var store = [{
        "title": "Home",
        "collection": "1.0",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "1.0",
        "excerpt":"IBM FHIR Server Introduction IBM FHIR Server is capable of processing, validating, and storing healthcare data in an industry-standard format that can be used for running search and other reporting capabilities. Built for the enterprise and offering cloud portability, it’s an integral component of a data integration pipeline. By applying standardized semantics and data models in accordance with the HL7 Fast Healthcare Interoperability Resources (FHIR) specification, IBM FHIR Server makes it easier to maximize insights across systems. It enables capabilities like advanced analytics and machine learning, which can drive better health outcomes and organizational impact. IBM FHIR Server is built on an open-source Java™ implementation of HL7 FHIR Release 4. Organizations can receive enterprise-grade services and support, including troubleshooting, maintenance, and automated upgrades. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/about/overview/",
        "teaser":null},{
        "title": "Documentation",
        "collection": "1.0",
        "excerpt":"Documentation   IBM FHIR Server WebSite  IBM FHIR Server User’s Guide  DockerHub: IBM FHIR Server - Schema Tool  DockerHub: IBM FHIR Server","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/about/documentation/",
        "teaser":null},{
        "title": "License",
        "collection": "1.0",
        "excerpt":"License The IBM FHIR Server is licensed under the Apache 2.0 license. Full license text is available at LICENSE. FHIR® is the registered trademark of HL7 and is used with the permission of HL7. Use of the FHIR trademark does not constitute endorsement of this product by HL7. IBM and the IBM logo are trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on https://ibm.com/trademark. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/about/license/",
        "teaser":null},{
        "title": "Installing IBM FHIR Server",
        "collection": "1.0",
        "excerpt":"  The IBM FHIR Server operator can be installed in an on-line cluster through the OpenShift CLI.  Multiple instances of the IBM FHIR Server operator may be deployed into different namespaces, one per namespace.Limitations   The Operator may be deployed into different namespaces, one per namespace.  The Operator has limited support for IBM FHIR Server configuration.Schema upgrades require downtime: The IBM FHIR Server requires downtime to complete upgrades of the IBM FHIR Server’s relational data. During the upgrade Values tables are refreshed, updated and optimized for the workloads that the FHIR specification supports. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/trying-out/",
        "teaser":null},{
        "title": "Prerequisites",
        "collection": "1.0",
        "excerpt":"Prerequisites Red Hat OpenShift Container Platform 4.5 or later installed on one of the following platforms:   Linux x86_64Connectivity to any of the following database systems:   IBM Db2 11.5 or later  PostgreSQL 12.1 or laterConnectivity to any of the following event streaming platforms (optional):   Kafka 1.0 or higherStorage   Storage for the database instance that the IBM FHIR Server connects to is outside the scope of this Operator.Resources Required       Describe Minimum System Resources Required     Minimum scheduling capacity:                             Software          Memory (GB)          CPU (cores)          Disk (GB)          Nodes                                      IBM FHIR Server          6          2          N/A          2                          Total          6          2          N/A          2                      Recommended scheduling capacity:                             Software          Memory (GB)          CPU (cores)          Disk (GB)          Nodes                                      IBM FHIR Server          64          16          N/A          3                          Total          64          16          N/A          3                      Note: There is an initContainer with the IBM FHIR Server called the IBM FHIR Server Schema Tool. This tool has a small memory footprint used on initialization of a pod and is accounted for in the above capacities.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/prerequisites/",
        "teaser":null},{
        "title": "Installing",
        "collection": "1.0",
        "excerpt":"The following sections provide instructions about installing IBM FHIR Server on the Red Hat OpenShift Container Platform. The instructions are based on using the OpenShift Container Platform web console and oc command line utility. When deploying in an air-gapped environment, link to airgap instructions. Overview IBM FHIR Server is an operator-based release and uses custom resources to define your IBM FHIR Server configurations. The IBM FHIR Server operator uses the custom resources to deploy and manage the entire lifecycle of your IBM FHIR Server instances. Custom resources are presented as YAML configuration documents that define instances of the IBMFHIRServer custom resource type. Installing IBM FHIR Server has two phases:   Install the IBM FHIR Server operator: this will deploy the operator that will install and manage your IBM FHIR Server instances.  Install one or more instances of IBM FHIR Server by using the operator.Before you begin   Ensure you have set up your environment , including setting up your OpenShift Container Platform.  Obtain the connection details for your OpenShift Container Platform cluster from your administrator.Create a project (namespace) Create a namespace into which the IBM FHIR Server instance will be installed by creating a project.When you create a project, a namespace with the same name is also created. Ensure you use a namespace that is dedicated to a single instance of IBM FHIR Server. A single namespace per instance also allows for finer control of user accesses. Important: Do not use any of the default or system namespaces to install an instance of IBM FHIR Server (some examples of these are: default, kube-system, kube-public, and openshift-operators). Add the IBM FHIR Server operator to the catalog Before you can install the IBM FHIR Server operator and use it to create instances of IBM FHIR Server, you must have the IBM Operator Catalog available in your cluster. If you have other IBM products installed in your cluster, then you already have the IBM Operator Catalog available, and you can continue to installing the IBM FHIR Server operator. If you are installing IBM FHIR Server as the first IBM product in your cluster, complete the following steps. To make the IBM FHIR Server operator available in the OpenShift OperatorHub catalog, create the following YAML files and apply them as  follows. To add the IBM Operator Catalog:       Create a file for the IBM Operator Catalog source with the following content, and save as IBMCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: ibm-operator-catalog   namespace: openshift-marketplacespec:   displayName: \"IBM Operator Catalog\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-operator-catalog   updateStrategy:     registryPoll:       interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCatalogSource.yaml   The IBM Operator Catalog source is added to the OperatorHub catalog, making the IBM FHIR Server operator available to install. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/installing/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "1.0",
        "excerpt":"Prerequisites Installing an instance of IBM FHIR Server requires an installed IBM FHIR Server Operator.See inventory item ibmFhirServerOperatorSetup for instructions to install the IBM FHIR Server Operator. Installing an instance of IBM FHIR Server requires at least namespace administration privileges. If you intend to use CLI commands, ensure you have the following installed:   IBM Cloud Pak CLI (cloudctl)  Red Hat OpenShift Container Platform CLI (oc)Creating an instance Complete the following steps to create an instance of IBM FHIR Server. 1. Define IBM FHIR Server configuration Create Secret resource containing IBM FHIR Server configuration.   Define values for the following secret keys:      IBM_FHIR_SCHEMA_TOOL_INPUT     The value must be set to the contents of the configuration file for IBM FHIR Server Schema Tool.     See IBM FHIR Server Schema Tool for how to create a configuration file.     Put the contents of the configuration file in a file named persistence.json.         IBM_FHIR_SERVER_CONFIG     The value must be set to the contents of the fhir-server-config.json configuration file for IBM FHIR Server.     See IBM FHIR Server for how to create a fhir-server-config.json configuration file.     Put the contents of the configuration file in a file named fhir-server-config.json.         IBM_FHIR_SERVER_CERT     If using Postgres as the database, this value must be set to the public key of the intermediate CA certificate.     Put the public key of the intermediate CA certificate in a file named db.cert.     If using Db2 as the database, leave the file named db.cert empty.         IBM_FHIR_SERVER_ADMIN_PASSWORD     The value must be set to the admin password to use for the IBM FHIR Server.     Put the admin password in a file named admin.txt.         IBM_FHIR_SERVER_USER_PASSWORD     The value must be set to the user password to use for the IBM FHIR Server.     Put the user password in a file named user.txt.         IBM_FHIR_SERVER_HOST     The value must be set to the contents of a configuration file for defining the host.     See Defining the host for how to define the host.     Put the contents of the configuration file in a file named host.xml.     If you do not want to define the host, leave the file named host.xml empty.     Create the secret from the files.  $ oc create secret generic &lt;secret-name&gt; \\      --from-file=IBM_FHIR_SCHEMA_TOOL_INPUT=./persistence.json \\      --from-file=IBM_FHIR_SERVER_CONFIG=./fhir-server-config.json \\      --from-file=IBM_FHIR_SERVER_CERT=./db.cert \\      --from-file=IBM_FHIR_SERVER_ADMIN_PASSWORD=./admin.txt \\      --from-file=IBM_FHIR_SERVER_USER_PASSWORD=./user.txt \\      --from-file=IBM_FHIR_SERVER_HOSTNAME=./host.xml \\      --namespace=&lt;target-namespace&gt;  &lt;secret-name&gt; is the name of the secret to contain the IBM FHIR Server configuration.  &lt;target-namespace&gt; is the target namespace.2. Create IBM FHIR Server instance Create an instance of IBM FHIR Server of the following methods: Red Hat OpenShift Container Platform web console       Log into the OpenShift Container Platform web console using your login credentials.         Navigate to Installed Operators and click on IBM FHIR Server.         Click Create Instance on the IBMFHIRServer tile.         Enter a name for the instance, and enter the name of the Secret resource containing the IBM FHIR Server configuration.         Click Create.   Red Hat OpenShift Container Platform CLI   Create an IBMFHIRServer resource in the target namespace by editing the namespace and secret name in the sample file, files/fhirserver_v1beta1_ibmfhirserver.yaml, and then apply the file.  $ oc apply -f files/fhirserver_v1beta1_ibmfhirserver.yamlIBM Cloud Pak CLI   Run the apply-custom-resources action.  $ cloudctl case launch \\      --case case/ibm-fhir-server-case \\      --namespace &lt;target-namespace&gt; \\      --inventory ibmFhirServerOperator \\      --action apply-custom-resources \\      --args \"--secretName &lt;secret-name&gt;\"  &lt;target-namespace&gt; is the target namespace.  &lt;secret-name&gt; is the name of the Secret resource containing the IBM FHIR Server configuration.3. Accessing IBM FHIR Server instance   Verify the IBM FHIR Server instance is functional.  $ oc get ibmfhirservers -n &lt;target-namespace&gt;      &lt;target-namespace&gt; is the target namespace.     The READY value of “True” indicates the IBM FHIR Server instance is functional.     Connect to the IBM FHIR Server instance.  $ oc get services -n &lt;target-namespace&gt;      &lt;target-namespace&gt; is the target namespace.     To make external connections to the IBM FHIR Server instance, either port-forward or create a route to the service.     See the IBM FHIR Server User’s Guide for how to verify that it’s running properly by invoking the $healthcheck endpoint.   Scaling By default, the deployment will contain 2 replicas. Use the oc scale command to manually scale an IBM FHIR Server deployment. Defining the host It is recommended, for security purposes, to explicitly define the hosts for which the IBM FHIR Server will handle requests. For any Route resources created for the IBM FHIR Server, ensure those hosts (e.g. test-fhir-server) are represented in the configuration file. &lt;server description=\"fhir-server\"&gt;    &lt;httpEndpoint host=\"*\" httpPort=\"-1\" httpsPort=\"9443\" id=\"defaultHttpEndpoint\" onError=\"FAIL\" /&gt;    &lt;virtualHost id=\"default_host\" allowedFromEndpointRef=\"defaultHttpEndpoint\"&gt;      &lt;hostAlias&gt;*:9443&lt;/hostAlias&gt;      &lt;hostAlias&gt;test-fhir-server:443&lt;/hostAlias&gt;    &lt;/virtualHost&gt;&lt;/server&gt;Deleting an instance Delete an instance of IBM FHIR Server using the Red Hat OpenShift Container Platform web console.       Log into the OpenShift Container Platform web console using your login credentials.         Navigate to Installed Operators and click on IBM FHIR Server.         Navigate to the IBMFHIRServer tab and click on the instance you want to delete.         Select Delete IBMFHIRServer from the Actions menu.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/configuring/",
        "teaser":null},{
        "title": "Air Gap Installation",
        "collection": "1.0",
        "excerpt":"Air Gap Installation Since air gap environments do not have access to the public internet, and therefore no access to DockerHub, the following preparation steps are necessary to make the required images accessable to the Red Hat OpenShift Container Platform cluster. If the Red Hat OpenShift Container Platform cluster has a Bastion host, ensure that the Bastion host can access:   The public internet to download the CASE and images.  The target (air gap) image registry where all the images will be mirrored to.  The Red Hat OpenShift Container Platform cluster to install the Operator on.In the absence of a Bastion host, a portable host with access to the public internet may used. By downloading the CASE and images onto the portable host, and then transporting the portable host into the air gap environment, the images can then be mirrored to the target (air gap) image registry. If using a Bastion host, refer to Using a Bastion host.If using a portable host, refer to Using a portable host. Using a Bastion host 1. Prepare the Bastion host Ensure you have the following installed on the Bastion host:   Docker CLI (docker)  IBM Cloud Pak CLI (cloudctl)  Red Hat OpenShift Container Platform CLI (oc)  Skopeo (skopeo)2. Download the CASE   Create a local directory in which to save the CASE.$ mkdir -p $HOME/offline  Save the CASE.$ cloudctl case save --case &lt;case-path&gt; --outputdir $HOME/offline  &lt;case-path&gt; is the path or URL to the CASE to save.The following output is displayed: Downloading and extracting the CASE ...- SuccessRetrieving CASE version ...- SuccessValidating the CASE ...- SuccessCreating inventory ...- SuccessFinding inventory items- SuccessResolving inventory items ...Parsing inventory items- Success  Verify the CASE (.tgz) file and images (.csv) file have been downloaded.$ ls $HOME/offlinechartsibm-fhir-server-case-&lt;version&gt;-charts.csvibm-fhir-server-case-&lt;version&gt;-images.csvibm-fhir-server-case-&lt;version&gt;.tgz  &lt;version&gt; is the CASE version.3. Log into cluster Log into the Red Hat OpenShift Container Platform cluster as a cluster administrator using the oc login command. 4. Configure target registry authentication secret For IBM FHIR Server, all images are available publicly in DockerHub, so no authentication secret for the source (public) registry is needed.   Create the authentication secret for the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry &lt;target-registry&gt; --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.  &lt;registry-user&gt; is the username for the target registry.  &lt;registry-password&gt; is the password for the target registry.The credentials are saved to $HOME/.airgap/secrets/&lt;target-registry&gt;.json 5. Mirror images to target registry   Copy the images in the CASE from the source (public) registry to the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.6. Configure cluster to access target registry   Configure a global image pull secret and ImageContentSourcePolicy resource.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --namespace openshift-marketplace \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-cluster-airgap \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.WARNING: This step may restart all cluster nodes. The cluster resources might be unavailable until the time the new pull secret is applied.   Optional: If you are using an insecure target registry, you must add the target registry to the cluster insecureRegistries list.$ oc patch image.config.openshift.io/cluster --type=merge \\    -p '{\"spec\":{\"registrySources\":{\"insecureRegistries\":[\"'&lt;target-registry&gt;'\"]}}}'  &lt;target-registry&gt; is the target registry.7. Proceed with installation Now that the air gap installation preparation steps are complete, you may continue with the IBM FHIR Server Operator installation. Using a portable host 1. Prepare the portable host Ensure you have the following installed on the portable host:   Docker CLI (docker)  IBM Cloud Pak CLI (cloudctl)  Red Hat OpenShift Container Platform CLI (oc)  Skopeo (skopeo)2. Download the CASE   Create a local directory in which to save the CASE.$ mkdir -p $HOME/offline  Save the CASE.$ cloudctl case save --case &lt;case-path&gt; --outputdir $HOME/offline  &lt;case-path&gt; is the path or URL to the CASE to save.The following output is displayed: Downloading and extracting the CASE ...- SuccessRetrieving CASE version ...- SuccessValidating the CASE ...- SuccessCreating inventory ...- SuccessFinding inventory items- SuccessResolving inventory items ...Parsing inventory items- Success3. Configure portable registry and authentication secret For IBM FHIR Server, all images are available publicly in DockerHub, so no authentication secret for the source (public) registry is needed.   Initialize the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action init-registry \\    --args \"--registry localhost --user &lt;registry-user&gt; --pass &lt;registry-password&gt; \\        --dir $HOME/offline/imageregistry\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry, which is initialized to this value.  &lt;registry-password&gt; is the password for the registry, which is initialized to this value.  Start the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action start-registry \\    --args \"--registry localhost --port 443 --user &lt;registry-user&gt; --pass &lt;registry-password&gt; \\        --dir $HOME/offline/imageregistry\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry.  &lt;registry-password&gt; is the password for the registry.  Create the authentication secret for the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry localhost:443 --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry.  &lt;registry-password&gt; is the password for the registry.The credentials are saved to $HOME/.airgap/secrets/localhost:443.json 4. Mirror images to portable registry   The following step copies the images in the CASE from the source (public) registry to the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--registry localhost:443 --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.5. Transport portable device Now that the images are in the portable registry, transport the portable host into the air gap environment. 6. Log into the cluster Log into the Red Hat OpenShift Container Platform cluster as a cluster administrator using the oc login command. 7. Configure target registry authentication secret   Create the authentication secret for the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry &lt;target-registry&gt; --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.  &lt;registry-user&gt; is the username for the target registry.  &lt;registry-password&gt; is the password for the target registry.The credentials are saved to $HOME/.airgap/secrets/$TARGET_REGISTRY.json 8. Mirror images to target registry   The following step copies the images in the CASE from the portable registry to the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--fromRegistry localhost:443 --registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.9. Configure cluster to access target registry   Configure a global image pull secret and ImageContentSourcePolicy resource.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --namespace openshift-marketplace \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-cluster-airgap \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.WARNING: This step may restart all cluster nodes. The cluster resources might be unavailable until the time the new pull secret is applied.   Optional: If you are using an insecure target registry, you must add the target registry to the cluster insecureRegistries list.$ oc patch image.config.openshift.io/cluster --type=merge \\    -p '{\"spec\":{\"registrySources\":{\"insecureRegistries\":[\"'&lt;target-registry&gt;'\"]}}}'  &lt;target-registry&gt; is the target registry.10. Proceed with installation Now that the air gap installation preparation steps are complete, you may continue with the installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/air-gap-installation/",
        "teaser":null},{
        "title": "Tracking license consumption of IBM FHIR Server",
        "collection": "1.0",
        "excerpt":"Tracking license consumption of IBM FHIR Server License Service is required for monitoring and measuring license usage of IBM FHIR Server in accord with the pricing rule for containerized environments. Manual license measurements are not allowed. Deploy License Service on all clusters where IBM FHIR Server is installed. The IBM FHIR Server Operator contains an integrated service for measuring the license usage at the cluster level for license evidence purposes. Overview The integrated licensing solution collects and stores the license usage information which can be used for audit purposes and for tracking license consumption in cloud environments. The solution works in the background and does not require any configuration. Only one instance of the License Service is deployed per cluster regardless of the number of Cloud Paks and containerized products that you have installed on the cluster. Deploying License Service Deploy License Service on each cluster where IBM FHIR Server is installed. License Service can be deployed on any Kubernetes cluster. For more information about License Service, how to install and use it, see the License Service documentation. Validating if License Service is deployed on the cluster To ensure license reporting continuity for license compliance purposes make sure that License Service is successfully deployed. It is recommended to periodically verify whether it is active. To validate whether License Service is deployed and running on the cluster, you can, for example, log into the Red Hat OpenShift Container Platform cluster and run the following command: $ oc get pods --all-namespaces | grep ibm-licensing | grep -v operatorThe following response is a confirmation of successful deployment: 1/1     RunningArchiving license usage data Remember to archive the license usage evidence before you decommission the cluster where IBM FHIR Server was deployed. Retrieve the audit snapshot for the period when IBM FHIR Server was on the cluster and store it in case of audit. For more information about the licensing solution, see License Service documentation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/installing/tracking-license/",
        "teaser":null},{
        "title": "Security",
        "collection": "1.0",
        "excerpt":"Security   The IBM FHIR Server is a stateless offering. It is the responsibility of the user to ensure that the proper security measures are established when using the server.Data in motion   All transports used to interact with IBM FHIR Server must be encrypted. TLS 1.2 is recommended.  Users are expected to use TLS when configuring their IBM FHIR Server to connect with their database instance.Data at rest   The prerequisite database must have data encryption enabled.  Each instance is responsible for Backup and Recovery of the Database and must backup solution specific configurations.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/security/security/",
        "teaser":null},{
        "title": "SecurityContextConstraints Requirements",
        "collection": "1.0",
        "excerpt":"SecurityContextConstraints Requirements By default, the IBM FHIR Server Operator uses the restricted SecurityContextConstraints resource. If desired, the following custom SecurityContextConstraints resource can be applied and used instead. apiVersion: security.openshift.io/v1kind: SecurityContextConstraintsmetadata:  name: ibm-fhir-server-operator-scc  annotations:    kubernetes.io/description: ibm-fhir-server-operator-scc denies access to all      host features and requires pods to be run with a UID, and SELinux context      that are allocated to the namespace, enforces readOnlyRootFilesystem, and      drops all capabilities.allowHostDirVolumePlugin: falseallowHostIPC: falseallowHostNetwork: falseallowHostPID: falseallowHostPorts: falseallowPrivilegeEscalation: falseallowPrivilegedContainer: falseallowedCapabilities: []defaultAddCapabilities: []groups: []fsGroup:  type: MustRunAspriority: nullreadOnlyRootFilesystem: truerequiredDropCapabilities:  - ALLrunAsUser:  type: MustRunAsRangeseLinuxContext:  type: MustRunAssupplementalGroups:  type: RunAsAnyusers: []volumes:  - configMap  - downwardAPI  - emptyDir  - persistentVolumeClaim  - projected  - secretTo cause the IBM FHIR Server Operator to use the custom SecurityContextConstraints resource.       Find the ibm-fhir-server-operator-sa ServiceAccount resource in the same namespace as the Operator.         Add the following to the rules in the ClusterRole resource that the ServiceAccount resource is bound to, and apply.   - apiGroups:    - security.openshift.io  resourceNames:    - ibm-fhir-server-operator-scc  resources:    - securitycontextconstraints  verbs:    - use  The IBM FHIR Server Operator also creates custom ClusterRole, ClusterRoleBinding, Role, RoleBinding, SecurityContextConstraints, and ServiceAccount resources to ensure separation of duties.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/security/security-context-constraints/",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-arangodb/Licensing%20and%20support",
        "teaser":null},{
        "title": "ArangoDB",
        "collection": "connectors",
        "excerpt":"Kafka Connect ArangoDB is a Kafka connector that takes records from Apache Kafka, translates them into database changes and performs them against ArangoDB. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-arangodb/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/jaredpetersen/kafka-connect-arangodb.git cd kafka-connect-arangodb mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-arangodb/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-cos/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM Cloud Object Storage",
        "collection": "connectors",
        "excerpt":"IBM Cloud Object Storage sink connector for copying data from a Kafka topic into IBM Cloud Object Storage. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-cos/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink cd kafka-connect-ibmcos-sink gradle shadowJar            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-cos/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-couchbase/Licensing%20and%20support",
        "teaser":null},{
        "title": "Couchbase",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector subscribes to one or more Kafka topics and writes the messages to Couchbase. This plugin also includes the corresponding source connector for Couchbase. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-couchbase/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-couchbase/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-elastic/Licensing%20and%20support",
        "teaser":null},{
        "title": "Elasticsearch",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Elasticsearch subscribes to one or more Kafka topics and writes the records to Elasticsearch. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-elastic/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the connector plugin JAR file.     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-elastic/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-filestream/Licensing%20and%20support",
        "teaser":null},{
        "title": "FileStream",
        "collection": "connectors",
        "excerpt":"FileStream sink connector for reading data from a Kafka topic and writing it to a local file. This connector is meant for use in standalone mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-filestream/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Add the connector plugin to your Kafka Connect environment.     The FileStream sink connector plugin JAR file, connect-file-&lt;version&gt;.jar, is included in the libs directory of Kafka. To make the plugin available to your Kafka Connect runtime, ensure it is in the libs directory alongside the Kafka Connect runtime JAR files.         Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-filestream/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-http/Licensing%20and%20support",
        "teaser":null},{
        "title": "HTTP",
        "collection": "connectors",
        "excerpt":"The HTTP sink connector is a Kafka connector for invoking HTTP APIs with data from Apache Kafka. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-http/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/thomaskwscott/kafka-connect-http.git cd kafka-connect-http mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-http/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-memcached/Licensing%20and%20support",
        "teaser":null},{
        "title": "Memcached",
        "collection": "connectors",
        "excerpt":"Kafka Connect for Memcached provides a sink connector that can write data in real time from Apache Kafka to a Memcached environment. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-memcached/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest tar.gz and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-memcached/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mongodb/Licensing%20and%20support",
        "teaser":null},{
        "title": "MongoDB",
        "collection": "connectors",
        "excerpt":"The Kafka Connect MongoDB connector allows you to import data from Kafka topics into MongoDB. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mongodb/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/hpgrahsl/kafka-connect-mongodb.git cd kafka-connect-mongodb mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mongodb/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mq/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM MQ",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for IBM MQ for copying data from Apache Kafka into IBM MQ. Supports connecting to MQ in both bindings and client mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mq/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"IBM Event Streams provides additional help for setting up a Kafka Connect environment and starting the MQ sink connector. Log in to the Event Streams UI, click the Toolbox tab and scroll to the Connectors section. Alternatively, you can download the MQ sink connector from GitHub:       Download the connector plugin JAR file:     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-mq/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-redis/Licensing%20and%20support",
        "teaser":null},{
        "title": "Redis",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Redis is used to write data from Apache Kafka to a Redis cache. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-redis/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the connector plugin JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-redis/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-solr/Licensing%20and%20support",
        "teaser":null},{
        "title": "Solr",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Solr takes plain JSON data from Kafka topics and pushes it to Solr. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-solr/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/bkatwal/kafka-solr-sink-connector.git cd kafka-solr-sink-connector mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-sink-solr/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-couchbase/Licensing%20and%20support",
        "teaser":null},{
        "title": "Couchbase",
        "collection": "connectors",
        "excerpt":"Kafka Connect source connector publishes document change notifications from Couchbase to a Kafka topic. This plugin also includes the corresponding sink connector for Couchbase. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-couchbase/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-couchbase/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-filestream/Licensing%20and%20support",
        "teaser":null},{
        "title": "FileStream",
        "collection": "connectors",
        "excerpt":"FileStream source connector for reading data from a local file and sending it to a Kafka topic. This connector is meant for use in standalone mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-filestream/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Add the connector plugin to your Kafka Connect environment.     The FileStream source connector plugin JAR file, connect-file-&lt;version&gt;.jar, is included in the libs directory of Kafka. To make the plugin available to your Kafka Connect runtime, ensure it is in the libs directory alongside the Kafka Connect runtime JAR files.         Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-filestream/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mongodbz/Licensing%20and%20support",
        "teaser":null},{
        "title": "MongoDB (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s MongoDB connector can monitor a MongoDB replica set or a MongoDB sharded cluster for document changes in databases and collections, recording those changes as events in Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mongodbz/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mongodbz/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mq/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM MQ",
        "collection": "connectors",
        "excerpt":"Kafka Connect source connector for IBM MQ for copying data from Apache Kafka into IBM MQ. Supports connecting to MQ in both bindings and client mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mq/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"IBM Event Streams provides additional help for setting up a Kafka Connect environment and starting the MQ source connector. Log in to the Event Streams UI, click the Toolbox tab and scroll to the Connectors section. Alternatively, you can download the MQ source connector from GitHub:       Download the connector plugin JAR file:     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mq/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mysql/Licensing%20and%20support",
        "teaser":null},{
        "title": "MySQL (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s MySQL connector can monitor all of the row-level changes in the databases on a MySQL server or HA MySQL cluster and record them in Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mysql/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-mysql/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-postgresql/Licensing%20and%20support",
        "teaser":null},{
        "title": "PostgreSQL (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s PostgreSQL connector can monitor the row-level changes in the schemas of a PostgreSQL database and record them in separate Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-postgresql/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-postgresql/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-rss/Licensing%20and%20support",
        "teaser":null},{
        "title": "RSS",
        "collection": "connectors",
        "excerpt":"Kafka Connect RSS is a source connector that supports polling multiple URLs and sending output to a single Kafka topic. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-rss/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the connector plugin JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-rss/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-sqlserver/Licensing%20and%20support",
        "teaser":null},{
        "title": "SQL Server (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s SQL Server Connector can monitor the row-level changes in the schemas of a SQL Server database and record to separate Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-sqlserver/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-sqlserver/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-weathercompany/Licensing%20and%20support",
        "teaser":null},{
        "title": "Weather Company Data",
        "collection": "connectors",
        "excerpt":"Kafka Connect for Weather Company Data is a source connector for importing data from the IBM Cloud Weather Company Data service into Apache Kafka. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-weathercompany/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/ibm-messaging/kafka-connect-weather-source cd kafka-connect-weather-source gradle build            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/connectors/kc-source-weathercompany/installation",
        "teaser":null},{
        "title": "Schema registry",
        "collection": "keyFeatures",
        "excerpt":"  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/feature/schemaRegistry",
        "teaser":null},{
        "title": "Multizone support",
        "collection": "keyFeatures",
        "excerpt":"  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/feature/multizone",
        "teaser":null},{
        "title": "Open the Connector catalog",
        "collection": "keyFeatures",
        "excerpt":"No content ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/3.connectorsCatalog",
        "teaser":null},{
        "title": "Chat with us on Slack",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/support/chatOnSlack/",
        "teaser":null},{
        "title": "Give us feedback",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/support/giveUsFeedback/",
        "teaser":null},{
        "title": "IBM Support",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/support/ibmSupport/",
        "teaser":null},{
        "title": "Running Kafka Streams applications",
        "collection": "tutorials",
        "excerpt":"You can run Kafka Streams applications in IBM Event Streams. Follow the steps in this tutorial to understand how to set up your existing Kafka Streams application to run in Event Streams, including how to set the correct connection and permission properties to allow your application to work with Event Streams. The examples mentioned in this tutorial are based on the WordCountDemo.java sample which reads messages from an input topic called streams-plaintext-input and writes the words, together with an occurrence count for each word, to an output topic called streams-wordcount-output. Prerequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  Ensure you have a Kafka Streams application ready to use. You can also use one of the Kafka Streams sample applications such as the  WordCountDemo.java sample used here.Creating input and output topics Create the input and output topics in Event Streams. For example, you can create the topics and name them as they are named in the WordCountDemo.java sample application. For demonstration purposes, the topics only have 1 replica and 1 partition. To create the topics:   Log in to your IBM Event Streams UI.  Click the Topics tab and click Create topic.  Enter the name streams-plaintext-input and click Next.  Set 1 partition for the topic, leave the default retention period, and select 1 replica.  Click Create topic.  Repeat the same steps to create a topic called streams-wordcount-output.Sending data to input topic To send data to the topic, first set up permissions to produce to the input topic, and then run the Kafka Streams producer to add messages to the topic. To set up permissions:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Select your input topic you created earlier from the list, for example streams-plaintext-input.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Produce only. The name of the input topic is filled in automatically, for example streams-plaintext-input.  Click the Sample code tab, and copy the snippet from the Sample configuration properties section into a new file called streams-demo-input.properties. This creates a new properties file for your Kafka Streams application.  Replace &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the API key generated for the input topic. For example:    security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/Users/john.smith/Downloads/es-cert.jksssl.truststore.password=passwordsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"abcAt0vjYZ1hEwXsRIuy8pxxXHNbEppOF\";      To send messages to the input topic, use the bootstrap address, the input topic name, and the new properties file you created. For example, run kafka-console-producer.sh with the following options:   --broker-list &lt;broker_url&gt;: where &lt;broker_url&gt; is your cluster’s broker URL copied earlier from the Bootstrap server section.  --topic &lt;topic_name&gt;: where &lt;topic_name&gt; is the name of your input topic, in this example, streams-plaintext-input.  --producer.config &lt;properties_file&gt;: where &lt;properties_file&gt; is the new properties file including full path to it, in this example, streams-demo-input.properties.For example: kafka_2.12-1.1.0 $ ./bin/kafka-console-producer.sh \\              --broker-list 192.0.2.24:31248 \\              --topic streams-plaintext-input \\              --producer.config streams-demo-input.properties&gt;This is a test message&gt;This will be used to demo the Streams sample app&gt;It is a Kafka Streams test message&gt;The words in these messages will be counted by the Streams appAnother method to produce messages to the topic is by using the Event Streams producer API. Running the application Set up your Kafka Streams application to connect to your Event Streams instance, have permission to create topics, join consumer groups, and produce and consume messages. You can then use your application to create intermediate Kafka Streams topics, consume from the input topic, and produce to the output topic. To set up permissions and secure the connection:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Produce, consume and create topics.The permissions are required to do the following:          Create topics: Kafka Streams creates intermediate topics for the operations performed in the stream.      Join a consumer group: to be able to read messages from the input topic, it joins the group streams-wordcount.      Produce and consume messages.        Click the Sample code tab, and copy the snippet from the Sample connection code section into your Kafka Streams application to set up a secure connection from your application to your Event Streams instance.  Using the snippet, import the following libraries to your application:    import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;        Using the snippet, reconstruct the Properties object as follows, replacing &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the generated API key, for example:    Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.24:31248\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\";properties.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);        Ensure you download the JAR files for SLF4J and add them to your classpath.Run your Kafka Streams application. To view the topics, log in to your Event Streams UI and click the Topics tab. For example, the following topics are created by the WordCountDemo.java Kafka Streams application: streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelogstreams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartitionViewing messages on output topic To receive messages from the input topic, first set up permissions so that the output topic can consume messages, and then run the Kafka Streams consumer to send messages to the topic. To set up permissions:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Select your output topic you created earlier from the list, for example streams-wordcount-output.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Consume only. The name of the output topic is filled in automatically, for example streams-wordcount-output.  Click the Sample code tab, and copy the snippet from the Sample configuration properties section into a new file called streams-demo-output.properties. This creates a new properties file for your Kafka Streams application.  Replace &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the API key generated for the input topic.To view messages on the output topic, use the bootstrap address, the output topic name, and the new properties file you created. For example, run kafka-console-consumer.sh with the following options:   --bootstrap-server &lt;broker_url&gt;: where &lt;broker_url&gt; is your cluster’s broker URL copied earlier from the Bootstrap server section.  --topic &lt;topic_name&gt;: where &lt;topic_name&gt; is the name of your output topic, in this example, streams-wordcount-output.  --consumer.config &lt;properties_file&gt;: where &lt;properties_file&gt; is the new properties file including full path to it, in this example, streams-demo-output.properties.For example: $ ./bin/kafka-console-consumer.sh \\&gt; --bootstrap-server 192.0.2.24:31248 \\&gt; --topic streams-wordcount-output \\&gt; --consumer.config streams-demo-output.properties \\&gt; --from-beginning \\&gt; --group streams-demo-group-consumer \\&gt; --formatter kafka.tools.DefaultMessageFormatter \\&gt; --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\&gt; --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer \\&gt; --property print.key=truethis    1is    1a    1test    1message    1this    2will    1be    1used    1to    3demo    1the    1streams    5sample    1app    1it    1is    2a    2kafka    7streams    6test    2message    2the    2words    1in    1these    1messages    1will    2be    2counted    1by    1the    3streams    7app    2Processed a total of 34 messages","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/kafka-streams-app/",
        "teaser":null},{
        "title": "Monitoring cluster health with Datadog",
        "collection": "tutorials",
        "excerpt":"Event Streams can be configured such that Datadog can capture Kafka broker JMX metrics via its Autodiscovery service. For more information about Autodiscovery, see the Datadog documentation. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  Ensure you have the Datadog Kubernetes agent with JMX installed deployed into the IBM Cloud Private cluster.When installing the agent, ensure the following settings:   The Kubernetes agent requires a less restrictive PodSecurityPolicy than required for Event Streams. It is recommended that you install the agent into a different namespace than where Event Streams is deployed.  For the namespace where you deploy the agent, apply a PodSecurityPolicy to allow the following:          volumes:                  hostPath                    Configuring  for Autodiscovery When installing Event Streams, firstly ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure the Kafka brokers’ JMX ports are accessible to the Datadog Agent. Then supply the YAML object containing the required Check Templates for configuring Kafka JMX monitoring. The example configuration supplied provides an overview of the required fields. You can set the YAML object on the Configuration page by using the configuration option External monitoring &gt; Datadog - Autodiscovery annotation check templates for Kafka brokers. The YAML object is then applied to the Kafka pods as annotations to enable the pods to be recognized by the Datadog agent AutoDiscovery service. The Datadog annotation format is ad.datadoghq.com/&lt;container identifier&gt;.&lt;template name&gt;. However, Event Streams automatically adds the Datadog prefix and container identifier to the annotation, so the YAML object keys must only be &lt;template name&gt; (for example check_names). Providing Check Templates Each Check Template value is a YAML object: check_names: - kafkainstances:  - host: %%host%%See an example Kafka Check Template. Supplying JMX connection values As part of the Kafka instances Check Template, provide values to ensure the Datadog agent can communicate with the Kafka JMX port via SSL as an authenticated user.   rmi_registry_ssl=true  trust_store_path=&lt;path to trust store&gt;  trust_store_password=&lt;password for trust store&gt;  user=&lt;username for authenticating JMX connection&gt;  password=&lt;password for user&gt;Release-specific credentials for establishing the connection are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. Because these values are not known at install time, they cannot be supplied explictly as part of the check templates configuration. Template variables should be used to reference environment variables that will be supplied to each Datadog Agent pod after installing Event Streams. In addition, files contained inside the release-specific secret should be mounted into the Datadog Agent pod using the paths supplied in the configuration. Example Kafka check template content check_names:  - kafkainstances:  - host: %%host%%    rmi_registry_ssl: true    #This path must be used to mound the trust store into each Datadog Agent pod    trust_store_path: /etc/es-jmx/es-kafka-dd.jks    #Note the environment variable used    trust_store_password: %%env_TRUST_STORE_PASSWORD%%    #Note the environment variable used    user: %%env_JMX_USER%%    #Note the environment variable used    password: %%env_JMX_PASSWORD%%    #Port opened on Kafka broker for JMX    port: 9999    tags:      kafka: brokerinit_config:  - is_jmx: true    conf:    - include:        domain: kafka.server        bean: kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec        attribute:          count:            metric_type: rate            alias: kafka.net.bytes_in.rateInstalling through Helm CLI Check Templates can be supplied to Helm CLI installs using the following commands:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Supply -f values.yaml to the helm install command, where values.yaml contains:    externalMonitoring:   datadog:     instances:       key: value     check_names:       key: value     ... remaining template values      Configuring your Datadog agent After installation of Event Streams, the DataDog DaemonSet must be edited to supply values for the environment variables and trust store referenced in the check templates. First, the JMX secret must be copied into the Datadog Agent namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;datadog-namespace&gt; create -f - That will create the secret &lt;release-name&gt;-ibm-es-jmx-secret in the DataDog namespace, which can then be referenced in the DaemonSet. The Datadog Agent DaemonSet must now be edited to add in the following information: spec:  containers:    - name: datadog      env:       #Add in new environment variables from the jmx secret. Note that the variable names match the names supplied in the `instances` check template        - name: JMX_USER          secretKeyRef:            key: jmx_username            name: &lt;release-name&gt;-ibm-es-jmx-secret        - name: JMX_PASSWORD          secretRef:            key: jmx_password            name: &lt;release-name&gt;-ibm-es-jmx-secret        - name: TRUST_STORE_PASSWORD          secretRef:            key: trust_store_password            name: &lt;release-name&gt;-ibm-es-jmx-secret      ...      # Mount the secret volume with the mount path that matches the path to the trust store in the `instances` check template      volumeMounts:        - name: es-volume          mountPath: /etc/es-jmx  ...  volumes:    # Mount the jmx secret as a volume, selecting the trust store item    - name: es-volume      fromSecret:        secretName: &lt;release-name&gt;-ibm-es-jmx-secret        items:          - name: truststore.jks            #Note the path should match the name of the trust store file in the `instances` check template            path: es-kafka-dd.jks","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/monitor-with-datadog/",
        "teaser":null},{
        "title": "Monitoring cluster health with Prometheus",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial  details how to deploy a Prometheus JMX exporter into your IBM Cloud Private cluster and export Kafka JMX metrics to an external Prometheus system. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, firstly ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure the Kafka brokers’ JMX ports are accessible to the Prometheus Exporter.  Ensure you have a Prometheus server installed that has network access to your  cluster.  Ensure you have configured access to the Docker registry from the machine you will be using to deploy the JMX exporter.  Ensure you have downloaded the Prometheus JMX exporter httpserver jar file to the machine you will be using to deploy the JMX exporter.Prometheus JMX exporter The Prometheus JMX exporter can be run as a HTTP server which will provide an endpoint for the external Prometheus server to query for metrics data. In order to deploy to your  cluster, the JMX exporter needs to be packaged into a Kubernetes solution. Release-specific credentials for establishing the connection between the JMX exporter and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying the JMX exporter in a different namesapce to your Event Streams installation, the secret must be copied to the required namespace. kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - That will create the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the prometheus-exporter-&lt;broker-num&gt;.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   JMX exporter packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where the JMX exporter will be deployed.  Kubernetes pod specification created that exposes the configuration to the JMX exporter via environment variables and a ConfigMap.  Kubernetes ConfigMap containing the JMX exporter YAML configuration.  Kubernetes NodePort service to expose access to the JMX exporter for the external Prometheus server.Example Dockerfile Create a Dockerfile as follows. FROM &lt;base OS Docker image with Java&gt;WORKDIR /opt/prometheusCOPY jmx_prometheus_httpserver.jar .COPY run.sh .CMD ./run.shExample run.sh Create a run.sh script as follows. The script copies the YAML configuration and appends the release-specific connection values. It then runs the JMX exporter as an HTTP server. cp /etc/jmx-config/config.yaml /tmp/jmx-config.yamlcat &lt;&lt; EOF &gt;&gt; /tmp/jmx-config.yamlssl: trueusername: ${JMX_USER}password: ${JMX_PASSWORD}hostPort: ${JMX_HOST}:9999EOFjava -Djavax.net.ssl.trustStore=/etc/jmx-secret/store.jks -Djavax.net.ssl.trustStorePassword=${STORE_PW} -jar jmx_prometheus_httpserver.jar 0.0.0.0:8080 /tmp/jmx-config.yamlAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Building the Docker image Build the Docker image as follows.   Ensure that the Dockerfile, run.sh, and jmx-exporter.jar are in the same directory.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the image: docker build -t &lt;remote-registry&gt;:&lt;remote-registry-port&gt;/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt; .  Push the image to your IBM Cloud Private cluster Docker registry: docker push &lt;remote-registry&gt;:&lt;remote-registry-port&gt;/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create and expose a JMX exporter for each Kafka broker. However, the configuration ConfigMap can be shared across each instance.   Copy the following into a file called prometheus-config.yaml:    ---apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-configdata:  config.yaml: |-    startDelaySeconds: 10    lowercaseOutputName: true    rules:      # The following rules match the Kafka MBeans in the jconsole order      # Broker metrics    - pattern : kafka.server&lt;type=BrokerTopicMetrics, name=(BytesInPerSec|BytesOutPerSec)&gt;&lt;&gt;(Count)      name: kafka_server_BrokerTopicMetrics_$1_$2        Create the ConfigMap in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f prometheus-config.yaml  Copy the following into a file called prometheus-exporter-&lt;broker-num&gt;.yaml       apiVersion: v1kind: Podmetadata:    name: prometheus-export-broker-&lt;broker-num&gt;    labels:       app: prometheus-export-broker-&lt;broker-num&gt;spec:    containers:    - name: jmx-exporter      image: &lt;full name of docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          # mountPath must match path supplied to -Djavax.net.ssl.trustStore in run.sh          mountPath: /etc/jmx-secret        - name: config-volume          # mountPath must match path supplied to the cp command in run.sh          mountPath: /etc/jmx-config      ports:      - containerPort: 8080      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-&lt;broker-num&gt;.&lt;release-namespace&gt;.svc      - name: STORE_PW        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              # Path must match the filename supplied to -Djavax.net.ssl.trustStore in run.sh              path: store.jks        - name: config-volume          configMap:             name: prometheus-config---apiVersion: v1kind: Servicemetadata:  name: prometheus-svc-&lt;broker-num&gt;spec:  type: NodePort  selector:    app: prometheus-export-broker-&lt;broker-num&gt;  ports:  - port : 8080    protocol: TCP               Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f prometheus-exporter-&lt;broker-num&gt;.yaml.  Repeat for the total number of Kafka brokers deployed in your Event Streams installation.  After the resources are created, find the NodePort by using the command kubectl -n &lt;target-namespace&gt; get svc. The NodePort is listed for the service prometheus-svc-&lt;broker-num&gt;. The connection to be supplied to your external Prometheus as static scrape target in the Prometheus configuration file is the url &lt;cluster name/IP&gt;:&lt;node-port&gt;Troubleshooting If metrics are not appearing in your external Prometheus, check the logs for the Prometheus agent with the following command: kubectl -n &lt;target-namespace&gt; get logs prometheus-export-broker-&lt;broker-num&gt; ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/monitor-with-prometheus/",
        "teaser":null},{
        "title": "Monitoring cluster health with a Splunk HTTP Event Collector",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial details how to deploy jmxtrans into your IBM Cloud Private cluster to export Kafka JMX metrics as graphite output, and then use Logstash to write the metrics to an external Splunk system as an HTTP Event Collector. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure that each Kafka broker’s JMX port is accessible to jmxtrans.  Ensure you have a Splunk Enterprise server installed or a Splunk Universal Forwarder that has network access to your  cluster.  Ensure that you have an index to receive the data and an HTTP Event Collector configured on Splunk. Details can be found in the Splunk documentation  Ensure you have configured access to the Docker registry from the machine you will be using to deploy jmxtrans.jmxtrans Jmxtrans is a connector that reads JMX metrics and outputs a number of formats supporting a wide variety of logging, monitoring, and graphing applications. To deploy to your  cluster, you must package jmxtrans into a Kubernetes solution. Release-specific credentials for establishing the connection between jmxtrans and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying jmxtrans in a different namespace to your Event Streams installation, copy the secret to the required namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - The command creates the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the hec.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   Jmxtrans packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where Logstash and jmxtrans will be deployed.  Logstash packaged into a Docker image to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where Logstash and jmxtrans will be deployed.  Kubernetes pod specification created that exposes the configuration to jmxtrans and Logstash via environment variables.Example Dockerfile.jmxtrans Create a Dockerfile called Dockerfile.jmxtrans as follows. FROM jmxtrans/jmxtransCOPY run.sh .ENTRYPOINT [ \"./run.sh\" ]Example run.sh Create a run.sh script as follows. The script generates the JSON configuration file and substitute the release-specific connection values. It then runs jmxtrans. #!/bin/shcat &lt;&lt;EOF &gt;&gt; /var/lib/jmxtrans/config.json{  \"servers\": [    {      \"port\": 9999,      \"host\": \"${JMX_HOST_0}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_1}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_2}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    }  ]}EOFexec /docker-entrypoint.sh start-without-jmxAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Example Dockerfile.logstash Create a Dockerfile called Dockerfile.logstash as follows. FROM docker.elastic.co/logstash/logstash:&lt;required-logstash-version&gt;RUN /usr/share/logstash/bin/logstash-plugin install logstash-input-graphiteRUN rm -f /usr/share/logstash/pipeline/logstash.confCOPY pipeline/ /usr/share/logstash/pipeline/COPY config/ /usr/share/logstash/config/Example logstash.yml Create a Logstash settings file called logstash.yml as follows. path.config: /usr/share/logstash/pipeline/Example logstash.conf Create a Logstash configuration file called logstash.conf as follows. input {    graphite {        host =&gt; \"localhost\"        port =&gt; 9999        mode =&gt; \"server\"    }}output {    http {        http_method =&gt; \"post\"        url =&gt; \"http://&lt;splunk-host-name-or-ip-address&gt;:&lt;splunk-http-event-collector-port&gt;/services/collector/event\"        headers =&gt; [\"Authorization\", \"Splunk &lt;splunk-http-event-collector-token&gt;\"]        mapping =&gt; {\"event\" =&gt; \"%{message}\"}    }}Building the Docker image Build the Docker images as follows.   Ensure that Dockerfile.jmxtrans, Dockerfile.logstash and run.sh are in the same directory. Edit Dockerfile.logstash and replace &lt;required-logstash-version&gt; with the Logstash version you would like to use as a basis.  Ensure that logstash.yml is in a subdirectory called config/ of the directory in step 1.  Ensure that logstash.conf is in a subdirectory called pipeline/ of the directory in step 1.  Edit logstash.conf, and replace &lt;splunk-host-name-or-ip-address&gt; with the external Splunk Enterprise, Splunk Universal forwarder, or Splunk Cloud host name or IP address.Replace &lt;splunk-http-event-collector-port&gt; with the HTTP Event Collector port number.Replace &lt;splunk-http-event-collector-token&gt; with the HTTP Event Collector token setup on the Splunk HTTP Event Collector Data input.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the jmxtrans image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;jmxtrans-image-name&gt;:&lt;image-version&gt; -f Dockerfile.jmxtrans .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;jmxtrans-image-name&gt;:&lt;image-version&gt;  Create the logstash image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;logstash-image-name&gt;:&lt;image-version&gt; -f Dockerfile.logstash .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;logstash-image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create a pod for jmxtrans and Logstash as follows.   Copy the following into a file called hec.yaml.    apiVersion: v1kind: Podmetadata:    name: jmxtrans-broker    labels:       app: jmxtrans-brokerspec:    containers:    - name: logstash      image: &lt;full name of logstash docker image pushed to remote registry&gt;    - name: jmxtrans      image: &lt;full name of jmxtrans docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          mountPath: /etc/jmx-secret      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST_0        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-0.&lt;release-namespace&gt;.svc      - name: JMX_HOST_1        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-1.&lt;release-namespace&gt;.svc      - name: JMX_HOST_2        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-2.&lt;release-namespace&gt;.svc      - name: SSL_TRUSTSTORE        value: /etc/jmx-secret/store.jks      - name: SSL_TRUSTSTORE_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password      - name: JAVA_OPTS        value: -Djavax.net.ssl.trustStore=$(SSL_TRUSTSTORE) -Djavax.net.ssl.trustStorePassword=$(SSL_TRUSTSTORE_PASSWORD)      # The SECONDS_BETWEEN_RUNS is the scrape frequency of the JMX values. The default value is 60 seconds. Change it to a value to suit your requirements.      - name: SECONDS_BETWEEN_RUNS        value: \"15\"    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              path: store.jks        Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f hec.yaml.Events start appearing in Splunk after running the command. Troubleshooting If metrics are not appearing in your external Splunk, check the logs for jmxtrans and for Logstash with the following commands: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c jmxtrans kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c logstash To get debug-level logs from jmxtrans, use the following steps:   Copy the following into a file called logback.xml.    &lt;configuration debug=\"false\"&gt;  &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;    &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;    &lt;encoder&gt;      &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;logger name=\"com.googlecode.jmxtrans\" level=\"${logLevel}\"/&gt;  &lt;root level=\"info\"&gt;    &lt;appender-ref ref=\"console\" /&gt;  &lt;/root&gt;&lt;/configuration&gt;        Add the file to the same directory as the Dockerfile and run.sh files.  Edit the Dockerfile to include the logback.xml file, for example:    FROM jmxtrans/jmxtransCOPY logback.xml /usr/share/jmxtrans/conf/logback.xmlCOPY configure.sh .ENTRYPOINT [ \"./configure.sh\" ]        Follow the instructions for building the docker image.  Add the following environment variable to the hec.yaml file after the env: property:      - name: JMXTRANS_OPTS    value: -Djmxtrans.log.level=debug        Delete jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; delete pod jmxtrans-broker  Check that it has been deleted with the following command kubectl -n &lt;target-namespace&gt; get pods  When it has been deleted, create it again with the following command: kubectl -n &lt;target-namespace&gt; apply -f hec.yaml  View the logs with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c jmxtrans","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/monitor-with-splunk-hec/",
        "teaser":null},{
        "title": "Monitoring cluster health with Splunk",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial  details how to deploy jmxtrans into your IBM Cloud Private cluster to export Kafka JMX metrics as graphite output to an external Splunk system using a TCP data input. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure that each Kafka broker’s JMX port is accessible to jmxtrans.  Ensure you have a Splunk Enterprise server installed or a Splunk Universal Forwarder that has network access to your  cluster.  Ensure that you have an index to receive the data and a TCP Data input configured on Splunk. Details can be found in the Splunk documentation.  Ensure you have configured access to the Docker registry from the machine you will be using to deploy jmxtrans.jmxtrans Jmxtrans is a connector that reads JMX metrics and outputs a number of formats supporting a wide variety of logging, monitoring, and graphing applications. To deploy to your  cluster, you must package jmxtrans into a Kubernetes solution. Release-specific credentials for establishing the connection between jmxtrans and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying jmxtrans in a different namespace to your Event Streams installation, copy the secret to the required namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - The command creates the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the jmxtrans.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   Jmxtrans packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where jmxtrans will be deployed.  Kubernetes pod specification created that exposes the configuration to jmxtrans via environment variables.Example Dockerfile Create a Dockerfile as follows. FROM jmxtrans/jmxtransCOPY run.sh .ENTRYPOINT [ \"./run.sh\" ]Example run.sh Create a run.sh script as follows. The script generates the JSON configuration file and substitutes the release-specific connection values. It then runs jmxtrans. #!/bin/shcat &lt;&lt;EOF &gt;&gt; /var/lib/jmxtrans/config.json{  \"servers\": [    {      \"port\": 9999,      \"host\": \"${JMX_HOST_0}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_1}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_2}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    }  ]}EOFexec /docker-entrypoint.sh start-without-jmxAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Building the Docker image Build the Docker image as follows.   Ensure that the Dockerfile and run.sh are in the same directory.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt; .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create a jmxtrans pod as follows.   Copy the following into a file called jmxtrans.yaml.    apiVersion: v1kind: Podmetadata:    name: jmxtrans-broker    labels:       app: jmxtrans-brokerspec:    containers:    - name: jmxtrans      image: &lt;full name of docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          mountPath: /etc/jmx-secret      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST_0        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-0.&lt;release-namespace&gt;.svc      - name: JMX_HOST_1        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-1.&lt;release-namespace&gt;.svc      - name: JMX_HOST_2        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-2.&lt;release-namespace&gt;.svc      - name: SSL_TRUSTSTORE        value: /etc/jmx-secret/store.jks      - name: SSL_TRUSTSTORE_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password      - name: JAVA_OPTS        value: -Djavax.net.ssl.trustStore=$(SSL_TRUSTSTORE) -Djavax.net.ssl.trustStorePassword=$(SSL_TRUSTSTORE_PASSWORD)      # The SECONDS_BETWEEN_RUNS is the scrape frequency of the JMX values. The default value is 60 seconds. Change it to a value to suit your requirements.      - name: SECONDS_BETWEEN_RUNS        value: \"15\"      - name: SPLUNK_HOST        value: &lt;splunk-hostname-or-ip-address&gt;    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              path: store.jks        Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f jmxtrans.yamlEvents start appearing in Splunk after running the command. The amount of time it takes before events appear in the Splunk index depends on a combination of the scrape interval on jmxtrans and the size of the receive queue on Splunk. You can increase or decrease the frequency of samples in jmxtrans and the size of the receive queue. To modify the receive queue on Splunk, create an inputs.conf file, and specify the queueSize and persistentQueueSize settings of the [tcp://&lt;remote server&gt;:&lt;port&gt;] stanza. Troubleshooting If metrics are not appearing in your external Splunk, check the logs for jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker To get debug-level logs from jmxtrans, use the following steps:   Copy the following into a file called logback.xml.    &lt;configuration debug=\"false\"&gt;  &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;    &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;    &lt;encoder&gt;      &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;logger name=\"com.googlecode.jmxtrans\" level=\"${logLevel}\"/&gt;  &lt;root level=\"info\"&gt;    &lt;appender-ref ref=\"console\" /&gt;  &lt;/root&gt;&lt;/configuration&gt;        Add the file to the same directory as the Dockerfile and run.sh files.  Edit the Dockerfile to include the logback.xml file, for example:    FROM jmxtrans/jmxtransCOPY logback.xml /usr/share/jmxtrans/conf/logback.xmlCOPY configure.sh .ENTRYPOINT [ \"./configure.sh\" ]        Follow the instructions for building the docker image.  Add the following environment variable to the jmxtrans.yaml file after the env: property:      - name: JMXTRANS_OPTS    value: -Djmxtrans.log.level=debug        Delete jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; delete pod jmxtrans-broker  Check that it has been deleted with the following command kubectl -n &lt;target-namespace&gt; get pods  When it has been deleted, create it again with the following command: kubectl -n &lt;target-namespace&gt; apply -f jmxtrans.yaml  View the logs with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/monitor-with-splunk/",
        "teaser":null},{
        "title": "Setting up alert notifications to Slack",
        "collection": "tutorials",
        "excerpt":"Receiving notifications based on monitored metrics is an important way of keeping an eye on the health of your cluster. You can set up notifications to be sent to applications like Slack based on pre-defined triggers. The following tutorial shows an example of how to set up alert notifications to be sent to Slack based on metrics from Event Streams. Prerequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1 installed on IBM Cloud Private 3.1.1, using the default master port 8443.  Ensure you have Slack installed and ready to use. This tutorial is based on Slack version 3.3.8.  You need to be a Workplace Administrator to add apps to a Slack channel.Preparing Slack To send notifications from Event Streams to your Slack channel, configure an incoming webhook URL within your Slack service. The webhook URL provided by Slack is required for the integration steps later in this section. To create the webhook URL:   Open Slack and go to your Slack channel where you want the notifications to be sent.  From your Slack channel click the icon for Channel Settings, and select Add apps or Add an app depending on the Slack plan you are using.  Search for “Incoming Webhooks”.  Click Add configuration.  Select the channel that you want to post to.  Click Add Incoming Webhooks integration.  Copy the URL in the Webhook URL field.For more information about incoming webhooks in Slack, see the Slack documentation. Selecting the metric to monitor To retrieve a list of available metrics, use an HTTP GET request on your ICP cluster URL as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:8443.  Use the following request: https://&lt;Cluster Master Host&gt;:8443/prometheus/api/v1/label/__name__/valuesThe list of available metrics is displayed.  Select a metric to monitor.For example, to test the triggering of alerts, you can monitor the total number of partitions for all topics by using the kafka_server_replicamanager_partitioncount_value metric. When topics are created, this metric can trigger notifications. For production environments, a good metric to monitor is the number of under-replicated partitions as it tells you about potential problems with your Kafka cluster, such as load or network problems where the cluster becomes overloaded and followers are not able to catch up on leaders. Under-replicated partitions might be a temporary problem, but if it continues for longer, it probably requires urgent attention. An example is to set up a notification trigger to your Slack channel if the number of under-replicated partitions is greater than 0 for more than a minute. You can do this with the kafka_server_replicamanager_underreplicatedpartitions_value metric. The examples in this tutorial show you how to set up monitoring for both of these metrics, with the purpose of testing notification triggers, and also to have a production environment example. Note: Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap. For information about the different metrics, see Monitoring Kafka. Setting the alert rule To set up the alert rule and define the trigger criteria, use the monitoring-prometheus-alertrules ConfigMap. By default, the list of rules is empty. See the data section of the ConfigMap, for example: user$ kubectl get configmap -n kube-system monitoring-prometheus-alertrules -o yamlapiVersion: v1data:  alert.rules: \"\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"4564\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Example test setup As mentioned earlier, to test the triggering of alerts, you can monitor the total number of partitions for all topics by using the kafka_server_replicamanager_partitioncount_value metric. Define an alert rule that creates a notification if the number of partitions increases. To achieve this, add a new rule for kafka_server_replicamanager_partitioncount_value, and set the trigger conditions in the data section, for example: Note: In this example, we are setting a threshold value of 50 as the built-in consumer-offsets topic has 50 partitions by default already, and this topic is automatically created the first time a consumer application connects to the cluster. We will create a topic later with 10 partitions to test the firing of the alert and the subsequent notification to the Slack channel. user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertrulesapiVersion: v1data:  sample.rules: |-    groups:    - name: alert.rules      #      # Each of the alerts you want to create will be listed here      rules:      # Posts an alert if the number of partitions increases      - alert: PartitionCount        expr: kafka_server_replicamanager_partitioncount_value &gt; 50        for: 10s        labels:          # Labels should match the alert manager so that it is received by the Slack hook          severity: critical        # The contents of the Slack messages that are posted are defined here        annotations:          identifier: \"Partition count\"          description: \"There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"84156\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Important: As noted in the prerequisites, this tutorial is based on IBM Cloud Private 3.1.1. Setting up alert rules is different if you are using IBM Cloud Private 3.1.2 or later, as each alert rule is a dedicated Kubernetes resource instead of being defined in a ConfigMap. This means that instead of adding alert rule entries to a ConfigMap, you create a separate alert rule resource for each alert you want to enable. In addition, the alert rules don’t need to be in the kube-system namespace, they can be added to the namespace where your release is deployed. This also means you don’t have to be a Cluster administrator to add alert rules. For example, to create the rule by using a dedicated alert rule, you can save it to a file as follows: apiVersion: monitoringcontroller.cloud.ibm.com/v1kind: AlertRulemetadata:  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.4.0    component: prometheus    heritage: Tiller    release: RELEASENAME  name: partition-count  namespace: NAMESPACEspec:  data: |-    groups:      - name: PartitionCount        rules:          - alert: PartitionCount            expr: kafka_server_replicamanager_partitioncount_value &gt; 50            for: 10s            labels:              severity: critical            annotations:              identifier: 'Partition count'              description: 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}'  enabled: trueTo review your alert rules set up this way, use the kubectl get alertrules command, for example: $ kubectl get alertrulesNAME                          ENABLED   AGE   CHART                     RELEASE         ERRORSpartition-count               true      1h    ibm-icpmonitoring-1.4.0   es-demoExample production setup As mentioned earlier, a good metric to monitor in production environments is the metric kafka_server_replicamanager_underreplicatedpartitions_value, for which we want to define an alert rule that creates a notification if the number of under-replicated partitions is greater than 0 for more than a minute. To achieve this, add a new rule for kafka_server_replicamanager_underreplicatedpartitions_value, and set the trigger conditions in the data section, for example: user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertrulesapiVersion: v1data:  sample.rules: |-    groups:    - name: alert.rules      #      # Each of the alerts you want to create will be listed here      rules:      # Posts an alert if there are any under-replicated partitions      #  for longer than a minute      - alert: under_replicated_partitions        expr: kafka_server_replicamanager_underreplicatedpartitions_value &gt; 0        for: 1m        labels:          # Labels should match the alert manager so that it is received by the Slack hook          severity: critical        # The contents of the Slack messages that are posted are defined here        annotations:          identifier: \"Under-replicated partitions\"          description: \"There are {{ $value }} under-replicated partition(s) reported by broker {{ $labels.kafka }}\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"84156\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Important: As noted in the prerequisites, this tutorial is based on IBM Cloud Private 3.1.1. Setting up alert rules is different if you are using IBM Cloud Private 3.1.2 or later, as each alert rule is a dedicated Kubernetes resource instead of being defined in a ConfigMap. This means that instead of adding alert rule entries to a ConfigMap, you create a separate alert rule resource for each alert you want to enable. In addition, the alert rules don’t need to be in the kube-system namespace, they can be added to the namespace where your release is deployed. This also means you don’t have to be a Cluster administrator to add alert rules. For example, to create the rule by using a dedicated alert rule, you can save it to a file as follows: apiVersion: monitoringcontroller.cloud.ibm.com/v1kind: AlertRulemetadata:  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.4.0    component: prometheus    heritage: Tiller    release: RELEASENAME  name: under-replicated-partitions  namespace: NAMESPACEspec:  data: |-    groups:      - name: UnderReplicatedPartitions        rules:          - alert: UnderReplicatedPartitions            expr: kafka_server_replicamanager_underreplicatedpartitions_value &gt; 0            for: 1m            labels:              severity: critical            annotations:              identifier: 'Under-replicated partitions'              description: 'There are {{ $value }} under-replicated partition(s) reported by broker {{ $labels.kafka }}'  enabled: trueTo review your alert rules set up this way, use the kubectl get alertrules command, for example: $ kubectl get alertrulesNAME                          ENABLED   AGE   CHART                     RELEASE         ERRORSunder-replicated-partitions   true      1h    ibm-icpmonitoring-1.4.0   es-prodDefining the alert destination To define where to send the notifications triggered by the alert rule, specify Slack as a receiver by adding details about your Slack channel and the webhook you copied earlier to the monitoring-prometheus-alertmanager ConfigMap. For more information about Prometheus Alertmanager, see the Prometheus documentation. By default, the list of receivers is empty. See the data section of the ConfigMap, for example: user$ kubectl get configmap -n kube-system monitoring-prometheus-alertmanager -o yamlapiVersion: v1data:  alertmanager.yml: |-    global:    receivers:      - name: default-receiver    route:      group_wait: 10s      group_interval: 5m      receiver: default-receiver      repeat_interval: 3hkind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: alertmanager    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertmanager  namespace: kube-system  resourceVersion: \"4565\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertmanager  uid: a87bdb44-c89f-11e8-9f94-00000a3304c0Define the Slack channel as the receiver using the incoming webhook you copied earlier, and also set up the notification details such as the channel to post to, the content format, and criteria for the events to send to Slack. Settings to configure include the following:   slack_api_url: The incoming webhook generated in Slack earlier.  send_resolved: Set to true to send notifications about resolved alerts.  channel: The Slack channel to send the notifications to.  username: The username that posts the alert notifications to the channel.For more information about the configuration settings to enter for Slack notifications, see the Prometheus documentation. The content for the posts can be customized, see the following blog for Slack alert examples from Prometheus. For example, to set up Slack notifications for your alert rule created earlier: user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertmanagerapiVersion: v1data:  alertmanager.yml: |-    global:      # This is the URL for the Incoming Webhook you created in Slack      slack_api_url:  https://hooks.slack.com/services/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    receivers:      - name: default-receiver        #        # Adding a Slack channel integration to the default Prometheus receiver        #  see https://prometheus.io/docs/alerting/configuration/#slack_config        #  for details about the values to enter        slack_configs:        - send_resolved: true          # The name of the Slack channel that alerts should be posted to          channel: \"#ibm-eventstreams-demo\"          # The username to post alerts as          username: \"IBM Event Streams\"          # An icon for posts in Slack          icon_url: https://developer.ibm.com/messaging/wp-content/uploads/sites/18/2018/09/icon_dev_32_24x24.png          #          # The content for posts to Slack when alert conditions are fired          # Improves on the formatting from the default, with support for handling          #  alerts containing multiple events.          # (Modified from the examples in          #   https://medium.com/quiq-blog/better-slack-alerts-from-prometheus-49125c8c672b)          title: |-            [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}{{ range .Alerts.Firing }} @ {{ .Annotations.identifier }}{{ end }}{{ range .Alerts.Resolved }} @ {{ .Annotations.identifier }}{{ end }}{{ end }}          text: |-            {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}            {{ range .Alerts.Firing }}{{ .Annotations.description }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.description }}{{ end }}            {{ else }}            {{ if gt (len .Alerts.Firing) 0 }}            *Alerts Firing:*            {{ range .Alerts.Firing }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}            {{ end }}{{ end }}            {{ if gt (len .Alerts.Resolved) 0 }}            *Alerts Resolved:*            {{ range .Alerts.Resolved }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}            {{ end }}{{ end }}            {{ end }}    route:      group_wait: 10s      group_interval: 5m      receiver: default-receiver      repeat_interval: 3h      #      # The criteria for events that should go to Slack      routes:      - match:          severity: critical        receiver: default-receiverkind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: alertmanager    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertmanager  namespace: kube-system  resourceVersion: \"4565\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertmanager  uid: a87bdb44-c89f-11e8-9f94-00000a3304c0To check that the new alert is set up, use the Prometheus UI as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:8443.  Go to the Prometheus UI at https://&lt;Cluster Master Host&gt;:8443/prometheus, and click the Alerts tab to see the active alerts. You can also go to Status &gt; Rules to view the defined alert rules.For example:   Testing Example test setup To create a notification for the test setup, create a topic with 10 partitions as follows:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Click Create topic.  Follow the instructions to create the topic, and set the Partitions value to 10.The following notification is sent to the Slack channel when the topic is created:  To create a resolution alert, delete the topic you created previously:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Go to the topic you created and click  More options &gt; Delete this topic.When the topic is deleted, the following resolution alert is posted:  Example production setup For the production environment example, the following notification is posted to the Slack channel if the number of under-replicated partitions remains above 0 for a minute:  When the cluster recovers, a new resolution alert is posted when the number of under-replicated partitions returns to 0. This is based on the send_resolved setting (was set to true).  Setting up other notifications You can use this example to set up alert notifications to other applications, including HipChat, PagerDuty, emails, and so on. You can also use this technique to generate HTTP calls, which lets you customize alerts when defining a flow in tools like Node-RED or IBM App Connect. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/monitoring-alerts/",
        "teaser":null},{
        "title": "Installing a multizone cluster",
        "collection": "tutorials",
        "excerpt":"The following tutorial explains how to set up a multizone Event Streams cluster in a non-zone-aware cluster. The example used in this tutorial is for installing a 3 zone cluster. The example shows how to prepare your cluster for multiple zones by labelling your nodes, and then using those labels to set up the zones when installing Event Streams. Prerequisites This tutorial is based on the following software versions:   IBM Cloud Private version 3.2.1  IBM Event Streams version 2019.4.1Labelling the worker nodes To make your IBM Cloud Private cluster zone aware, label your worker nodes to be able to specify later the zones that each node will be added to. Labelling for zones Label your IBM Cloud Private worker nodes, so that they can later be allocated to zones. Run the following command to retrieve the nodes in your cluster: kubectl get nodes This will list the nodes of your cluster. In this example there are 12 nodes: 192.0.0.1 192.0.0.2 192.0.0.3 192.0.0.4 192.0.0.5 192.0.0.6 192.0.0.7 192.0.0.8 192.0.0.9 192.0.0.10 192.0.0.11 192.0.0.12Using the node IP addresses, set which zone you want each node to be in. In this tutorial, you are setting up a 3 zone cluster by using the following labels, each representing a data center:   es-zone-0  es-zone-1  es-zone-2Label your nodes with these zone labels by using the kubectl label nodes command. In this example, as you are creating a 3 zone cluster, and have 12 nodes, label every 4 nodes with the same zone label. Label the first 4 nodes 192.0.0.1-4 to allocate them to zone es-zone-0: kubectl label nodes 192.0.0.1 192.0.0.2 192.0.0.3 192.0.0.4 failure-domain.beta.kubernetes.io/zone=\"es-zone-0\" Label the next 4 nodes 192.0.0.5 192.0.0.6 192.0.0.7 192.0.0.8 to allocate them to zone es-zone-1: kubectl label nodes 192.0.0.5-8 failure-domain.beta.kubernetes.io/zone=\"es-zone-1\" Label the next 4 nodes 192.0.0.9 192.0.0.10 192.0.0.11 192.0.0.12 to allocate them to zone es-zone-2: kubectl label nodes 192.0.0.9-12 failure-domain.beta.kubernetes.io/zone=\"es-zone-2\" As a result, all nodes that Event Streams will use now have a label of failure-domain.beta.kubernetes.io/zone with value es-zone-0, es-zone-1, or es-zone-2. Run the following command to verify this: kubectl get nodes --show-labels Labelling for Kafka Kafka broker pods need to be distributed as evenly as possible across the zones by dedicating a node in each zone to a Kafka pod. In this example, there are 6 Kafka brokers. This means 6 worker nodes are needed to host our Kafka brokers. Splitting the 6 brokers across 3 zones equally means you will have 2 Kafka brokers in each zone. To achieve this, label 2 nodes in each zone. In this example, label the first 2 nodes of each zone: 192.0.0.1, 192.0.0.2, 192.0.0.5, 192.0.0.6, 192.0.0.9, and 192.0.0.10. For example: kubectl label node 192.0.0.1 192.0.0.2 node-role.kubernetes.io/kafka=true Labelling for ZooKeeper Event Streams deploys 3 ZooKeeper nodes. Distribute the 3 ZooKeeper pods across the zones by dedicating 1 or 2 nodes in each zone to a ZooKeeper pod. Note: Do not label more than 2 in each zone. In addition, it is preferred that you label nodes that are not already labelled with Kafka. In this example, label the last 2 nodes of each zone: 192.0.0.3, 192.0.0.4, 192.0.0.7, 192.0.0.8, 192.0.0.11, and 192.0.0.12. For example: kubectl label node 192.0.0.3 192.0.0.4 node-role.kubernetes.io/zk=true You can check that your nodes are labelled as required by using the following command: kubectl get nodes --show-labels Installing Event Streams When installing Event Streams, configure the Kafka broker and multizone options as follows. If you are installing by using the UI, set the following options for the example in this tutorial:   Set the number of Kafka brokers to 6 in the Kafka brokers field.  Set the  Number of zones field to 3.  Enter the zone label names for each zone in the Zone labels field:     es-zone-0 es-zone-1 es-zone-2      If you are installing by using the CLI, use the following settings for the example in this tutorial: helm install --tls --name my-es --namespace=eventstreams \\--set license=accept \\--set global.image.pullSecret=\"my-ips\" \\--set global.image.repository=\"&lt;image-pull-repository&gt;\" \\--set messageIndexing.messageIndexingEnabled=false \\--set global.zones.count=3 \\--set global.zones.labels[0]=\"es-zone-0\" \\--set global.zones.labels[1]=\"es-zone-1\" \\--set global.zones.labels[2]=\"es-zone-2\" \\--set kafka.brokers=6 \\ibm-eventstreams-prodCreating topics for multizone setup It is important that you do not configure topics where the minimum in-sync replicas setting cannot be met in the event of a zone failure. Warning: Do not create a topic with 1 replica. Setting 1 replica means the topic will become unavailable during an outage and will lose data. In this example, create a topic with 6 replicas, setting the minimum in-sync replicas configuration to 4. This means if a zone is lost, 2 brokers would be lost and therefore 2 replicas. The minimum in-sync replicas would still mean the system remains operational with no data loss, as 4 brokers still remain, with four replicas of the topics data. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/FHIR-Server-Docs/tutorials/multi-zone-tutorial/",
        "teaser":null}]
