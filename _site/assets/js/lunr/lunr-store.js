var store = [{
        "title": "Home",
        "collection": "10.0",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "10.0",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the Apache Kafka® project and incorporates the open-source Strimzi technology. Event Streams version 10.0.0 includes Kafka release 2.5.0, and supports the use of all Kafka interfaces. IBM Event Streams uses Strimzi to deploy Apache Kafka in a resilient and manageable way, and provides a range of additional capabilities to extend the core functionality. IBM Event Streams features include:       Apache Kafka deployed by Strimzi that distributes Kafka brokers across the nodes of a Red Hat OpenShift Container Platform cluster, creating a highly-available and resilient deployment.         An administrative user interface (UI) focused on providing essential features for managing production clusters, in addition to enabling developers to get started with Apache Kafka. The Event Streams UI facilitates topic lifecycle control, message filtering and browsing, client metric analysis, schema and geo-replication management, connection details and credentials generation, and a range of tools, including a sample starter and producer application.         A command-line interface (CLI) to enable manual and scripted cluster management. For example, you can use the Event Streams CLI to inspect brokers, manage topics, deploy schemas, and manage geo-replication.         Geo-replication of topics between clusters to enable disaster recovery.         A REST API for producing messages to Event Streams topics, expanding event source possibilities.         A schema registry to support the definition and enforcement of message formats between producers and consumers by using Apache Avro schemas.         Health check information to help identify issues with your clusters and brokers.         Secure by default production cluster templates with authentication, authorization and encryption included, and optional configurations to simplify proof of concept or lite development environments.         Granular security configuration through Kafka access control lists to configure authorization and quotas for connecting applications.   Operators Kubernetes Operators are designed to simplify the deployment and maintenance of complex applications. They do this by packaging up and abstracting away domain-specific knowledge of how to deploy, configure and operate the application and its component parts. An Operator can then extend Kubernetes by providing new Kubernetes constructs to support these abstractions, simplifying the initial deployment and subsequent lifecycle management of the application. Strimzi uses Operators in this manner to facilitate the deployment of Kafka clusters. Event Streams builds on top of Strimzi to deploy not only the Kafka components but also the additional features outlined earlier. A new Kubernetes resource EventStreams is provided (in Kubernetes referred to as a “kind”), to allow the definition of a complete deployment that brings together the components provided by Strimzi and Event Streams. This information is defined in a standard YAML document and deployed in a single operation. Operators provided by Event Streams The following diagram shows the operators involved in an Event Streams deployment along with the resources they manage. Event Streams builds on the Strimzi core, and adds additional components to extend the base capabilities.  The EventStreams Cluster Operator The Strimzi Cluster Operator manages the deployment of core components within the Kafka cluster such as Kafka and ZooKeeper nodes. The EventStreams Cluster Operator extends the Strimzi Cluster Operator to provision the additional components provided by  Event Streams alongside these core components. A new custom resource Type called EventStreams is provided to manage this overall deployment. The Entity Operator Kafka provides authentication and authorization through Users and Access Control Lists. These define the operations that users can perform on specific resources within the cluster. The User operator provides a new custom resource type called KafkaUser to manage these Kafka users and associated Access Control Lists and is a mandatory component of an Event Streams deployment. Kafka topics are a resource within the cluster to which a series of records can be produced. The optional Topic operator provides a new custom resource type called KafkaTopic to manage these Kafka topics. By default, the Topic operator is not part of the Event Streams cluster. Instead, to create and manage Kafka topics, use the Event Streams UI, CLI and REST API provided mechanisms. If there is a need to have Kafka topics represented as Kubernetes resources, the optional Topic operator can be included in the EventStreams definition, and will then be deployed as a container alongside the User operator within the Entity operator Pod. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "10.0",
        "excerpt":"Find out what is new in IBM Event Streams version 10.0.0. Operator-based release The Kubernetes operator replaces Helm as the method for installing and managing Event Streams. The Event Streams operator uses custom resources to manage the required components. The operator can deploy instances of Event Streams as required, and manage the lifecycle of each instance, including configuration changes and upgrades. Find out more about Event Streams and operators. New security model Event Streams version 10.0 introduces changes to the way security works. The main changes are as follows:   Support for SCRAM-SHA-512 and Mutual TLS security mechanisms when connecting Kafka clients to the Event Streams Cluster.  Support for Kafka Access Control Lists for permissions to Kafka resources.  Support for insecure cluster installation, for proof of concept or development environments.  Support for multiple REST endpoints with configurable security definitions.Kafka Connect framework hosted by Event Streams The Event Streams operator now deploys and manages Kafka Connect clusters using the KafkaConnectS2I and KafkaConnector custom resources. The KafkaConnectS2I custom resource enables users to install new connectors without needing to run local Docker builds. The KafkaConnector custom resource provides a Kubernetes-native way to start, pause and stop connectors and tasks. For more information, see the documentation on creating Kafka Connect clusters. Additional tested connectors The connector catalog now includes additional connectors that are commercially supported for customers with a support entitlement for IBM Cloud Pak for Integration:   An Elasticsearch sink connector that subscribes to one or more Kafka topics and writes the records to Elasticsearch.  Debezium’s PostgreSQL source connector that can monitor the row-level changes in the schemas of a PostgreSQL database and record them in separate Kafka topics.  Debezium’s MySQL source connector that can monitor all of the row-level changes in the databases on a MySQL server or HA MySQL cluster and record them in Kafka topics.Geo-replication now uses MirrorMaker2 Kafka Mirror Maker 2 provides the implementation for geo-replication in IBM Event Streams 10.0.0. Find out more about Event Streams geo-replication. Support for Red Hat OpenShift Container Platform 4.4 IBM Event Streams 10.0.0 introduces support for Red Hat OpenShift Container Platform 4.4. Kafka version upgraded to 2.5.0 Event Streams version 10.0.0 includes Kafka release 2.5.0, and supports the use of all Kafka interfaces. Default resource requirements have changed The minimum footprint for Event Streams has been reduced. See the updated tables for the resource requirements. Technology Preview features Technology Preview features are available to evaluate potential upcoming features. Such features are intended for testing purposes only and not for production use. IBM does not support these features, but might help with any issues raised against them. IBM welcomes feedback on Technology Preview features to improve them. As the features are still under development, functions and interfaces can change, and it might not be possible to upgrade when updated versions become available. IBM offers no guarantee that Technology Preview features will be part of upcoming releases and as such become fully supported. Event Streams version 10.0.0 includes Cruise Control for Apache Kafka as a Technology Preview feature. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "10.0",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers (Kafka brokers). The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "10.0",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the required rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "10.0",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "10.0",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "10.0",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "10.0",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "10.0",
        "excerpt":"To try out Event Streams, you have the following options:   Create a subscription for a fully managed Kafka service on IBM Cloud.      Install IBM Event Streams on the Red Hat OpenShift Container Platform for development purposes or to set up Kafka for production use, and benefit from both the support of IBM and the open-source community.     Event Streams comes with a host of useful features such as a user interface (UI) to help get started with Kafka and help operate a production cluster, geo-replication of topics between clusters, a schema registry to enforce correct and consistent message formats, a connector catalog, and more.         Use Strimzi if you want to install your own basic Kafka cluster on Kubernetes for testing and proof-of-concept purposes.     As Event Streams is based on Strimzi, you can easily move your deployment to Event Streams later, and keep your existing configurations and preferences from the Strimzi setup. Moving to Event Streams adds the benefit of full enterprise-level support from IBM.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/trying-out/",
        "teaser":null},{
        "title": "Prerequisites",
        "collection": "10.0",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. Container environment IBM Event Streams 10.0.0 is supported on the Red Hat OpenShift Container Platform. Version 10.0.0 is installed by the Event Streams operator 2.0.0, and includes Kafka version 2.5.0. For an overview of supported component and platform versions, see the support matrix. Ensure you have the following set up for your environment:   A supported version of OpenShift Container Platform installed. See the support matrix for supported versions.  The OpenShift Container Platform CLI installed.  The IBM Cloud Pak CLI (cloudctl) installed.Hardware requirements Ensure your hardware can accommodate the resource requirements for your planned deployment. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other Event Streams components which might be required to reside on the same node. For production systems, it is recommended to have Event Streams configured with at least 3 Kafka brokers, and to have one worker node for each Kafka broker. This requires a minimum of 3 worker nodes available for use by Event Streams. Ensure each worker node runs on a separate physical server. See the guidance about Kafka high availability for more information. Resource requirements Event Streams resource requirements depend on several factors. The following sections provide guidance about minimum requirements for a starter deployment, and options for initial production configurations. Minimum resource requirements are as follows.Always ensure you have sufficient resources in your environment to deploy the Event Streams operator together with a development or a production Event Streams operand configuration.             Deployment      CPU (cores)      Memory (Gi)      VPCs (see licensing)                  Operator      1      1      N/A              Development      8.8      9.1      0.5              Production      13      15.3      3      Note: Event Streams provides samples to help you get started with deployments. The resource requirements for these specific samples are detailed in the planning section. If you do not have an Event Streams installation on your system yet, always ensure you include the resource requirements for the operator together with the intended Event Streams instance requirements (development or production). Important: Licensing is based on the number of Virtual Processing Cores (VPCs) used by your Event Streams instance. See licensing considerations for more information. For a production installation of Event Streams, the ratio is 1 license required for every 1 VPC being used. For a non-production installation of Event Streams, the ratio is 1 license required for every 2 VPCs being used. Event Streams is a Kubernetes operator-based release and uses custom resources to define your Event Streams configurations.The Event Streams operator uses the declared required state of your Event Streams in the custom resources to deploy and manage the entire lifecycle of your Event Streams instances. Custom resources are presented as YAML configuration documents that define instances of the EventStreams custom resource type. The provided samples define typical configuration settings for your Event Streams instance, including broker configurations, security settings, and default values for resources such as CPU and memory defined as “request” and “limit” settings. Requests and limits are Kubernetes concepts for controlling resource types such as CPU and memory.   Requests set the minimum requirements a container requires to be scheduled. If your system does not have the required request value, then the services will not start up.  Limits set the value beyond which a container cannot consume the resource. It is the upper limit within your system for the service. Containers that exceed a CPU resource limit are throttled, and containers that exceed a memory resource limit are terminated by the system.Ensure you have sufficient CPU capacity and physical memory in your environment to service these requirements. Your Event Streams instance can be dynamically updated later through the configuration options provided in the custom resource. Installing Event Streams has two phases:   Install the Event Streams operator: this will deploy the operator that will install and manage your Event Streams instances.  Install one or more instances of Event Streams by applying configured custom resources.Operator requirements The Event Streams operator requires the following minimum resource requirements. Ensure you always include sufficient CPU capacity and physical memory in your environment to service the operator requirements.             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)                  0.2      1.0      1.0      1.0      The Event Streams operator will automatically deploy the required IBM Cloud Platform Common Services if not present. Ensure you include the following Common Services component resource requirements in your calculations:   Catalog UI  Certificate Manager  Common Web UI  IAM  Ingress NGINX  Installer  Licensing  Management ingress  Metering  Mongo DB  Monitoring Exporters  Monitoring Grafana  Monitoring Prometheus Ext  Platform APINote: If you are installing Event Streams in an existing IBM Cloud Pak for Integration deployment, the required Common Services might already be installed. Adding Event Streams geo-replication to a deployment The Event Streams geo-replicator allows messages sent to a topic on one Event Streams cluster to be automatically replicated to another Event Streams cluster. This capability ensures messages are available on a separate system to form part of a disaster recovery plan. To use this feature, ensure you have the following additional resources available. The following table shows the prerequisites for each geo-replicator node.             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  1.0      2.0      2.0      2.0      1.0      For instructions about installing geo-replication, see configuring. Red Hat OpenShift Security Context Constraints Event Streams requires a Security Context Constraint (SCC) to be bound to the target namespace prior to installation. By default, Event Streams uses the default restricted SCC that comes with the OpenShift Container Platform. If you use a custom SCC (for example, one that is more restrictive), or have an operator that updates the default SCC, the changes might interfere with the functioning of your Event Streams deployment. To resolve any issues, apply the SCC provided by Event Streams as described in troubleshooting. Network requirements IBM Event Streams is supported for use with IPv4 networks only. Data storage requirements If you want to set up persistent storage, Event Streams requires block storage configured to use the XFS or ext4 file system. The use of file storage (for example, NFS) is not recommended. For example, you can use one of the following systems:   Kubernetes local volumes  Amazon Elastic Block Store (EBS)  Rook Ceph  Red Hat OpenShift Container StorageIBM Event Streams UI The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterSee the post-installation tasks for information about installing the CLI Kafka clients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8  Oracle Java 8You can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning your installation",
        "collection": "10.0",
        "excerpt":"Consider the following when planning your installation of Event Streams. Decide the purpose of your deployment, for example, whether you want to try a starter deployment for testing purposes, or start setting up a production deployment.   Use the sample deployments as a starting point if you need something to base your deployment on.  Size your planned deployment by considering potential throughput, the number of producers and consumers, Kafka performance tuning, and other aspects. For more details, see the performance considerations section.  For production use, and whenever you want your data to be saved in the event of a restart, set up persistent storage.  Consider the options for securing your deployment.  Plan for resilience by understanding Kafka high availability and how to support it, set up multiple availability zones for added resilience, and consider geo-replication to help with your disaster recovery planning.  Consider setting up logging for your deployment to help troubleshoot any potential issues.Sample deployments A number of sample configurations are provided when installing Event Streams on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload.   Development deployments  Production deploymentsThe sample configurations can be found in the OpenShift Container Platform web console as explained in installing or on GitHub. Development deployments If you want to try Event Streams, use one of the development samples when configuring your instance. Installing with these settings is suitable for a starter deployment intended for testing purposes and trying out Event Streams. It is not suitable for production environments. For samples appropriate for production use, see production deployments The following development samples are available:   Lightweight without security  DevelopmentExample deployment: Lightweight without security Overview: A non-production deployment suitable for basic development, and test activities. For environments where minimum resource requirements, persistent storage, access control and encryption are not required. This example provides a starter deployment that can be used if you simply want to try Event Streams with a minimum resource footprint. It installs an Event Streams instance with the following characteristics:   A small single broker Kafka cluster and a single node ZooKeeper.  As there is only 1 broker, no message replication takes place between brokers, and the system topics (message offset and transaction state) are configured accordingly for this.  There is no encryption internally between containers.  External connections use TLS encryption, but no authentication to keep the configuration to a minimum, making it easy to experiment with the platform.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  2.9      8.8      6.3      9.1      0.5      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: This deployment is not suitable for a production system even if storage configuration is applied. This is due to the number of Kafka and ZooKeeper nodes not being appropriate for data persistence and high availability. For a production system, at least three Kafka brokers and ZooKeeper nodes are required for an instance, see production sample deployments later for alternatives. Example deployment: Development Overview: A non-production deployment for experimenting with Event Streams configured for high availability, authentication, and no persistent storage. Suitable for basic development and testing activities. This example provides a starter deployment that can be used if you want to try Event Streams with a minimum resource footprint. It installs an Event Streams instance with the following settings:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  3.3      13.0      7.1      15.3      1.5      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Production deployments To start setting up a production instance, use one of the following samples.   Minimal production  Production 3 brokers  Production 6 brokers  Production 9 brokersImportant: For a production setup, the sample configuration values are for guidance only, and you might need to change them. Ensure you set your resource values as required to cope with the intended usage, and also consider important configuration options for your environment and Event Streams requirements as described in the rest of this planning section. Example deployment: Minimal production Overview: A minimal production deployment for Event Streams. This example provides the smallest possible production deployment that can be configured for Event Streams. It installs an Event Streams instance with the following settings:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.  Kafka tuning settings consistent with 3 brokers are applied as follows:num.replica.fetchers: 3num.io.threads: 24num.network.threads: 9log.cleaner.threads: 6If a storage solution has been configured, the following characteristics make this a production-ready deployment:   Messages are replicated between brokers to ensure that no single broker is a point of failure. If a broker restarts, producers and consumers of messages will not experience any loss of service.  The number of threads made available for replicating messages between brokers, is increased to 3 from the default 1. This helps to prevent bottlenecks when replicating messages between brokers, which might otherwise prevent the Kafka brokers from being fully utilized.  The number of threads made available for processing requests is increased to 24 from the default 8, and the number of threads made available for managing network traffic is increased to 9 from the default 3. This helps prevent bottlenecks for producers or consumers, which might otherwise prevent the Kafka brokers from being fully utilized.  The number of threads made available for cleaning the Kafka log is increased to 6 from the default 1. This helps to ensure that records that have exceeded their retention period are removed from the log in a timely manner, and prevents them from accumulating in a heavily loaded system.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  9.7      13.0      15.3      15.3      3.0      You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 3 brokers Overview: A small production deployment for Event Streams. This example installs a production-ready Event Streams instance similar to the Minimal production setup, but with added resource requirements:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.  The memory and CPU requests and limits for the Kafka brokers are increased compared to the Minimal production sample described previously to give them the bandwidth to process a larger number of messages.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  15.0      22.0      30.7      33.3      12.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 6 brokers Overview: A medium sized production deployment for Event Streams. This sample configuration is similar to the Production 3 brokers sample described earlier, but with an increase in the following settings:   Uses 6 brokers rather than 3 to allow for additional message capacity.  The resource settings for the individual brokers are the same, but the number of threads made available for replicating messages between brokers is increased to 6 to cater for the additional brokers to manage.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  27.1      34.3      55.1      57.7      24.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. Important: This sample is not provided in the OpenShift Container Platform web console and can only be obtained through GitHub. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 9 brokers Overview: A large production deployment for Event Streams. This sample configuration is similar to the Production 6 brokers sample described earlier, but with an increase in the following settings:   Uses 9 Brokers rather than 6 to allow for additional message capacity.  The resource settings for the individual brokers are the same, but the number of threads made available for replicating messages between brokers is increased to 9 to cater for the additional brokers to manage.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  39.1      46.6      79.4      82.1      36.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. Important: This sample is not provided in the OpenShift Container Platform web console and can only be acquired through GitHub. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Planning for persistent storage If you plan to have persistent volumes, consider the disk space required for storage.       Both Kafka and ZooKeeper rely on fast write access to disks. Use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation.     If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. The number of Kafka brokers and ZooKeeper servers depends on your setup (for example, see the provided samples described in resource requirements).         Schema registry requires a single physical volume.   You either need to create a persistent volume for each physical volume, or specify a storage class that supports dynamic provisioning. Each component can use a different storage class to control how physical volumes are allocated. See the OpenShift Container Platform documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the Cluster Administrator role.   If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. These will then be claimed from a central pool when the IBM Event Streams instance is deployed. The installation will then claim the required number of persistent volumes from this pool.  If these persistent volumes are to be created automatically, ensure a dynamic provisioner is configured for the storage class you want to use. See data storage requirements for information about storage systems supported by Event Streams.Important: When creating persistent volumes for each component, ensure the correct Access mode is set for the volumes as described in the following table.             Component      Access mode                  Kafka      ReadWriteOnce              ZooKeeper      ReadWriteOnce              Schema registry      ReadWriteMany or ReadWriteOnce      To use persistent storage, configure the storage properties in your EventStreams custom resource. Planning for security Event Streams has highly configurable security options that range from the fully secured default configuration to no security for basic development and testing. The main security vectors to consider are:   Kafka listeners  Pod-to-Pod communication  UI access  REST endpoints (REST Producer, Admin API, Schema Registry)Secure instances of Event Streams will make use of TLS to protect network traffic. Certificates will be generated by default, or you can use custom certificates. Note: If you want to use custom certificates, ensure you configure them before installing Event Streams. Event Streams UI Access As explained in the Managing access section, the IBM Cloud Platform Common Services Identity and Access Management (IAM) is used to bind a role to an identity. By default, the secure Event Streams instance will require an Administrator or higher role to authorize access. To setup LDAP (Lightweight Directory Access Protocol), assign roles to LDAP users, and create teams, see the instructions about configuring LDAP connections. Whilst it is highly recommended to always configure Event Streams with security enabled, it is also possible to configure the Event Streams UI to not require a login, which can be useful for proof of concept (PoC) environments. For details, see configuring Event Streams UI access. REST endpoint security Review the security and configuration settings of your development and test environments.The REST endpoints of Event Streams have a number of configuration capabilities. See configuring access for details. Securing communication between pods By default, Pod-to-Pod encryption is enabled. You can configure encryption between pods when configuring your Event Streams installation. Kafka listeners Event Streams has both internal and external configurable Kafka listeners. Optionally, each Kafka listener can be secured with TLS or SCRAM. Planning for resilience If you are looking for a more resilient setup, or want plan for disaster recovery, consider setting up multiple availability zones and creating geo-replication clusters. Also, set up your environment to support Kafka’s inherent high availability design. Kafka high availability Kafka is designed for high availability and fault tolerance. To reduce the impact of Event Streams Kafka broker failures, configure your installation with at least three brokers and spread them across several Red Hat OpenShift Container Platform worker nodes by ensuring you have at least as many worker nodes as brokers. For example, for 3 Kafka brokers, ensure you have at least 3 worker nodes running on separate physical servers. Kafka ensures that topic-partition replicas are spread across available brokers up to the replication factor specified. Usually, all of the replicas will be in-sync, meaning that they are all fully up-to-date, although some replicas can temporarily be out-of-sync, for example, when a broker has just been restarted. The replication factor controls how many replicas there are, and the minimum in-sync configuration controls how many of the replicas need to be in-sync for applications to produce and consume messages with no loss of function. For example, a typical configuration has a replication factor of 3 and minimum in-sync replicas set to 2. This configuration can tolerate 1 out-of-sync replica, or 1 worker node or broker outage with no loss of function, and 2 out-of-sync replicas, or 2 worker node or broker outages with loss of function but no loss of data. The combination of brokers spread across nodes together with the replication feature make a single Event Streams cluster highly available. Multiple availability zones To add further resilience to your Event Streams cluster, you can split your servers across multiple data centers or zones, so that even if one zone experiences a failure, you still have a working system. Multizone support provides the option to run a single Kubernetes cluster in multiple availability zones within the same region. Multizone clusters are clusters of either physical or virtual servers that are spread over different locations to achieve greater resiliency. If one location is shut down for any reason, the rest of the cluster is unaffected. Note: For Event Streams to work effectively within a multizone cluster, the network latency between zones must not be greater than 20 ms for Kafka to replicate data to the other brokers. Typically, high availability requires a minimum of 3 zones (sites or data centers) to ensure a quorum with high availability for components, such as Kafka and ZooKeeper. Without the third zone, you might end up with a third quorum member in a zone that already has a member of the quorum, consequently if that zone goes down, the majority of the quorum is lost and loss of function is inevitable. OpenShift Container Platform requires a minimum of 3 zones for high availability topologies and Event Streams supports that model. This is different from the traditional primary and backup site configuration, and is a move to support the quorum-based application paradigm. With zone awareness, Kubernetes automatically distributes pods in a replication controller across different zones. For workload-critical components, for example Kafka, ZooKeeper and REST Producer, set the number of replicas of each component to at least match the number of zones. This provides at least one replica of each component in each zone, so in the event of loss of a zone the service will continue using the other working zones. For information about how to prepare multiple zones, see preparing for multizone clusters. Geo-replication Consider configuring geo-replication to aid your disaster recovery and resilience planning. You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. No additional preparation is needed on the origin cluster, IBM Event Streams as geo-replication runs on the destination cluster. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Geo-replication is based on MirrorMaker 2.0, which uses Kafka Connect, enabling interoperability with other Kafka distributions. Use geo-replication to replicate data between Event Streams clusters.  Use MirrorMaker2 to move data between Event Streams clusters and other Kafka clusters. Planning for log management Event Streams uses the cluster logging provided by the OpenShift Container Platform to collect, store, and visualize logs. The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). You can use this EFK stack logging capability in your environment to help resolve problems with your deployment and aid general troubleshooting. You can use log data to investigate any problems affecting your system health. Kafka static configuration properties You can set Kafka broker configuration settings in your EventStreams custom resource under the property spec.strimziOverrides.kafka. These settings will override the default Kafka configuration defined by Event Streams. You can also use this configuration property to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. Connecting clients By default, Kafka client applications connect to cluster using the Kafka bootstrap route address. Find out more about connecting external clients to your installation. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Platform Common Services monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing Licensing considerations Licensing is based on a Virtual Processing Cores (VPC) metric. To use Event Streams you must have a license for all of the virtual cores that are available to all of the following Event Streams components:   Kafka brokers  Geo-Replicator nodes  MirrorMaker2 nodes  Kafka Connect nodes hosted by Event StreamsAll other container types are pre-requisite components that are supported as part of Event Streams, and do not require additional licenses. If you are using one of the samples provided, see the sample deployments section for information about the number of VPCs required. The number of VPCs indicate the licenses required. Note: For a production installation of Event Streams, the ratio is 1 license required for every 1 VPC being used. For a non-production installation of Event Streams, the ratio is 1 license required for every 2 VPCs being used. To flag an installation of Event Streams as production or non-production, set the spec.license.use correctly during installation. See license usage for more information about selecting the correct value. If you add more Kafka replicas, geo-replicator nodes, MirrorMaker2 nodes, or Kafka Connect nodes, each one is an additional, separate chargeable unit. See license usage to learn how you can find out more about the number of virtual cores used by your deployment. License usage The license usage of IBM Event Streams is collected by the IBM Cloud Platform Common Services License Service which is automatically deployed with IBM Event Streams. This provides the service that tracks the licensed containers and their resource usage based on the product use. When creating an instance of Event Streams, ensure that you select the correct value for spec.license.use in the custom resource. This value is used for metering purposes and could result in inaccurate charging and auditing if set incorrectly. Select one of the following values based on the purpose of your deployment:   CloudPakForIntegrationNonProduction for non-production deployments suitable for basic development and test activities.  CloudPakForIntegrationProduction for production deployments.The sample deployments provided by Event Streams have the correct value set by default based on the sample deployment purposes. The license usage information can be viewed by obtaining an API token that is required to make the API calls to retrieve license usage data, and then accessing provided APIs for retrieving the license usage data. There are 3 APIs that can be viewed:   Snapshot (last 30 days) This provides audit level information in a .zip file and is a superset of the other reports.  Products report (last 30 days) This shows the VPC usage for all products that are deployed in IBM Cloud Pak for Integration, for example:    [{\"name\":\"IBM Cloud Pak for Integration\",\"id\":\"c8b82d189e7545f0892db9ef2731b90d\",\"metricPeakDate\":\"2020-06-10\",\"metricQuantity\":3,\"metricName\":\"VIRTUAL_PROCESSOR_CORE\"}]        In this example, the metricQuantity is 3 indicating that the peak VPC usage is 3.     Bundled products report (last 30 days) This shows the breakdown of bundled products that are included in IBM Cloud Paks that are deployed on a cluster with the highest VPC usage within the requested period. For Event Streams this shows the peak number of VPCs in use, the conversion ratio and the number of licenses used. For example:    [{\"productName\":\"IBM Event Streams for Non Production\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-10\",\"metricMeasuredQuantity\":6,\"metricConversion\":\"2:1\",\"metricConvertedQuantity\":3}]        In this example, the productName shows the license metrics for a IBM Event Streams for Non Production deployment. The metricMeasuredQuantity is 6 VPCs, the metricConversion is 2:1 and metricConvertedQuantity is 3 VPCs so the license usage is 3.   Note: The metricMeasuredQuantity is the peak number of VPCs used over the timeframe. If an Event Streams instance is deleted and a new instance installed, then the quantity will be the maximum used at any one time. The following examples show the number of licenses required for specific installations: Example 1 - Non-production 3 brokers This example is for an Event Streams installation configured with 3 Kafka brokers, no Mirror Maker or Kafka Connect containers. In this example installation, each Kafka container requires 2 VPCs so in total 6 VPCs are being used. For non-production deployment the metrics conversion ratio is 2:1, therefore 3 licenses are required. Example 2 - Production 3 brokers This example is for an Event Streams installation configured with 3 Kafka brokers, no Mirror Maker or Kafka Connect containers. In this example installation, each Kafka container requires 4 VPCs so in total 12 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 12 licenses are required. Example 3 - Production 6 brokers with Geo-Replication This example is for an Event Streams installation configured with 6 Kafka brokers, 1 Mirror Maker container and no Kafka Connect containers. In this example installation, each Kafka container requires 4 VPCs and each Mirror Maker container requires 1 VPC, so in total 25 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 25 licenses are required. Example 4 - Production 9 brokers with Geo-Replication and Kafka-Connect This example is for an Event Streams installation configured with 9 Kafka brokers, 1 Mirror Maker and 1 Kafka Connect container. In this example installation, each Kafka container requires 4 VPCs, each Mirror Maker container requires 1 VPC and each Kafka-Connect container requires 1 VPC, so in total 38 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 38 licenses are required. If there are multiple production or non-production installations in a cluster then the API will show the total peak VPC usage for all production or non-production instances in that cluster. For example if you have 2 production instances of IBM Event Streams where each instance has 3 Kafka brokers that each use 2 VPS, then the total peak usage is 12 VPCs which converts to 12 licenses. If there are production and non-production IBM Event Streams instances installed in the cluster, then the metricConvertedQuantity under IBM Event Streams and IBM Event Streams for Non Production will need to be added to determine the total license usage. For example: [{\"productName\":\"IBM Event Streams for Non Production\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-10\",\"metricMeasuredQuantity\":6,\"metricConversion\":\"2:1\",\"metricConvertedQuantity\":3},{\"productName\":\"IBM Event Streams\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-11\",\"metricMeasuredQuantity\":8,\"metricConversion\":\"1:1\",\"metricConvertedQuantity\":8}]In this example there are Event Streams installations for non-production and for production. The non-production usage is 6 VPCs which converts to 3 licenses. The production usage is 8 VPCs which converts to 8 licenses. Therefore the total license usage is 11. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/planning/",
        "teaser":null},{
        "title": "Performance and capacity planning",
        "collection": "10.0",
        "excerpt":"Guidance for production environments You can use one of the sample configurations provided by Event Streams to set up a production deployment. For information about the provided samples, see the sample deployments section. If needed, you can modify the selected sample configuration when you deploy a new instance, or make changes at a later time. Tuning Event Streams Kafka performance When preparing for your Event Streams installation, review your workload requirements and consider the configuration options available for performance tuning your Event Streams installation. Kafka offers a number of configuration settings that can be adjusted as necessary for an Event Streams deployment. These can be used to address any bottlenecks in the system as well as perform fine tuning of Kafka performance. Kafka provides a wide range of configuration properties to set, but consider the following when reviewing performance requirements:   The num.replica.fetchers property sets the number of threads available on each broker to replicate messages from topic leaders. Increasing this setting increases I/O parallelism in the follower broker, and can help reduce bottlenecks and message latency. You can start by setting this value to match the number of brokers deployed in the system.Note: Increasing this value results in brokers using more CPU resources and network bandwidth.  The num.io.threads property sets the number of threads available to a broker for processing requests. As the load on each broker increases, handling requests can become a bottleneck. Increasing this property value can help mitigate this issue. The value to set depends on the overall system load and the processing power of the worker nodes, which varies for each deployment. There is a correlation between this setting and the num.network.threads setting.  The num.network.threads property sets the number of threads available to the broker for receiving and sending requests and responses to the network. The value to set depends on the overall network load, which varies for each deployment. There is a correlation between this setting and the num.io.threads setting.  The replica.fetch.min.bytes, replica.fetch.max.bytes, and replica.fetch.response.max.bytes properties control the minimum and maximum sizes for message payloads when performing inter-broker replication. Set these values to be greater than the message.max.bytes property to ensure that all messages sent by a producer can be replicated between brokers. The value to set depends on message throughput and average size, which varies for each deployment.These properties are configured in the EventStreams custom resource for an instance when it is first created and can be modified at any time. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster-level configuration property log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster-level configuration property log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration property segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration property cleanup.policy determines the way the log segments for the topic are managed. For more information about these settings, see the Kafka documentation. The cluster-level settings are configured in the EventStreams custom resource for an instance when it is first created and can be modified at any time. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click Create a topic, and set Show all available options to On). Note: When using ephemeral storage, ensure you set retention limits for Kafka topics so that you do not run out of disk space.If message retention is set to long periods and the message volume is high, the storage requirements for the topics could impact the OpenShift nodes that host the Kafka pods, and cause the nodes to run out of allocated disk space, which could impact normal operation. Log cleanup by deletion If the topic-level configuration property cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The cluster-level configuration property log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The cluster-level configuration property log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration property cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding all but the last of each duplicate key in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration property at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/capacity-planning/",
        "teaser":null},{
        "title": "Preparing for multizone clusters",
        "collection": "10.0",
        "excerpt":"IBM Event Streams supports multiple availability zones for your clusters. Multizone clusters add resilience to your Event Streams installation. For guidance about handling outages in a multizone setup, see managing a multizone setup. Zone awareness Kubernetes uses zone-aware information to determine the zone location of each of its nodes in the cluster to enable scheduling of pod replicas in different zones. Some clusters, typically AWS, will already be zone aware. For clusters that are not zone aware, each Kubernetes node will need to be set up with a zone label. To determine if your cluster is zone aware:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command as cluster administrator:     oc get nodes --show-labels   If your Kubernetes cluster is zone aware, the following label is displayed against each node: failure-domain.beta.kubernetes.io/zone. The value of the label is the zone the node is in, for example, es-zone-1. If your Kubernetes cluster is not zone aware, all cluster nodes will need to be labeled with failure-domain.beta.kubernetes.io/zone using a value that identifies the zone that each node is in. For example, run the following command to allocate a node to es-zone-1: oc label node &lt;node-name&gt; failure-domain.beta.kubernetes.io/zone=es-zone-1 The zone label is needed to set up rack awareness when installing for multizone. Kafka rack awareness In addition to zone awareness, Kafka rack awareness helps to spread the Kafka broker pods and Kafka topic replicas across different availability zones, and also sets the brokers’ broker.rack configuration property for each Kafka broker. To set up Kafka rack awareness, Kafka brokers require a cluster role to provide permission to view which Kubernetes node they are running on. Before applying Kafka rack awareness to an Event Streams installation, apply a cluster role:   Download the cluster role YAML file from GitHub.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the cluster role by using the following command and the downloaded file:     oc apply -f eventstreams-kafka-broker.yaml   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/preparing-multizone/",
        "teaser":null},{
        "title": "Installing",
        "collection": "10.0",
        "excerpt":"The following sections provide instructions about installing IBM Event Streams on the Red Hat OpenShift Container Platform. The instructions are based on using the OpenShift Container Platform web console and oc command line utility. When deploying in an air-gapped environment, ensure you have access to this documentation set, and see the instructions in the offline installation README that is provided as part of the downloaded package. Event Streams can also be installed as part of IBM Cloud Pak for Integration. Overview Event Streams is an operator-based release and uses custom resources to define your Event Streams configurations. The Event Streams operator uses the custom resources to deploy and manage the entire lifecycle of your Event Streams instances. Custom resources are presented as YAML configuration documents that define instances of the EventStreams custom resource type. Installing Event Streams has two phases:   Install the Event Streams operator: this will deploy the operator that will install and manage your Event Streams instances.  Install one or more instances of Event Streams by using the operator.Before you begin   Ensure you have set up your environment according to the prerequisites, including setting up your OpenShift Container Platform.  Ensure you have planned for your installation, such as preparing for persistent storage, considering security options, and considering adding resilience through multiple availability zones.  Obtain the connection details for your OpenShift Container Platform cluster from your administrator.Create a project (namespace) Create a namespace into which the Event Streams instance will be installed by creating a project.When you create a project, a namespace with the same name is also created. Ensure you use a namespace that is dedicated to a single instance of Event Streams. This is required because Event Streams uses network security policies to restrict network connections between its internal components. A single namespace per instance also allows for finer control of user accesses. Important: Do not use any of the default or system namespaces to install an instance of Event Streams (some examples of these are: default, kube-system, kube-public, and openshift-operators). Creating a project by using the web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Home dropdown and select Projects to open the Projects panel.  Click Create Project.  Enter a new project name in the Name field, and optionally, a display name in the Display Name field, and a description in the Description field.  Click Create.Creating a project by using the CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to create a new project:     oc new-project &lt;project_name&gt; --description=\"&lt;description&gt;\" --display-name=\"&lt;display_name&gt;\"     where description and display-name are optional flags to set a description and custom descriptive name for your project.         Ensure you are using the project you created by selecting it as follows:     oc project &lt;new-project-name&gt;     The following message is displayed if successful:     Now using project \"&lt;new-project-name&gt;\" on server \"https://&lt;OpenShift-host&gt;:6443\".      Add the Event Streams operator to the catalog Before you can install the Event Streams operator and use it to create instances of Event Streams, you must have the IBM Operator Catalog and the IBM Common Services Catalog available in your cluster. If you have other IBM products installed in your cluster, then you already have the IBM Operator Catalog available, and you can continue to installing the Event Streams operator. Ensure you also have the IBM Common Services Catalog available, as described in the following steps. If you are installing Event Streams as the first IBM product in your cluster, complete the following steps. To make the IBM Event Streams operator and related Common Services dependencies available in the OpenShift OperatorHub catalog, create the following YAML files and apply them as  follows. To add the IBM Operator Catalog:       Create a file for the IBM Operator Catalog source with the following content, and save as IBMCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: ibm-operator-catalog   namespace: openshift-marketplacespec:   displayName: \"IBM Operator Catalog\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-operator-catalog   updateStrategy:     registryPoll:       interval: 45m        Important: If you are using OpenShift Container Platform 4.3, do not include the last 3 lines in your file:     updateStrategy:  registryPoll:    interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCatalogSource.yaml   The IBM Operator Catalog source is added to the OperatorHub catalog, making the Event Streams operator available to install. To add the IBM Common Services Catalog:       Create a file for the IBM Common Services Catalog source with the following content, and save as IBMCSCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: opencloud-operators   namespace: openshift-marketplacespec:   displayName: \"IBMCS Operators\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-common-service-catalog:latest   updateStrategy:     registryPoll:       interval: 45m        Important: If you are using OpenShift Container Platform 4.3, do not include the last 3 lines in your file:     updateStrategy:  registryPoll:    interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCSCatalogSource.yaml   The IBM Common Services Catalog source is added to the OperatorHub catalog, making the IBM Cloud Platform Common Services items available to install for Event Streams. OpenShift 4.4.8 to 4.4.12 only: manually install the Common Services operator OpenShift Container Platform version 4.4.8 introduced a regression where the default channels are not selected when a dependency is installed by the Operator Lifecycle Manager. This results in the wrong version of some operators, including IBM Cloud Platform Common Services, to be installed by default. If you are installing Event Streams on OpenShift Container Platform 4.4.8 to 4.4.12, manually install the Common Services operator on the stable-v1 channel first, or upgrade the OpenShift Container Platform to 4.4.13 before installing the Event Streams operator. For other OpenShift versions, the Event Streams operator will automatically deploy the required IBM Cloud Platform Common Services if not present. To install the Common Services operator on the stable-v1 channel when using OpenShift Container Platform 4.4.8 to 4.4.12:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select OperatorHub to open the OperatorHub dashboard.  Select the project you want to deploy the operator in.  In the All Items search box enter Common Services to locate the operator title.  Click the IBM Cloud Platform Common Services tile to open the install side panel.  Click the Install button to open the Create Operator Subscription dashboard, and select the stable-v1 channel.  Select the chosen installation mode that suits your requirements.If the installation mode is A specific namespace on the cluster, select the target namespace you created previously.  Click Subscribe to begin the installation.Install the Event Streams operator Choosing operator installation mode Before installing the Event Streams operator, decide if you want the operator to:       Manage instances of Event Streams in any namespace.     To use this option, select All namespaces on the cluster (default) later. The operator will be deployed into the system namespace openshift-operators, and will be able to manage instances of Event Streams in any namespace.         Only manage instances of Event Streams in a single namespace.     To use this option, select A specific namespace on the cluster later. The operator will be deployed into the specified namespace, and will not be able to manage instances of Event Streams in any other namespace.   Installing by using the web console To install the operator by using the OpenShift Container Platform web console, do the following:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select OperatorHub to open the OperatorHub dashboard.  Select the project you want to deploy the Event Streams instance in.  In the All Items search box enter IBM Event Streams to locate the operator title.  Click the IBM Event Streams tile to open the install side panel.  Click the Install button to open the Create Operator Subscription dashboard.  Select the chosen installation mode that suits your requirements.If the installation mode is A specific namespace on the cluster, select the target namespace you created previously.  Click Subscribe to begin the installation.The installation can take a few minutes to complete. Checking the operator status You can see the installed operator and check its status as follows:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Scroll down to the ClusterServiceVersion Overview section of the page.  Check the Status field. After the operator is successfully installed, this will change to Succeeded.In addition to the status, information about key events that occur can be viewed under the Conditions section of the same page. After a successful installation, a condition with the following message is displayed: install strategy completed with no errors. Note: If the operator is installed into a specific namespace, then it will only appear under the associated project. If the operator is installed for all namespaces, then it will appear under any selected project. If the operator is installed for all namespaces and you select all projects from the Project drop down, the operator will be shown multiple times in the resulting list, once for each project. Note: If the required IBM Cloud Platform Common Services are not installed, they will be automatically deployed when the Event Streams operator is installed and the following additional operators will appear in the installed operator list:   Operand Deployment Lifecycle Manager.  IBM Common Service Operator.Install an Event Streams instance Instances of Event Streams can be created after the Event Streams operator is installed. If the operator was installed into a specific namespace, then it can only be used to manage instances of Event Streams in that namespace. If the operator was installed for all namespaces, then it can be used to manage instances of Event Streams in any namespace, including those created after the operator was deployed. When installing an instance of Event Streams, ensure you are using a namespace that an operator is managing. Creating an image pull secret Before installing an Event Streams instance, create an image pull secret called ibm-entitlement-key in the namespace where you want to create an instance of Event Streams. The secret enables container images to be pulled from the registry.   Obtain an entitlement key from the IBM Container software library.      Create the secret in the namespace that will be used to deploy an instance of Event Streams as follows.     Name the secret ibm-entitlement-key, use cp as the username, your entitlement key as the password, and cp.icr.io as the docker server:     oc create secret docker-registry ibm-entitlement-key --docker-username=cp --docker-password=\"&lt;your-entitlement-key&gt;\" --docker-server=\"cp.icr.io\" -n &lt;target-namespace&gt;   Note: If you do not create the required secret, pods will fail to start with ImagePullBackOff errors. In this case, ensure the secret is created and allow the pod to restart. Installing an instance by using the web console To install an Event Streams instance through the OpenShift Container Platform web console, do the following:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.      Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.     Note: If the operator is not shown, it is either not installed or not available for the selected namespace.     In the Operator Details dashboard, click the Event Streams tab.  Click the Create EventStreams button to open the Create EventStreams panel. You can use this panel to define an EventStreams custom resource.From here you can install by using the form view. For more advanced configurations or to install one of the samples, see installing by using the YAML view. Installing by using the form view To configure an EventStreams custom resource, do the following:   Enter a name for the instance in the Name field.  Click the license accept toggle to set it to True.  Ensure that the correct value is selected for the Product use from the dropdown. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  You can optionally configure other components such as Kafka, ZooKeeper, Schema Registry, and Security to suit your requirements.  Scroll down and click the Create button at the bottom of the page to deploy the Event Streams instance.  Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.Installing by using the YAML view Alternatively, you can configure the EventStreams custom resource by editing YAML documents. To do this, click the Edit YAML tab. A number of sample configurations are provided on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload. Alternatively, a pre-configured YAML file containing the custom resource sample can be dragged and dropped onto this screen to apply the configuration. To view the samples, do the following:   Select the Samples tab to show the available sample configurations.  Click the Try it link under any of the samples to open the configuration in the Create EventStreams panel.More information about these samples is available in the planning section. You can base your deployment on the sample that most closely reflects your requirements and apply customizations on top as required. When modifying the sample configuration, the updated document can be exported from the Create EventStreams panel by clicking the Download button and re-imported by dragging the resulting file back into the window. Important: You must ensure that the spec.license.accept field in the custom resource YAML is set to true and that the correct value is selected for the spec.license.use field before deploying the Event Streams instance. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  Note: If experimenting with Event Streams for the first time, the Lightweight without security sample is the smallest and simplest example that can be used to create an experimental deployment. For the smallest production setup, use the Minimal production sample configuration. To deploy an Event Streams instance, use the following steps:   Complete any changes to the sample configuration in the Create EventStreams panel.  Click Create to begin the installation process.  Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.Installing an instance by using the CLI To install an instance of Event Streams from the command line, you must first prepare an EventStreams custom resource configuration in a YAML file. A number of sample configuration files have been provided on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload. More information about these samples is available in the planning section. You can base your deployment on the sample that most closely reflects your requirements and apply customizations on top as required. Important: You must ensure that the spec.license.accept field in the configuration is set to true and that the correct value is selected for the spec.license.use field before deploying the Event Streams instance. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  Note: If experimenting with Event Streams for the first time, the Lightweight without security sample is the smallest and simplest example that can be used to create an experimental deployment. For the smallest production setup, use the Minimal production sample configuration. To deploy an Event Streams instance, run the following commands:       Set the project where your EventStreams custom resource will be deployed in:     oc project &lt;project-name&gt;         Apply the configured EventStreams custom resource:     oc apply -f &lt;custom-resource-file-path&gt;     For example: oc apply -f development.yaml     Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/installing/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "10.0",
        "excerpt":"Event Streams provides samples to help you get started with deployments, as described in the planning section. Choose one of the samples suited to your requirements to get started:   Lightweight without security  Development  Minimal production  Production 3 brokers  Production 6 brokers  Production 9 brokersYou can modify the samples, save them, and apply custom configuration settings as well. See the following sections for guidance about configuring your instance of Event Streams. Note: The Production 6 brokers and Production 9 brokers samples are only available on GitHub. You can configure and apply them by using the command line or by dragging and dropping them onto the OpenShift Container Platform web console, and editing them. Checking configuration settings This page gives information about many configuration options. To see further information about specific configuration options, or to see what options are available, you can use the oc explain command. To see information about a specific field, run the following: oc explain eventstreams.&lt;path-of-field&gt; Where path-of-field is the JSON path of the field of interest. For example, if you want to see more information about configuring external listeners for Kafka you can run the following command: oc explain eventstreams.spec.strimziOverrides.kafka.listeners.external Enabling persistent storage If you want your data to be preserved in the event of a restart, configure persistent storage for Kafka, ZooKeeper, and Schema Registry in your IBM Event Streams installation. Note: Ensure you have sufficient disk space for persistent storage. These settings are specified in the YAML configuration document that defines an instance of the EventStreams custom resource and can be applied when defining a new Event Streams instance under the “IBM Event Streams” operator in the OpenShift Container Platform web console.   To enable persistent storage for Kafka, add the storage property under spec.strimziOverrides.kafka  To enable persistent storage for ZooKeeper, add the storage property under spec.strimziOverrides.zookeeper  To enable persistent storage for Schema Registry, add the storage property under spec.schemaRegistryComplete the configuration by adding additional fields to these storage properties as follows:       Specify the storage type in storage.type (for example, \"ephemeral\" or \"persistent-claim\").     Note: When using ephemeral storage, ensure you set retention limits for Kafka topics so that you do not run out of disk space.If message retention is set to long periods and the message volume is high, the storage requirements for the topics could impact the OpenShift nodes that host the Kafka pods, and cause the nodes to run out of allocated disk space, which could impact normal operation.     Specify the storage size in storage.size (for example, \"100Gi\").  Optionally, specify the storage class in storage.class (for example, \"rook-ceph-block-internal\").  Optionally, specify the retention setting for the storage if the cluster is deleted in storage.deleteClaim (for example, \"true\").An example of these configuration options: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  schemaRegistry:    # ...    storage:      type: \"persistent-claim\"      size: \"10Gi\"      class: \"cephfs\"  strimziOverrides:    kafka:      # ...      storage:        type: \"persistent-claim\"        size: \"100Gi\"        class: \"ceph-block\"    zookeeper:      # ...      storage:        type: \"persistent-claim\"        size: \"100Gi\"        class: \"ceph-block\"# ...If present, existing persistent volumes with the specified storage class are used after installation, or if a dynamic provisioner is configured for the specified storage class, new persistent volumes are created. Where optional values are not specified:   If no storage class is specified and a default storage class has been defined in the OpenShift Container Platform settings, the default storage class will be used.  If no storage class is specified and no default storage class has been defined in the OpenShift Container Platform settings, the deployment will use any persistent volume claims that have at least the set size value.  If no retention setting is provided, the storage will be retained when the cluster is deleted.The following example YAML document shows an example EventStreams custom resource with dynamically allocated storage provided using CephFS for Kafka and ZooKeeper. To try this deployment, set the required namespace and accept the license by changing the spec.license.accept value to \"true\". apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-storage  namespace: myprojectspec:  license:    accept: false  version: 10.0.0  adminApi: {}  adminUI: {}  collector: {}  restProducer: {}  schemaRegistry:    storage:      type: ephemeral  strimziOverrides:    kafka:      replicas: 1      config:        interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor        offsets.topic.replication.factor: 1        transaction.state.log.min.isr: 1        transaction.state.log.replication.factor: 1      listeners:        external:          type: route        plain: {}        tls: {}      storage:        type: persistent-claim        size: 100Gi        class: rook-ceph-block-internal        deleteClaim: true      metrics: {}    zookeeper:      replicas: 1      storage:        type: persistent-claim        size: 100Gi        class: rook-ceph-block-internal      deleteClaim: true      metrics: {}Configuring encryption between pods Pod-to-Pod encryption is enabled by default for all Event Streams pods. Unless explicitly overridden in an EventStreams custom resource, the configuration option spec.security.internalTls will be set to TLSv1.2. This value can be set to NONE which will disable Pod-to-Pod encryption. For example, the following YAML snippet disables encryption between pods: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-internal-disabled  namespace: myprojectspec:  # ...  security:    # ...    internalTls: NONE# ...Configuring UI Security By default, accessing the Event Streams UI requires an IBM Cloud Platform Common Services Identity and Access Management (IAM) user that has been assigned access to Event Streams (see managing access for details). The login requirement for the UI is disabled when all Kafka authentication and authorization is disabled. This is demonstrated by the proof-of-concept lightweight without security sample. Important: This configuration will limit UI capability due to the security requirements of other components. The following features will be disabled:   Geo-replication  Metrics panel  Producers panel  Connect to schema  Connect to cluster  Connect to topicApplying Kafka broker configuration settings Kafka supports a number of broker configuration settings, typically provided in a properties file. When creating an instance of Event Streams, these settings are defined in an EventStreams custom resource under a the spec.strimziOverrides.kafka.config property. The following example uses Kafka broker settings to configure replication for system topics: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    # ...    kafka:      # ...      config:        offsets.topic.replication.factor: 1        transaction.state.log.min.isr: 1        transaction.state.log.replication.factor: 1This custom resource can be created using the oc command or the OpenShift Container Platform web console under the IBM Event Streams operator page. You can specify all the broker configuration options supported by Kafka except from those managed directly by Event Streams. For further information, see the list of supported configuration options. After deployment, these settings can be modified by updating the EventStreams custom resource. Applying Kafka rack awareness Kafka rack awareness is configured by setting the rack property in the EventStreams custom resource using the zone label as the topology key in the spec.strimziOverrides.kafka.rack field. This key needs to match the zone label name applied to the nodes. Note: Before this is applied, ensure the Kafka cluster role for rack awareness has been applied. The following example sets the rack topologyKey to failure-domain.beta.kubernetes.io/zone: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    # ...    kafka:      # ...      rack:        topologyKey: failure-domain.beta.kubernetes.io/zone      # ...Setting geo-replication nodes You can install geo-replication in a cluster to enable messages to be automatically synchronized between local and remote topics. A cluster can be a geo-replication origin or destination. Origin clusters send messages to a remote system, while destination clusters receive messages from a remote system. A cluster can be both an origin and a destination cluster at the same time. To enable geo-replication, create an EventStreamsGeoReplicator custom resource alongside the EventStreams custom resource. This can be defined in a YAML configuration document under the IBM Event Streams operator in the OpenShift Container Platform web console. When setting up geo-replication, consider the number of geo-replication worker nodes (replicas) to deploy and configure this in the spec.replicas property. Ensure that the following properties match the name of the Event Streams instance:   metadata.name  metadata.labels[\"eventstreams.ibm.com/cluster\"]For example, to configure geo-replication with 2 replicas for an Event Streams instance called sample-three in the namespace myproject, create the following EventStreamsGeoReplicator configuration: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsGeoReplicatormetadata:  labels:    eventstreams.ibm.com/cluster: sample-three  name: sample-three  namespace: myprojectspec:  # ...  replicas: 2Note: Geo-replication can be deployed or reconfigured at any time. For more information, see Setting up geo-replication. Configuring access External access using OpenShift Container Platform routes is automatically configured for the following services if they are included in the Event Streams installation:   The Event Streams UI  The Schema Registry  The Admin API  The REST ProducerREST services access The REST services for Event Streams are configured with defaults for the container port, type, TLS version, certificates, and authentication mechanisms. If the Kafka listeners have been configured without authentication requirements then the authentication mechanisms are automatically removed from the REST endpoints. The schema for REST endpoint configuration is described in the following table, followed by an example of an endpoint configuration for the Admin API. In the example, the potential values for &lt;component&gt; in spec.&lt;component&gt;.endpoints are:   adminApi for the Admin API  restProducer for the REST Producer  schemaRegistry for the Schema Registry            Key      Type      Description                  name      String      Name to uniquely identify the endpoint among other endpoints in the list for a component.              containerPort      Integer      A unique port to open on the container that this endpoint will serve requests on. Restricted ranges are 0-1000 and 7000-7999.              type      String [internal, route]      Event Streams REST components support internal type endpoints and OpenShift Container Platform Routes.              tlsVersion      String [TLSv1.2,NONE]      Specifies the TLS version where NONE will disable HTTPS.              authenticationMechanisms      List of Strings      List of authentication mechanisms to be supported at this endpoint. By default, all authentication mechanisms: [iam-bearer,tls,scram-sha-512] are enabled. Optionally a subset or even none ([]) can be configured.              certOverrides.certificate      String      The name of the key in the provided certOverrides.secretName secret that contains the base64 encoded certificate.              certOverrides.key      String      The name of the key in the provided certOverrides.secretName secret that contains the base64 encoded key.              certOverrides.secretName      String      The name of the secret in the instance namespace that contains the encoded certificate and key to secure the endpoint with.              host      String (DNS rules apply)      An optional override for the default host that an OpenShift Container Platform route will generate.      # ...spec:  # ...  adminApi:    # ...    endpoints:      - name: example        containerPort: 9080        type: route        tlsVersion: TLSv1.2        authenticationMechanisms:          - iam-bearer          - tls          - scram-sha-512        certOverrides:            certificate: mycert.crt            key: mykey.key            secretName: custom-endpoint-cert        host: example-host.apps.example-domain.comNote: Changing an endpoint in isolation might have adverse effects if Kafka is configured to require authentication and the configured endpoint has no authentication mechanisms specified. In such cases, a warning message might be displayed in the instance status conditions. The Event Streams REST components also allow for the default set of cipher suites to be overridden. Though not a recommended practice, it is possible to enable alternative cipher suites to facilitate connectivity of legacy systems. This capability is provided through the CIPHER_SUITES environment variable as shown in this example: # ...spec:  # ...  restProducer:    # ...    env:      - name: CIPHER_SUITES        value: &gt;-          TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256Kafka access All examples provided for Event Streams include an external listener for Kafka and varying internal listener types by default. The supported external listener is of type route. This indicates the use of an OpenShift Container Platform route, and it can have either tls or scram-sha-512 configured as the authentication mechanisms. The following example snippet defines an external listener that exposes the Kafka brokers using an OpenShift Container Platform route with SCRAM-SHA-512 authentication enabled. # ...spec:  # ...  strimziOverrides:    # ...    kafka:      listeners:        external:          type: route          authentication:            type: scram-sha-512Internal listeners for Kafka can also be configured. In addition to the external listener, there are plain and tls internal listeners. Each of these can be configured to have an authentication mechanism as shown in the following example. # ...spec:  # ...  strimziOverrides:    # ...    kafka:      listeners:        plain:          authentication:            type: scram-sha-512        tls:          authentication:            type: tlsThe Kafka listener security protocols are mapped to the internal listener configurations as shown in the following table:             Security protocol      Listener configuration                  PLAINTEXT      spec.strimziOverrides.kafka.listeners.plain: {}              SSL (no-authentication)      spec.strimziOverrides.kafka.listeners.tls: {}              SSL (mutual-authentication)      spec.strimziOverrides.kafka.listeners.tls.authentication.type: tls              SASL_PLAINTEXT      spec.strimziOverrides.kafka.listeners.plain.authentication.type: scram-sha-512              SASL_SSL      spec.strimziOverrides.kafka.listeners.tls.authentication.type: scram-sha-512      Configuring external monitoring through Prometheus Metrics provide information about the health and operation of the Event Streams instance. Metrics can be enabled for Kafka, ZooKeeper, geo-replicator, and Kafka Connect pods. Note: Kafka metrics can also be exposed externally through JMX by configuring external monitoring tools. Kafka metrics can be enabled by setting spec.strimziOverrides.kafka.metrics to {} in the EventStreams custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    kafka:      # ...      metrics: {}# ...ZooKeeper metrics can be enabled by setting spec.strimziOverrides.zookeeper.metrics to {} in the EventStreams custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    zookeeper:      # ...      metrics: {}# ...Geo-replicator metrics can be enabled by setting spec.metrics to {} in the KafkaMirrorMaker2 custom resource. For example: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaMirrorMaker2# ...spec:  # ...  metrics: {}# ...Note: The Event Streams operator automatically applies a KafkaMirrorMaker2 custom resource when a EventStreamsGeoReplicator custom resource is created. Metrics can then be enabled by editing the generated KafkaMirrorMaker2 custom resource. Kafka Connect metrics can be enabled by setting spec.metrics to {} in the KafkaConnectS2I custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: KafkaConnectS2I# ...spec:  # ...  metrics: {}# ...To complement the default Kafka metrics, Event Streams can be configured to publish additional information about the Event Streams instance by setting the spec.strimziOverrides.kafka.config.interceptor.class.name to com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor, for example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    kafka:      # ...        config:          # ...          interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor# ...Note: For details about viewing metrics information, see the cluster health and topic health sections. Configuring external monitoring through JMX You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by collecting Kafka metrics. To set this up, you need to:   Have a third-party monitoring tool set up to be used within your OpenShift Container Platform cluster.  Enable access to the broker JMX port by setting spec.strimizOverrides.kafka.jmxOptions.    apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      jmxOptions: {}        Include any configuration settings for Event Streams as required by your monitoring tool. For example, Datadog’s autodiscovery requires you to annotate Kafka broker pods (strimziOverrides.kafka.template.statefulset.metadata.annotations)  Configure your monitoring applications to consume JMX metrics.Configuring the Kafka Exporter You can configure the Kafka Exporter to expose additional metrics to Prometheus on top of the default ones. For example, you can obtain the consumer group lag information for each topic. The Kafka Exporter can be configured using a regex to expose metrics for a collection of topics and consumer groups that match the expression. For example, to enable JMX metrics collection for the topic orders and the group buyers, configure the EventStreams custom resource as follows:   apiVersion: eventstreams.ibm.com/v1beta1  kind: EventStreams  # ...  spec:  # ...  strimziOverrides:    # ...    kafkaExporter:      groupRegex: orders      topicRegex: buyers      template:        pod:          metadata:            annotations:              prometheus.io/port: '9404'              prometheus.io/scheme: https              prometheus.io/scrape: 'true'For more information about configuration options, see configuring the Kafka Exporter. Configuring the JMX Exporter You can configure the JMX Exporter to expose JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes to Prometheus. To enable the collection of all JMX metrics available on the Kafka brokers and ZooKeeper nodes, configure the EventStreams custom resource as follows:   apiVersion: eventstreams.ibm.com/v1beta1  kind: EventStreams  # ...  spec:  # ...  strimziOverrides:    kafka:      metrics: {}      # ...    zookeepers:      #    # ...For more information about configuration options, see the following documentation:   Kafka and ZooKeeper JMX metrics configuration  Kafka JMX metrics configurationUsing your own certificates Event Streams offers the capability to provide your own CA certificates and private keys instead of using the ones generated by the operator. Note: You must complete the process of providing your own certificates before installing an instance of Event Streams. You must provide your own X.509 certificates and keys in PEM format with the addition of a PKCS12-formatted certificate and the CA password. If you want to use a CA which is not a Root CA, you have to include the whole chain in the certificate file. The chain should be in the following order:   The cluster or clients CA  One or more intermediate CAs  The root CAAll CAs in the chain should be configured as a CA with the X509v3 Basic Constraints. Note: In the following instructions, the CA public certificate file is denoted CA.crt and the CA private key is denoted CA.key. As Event Streams also serves the truststore in PKCS12 format, the following command can be used to generate the required .p12 file from the PEM format certificate and key: openssl pkcs12 -export -inkey CA.key -in CA.crt -out CA.p12 The cluster and/or clients certificates, and keys must be added to secrets in the namespace that the Event Streams instance is intended to be created in. The naming of the secrets and required labels must follow the conventions detailed in the following command templates. The following four commands can be used to create and label the secrets for custom certificates and keys. The templates demonstrate providing cluster certificates but the same commands can be re-used substituting cluster with clients in each secret name. For each command, provide the intended name and namespace for the Event Streams instance. oc create --namespace &lt;namespace&gt; secret generic &lt;instance-name&gt;-cluster-ca --from-file=ca.key=CA.key oc label --namespace &lt;namespace&gt; secret &lt;instance-name&gt;-cluster-ca eventstreams.ibm.com/kind=Kafka eventstreams.ibm.com/cluster=&lt;instance-name&gt; oc create --namespace &lt;namespace&gt; secret generic &lt;instance-name&gt;-cluster-ca-cert --from-file=ca.crt=CA.crt --from-file=ca.p12=CA.p12 --from-literal=ca.password='&lt;CA_PASSWORD&gt;' oc label --namespace &lt;namespace&gt; secret &lt;instance-name&gt;-cluster-ca-cert eventstreams.ibm.com/kind=Kafka eventstreams.ibm.com/cluster=&lt;instance-name&gt; To make use of the provided secrets, Event Streams will require the following overrides to be added to the custom resource. spec:  # ...  strimziOverrides:    clusterCa:      generateCertificateAuthority: false  # And/Or    clientsCa:      generateCertificateAuthority: falseIt is also possible to configure the renewalDays (default 30) and validityDays (default 365) under the spec.strimziOverrides.clusterCa and spec.strimziOverrides.clientsCa keys. Validity periods are expressed as a number of days after certificate generation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/configuring/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "10.0",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying an installation To verify that your Event Streams installation deployed successfully, you can check the status of your instance through the OpenShift Container Platform web console or command line. Check the status of the EventStreams instance through the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  The Phase field will display the current state of the EventStreams custom resource. When the Event Streams instance is ready, the phase will display Ready, meaning the deployment has completed.Check the status of the Event Streams instance through the command line After all the components of an Event Streams instance are active and ready, the EventStreams custom resource will have a Ready phase in the status.To verify the status:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the oc get command as follows: oc get eventstreamsFor example, the installation of the instance called development is complete when the STATUS returned by the oc get command displays Ready:oc get eventstreamsAn example output: $ oc get eventstreams&gt;NAME             STATUSdevelopment      ReadyNote: It might take several minutes for all the resources to be created and the EventStreams instance to become ready. Installing the Event Streams command-line interface The Event Streams CLI is a plugin for the cloudctl CLI. Use the Event Streams CLI to manage your Event Streams instance from the command line.Examples of management activities include:   Creating, deleting, and updating Kafka topics.  Creating, deleting, and updating Kafka users.  Creating, deleting, and updating Kafka message schemas.  Managing geo-replication.  Displaying the cluster configuration and credentials.To install the Event Streams CLI:   Ensure you have the IBM Cloud Pak CLI (cloudctl) installed either by retrieving the binary from your cluster or downloading the binary from a release on the GitHub project.Note: Ensure you download the correct binary for your architecture and operating system.  Log in to your Event Streams instance as an administrator.  Click Toolbox in the primary navigation.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;path-to-plugin&gt;To start the Event Streams CLI and check all available command options in the CLI, use the cloudctl es command.For an exhaustive list of commands, you can run: cloudctl es --help To get help for a specific command, run: cloudctl es &lt;command&gt; --help To use the Event Streams CLI against an OpenShift Container Platform cluster, do the following: Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt; To configure the CLI to connect to a specific Event Streams instance running a namespace: cloudctl es init -n &lt;namespace&gt; Firewall and load balancer settings Consider the following guidance about firewall and load balancer settings for your deployment. Using OpenShift Container Platform routes Event Streams uses OpenShift routes. Ensure your OpenShift router is set up as required. Connecting clients For instructions about connecting a client to your Event Streams instance, see connecting clients. Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Platform Common Services teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment. Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/post-installation/",
        "teaser":null},{
        "title": "Migrating from open-source Apache Kafka to Event Streams",
        "collection": "10.0",
        "excerpt":"If you are using open-source Apache Kafka as your event-streaming platform, you can move to IBM Event Streams and benefit from its features and enterprise-level support. Prerequisites Ensure you have an Event Streams deployment available. See the instructions for installing on OpenShift Container Platform. For many of the tasks, you can use the Kafka console tools. Many of the console tools work with Event Streams, as described in the using console tools topic. Create the required topics in Event Streams In Event Streams, create the same set of topics that have been deployed in the open-source Kafka cluster. To list these topics, run the following Kafka console tool: ./kafka-topics.sh --bootstrap-server &lt;host&gt;:&lt;port&gt; --describe For each existing topic, create a new topic in Event Streams with the same name. Ensure you use the same partition and replica settings, as well as any other non-default settings that were applied to the existing topic in your open-source Kafka instance. Change producer configuration Change the configuration for applications that produce messages to your open-source Kafka cluster to connect to Event Streams instead as described in connecting clients. If you are using the Kafka console tools, see the instructions for the example console producer in using the console tools to change where the messages are produced to. Change consumer configuration Change the configuration for applications that consume messages from your open-source Kafka cluster to connect to Event Streams instead as described in connecting clients. If you are using the Kafka console tools, see the instructions for the example console consumer in using the console tools to change where messages are consumed from. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/moving-from-oss-kafka/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "10.0",
        "excerpt":"You can remove an Event Streams instance by using the oc utility or the OpenShift Container Platform web console.You can also remove the Event Streams operator itself. Uninstalling an Event Streams instance Uninstalling using the OpenShift Container Platform web console To delete an Event Streams instance:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  In the Operator Details panel, select the Event Streams tab to show the  Event Streams instances in the selected namespace.  Click  More options next to the instance to be deleted to open the actions menu.  Click the Delete EventStreams menu option to open the confirmation panel.  Check the namespace and instance name and click Delete to shutdown the associated pods and delete the instance.Check uninstallation progress   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Workloads dropdown and select Pods to open the Pods dashboard.  Click Select All Filters to display pods in any state.  Enter the name of the Event Streams instance being deleted in the Filter by name box.  Wait for all the Event Streams pods to be displayed as Terminating and then be removed from the list.Removing persistence resources If you had enabled persistence for the Event Streams instance but set the deleteClaim storage property to false, you will need to manually remove the associated Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) that were created at installation time. The deleteClaim property is configured in the EventStreams custom resource and can be set to true during installation to ensure the PVs and PVCs are automatically removed when the instance is deleted. For Kafka and ZooKeeper, this property can be found as follows:   spec.strimziOverrides.kafka.storage.deleteClaim  spec.strimziOverrides.zookeeper.storage.deleteClaimFor other components, this property can be found as follows:   spec.&lt;component_name&gt;.storage.deleteClaimImportant: This change will cause data to be removed during an upgrade. For example, to configure automatic deletion for the Kafka storage when uninstalling: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams...spec:  ...  strimziOverrides:    ...    kafka:      ...      storage:        type: persistent-claim        ...        deleteClaim: trueTo remove any remaining storage, delete the PVCs first then delete any remaining PVs. Delete the Persistent Volume Claims (PVCs):   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Storage dropdown and select Persistent Volume Claims to open the Persistent Volume Claims page.  In the Project dropdown select the required namespace.  Click Select All Filters to display PVCs in any state.  Enter the name of the Event Streams instance in the Filter by name box.  For each PVC to be deleted, make a note of the Persistent Volume listed for that PVC and then click  More options to open the actions menu.  Click the Delete Persistent Volume Claim menu option to open the confirmation panel.  Check the PVC name and namespace, then click Delete to remove the PVC.Delete remaining Persistent Volumes (PVs):   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Storage dropdown and select Persistent Volumes to open the Persistent Volumes page.  In the Project dropdown select the required namespace.  For each PV you made a note of when deleting PVCs, click  More options to open the actions menu.  Click the Delete Persistent Volume menu option to open the confirmation panel.  Check the PV name and click Delete to remove the PV.Uninstalling using the CLI You can delete an Event Streams installation using the oc command line tool:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Ensure you are using the project where your Event Streams instance is located:oc project &lt;project_name&gt;  Run the following command to display the Event Streams instances:oc get es  Run the following command to delete your instance:oc delete es --selector app.kubernetes.io/instance=&lt;instance_name&gt;Check uninstallation progress Run the following command to check the progress:oc get pods --selector app.kubernetes.io/instance=&lt;instance_name&gt; Pods will initially display a STATUS Terminating and then be removed from the output as they are deleted. $ oc get pods --selector app.kubernetes.io/instance=minimal-prod&gt;NAME                                            READY     STATUS        RESTARTS   AGEminimal-prod-entity-operator-77dfff7c79-cnrx5   0/2       Terminating   0          5h35mminimal-prod-ibm-es-admapi-b49f976c9-xhsrv      0/1       Terminating   0          5h35mminimal-prod-ibm-es-recapi-6f6bd784fc-jvf9z     0/1       Terminating   0          5h35mminimal-prod-ibm-es-schema-6dffdb54f9-dfdpl     0/3       Terminating   0          5h35mminimal-prod-ibm-es-ui-5dd7496dbc-qks7m         0/2       Terminating   0          5h35mminimal-prod-kafka-0                            2/2       Terminating   0          5h36mminimal-prod-zookeeper-0                        0/2       Terminating   0          5h37mRemoving persistence resources If you had enabled persistence for the Event Streams instance but set the deleteClaim storage property to false, you will need to manually remove the associated Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) that were created at installation time. The deleteClaim property is configured in the EventStreams custom resource and can be set to true during installation to ensure the PVs and PVCs are automatically removed when the instance is deleted. For Kafka and ZooKeeper, this property can be found as follows:   spec.strimziOverrides.kafka.storage.deleteClaim  spec.strimziOverrides.zookeeper.storage.deleteClaimFor other components, this property can be found as follows:   spec.&lt;component_name&gt;.storage.deleteClaimImportant: This change will cause data to be removed during an upgrade. For example, to configure automatic deletion for the Kafka storage when uninstalling: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams...spec:  ...  strimziOverrides:    ...    kafka:      ...      storage:        type: persistent-claim        ...        deleteClaim: trueTo remove any remaining storage, delete the PVCs first then delete any remaining PVs. Delete the PVCs:       Run the following command to list the remaining PVCs associated with the deleted instance:oc get pvc --selector app.kubernetes.io/instance=&lt;instance_name&gt;         Run the following to delete a PVC:oc delete pvc &lt;pvc_name&gt;   Delete remaining PVs:   Run the following command to list the remaining PVs:oc get pv  Run the following command to delete any PVs that were listed in the Volume column of the deleted PVCs.oc delete pv &lt;pv_name&gt;Note: Take extreme care to select the correct PV name to ensure you do not delete storage associated with a different application instance. Uninstalling an Event Streams operator To delete an Event Streams operator:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Operators and click Installed Operators.  In the Project dropdown select the required namespace. For cluster-wide operators, select the openshift-operators project.  Click  More options next to the IBM Event Streams operator to be deleted to open the actions menu.  Click the Uninstall Operator menu option to open the confirmation panel.  Check the namespace and operator name, then click Remove to uninstall the operator.Removing Event Streams Custom Resoure Definitions The Event Streams Custom Resource Definitions (CRDs) are not deleted automatically. You must manually delete any CRDs that you do not want:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Administration and click Custom Resource Definitions.  Enter eventstreams in the Filter by name box to filter the CRDs associated with Event Streams.  Click  More options next to the CRD to be deleted to open the actions menu.  Click the Delete Custom Resource Definition menu option to open the confirmation panel.  Check the name of the CRD and click Delete to remove the CRD.Uninstalling IBM Cloud Platform Common Services For information about uninstalling IBM Cloud Platform Common Services see the IBM Cloud Platform Common Services documentation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading and migrating",
        "collection": "10.0",
        "excerpt":"Event Streams version 10.0.0 is an operator-based release. Previous Event Streams versions were helm-based releases. To upgrade to version 10.0.0 and migrate your previous Event Streams installation, contact us for help with moving to the latest operator-based release. Note: There is no option to roll back to a previous Event Streams version after you have upgraded and migrated to version 10.0.0. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/installing/upgrading/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "10.0",
        "excerpt":"Log in to your IBM Event Streams UI from a supported web browser. Determining the URL depends on your platform. Using the OpenShift Container Platform CLI Event Streams uses OpenShift routes. To retrieve the URL for your Event Streams UI, use the following command:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command:     oc get routes -n &lt;namespace&gt; -l app.kubernetes.io/name=admin-ui        The following is an example output, and you use the value from the HOST/PORT column to log in to your UI in a web browser:     NAME                        HOST/PORT                                                           PATH   SERVICES                    PORT   TERMINATION   WILDCARDmy-eventstreams-ibm-es-ui   my-eventstreams-ibm-es-ui-myproject.apps.my-cluster.my-domain.com          my-eventstreams-ibm-es-ui   3000   reencrypt     None        Enter the address in a web browser. Add https:// in front of the HOST/PORT value. For example:    https://my-eventstreams-ibm-es-ui-myproject.apps.my-cluster.my-domain.com        Use your credentials provided to you by your cluster administrator.A cluster administrator can manage access rights by following the instructions in managing access.Enter your username and password to access the Event Streams UI.Using OpenShift Container Platform UI Event Streams uses OpenShift routes. To retrieve the URL for your Event Streams UI, you can find it in the OpenShift UI:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Operators in the navigation on the left, and click Installed Operators.  Locate the operator that manages your Event Streams instance in the namespace. It is called IBM Event Streams in the NAME column.  Click the IBM Event Streams link in the row and click the Event Streams tab. This lists the Event Streams operands related to this operator.  Find your instance in the Name column and click the link for the instance.   A link to the IBM Event Streams UI is displayed under the label Admin UI. Click the link to open the IBM Event Streams UI login page in your browser tab.  Use your credentials provided to you by your cluster administrator.A cluster administrator can manage access rights by following the instructions in managing access.Enter your username and password to access the Event Streams UI.Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Platform Common Services session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/logging-in/",
        "teaser":null},{
        "title": "Creating a Kafka topic",
        "collection": "10.0",
        "excerpt":"To use Kafka topics to store events in IBM Event Streams, create and configure a Kafka topic. Using the UI   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Home in the primary navigation.  Click the Create a topic tile.      Enter a topic name in the Topic name field, for example, my-topic.This is the name of the topic that an application will be producing to or consuming from.     Click Next.         Enter the number of Partitions, for example, 1.Partitions are used for scaling and distributing topic data across the Apache Kafka brokers.For the purposes of a basic starter application, using only 1 partition is sufficient.     Click Next.         Select a Message retention,  for example,  A day.This is how long messages are retained before they are deleted.     Click Next.         Select a replication factor in Replicas,  for example, Replication factor: 1.This is how many copies of a topic will be made for high availability. For production environments, select Replication factor: 3 as a minimum.     Click Create topic. The topic is created and can be viewed from the Topics tab located in the primary navigation.Note: To view all configuration options you can set for topics, set Show all available options to On. Note: Kafka supports additional topic configuration settings. Enable Show all available options to access more detailed configuration settings if required. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init      Run the following command to create a topic:     cloudctl es topic-create --name &lt;topic-name&gt; --partitions &lt;number-of-partitions&gt; --replication-factor &lt;replication-factor&gt;     For example, to create a topic called my-topic that has 1 partition, a replication factor of 1, and 1 day set for message retention time (provided in milliseconds):     cloudctl es topic-create --name my-topic --partitions 1 --replication-factor 1 --config retention.ms=86400000     Important: Do not set &lt;replication-factor&gt; to a greater value than the number of available brokers.   Note: To view all configuration options you can set for topics, use the help option as follows: cloudctl es topic-create --help Kafka supports additional topic configuration settings. Extend the topic creation command with one or more --config &lt;property&gt;=&lt;value&gt; properties to apply additional configuration settings. The following additinal properties are currently supported:   cleanup.policy  compression.type  delete.retention.ms  file.delete.delay.ms  flush.messages  flush.ms  follower.replication.throttled.replicas  index.interval.bytes  leader.replication.throttled.replicas  max.message.bytes  message.format.version  message.timestamp.difference.max.ms  message.timestamp.type  min.cleanable.dirty.ratio  min.compaction.lag.ms  min.insync.replicas  preallocate  retention.bytes  retention.ms  segment.bytes  segment.index.bytes  segment.jitter.ms  segment.ms  unclean.leader.election.enable","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/creating-topics/",
        "teaser":null},{
        "title": "Running a starter application",
        "collection": "10.0",
        "excerpt":"To learn more about how to create applications that can take advantage of IBM Event Streams capabilities, you can use the starter application. The starter application can produce and consume messages, and you can specify the topic to send messages to and the contents of the message. About the application The starter application provides a demonstration of a Java application that uses the Vert.x Kafka Client to send events to, and receive events from, Event Streams. It also includes a user interface to easily view message propagation. The source code is provided in GitHub to allow you to understand the elements required to create your own Kafka application. Downloading the application If you do not already have the application, download the JAR file from the Event Streams UI.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Try the starter application tile, or click Toolbox in the primary navigation, go to the Starter application section, and click Get started.  Click Download JAR from GitHub.  Download the JAR file for the latest release.Generate security and configuration files Before you can run the application, generate security and configuration files to connect to your Event Streams and the target topic. Some of the following steps depend on your access permissions. If you are not permitted to generate credentials you will not see the Generate properties button and will have to obtain security and configuration files from your administrator before running the application. If you are not permitted to create topics, you will not be able to create a topic as part of this lesson and will have to use a pre-existing topic.   From the Starter application menu opened in the previous step, click Generate properties to open the side panel.      Enter an application name in the Starter application name field. This value is used by Event Streams to create a KafkaUser, which will provide your application with credentials to connect to Event Streams securely.Note: This name must be unique to avoid potential clashes with pre-existing KafkaUser resources.     Select a new or existing topic to connect to.Note: When creating a new topic the name must be unique to avoid potential clashes with pre-existing topics.  Click the Generate and download .zip button to download the compressed file, then extract the contents to your preferred location.Running the application Before running the application, ensure you have the following available:   The application JAR file.  A directory containing security and configuration filesRun the following command to start the application: java -Dproperties_path=&lt;configuration_properties_path&gt; -jar &lt;jar_path&gt;/demo-all.jarWhere:   configuration_properties_path is the path to the directory containing the extracted security and configuration files.  jar_path is the path to the downloaded application JAR file.Wait for the application to be ready. It will print out the following message: Application started in XmsWhen the application is ready, access the UI by using the following URL: http://localhost:8080. Use the start button in the UI to produce messages and see them consumed. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads",
        "collection": "10.0",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory of the event-streams-sample-producer project.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the bootstrap.servers and any required security configuration details in the configuration file. Obtaining configuration details The bootstrap servers address can be obtained from the Event Streams UI as described in the following steps. Other methods to obtain the bootstrap servers address are described in connecting client.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Click the Resources tab.  Go to the Kafka listener and credentials section.  Copy the address from one of the External listeners.The producer application might require credentials for the listener chosen in the previous step. For more information about these credentials, see the information about managing access. Obtain the required credentials as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Kafka listener and credentials section.  Click the button next to the listener chosen as the bootrap.servers configuration. If present, the button will either be labelled Generate SCRAM credentials or Generate TLS credentials.  Select Produce messages, consume messages and create topics and schemas and click Next.  Select A specific topic, enter the name of a topic to produce to and click Next.  Select All consumer groups and click Next.  Select No transactional IDs and click Generate credentials.  Retrieve the generated credentials:          If using SCRAM note down the Username and password.      If using TLS click Download certificates and extract the contents of the resulting .zip file to a preferred location.      Obtain the Event Streams certificate as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Certificates section.  In the PKCS12 certificate section click Download certificate.  Note down the generated password displayed in the Certificate password section.Updating the configuration file Before updating the file, obtain the credentials by following the steps in Obtaining configuration details. Update the producer.config file with your configuration details using the following table as guidance.             Attribute      Description                         bootstrap.servers      The bootstrap address for the chosen external listener.                     security.protocol      Set to SSL. Can be ommitted if the chosen external listener has TLS disabled.                     ssl.truststore.location      The full path and name of the Event Streams PKCS12 certificate file. Can be ommitted if the chosen external listener has TLS disabled.                     ssl.truststore.password      The password for the Event Streams PKCS12 certificate file. Can be ommitted if the chosen external listener has TLS disabled.                     sasl.mechanism      Set to SCRAM-SHA-512 if using SCRAM credentials, otherwise ommitted.                     sasl.jaas.config      Set to org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";, where &lt;username&gt; and &lt;password&gt; are replaced with the SCRAM credentials. Omitted if not using SCRAM credentials.                     ssl.keystore.location      Set to the full path and name of the user.p12 keystore file downloaded from the Event Streams UI. Ommitted if not using TLS credentials.                     ssl.keystore.password      Set to the password listed in the user.password file downloaded from the Event Streams UI. Ommitted if not using TLS credentials.             Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t my-topic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called my-topic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Specifying a load You can generate a custom message load using your own settings. For example, to test the load to the topic called my-topic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t my-topic -T 1000 -n 60000 -r 1024The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      integer      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      integer      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      boolean      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      integer      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      integer      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "10.0",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:    &lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;    &lt;version&gt;1.7.26&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;    &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;    &lt;version&gt;1.7.26&lt;/version&gt;&lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:    implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.5.0'implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.26'implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.26'        Ensure you set up security.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/client/",
        "teaser":null},{
        "title": "Connecting clients",
        "collection": "10.0",
        "excerpt":"Learn how to discover connection details to connect your clients to your Event Streams instance. Obtaining the bootstrap address Use one of the following methods to obtain the bootstrap address for your connection to your Event Streams instance, choosing the listener type appropriate for your client. More information on configuring listener types can be found in the configuring Kafka access section. Using the Event Streams UI Note: You can only use the UI to retrieve the bootstrap address if you installed your Event Streams instance with authentication enabled.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile.      Go to the Kafka listener and credentials section, and select the listener from the list.           Click the External tab for applications connecting from outside of the OpenShift Container Platform cluster.      Click the Internal tab for applications connecting from inside the OpenShift Container Platform cluster.        Note: The list reflects the listeners configured in spec.strimziOverrides.kafka.listeners. For example, you will have external listeners displayed if you have spec.strimziOverrides.kafka.listeners.external configured. If spec.strimziOverrides.kafka.listeners is empty for your instance (not configured), then no address is displayed here.   Using the Event Streams CLI Note: You can only use the Event Streams CLI to retreive the address if your Event Streams instance has an external listener configured in spec.strimziOverrides.kafka.listeners.external.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.      Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es initMake note of the Event Streams bootstrap address value. This is the Kafka bootstrap address that your application will use.     Note: If you have multiple listeners defined in spec.strimziOverrides.kafka.listeners, only the external listener is displayed. If you only have internal listeners defined, nothing is displayed.   Using the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Select the YAML tab.  Scroll down and look for status.kafkaListeners.  The kafkaListeners field will contain one or more listeners each with a bootstrapServers property.Find the type of listener you want to connect to and use the bootstrapServers value from the entry.Note: if using the external Kafka listener, the OpenShift route is a HTTPS address so the port in use is 443. Using the OpenShift Container Platform CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  To find the type and address for the Kafka bootstrap route for each listener run the following command:oc get eventstreams &lt;instance-name&gt; -o=jsonpath='{range .status.kafkaListeners[*]}{.type} {.bootstrapServers}{\"\\n\"}{end}'Where &lt;instance-name&gt; is the name of your Event Streams instance.Note: if using the external Kafka listener, the OpenShift route is a HTTPS address so the port in use is 443. Securing the connection To connect client applications to a secured IBM Event Streams, you must obtain the following:   A copy of the server-side public certificate to add to your client-side trusted certificates.  SCRAM-SHA-512 (username and password) or Mutual TLS (user certificates) Kafka credentials.Obtaining the server-side public certificate from the Event Streams UI   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  From the Certificates section, download the server certificate. If you are using a Java client, use the PKCS12 certificate, remembering to copy the truststore password presented during download. Otherwise, use the PEM certificate.Obtaining the server-side public certificate from the Event Streams CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init      Use the certificates command to download the cluster’s public certificate in the required format:cloudctl es certificates --format p12The truststore password will be displayed in the output for the command. The following example has a truststore password of mypassword:     $ cloudctl es certificates --format p12Trustore password is mypasswordCertificate successfully written to /es-cert.p12.OK        Note: You can optionally change the format to download a PEM Certificate if required.   Obtaining the server-side public certificate from the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Select the Resources tab.  To filter only secrets, deselect all resource types with the exception of Secret.  Locate and select the &lt;instance-name&gt;-cluster-ca-cert secret. Where &lt;instance-name&gt; is the name of your Event Streams instance.  In the Secret Overview panel scroll down to the Data section. Then, click the copy button to transfer the ca.p12 certificate to the clipboard. The password can be found under ca.password.Note: For a PEM certificate, click the copy button for ca.crt instead. Obtaining the server-side public certificate from the OpenShift Container Platform CLI To extract the server-side public certificate to a ca.p12 file, run the following command: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.p12 Where &lt;instance-name&gt; is the name of your Event Streams instance. To extract the password for the certificate to a ca.password file, run the following command: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.password Note: If a PEM certificate is required, run the following command to extract the certificate to a ca.crt file: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.crt Generating or Retrieving Client Credentials See the assigning access to applications section to learn how to create new application credentials or retrieve existing credentials. Configuring your SCRAM client Add the truststore certificate details and the SCRAM credentials to your Kafka client application to set up a secure connection from your application to your Event Streams instance. You can configure a Java application as follows: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;bootstrap-address&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.p12-file-location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore-password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.scram.ScramLoginModule required \"    + \"username=\\\"&lt;scram-username&gt;\\\" password=\\\"&lt;scram-password&gt;\\\";\");            Property Placeholder      Description                  &lt;bootstrap-address&gt;      Bootstrap servers address              &lt;certs.p12-file-location&gt;      Path to your truststore certificate. This must be a fully qualified path. As this is a Java application, the PKCS12 certificate is used.              &lt;truststore-password&gt;      Truststore password.              &lt;scram-username&gt;      SCRAM username.              &lt;scram-password&gt;      SCRAM password.      Configuring your Mutual TLS client Add the truststore and keystore certificate details to your Kafka client application to set up a secure connection from your application to your Event Streams instance. You can configure a Java application as follows: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;bootstrap-address&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.p12-file-location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore-password&gt;\");properties.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user.p12-file-location&gt;\");properties.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user.p12-password&gt;\");            Property Placeholder      Description                  &lt;bootstrap-address&gt;      Bootstrap servers address.              &lt;certs.p12-file-location&gt;      Path to your truststore certificate. This must be a fully qualified path. As this is a Java application, the PKCS12 certificate is used.              &lt;truststore-password&gt;      Truststore password.              &lt;user.p12-file-location&gt;      Path to user.p12 keystore file from credentials zip archive.              &lt;user.p12-password&gt;      The user.p12 keystore password found in the user.password file in the credentials zip archive.      Obtaining Java code samples from the Event Streams UI For a Java application, you can copy the connection code snippet from the Event Streams UI by doing the following:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile.  Click the Sample code tab.  Copy the snippet from the Sample connection code section into your Java Kafka client application. Uncomment the relevant sections and replace the property placeholders with the values from the relevant table for SCRAM or Mutual TLS.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/connecting/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "10.0",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      Yes                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No      cloudctl es entity-config              kafka-configs.sh --entity-type users      No      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      Yes      cloudctl es topics              kafka-topics.sh --describe      Yes      cloudctl es topic              kafka-topics.sh --create      Yes      cloudctl es topic-create              kafka-topics.sh --delete      Yes      cloudctl es topic-delete              kafka-topics.sh --alter --config      Yes      cloudctl es topic-update              kafka-topics.sh --alter --partitions      Yes      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      Yes      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Schemas overview",
        "collection": "10.0",
        "excerpt":"Apache Kafka can handle any data, but it does not validate the information in the messages. However, efficient handling of data often requires that it includes specific information in a certain format. Using schemas, you can define the structure of the data in a message, ensuring that both producers and consumers use the correct structure. Schemas help producers create data that conforms to a predefined structure, defining the fields that need to be present together with the type of each field. This definition then helps consumers parse that data and interpret it correctly. Event Streams supports schemas and includes a schema registry for using and managing schemas. It is common for all of the messages on a topic to use the same schema. The key and value of a message can each be described by a schema.  Schema registry Schemas are stored in the Event Streams schema registry. In addition to storing a versioned history of schemas, it provides an interface for retrieving them. Each Event Streams cluster has its own schema registry. Your producers and consumers validate the data against the specified schema stored in the schema registry. This is in addition to going through Kafka brokers. The schemas do not need to be transferred in the messages this way, meaning the messages are smaller than without using a schema registry.  If you are migrating to use Event Streams as your Kafka solution, and have been using a schema registry from a different provider, you can migrate to using the Event Streams schema registry. Apache Avro data format Schemas are defined using Apache Avro, an open-source data serialization technology commonly used with Apache Kafka. It provides an efficient data encoding format, either by using the compact binary format or a more verbose, but human-readable JSON format. The Event Streams schema registry uses Apache Avro data formats. When messages are sent in the Avro format, they contain the data and the unique identifier for the schema used. The identifier specifies which schema in the registry is to be used for the message. Avro has support for a wide range of data types, including primitive types (null, boolean, integer, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Learn more about how you can create schemas in Event Streams.  Serialization and deserialization A producing application uses a serializer to produce messages conforming to a specific schema. As mentioned earlier, the message contains the data in Avro format, together with the schema identifier. A consuming application then uses a deserializer to consume messages that have been serialized using the same schema. When a consumer reads a message sent in Avro format, the deserializer finds the identifier of the schema in the message, and retrieves the schema from the schema registry to deserialize the data. This process provides an efficient way of ensuring that data in messages conform to the required structure. Serializers and deserializers that automatically retrieve the schemas from the schema registry as required are provided or generated by IBM Event Streams. If you need to use schemas in an environment for which serializers or deserializers are not provided, you can use the command line or UI directly to retrieve the schemas.  Versions and compatibility Whenever you add a schema, and any subsequent versions of the same schema, Event Streams validates the format automatically and warns of any issues. You can evolve your schemas over time to accommodate changing requirements. You simply create a new version of an existing schema, and the schema registry ensures that the new version is compatible with the existing version, meaning that producers and consumers using the existing version are not broken by the new version. When you create a new version of the schema, you simply add it to the registry and version it. You can then set your producers and consumers that use the schema to start using the new version. Until they do, both producers and consumers are warned that a new version of the schema is available.  Lifecycle When a new version is used, you can deprecate the previous version. Deprecating means that producing and consuming applications still using the deprecated version are warned that a new version is available to upgrade to. When you upgrade your producers to use the new version, you can disable the older version so it can no longer be used, or you can remove it entirely from the schema registry. You can use the Event Streams UI or CLI to manage the lifecycle of schemas, including registering, versioning, and deprecating.  How to get started with schemas   Create schemas  Add schemas to schema registry  Set your Java or non-Java applications to use schemas  Manage schema lifecycle","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/overview/",
        "teaser":null},{
        "title": "Creating and adding schemas",
        "collection": "10.0",
        "excerpt":"You can create schemas in Avro format and then use the Event Streams UI or CLI to add them to the schema registry. Creating schemas Event Streams supports Apache Avro schemas. Avro schemas are written in JSON to define the format of the messages. For more information about Avro schemas, see the Avro documentation. The Event Streams schema registry imports, stores, and uses Avro schemas to serialize and deserialize Kafka messages. The schema registry supports Avro schemas using the record complex type. The record type can include multiple fields of any data type, primitive or complex. Define your Avro schema files and save them by using the .avsc or .json file extension. For example, the following Avro schema defines a Book record in the org.example namespace, and contains the Title, Author, and Format fields with different data types: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        }    ]}Adding schemas to the registry To use schemas in Kafka applications, import your schema definitions into the schema registry. Your applications can then retrieve the schemas from the registry as required. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation, and then click Add schema.  Click Upload definition and select your Avro schema file. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Optional: Edit the Schema name and Version fields.          The name of the record defined in the Avro schema file is added to the Schema name field. You can edit this field to add a different name for the schema. Changing the Schema name field does not update the Avro schema definition itself.      The value 1.0.0 is automatically added to the Version field as the initial version of the schema. You can edit this field to set a different version number for the schema.        Click Add schema. The schema is added to the list of schemas in the Event Streams schema registry.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to add a schema to the schema registry:cloudctl es schema-add --name &lt;schema-name&gt; --version &lt;schema-version&gt; --file &lt;path-to-schema-file&gt;Adding new schema versions The Event Streams schema registry can store multiple versions of the same schema. As your applications and environments evolve, your schemas need to change to accommodate the requirements. You can import, manage, and use different versions of a schema. As your schemas change, consider the options for managing their lifecycle. Note: A new version of a schema must be compatible with previous versions. This means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. For example, the following Avro schema defines a new version of the Book record, adding a PageCount field. By including a default value for this field, messages that were serialized with the previous version of this schema (which would not have a PageCount value) can still be deserialized using this version. {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },        {\"name\": \"PageCount\",  \"type\": \"int\", \"default\": 0}    ]}Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Locate your schema in the list of registered schemas and click its name. The list of versions for the schema is displayed.  Click Add new version to add a new version of the schema.  Click Upload definition and select the file that contains the new version of your schema. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Set a value in the Version field to be the version number for this iteration of the schema. For the current list of all versions, click View all versions.  Click Add schema. The schema version is added to the list of all versions for the schema.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to list all schemas in the schema registry, and find the schema name you want to add a new version to:cloudctl es schemas  Run the following command to add a new version of the schema to the registry:cloudctl es schema-add --name &lt;schema-name-from-previous-step&gt; --version &lt;new-schema-version&gt; --file &lt;path-to-new-schema-file&gt;","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/creating/",
        "teaser":null},{
        "title": "Managing schema lifecycle",
        "collection": "10.0",
        "excerpt":"Multiple versions of each schema can be stored in the Event Streams schema registry. Kafka producers and consumers retrieve the right schema version they use from the registry based on a unique identifier and version. When a new schema version is added, you can set both the producer and consumer applications to use that version. You then have the following options to handle earlier versions. The lifecycle is as follows:   Add schema  Add new schema version  Deprecate version or entire schema  Disable version or entire schema  Remove version or entire schemaDeprecating If you want your applications to use a new version of a schema, you can set the earlier version to Deprecated. When a version is deprecated, the applications using that version receive a message to warn them to stop using it. Applications can continue to use the old version of the schema, but warnings will be written to application logs about the schema version being deprecated. You can customize the message to be provided in the logs, such as providing information for what schema or version to use instead. Deprecated versions are still available in the registry and can be used again. Note: You can deprecate an entire schema, not just the versions of that schema. If the entire schema is set to deprecated, then all of its versions are reported as deprecated (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to deprecate from the list.  Set the entire schema or a selected version of the schema to be deprecated:          If you want to deprecate the entire schema and all its versions, click the Manage schema tab, and set Mark schema as deprecated to on.      To deprecate a specific version, select it from the list, and click the Manage version tab for that version. Then set Mark schema as deprecated to on.      Deprecated schemas and versions are marked with a Deprecated flag on the UI. You can re-activate a schema or its version by setting Mark schema as deprecated to off. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to deprecate a schema version:cloudctl es schema-modify --deprecate --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To deprecate an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     To re-activate a schema version:cloudctl es schema-modify --activate --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To re-activate an entire schema, do not specify the --version &lt;schema-version-id&gt; option.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Disabling If you want your applications to stop using a specific schema, you can set the schema version to Disabled. If you disable a version, applications will be prevented from producing and consuming messages using it. After being disabled, a schema can be enabled again to allow applications to use the schema. When a schema is disabled, applications that want to use the schema receive an error response. Java producers using the Event Streams schema registry’s serdes library will throw a SchemaDisabledException when attempting to produce messages using a disabled schema version. For example, the message and stack trace for a disabled schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaDisabledException: Schema \"Test_Schema\" version \"1.0.0\" is disabled.    at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:174)    at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:41)    at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:884)    at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:846)    at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:733)    at Producer.main(Producer.java:92)Note: You can disable a entire schema, not just the versions of that schema. If the entire schema is disabled, then all of its versions are disabled as well, which means no version of the schema can be used by applications (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to disable from the list.  Set the entire schema or a selected version of the schema to be disabled:          If you want to disable the entire schema and all its versions, click the Manage schema tab, and click Disable schema, then click Disable.      To disable a specific version, select it from the list, and click the Manage version tab for that version. Then click Disable version, then click Disable.You can re-enable a schema by clicking Enable schema, and re-enable a schema version by clicking  Re-enable version.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to disable a schema version:cloudctl es schema-modify --disable --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To disable an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     To re-enable a schema version:cloudctl es schema-modify --enable --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To re-enable an entire schema, do not specify the --version &lt;schema-version-id&gt; option.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Removing If a schema version has not been used for a period of time, you can remove it from the schema registry. Removing a schema version means it will be permanently deleted from the schema registry of your Event Streams instance, and applications will be prevented from producing and consuming messages using it. If a schema is no longer available in the registry, Java applications that want to use the schema receive a SchemaNotFoundException message. For example, the message and stack trace when producing a message with a missing schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaNotFoundException: Schema \"Test_Schema\" not found    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.handleErrorResponse(SchemaRegistryRestAPIClient.java:145)    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.get(SchemaRegistryRestAPIClient.java:120)    at com.ibm.eventstreams.serdes.SchemaRegistry.downloadSchema(SchemaRegistry.java:253)    at com.ibm.eventstreams.serdes.SchemaRegistry.getSchema(SchemaRegistry.java:239)Important: You cannot reverse the removal of a schema. This action is permanent. Note: You can remove an entire schema, including all of its versions. If the entire schema is removed, then all of its versions are permanently deleted from the schema registry of your Event Streams instance. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to remove from the list.  Remove the entire schema or a selected version of the schema:          If you want to remove the entire schema and all its versions, click the Manage schema tab, and click Remove schema, then click Remove.      To remove a specific version, select it from the list, and click the Manage version tab for that version. Then click Remove version, then click Remove.        Important: This action is permanent and cannot be reversed.   Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to remove a schema version:cloudctl es schema-remove --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To remove an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     Important: This action is permanent and cannot be reversed.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/manage-lifecycle/",
        "teaser":null},{
        "title": "Setting Java applications to use schemas",
        "collection": "10.0",
        "excerpt":"If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas. Note: If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas. Preparing the setup To use schemas stored in the Event Streams schema registry, your client applications need to be able to serialize and deserialize messages based on schemas.   Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.  Consuming applications then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.The Event Streams UI provides help with setting up your Java applications to use schemas. To set up your Java applications to use the Event Streams schemas and schema registry, prepare the connection for your application as follows:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Ensure you have added schemas to the registry.  Click Schema registry in the primary navigation.  Select a schema from the list and click the row for the schema.  Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.      Set the preferences for your connection in the Configure the schema connection section. Use the defaults or change them by clicking Change configuration.                   For producers, set the method for Message encoding.                   Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.          JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.                            For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.                   Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.          Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and will allow a Kafka consumer to continue to process further messages.                            For both producers and consumers, set in the Use generated or generic code section whether your schema is to use custom Java classes that are generated based on the schema, or generic code by using the Apache Avro API.                   Use schema-specific code (default): Your application will use custom Java classes that are generated based on this schema, using get and set methods to create and access objects. When you want to use a different schema, you will need to update your code to use a new set of specific schema Java classes.          Use generic Apache Avro schema code: Your application will create and access objects using the generic Apache Avro API. Producers and consumers that use the generic serializer and deserializer can be coded to produce or consume messages using any schema uploaded to this schema registry.                      If any configuration was changed, click Save.  Click Generate credentials to generate SCRAM or Mutual TLS credentials and follow the instructions.Important: Ensure you make note of the information provided upon completion as you will need this later.Alternatively you can generate credentials later by using the Event Streams UI or CLI.  Click Generate connection details.  Click Download certificate to download the cluster PKCS12 certificate. This is the Java truststore file which contains the server certificate. Take a copy of the Certificate password for use with the certificate in your application.  If your project uses Maven, select the Use Maven tab. Follow the instructions to copy the configuration snippets using either SCRAM or Mutual TLS for the Event Streams Maven repository to your project Maven settings.xml and POM file, and run the Maven install command to download and install project dependencies.Alternatively if your project does not use Maven, select the Use JARs tab to click Java dependencies and Schema JAR to download the java dependencies and schema JAR files to use for your application in its code.  Depending on your application, click the Producer or Consumer tab, and copy the sample Java code snippets displayed. The sample code snippets include the settings you configured to set up your applications to use the schema.  Add the required snippets into your application code as described in the following sections.          Setting up producers to use schemas      Setting up consumers to use schemas      Setting up producers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files if not using Maven, and copying code snippets for a producing application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your producer Kafka application.  Use the code snippets you copied from the UI for a producer and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets up the application to use the Event Streams schema registry’s serdes library and any generated schema-specific classes, for example: import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import com.ibm.eventstreams.serdes.SchemaInfo;import com.ibm.eventstreams.serdes.SchemaRegistry;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;The code snippet from the Connection properties section specifies connection and access permission details for your Event Streams cluster, for example: Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the SchemaRegistry and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//Schema Registry connectionprops.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://my-schema-route.my-cluster.com\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);//Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Follow the instructions in the code snippet to uncomment lines. Replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, and &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address). For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Producer code section defines properties for the producer application that set it to use the schema registry and the correct schema, for example: // Set the value serializer for produced messages to use the Event Streams serializerprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"com.ibm.eventstreams.serdes.EventStreamsSerializer\");// Set the encoding type used by the message serializerprops.put(SchemaRegistryConfig.PROPERTY_ENCODING_TYPE, SchemaRegistryConfig.ENCODING_BINARY);// Get a new connection to the Schema RegistrySchemaRegistry schemaRegistry = new SchemaRegistry(props);// Get the schema from the registrySchemaInfo schema = schemaRegistry.getSchema(\"ABC_Assets_Schema\", \"1.0.0\");// Get a new specific KafkaProducerKafkaProducer&lt;String, ABC_Assets_Schema&gt; producer = new KafkaProducer&lt;&gt;(props);// Get a new specific record based on the schemaABC_Assets_Schema specificRecord = new ABC_Assets_Schema();// Add fields and values to the specific record, for example:// specificRecord.setTitle(\"this is the value for a title field\");// Prepare the record, adding the Schema Registry headersProducerRecord&lt;String, ABC_Assets_Schema&gt; producerRecord =    new ProducerRecord&lt;String, ABC_Assets_Schema&gt;(\"&lt;my_topic&gt;\", specificRecord);producerRecord.headers().add(SchemaRegistryConfig.HEADER_MSG_SCHEMA_ID,    schema.getIdAsBytes());producerRecord.headers().add(SchemaRegistryConfig.HEADER_MSG_SCHEMA_VERSION,    schema.getVersionAsBytes());// Send the record to Kafkaproducer.send(producerRecord);// Close the producerproducer.close();The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsSerializer, telling Kafka to use the Event Streams serializer for message values when producing messages. You can also use the Event Streams serializer for message keys. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to set the field values in your message.   Specific Java classes that are generated from the schema definition will have set&lt;field-name&gt; methods that can be used to easily set the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named setAuthor which takes a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the put method in the GenericRecord class to set field names and values.Note: Replace &lt;my_topic&gt; with the name of the topic to produce messages to. Setting up consumers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files if not using Maven, and copying code snippets for a consuming application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your consumer Kafka application.  Use the code snippets you copied from the UI for a consumer and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets up the application to use the Event Streams schema registry’s serdes library and any generated schema-specific classes, for example: // Import the specific schema classimport java.time.Duration;import java.util.Arrays;import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.KafkaConsumer;The code snippet from the Connection properties section specifies connection and access permission details for your Event Streams cluster, for example: Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the SchemaRegistry and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//Schema Registry connectionprops.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://my-schema-route.my-cluster.com\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);//Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Follow the instructions in the code snippet to uncomment lines. Replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, and &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address). For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Consumer code section defines properties for the consumer application that set it to use the schema registry and the correct schema, for example: // Set the value deserializer for consumed messages to use the Event Streams deserializerprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"com.ibm.eventstreams.serdes.EventStreamsDeserializer\");// Set the behavior of the deserializer when a record cannot be deserializedprops.put(SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE, SchemaRegistryConfig.BEHAVIOR_STRICT);// Set the consumer group ID in the propertiesprops.put(\"group.id\", \"&lt;my_consumer_group&gt;\");KafkaConsumer&lt;String, ABC_Assets_Schema&gt; consumer = new KafkaConsumer&lt;&gt;(props);// Subscribe to the topicconsumer.subscribe(Arrays.asList(\"&lt;my_topic&gt;\"));// Poll the topic to retrieve recordswhile(true) {    ConsumerRecords&lt;String, ABC_Assets_Schema&gt; records = consumer.poll(Duration.ofSeconds(5));    for (ConsumerRecord&lt;String, ABC_Assets_Schema&gt; record : records) {        ABC_Assets_Schema specificRecord = record.value();        // Get fields and values from the specific record, for example:        // String titleValue = specificRecord.getTitle().toString();    }}The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsDeserializer, telling Kafka to use the Event Streams deserializer for message values when consuming messages. You can also use the Event Streams deserializer for message keys. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to read the field values from your message.   Specific Java classes that are generated from the schema definition will have get&lt;field-name&gt; methods that can be used to easily retrieve the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named getAuthor which returns a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the get method in the GenericRecord class to set field names and values.Note: Replace &lt;my_consumer_group&gt; with the name of the consumer group to use and &lt;my_topic&gt; with the name of the topic to consume messages from. Setting up Kafka Streams applications Kafka Streams applications can also use the Event Streams schema registry’s serdes library to serialize and deserialize messages. For example: // Set the Event Streams serdes properties, including the override option to set the schema// and version used for serializing produced messages.Map&lt;String, Object&gt; serdesProps = new HashMap&lt;String, Object&gt;();serdesProps.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://my-schema-route.my-cluster.com\");serdesProps.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE, \"ABC_Assets_Schema\");serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE, \"1.0.0\");// TLS PropertiesserdesProps.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");// SCRAM authentication properties - uncomment to connect using Scram//serdesProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//serdesProps.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//serdesProps.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//serdesProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//serdesProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//serdesProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");// Set up the Kafka StreamsBuilderStreamsBuilder builder = new StreamsBuilder();// Configure a Kafka Serde instance to use the Event Streams schema registry// serializer and deserializer for message valuesSerde&lt;IndexedRecord&gt; valueSerde = new EventStreamsSerdes();valueSerde.configure(serdesProps, false);// Get the stream of messages from the source topic, deserializing each message value with the// Event Streams deserializer, using the schema and version specified in the message headers.builder.stream(\"&lt;my_source_topic&gt;\", Consumed.with(Serdes.String(), valueSerde))    // Get the 'nextcount' int field from the record.    // The Event Streams deserializer constructs instances of the generated schema-specific    // ABC_Assets_Schema_Count class based on the values in the message headers.    .mapValues(new ValueMapper&lt;IndexedRecord, Integer&gt;() {        @Override        public Integer apply(IndexedRecord val) {            return ((ABC_Assets_Schema_Count) val).getNextcount();        }    })    // Get all the records    .selectKey((k, v) -&gt; 0).groupByKey()    // Sum the values    .reduce(new Reducer&lt;Integer&gt;() {        @Override        public Integer apply(Integer arg0, Integer arg1) {            return arg0 + arg1;        }    })    .toStream()    // Map the summed value to a field in the schema-specific generated ABC_Assets_Schema class    .mapValues(        new ValueMapper&lt;Integer, IndexedRecord&gt;() {            @Override            public IndexedRecord apply(Integer val) {                ABC_Assets_Schema record = new ABC_Assets_Schema();                record.setSum(val);                return record;            }     })     // Finally, put the result to the destination topic, serializing the message value     // with the Event Streams serializer, using the overridden schema and version from the     // configuration.    .to(\"&lt;my_destination_topic&gt;\", Produced.with(Serdes.Integer(), valueSerde));// Create and start the streamfinal KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig);streams.start();In this example, the Kafka StreamsBuilder is configured to use the com.ibm.eventstreams.serdes.EventStreamsSerdes class, telling Kafka to use the Event Streams deserializer for message values when consuming messages and the Event Streams serializer for message values when producing messages. Note: The Kafka Streams org.apache.kafka.streams.kstream API does not provide access to message headers, so to produce messages with the Event Streams schema registry headers, use the SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE and SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE configuration properties. Setting these configuration properties will mean produced messages are serialized using the provided schema version and the Event Streams schema registry message headers will be set. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: To re-use this example, replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, and &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address). For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier, and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/setting-java-apps/",
        "teaser":null},{
        "title": "Setting non-Java applications to use schemas",
        "collection": "10.0",
        "excerpt":"If you have producer or consumer applications created in languages other than Java, use the following guidance to set them up to use schemas. You can also use the REST producer API to send messages that are encoded with a schema. For a producer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Use an Apache Avro library for your programming language to read the schema definition from the local file and encode a Kafka message with it.  Set the schema registry headers in the Kafka message, so that consumer applications can understand which schema and version was used to encode the message, and which encoding format was used.  Send the message to Kafka.For a consumer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Consume a message from Kafka.  Check the headers for the Kafka message to ensure they match the expected schema ID and schema version ID.  Use the Apache Avro library for your programming language to read the schema definition from the local file and decode the Kafka message with it.Retrieving the schema definition from the schema registry Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation and find your schema in the list.  Copy the schema definition into a new local file.          For the latest version of the schema, expand the row. Copy and paste the schema definition into a new local file.      For a different version of the schema, click on the row and then select the version to use from the list of schema versions. Click the Schema definition tab and then copy and paste the schema definition into a new local file.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster: cloudctl es init  Run the following command to list all the schemas in the schema registry: cloudctl es schemas  Select your schema from the list and run the following command to list all the versions of the schema: cloudctl es schema &lt;schema-name&gt;  Select your version of the schema from the list and run the following command to retrieve the schema definition for the version and copy it into a new local file: cloudctl es schema &lt;schema-name&gt; --version &lt;schema-version-id&gt; &gt; &lt;schema-definition-file&gt;.avscNote: &lt;schema-version-id&gt;is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Setting headers in the messages you send to Event Streams Kafka Set the following headers in the message to enable applications that use the Event Streams serdes Java library to consume and deserialize the messages automatically. Setting these headers also enables the Event Streams UI to display additional details about the message. The required message header keys and values are listed in the following table.             Header name      Header key      Header value                  Schema ID      com.ibm.eventstreams.schemaregistry.schema.id      The schema ID as a string.              Schema version ID      com.ibm.eventstreams.schemaregistry.schema.version      The schema version ID as a string.              Message encoding      com.ibm.eventstreams.schemaregistry.encoding      Either JSON for Avro JSON encoding, or BINARY for Avro binary encoding.      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/setting-nonjava-apps/",
        "teaser":null},{
        "title": "Migrating existing applications to the Event Streams schema registry",
        "collection": "10.0",
        "excerpt":"If you are using the Confluent Platform schema registry, Event Streams provides a migration path for moving your Kafka consumers and producers over to use the Event Streams schema registry. Migrating schemas to Event Streams schema registry To migrate schemas, you can use schema auto-registration in your Kafka producer, or you can manually migrate schemas by downloading the schema definitions from the Confluent Platform schema registry and adding them to the Event Streams schema registry. Migrating schemas with auto-registration When using auto-registration, the schema will be automatically uploaded to the Event Streams schema registry, and named with the subject ID (which is based on the subject name strategy in use) and a random suffix. Auto-registration is enabled by default in the Confluent Platform schema registry client library. To disable it, set the auto.register.schemas property to false. Note: To auto-register schemas in the Event Streams schema registry, you need credentials that have operator role permissions (or higher) and permission to create schemas. You can generate credentials by using the Event Streams UI or CLI. Migrating schemas manually To manually migrate the schemas, download the schema definitions from the Confluent Platform schema registry, and add them to the Event Streams Schema Registry. When manually adding schemas to the Event Streams Schema Registry, the provided schema name must match the subject ID used by the Confluent Platform schema registry subject name strategy. If you are using the default TopicNameStrategy, the schema name must be &lt;TOPIC_NAME&gt;-&lt;'value'|'key'&gt; If you are using the RecordNameStrategy, the schema name must be &lt;SCHEMA_DEFINITION_NAMESPACE&gt;.&lt;SCHEMA_DEFINITION_NAME&gt; For example, if you are using the default TopicNameStrategy as your subject name strategy, and you are serializing your data into the message value and producing to the MyTopic topic, then the schema name you must provide when adding the schema in the UI must be MyTopic-value For example, if you are using the RecordNameStrategy as your subject name strategy, and the schema definition file begins with the following, then the schema name you must provide when adding the schema in the UI must be org.example.Book: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [...If you are using the CLI, run the following command when adding the schema: cloudctl es schema-add --create --name org.example.Book --version 1.0.0 --file /path/to/Book.avsc Migrating a Kafka producer application To migrate a Kafka producer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your producer application to secure the connection between the producer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the following command:  cloudctl es init      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");            Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:\\     export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\       -Djavax.net.ssl.trustStorePassword=password\"      Migrating a Kafka consumer application To migrate a Kafka consumer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your consumer application to secure the connection between the consumer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the following command:  cloudctl es init      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");            Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:\\     export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\        -Djavax.net.ssl.trustStorePassword=password\"      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/migrating/",
        "teaser":null},{
        "title": "Using schemas with the REST producer API",
        "collection": "10.0",
        "excerpt":"You can use schemas when producing messages with the Event Streams REST producer API. You simply add the following parameters to the API call:   schemaname: The name of the schema you want to use when producing messages.  schemaversion: The schema version you want to use when producing messages.For example, to use cURL to produce messages to a topic with the producer API, and specify the schema to be used, run the curl command as follows: Using SCRAM Basic Authentication: curl -H \"Authorization: &lt;basic-auth&gt;\" -H \"Accept: application/json\" -H \"Content-Type: text/plain\" -d '&lt;avro-encoded-message&gt;' --cacert es-cert.pem \"https://&lt;producer-endpoint&gt;/topics/&lt;my-topic&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" Using TLS Mutual Authentication: curl -H \"Accept: application/json\" -H \"Content-Type: text/plain\" -d '&lt;avro-encoded-message&gt;' --cacert es-cert.pem --key user.key --cert user.crt \"https://&lt;producer-endpoint&gt;/topics/&lt;my-topic&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" Note: Replace the values in brackets as follows:   &lt;basic-auth&gt; with the SCRAM Basic Authentication token which is generated using the Event Streams UI.  &lt;producer-endpoint&gt; with the producer URL which is available from Connect to this cluster or Connect to this topic and copy the URL in the Producer endpoint and credentials section.  &lt;my_topic&gt; with your topic name.  &lt;schema-name&gt; with the name of your schema.  &lt;schema-version-name&gt; with the version of your schema.  &lt;avro-encoded-message&gt; with your avro encoded message.The es-cert.pem certificate is downloaded by running the following command:cloudctl es certificates --format pem The user.key and user.crt files are downloaded in a .zip file by clicking Generate credentials in the Producer endpoint and credentials section of Connect to this cluster or Connect to this topic, selecting Mutual TLS certificate, following the instructions in the wizard and clicking Download certificates. By adding these parameters to the API call, a lookup is done on the specified schema and its version to check if it is valid. If valid, the correct message headers are set for the produced message. Important: When using the producer API, the lookup does not validate the data in the request to see if it matches the schema. Ensure the message conforms to the schema, and that it has been encoded in the Apache Avro binary or JSON encoding format. If the message does not conform and is not encoded with either of those formats, consumers will not be able to deserialize the data. If the message has been encoded in the Apache Avro binary format, ensure the HTTP Content-Type header is set to application/octet-stream. If the message has been encoded in the Apache Avro JSON format, ensure the HTTP Content-Type header is set to application/json. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/using-with-rest-producer/",
        "teaser":null},{
        "title": "Using the schema API",
        "collection": "10.0",
        "excerpt":"Event Streams provides a Java library to enable Kafka applications to serialize and deserialize messages using schemas stored in your Event Streams schema registry. Using the schema registry serdes library API, schema versions are automatically downloaded from your schema registry, checked to see if they are in a disabled or deprecated state, and cached. The schemas are used to serialize messages produced to Kafka and deserialize messages consumed from Kafka. Schemas downloaded by the schema registry serdes library API are cached in memory with a 10 minute expiration period. This means that if a schema is deprecated or disabled, it might take 10 minutes before consuming or producing applications will see the change. To change the expiration period, set the SchemaRegistryConfig.PROPERTY_SCHEMA_CACHE_REFRESH_RATE configuration property to a new milliseconds value. For more details, including code snippets that use the schema registry serdes API, see setting Java applications to use schemas. For full details of the Event Streams schema registry serdes API, see the Schema API Javadoc. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/schemas/schema-api/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "10.0",
        "excerpt":"You can secure your IBM Event Streams resources by managing the access each user and application has to each resource. An Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers. The bootstrap address is used for the initial connection to the cluster. The address will resolve to one of the brokers in the cluster and respond with metadata describing all the relevant connection information for the remaining brokers. Each Kafka listener providing a connection to Event Streams can be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. Additionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener. Note: Schemas in the Event Streams schema registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. Accessing the Event Streams UI and CLI When Kafka authentication is enabled, the Event Streams UI requires you to log in by using an IBM Cloud Platform Common Services Identity and Access Management (IAM) user. Access to the Event Streams CLI always requires a user to be supplied to the cloudctl login command, so the CLI is not affected by enabling Kafka authentication. The default cluster administrator (admin) IAM user credentials are stored in a secret within the ibm-common-services namespace. To retrieve the username and password:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Extract and decode the current IBM Cloud Platform Common Services admin username:oc --namespace ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_username}' | base64 --decode  Extract and decode the current IBM Cloud Platform Common Services admin password:oc --namespace ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 --decodeNote: The password is auto-generated in accordance with the default password rules for IBM Cloud Platform Common Services. To change the username or password of the admin user, see the instructions about changing the cluster administrator access credentials. The admin user has the Cluster Administrator role granting full access to all resources within the cluster, including Event Streams. Adding additional groups and users Access for groups and users is managed through IAM teams. If you have not previously created any teams, the admin user credentials can be used to set up a team. Managing access to the UI and CLI Access to the Event Streams UI and CLI requires an IAM user with a role of Cluster Administrator or Administrator. The role can be set for the user or for the group the user is part of. Any groups or users added to an IAM team with the Cluster Administrator role can log in to the Event Streams UI and CLI. Any groups or users with the Adminstrator role will not be able to log in until the namespace that contains the Event Streams cluster is added as a resource for the IAM team. If Kafka authorization is enabled by setting spec.strimziOverrides.kafka.authorization to type: runas, operations by IAM users are are automatically mapped to a Kafka principal with authorization for the required Kafka resources. Managing access to Kafka resources Each Kafka listener exposing an authenticated connection to Event Streams requires credentials to be presented when connecting. Credentials are created by using a KafkaUser custom resource, where the spec.authentication.type field has a value that matches the Kafka listener authentication type. You can create a KafkaUser by using the Event Streams UI or CLI. It is also possible to create a KafkaUser by using the OpenShift Container Platform UI or CLI, or the underlying Kubernetes API by applying a KafkaUser operand request. To assist in generating compatible KafkaUser credentials, the Event Streams UI indicates which authentication mechanism is being configured for each Kafka listener. Warning: Do not use or modify the internal Event Streams KafkaUsers named &lt;cluster&gt;-ibm-es-kafka-user and &lt;cluster&gt;-ibm-es-georep-source-user. These are reserved to be used internally within the Event Streams instance. Creating a KafkaUser in the IBM Event Streams UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile to view the Cluster connection panel.  Go to the Kafka listener and Credentials section.  To generate credentials for external clients, click External, or to generate credentials for internal clients, click Internal.  Click the Generate SCRAM credential or Generate TLS credential button next to the required listener to view the credential generation dialog.  Follow the instructions to generate credentials with desired permissions.Note: If your cluster does not have authorization enabled, the permission choices will not have any effect.The generated credential appears after the listener bootstrap address:   For SCRAM credentials, two tabs are displayed: Username and password and Basic Authentication. The SCRAM username and password combination is used by Kafka applications, while the Basic Authentication credential is for use as an HTTP authorization header.  For TLS credentials, a download button is displayed, providing a zip archive with the required certificates and keys.A KafkaUser will be created with the entered credential name. The cluster truststore is not part of the above credentials zip archive. This certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. Creating a KafkaUser in the IBM Event Streams CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Initialize the Event Streams plugin specifying your namespace and choose your instance from the numbered list:  cloudctl es init -n &lt;namespace&gt;  Use the kafka-user-create command to create a KafkaUser with the accompanying permissions.Note: If your cluster does not have authorization enabled, the permission choices will not have any effect.For example, to create SCRAM credentials with authorization to create topics &amp; schemas and produce (including transactions) &amp; consume from every topic: cloudctl es kafka-user-create \\  --name my-user \\  --consumer \\  --producer \\  --schema-topic-create \\  --all-topics \\  --all-groups \\  --all-txnids \\  --auth-type scram-sha-512For information about all options provided by the command, use the --help flag:cloudctl es kafka-user-create --help UI and CLI KafkaUser authorization KafkaUsers created by using the UI and CLI can only be configured with permissions for the most common operations against Kafka resources. You can later modify the created KafkaUser to add additional ACL rules. Creating a KafkaUser in the  UI Navigate to the IBM Event Streams installed operator menu and select the KafkaUser tab. Click Create KafkaUser. The YAML view contains sample KafkaUser definitions to consume, produce, or modify every resource. Retrieving credentials When a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type. You can retrieve the credentials later in the OpenShift Container Platform by using the name of the KafkaUser. For example, to retrieve the credentials by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Use the following command retrieve the required KafkaUser data, adding the KafkaUser name and your chosen namespace:     oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'   The command provides the following output:          The principal username      The name of the Kubernetes Secret, which includes the namespace, containing the SCRAM password or the TLS certificates.        Decode the credentials.For SCRAM, use the secret-name from step 2 to get the password and decode it:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.password}' | base64 --decodeFor TLS, get the credentials, decode them, and write each certificates and keys to files:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.ca\\.crt}' | base64 --decode &gt; ca.crtoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.crt}' | base64 --decode &gt; user.crtoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.key}' | base64 --decode &gt; user.keyoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.p12}' | base64 --decode &gt; user.p12oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.password}' | base64 --decodeThe cluster truststore certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. Similarly, these KafkaUser and Secret resources can be inspected by using the OpenShift Container Platform web console. Warning: Do not use or modify the internal Event Streams KafkaUsers named &lt;cluster&gt;-ibm-es-kafka-user and &lt;cluster&gt;-ibm-es-georep-source-user. These are reserved to be used internally within the Event Streams instance. Authorization What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules:   Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (transactionalId): you can control the ability to use the transaction capability in Kafka.  Cluster (cluster): you can control operations that affect the whole cluster, including idempotent writes.Note: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. What are the available Kafka operations? Access control in Apache Kafka is defined in terms of operations and resources. Operations are actions performed on a resource, and each operation maps to one or more APIs or requests.             Resource type      Operation      Kafka API                  topic      Alter      CreatePartitions                     AlterConfigs      AlterConfigs                            IncrementalAlterConfigs                     Create      CreateTopics                            Metadata                     Delete      DeleteRecords                            DeleteTopics                     Describe      ListOffsets                            Metadata                            OffsetFetch                            OffsetForLeaderEpoch                     DescribeConfigs      DescribeConfigs                     Read      Fetch                            OffsetCommit                            TxnOffsetCommit                     Write      AddPartitionsToTxn                            Produce                     All      All topic APIs                                           group      Delete      DeleteGroups                     Describe      DescribeGroup                            FindCoordinator                            ListGroups                     Read      AddOffsetsToTxn                            Heartbeat                            JoinGroup                            LeaveGroup                            OffsetCommit                            OffsetFetch                            SyncGroup                            TxnOffsetCommit                     All      All group APIs                                           transactionalId      Describe      FindCoordinator                     Write      AddOffsetsToTxn                            AddPartitionsToTxn                            EndTxn                            InitProducerId                            Produce                            TxnOffsetCommit                     All      All txnid APIs                                           cluster      Alter      AlterReplicaLogDirs                            CreateAcls                            DeleteAcls                     AlterConfigs      AlterConfigs                            IncrementalAlterConfigs                     ClusterAction      ControlledShutdown                            ElectPreferredLeaders                            Fetch                            LeaderAndISR                            OffsetForLeaderEpoch                            StopReplica                            UpdateMetadata                            WriteTxnMarkers                     Create      CreateTopics                            Metadata                     Describe      DescribeAcls                            DescribeLogDirs                            ListGroups                            ListPartitionReassignments                     DescribeConfigs      DescribeConfigs                     IdempotentWrite      InitProducerId                            Produce                     All      All cluster APIs      Implicitly-derived operations Certain operations provide additional implicit operation access to applications. When granted Read, Write, or Delete, applications implicitly derive the Describe operation.When granted AlterConfigs, applications implicitly derive the DescribeConfigs operation. For example, to Produce to a topic, the Write operation for the topic resource is required, which will implicitly derive the Describe operation required to get the topic metadata. Access Control List (ACL) rules Access to resources is assigned to applications through an Access Control List (ACL), which consists of rules. An ACL rule includes an operation on a resource together with the additional fields listed in the following tables. A KafkaUser custom resource contains the binding of an ACL to a principal, which is an entity that can be authenticated by the Event Streams instance. An ACL rule adheres to the following schema:             Property      Type      Description                  host      string      The host from which the action described in the ACL rule is allowed.              operation      string      The operation which will be allowed on the chosen resource.              resource      object      Indicates the resource for which the ACL rule applies.      The resource objects used in ACL rules adhere to the following schema:             Property      Type      Description                  type      string      Can be one of cluster, group, topic or transactionalId.              name      string      Name of the resource for which the ACL rule applies. Can be combined with the patternType field to use the prefix pattern.              patternType      string      Describes the pattern used in the resource field. The supported types are literal and prefix. With literal pattern type, the resource field will be used as a definition of a full topic name. With prefix pattern type, the resource name will be used only as a prefix.  The default value is literal.      Using the information about schemas and resource-operations described in the previous tables, the spec.authorization.acls list for a KafkaUser can be created as follows: # ...spec:# ...  authorization:    # ...    acls:      - host: '*'        resource:          type: topic          name: 'client-'          patternType: prefix        operation: WriteIn this example, an application using this KafkaUser would be allowed to write to any topic beginning with client- (for example, client-records or client-billing) from any host machine. Note: The write operation also implicitly derives the required describe operation that Kafka clients require to understand the data model of a topic. The following is an example ACL rule that provides access to read Schemas:     - host: '*'      resource:        type: topic        name: '__schema_'        patternType: prefix      operation: ReadRevoking access for an application As each application will be using credentials provided through a KafkaUser instance, deleting the instance will revoke all access for that application. Individual ACL rules can be deleted from a KafkaUser instance to remove the associated control on a resource operation. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "10.0",
        "excerpt":"The following encryption is always provided in IBM Event Streams:   Network connections into the Event Streams deployment from external clients are secured using TLS.  Kafka replication between brokers is also TLS encrypted.Consider the following for encryption as well:   Internal Kafka listeners can be configured with or without encryption as described in configuring access.  The REST producer endpoint can be configured with or without encryption as described in configuring access.In addition, you can supplement the existing data encryption with disk encryption where supported by your chosen storage provider. You can also encrypt messages within your applications before producing them. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/security/encrypting-data/",
        "teaser":null},{
        "title": "Configuring secure JMX connections",
        "collection": "10.0",
        "excerpt":"Java Management Extensions (JMX) Java Management Extensions (JMX) is a way of retrieving metrics from your specific processes dynamically at runtime. This can be used to get metrics that are specific to Java operations on Kafka To manage resources, management beans (MBeans) are used. MBeans represent a resource in the JVM. There are specific MBean attributes you can use with Kafka. Metrics can be retrieved from applications running inside your OpenShift Container Platform cluster by connecting to an exposed JMX port. The metrics can also be pushed in various formats to remote sinks inside or outside of the cluster by using JmxTrans. Exposing a JMX port on Kafka You can expose the JMX port (9999) of each Kafka broker to be accessible to secure connections from within the OpenShift Container Platform cluster. This grants applications deployed inside the cluster (including JmxTrans) read-only access to Kafka metrics. To expose the JMX port, set the spec.strimziOverrides.kafka.jmxOptions value to {}. This will create an open JMX port allowing any pod to read from it. The JMX port can be password-protected to prevent unauthorised pods from accessing it. It is good practice to secure the JMX port, as an unprotected port could allow a user to invoke an MBean operation on the Java JVM. To enable security for the JMX port, set the spec.strimiziOverrrides.kafka.jmxOptions.authentication.type field to password. For example: #...spec:  #...  strimziOverrides:    #...    kafka:      #...      jmxOptions:        authentication:          type: \"password\"    #...This will cause the JMX port to be secured using a generated username and password. When the JMX port is password protected, a Kubernetes secret named &lt;releasename&gt;ibm-es-jmx-secret is created inside the Event Streams namespace. The secret contains the following content:             Name      Description                  jmx_username      The user that is authenticated to connect to the JMX port.              jmx_password      The password for the authenticated user.      Connecting internal applications To connect your application to the Kafka JMX port, it must be deployed running inside the OpenShift Container Platform cluster. After your application is deployed, you can connect to each Kafka broker with the following URL pattern:&lt;cluster-name&gt;-kafka-&lt;kafka-ordinal&gt;.&lt;cluster-name&gt;-kafka-brokers.svc:9999 To connect to the JMX port, clients must use the following Java options:   javax.net.ssl.trustStore=&lt;path to trustStore&gt;  javax.net.ssl.trustStorePassword=&lt;password for trustStore&gt;In addition, when initiating the JMX connection, if the port is secured then clients must provide the username and password from the JMX secret. For example, the JConsole UI provide a username/password box to enter the credentials. Retrieving the truststore Using the Event Streams UI:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.  Click Connect to this cluster.  In the certificates section, click download certificate.You will now have the required certificate and the password will be displayed in the UI.Using the Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the command cloudctl es certificates to download the certificate. The password is displayed in the CLI.Retrieving the JMX username and password Using the oc CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following commands:     oc get secret &lt;cluster-name&gt;-jmx -o jsonpath='{.data.jmx\\-username}' -namespace &lt;name_of_your_namespace&gt; | base64 -decode &gt; jmx_username.txt     oc get secret &lt;cluster-name&gt;-jmx -o jsonpath='{.data.jmx\\-password}' -namespace &lt;name_of_your_namespace&gt; | base64 -decode &gt; jmx_password.txt   These will output the jmx_username and jmx_password values into the respective .txt files. Mounting the JMX secret directly into a pod Mounting the secret will project the jmx_username and jmx_password values as files under the mount path folder. apiVersion: v1kind: Podmetadata:  name: example-podspec:  volumes:      - name: jmx-secret        secret:          secretName: &lt;cluster-name&gt;-jmx  containers:    - name: example-container      image: example-image    volumeMounts:    - name: jmx-secret      mountPath: /path/to/jmx-secret      readOnly: trueFor more information, see Kubernetes Secrets If the connecting application is not installed inside the Event Streams namespace, it must be copied to the application namespace using the following command: oc -n &lt;instance_namespace&gt; get secret &lt;releasename&gt;ibm-es-jmx-secret -o yaml --export | oc -n &lt;application_namespace&gt; apply -f -JmxTrans The Kafka broker JMX ports are not exposed to applications outside of the cluster. However, a JmxTrans pod can be deployed that provides a mechanism to read JMX metrics from the Kafka brokers and push them to applications inside or outside the cluster. JmxTrans reads JMX metric data from the Kafka brokers and sends the data to applications in various data formats. Configuring a JmxTrans deployment To configure a JmxTrans deployment, you will need to use the spec.strimziOverrides.jmxTrans field to define the outputDefinitions and kafkaQueries. outputDefinitions: This specifies where the metric data will be pushed to and in what data format it will be provided in.             Attribute      Description                  outputType      The specified format you want to transform your query to. For a list of supported data formats, see the OutputWriters documentation              host      The target host address the data is pushed to.              port      The target port the data is pushed to.              flushDelay      Number of seconds JmxTrans agent waits before pushing new data.              name      Name of the property which is later referenced by spec.strimiziOverrides.jmxTrans.queries.      The following is an example configuration pushing JMX data to standardOut in the JmxTrans logs and another pushing JMX data every 10 seconds in the Graphite format to a Logstash database at the address mylogstash.com:31028: # ...spec:  # ...  strimziOverrides:    # ...    jmxTrans:      # ...      outputDefinitions:        - outputType: \"com.googlecode.jmxtrans.model.output.StdOutWriter\"          name: \"standardOut\"        - outputType: \"com.googlecode.jmxtrans.model.output.GraphiteOutputWriter\"          host: \"mylogstash.com\"          port: 31028          flushDelayInSeconds: 5          name: \"logstash\"kafkaQueries: This specifies what JMX metrics are read from the Kafka brokers. Note: Metrics are read from all Kafka brokers. There is no configuration option to obtain metrics from selected brokers only.             Attribute      Description                  targetMBean      Specifies what metrics you want to get from the JVM.              attributes      Specifies which MBean metric is read from the targetMBean as JMX metrics.              output      Defines where the metrics are pushed to, by choosing an output type.      The following is an example JmxTrans deployment that reads from all MBeans that match the pattern kafka.server:type=BrokerTopicMetrics,name=* and have name in the target MBeans name. From those MBeans, it obtains JMX metrics about the Count attribute, and pushes the metrics to a standard output as defined by the outputs attribute. #...spec:  #...  strimziOverrides:    #...    jmxTrans:      #...      kafkaQueries:        - targetMBean: \"kafka.server:type=BrokerTopicMetrics,name=*\"          attributes: [\"Count\"]          outputs: [\"standardOut\"]","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/security/secure-jmx-connections/",
        "teaser":null},{
        "title": "Network policies",
        "collection": "10.0",
        "excerpt":"Network policies are used to control inbound connections into pods. These connections can be from pods within the cluster, or from external sources. When you install an instance of Event Streams, the required network policies will be automatically created. To review the network policies that have been applied:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to display the installed network policies for a specific namespace:    oc get netpol -n &lt;namespace&gt;      The following tables provide information about the network policies that are applicable to each pod within the Event Streams instance. If a particular pod is not required by a given Event Streams configuration, the associated network policy will not be applied. Note: Where a network policy exposes a port to the Event Streams Cluster operator, it is configured to allow connections from any namespace. Kafka pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      REST API, REST Producer and Schema Registry pods      8091      Broker communication      Always              TCP      Kafka, Cluster operator, Entity operator, Kafka Cruise Control and Kafka Exporter pods      9091      Broker communication      Always              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9092      Kafka access for Plain listener      If the Plain listener is enabled              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9093      Kafka access for TLS listener      If the TLS listener is enabled              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9094      Kafka access for External listener      If the External listener is enabled              TCP      Anywhere      9404      Prometheus access to Kafka metrics      If metrics are enabled              TCP      Anywhere      9999      JMX access to Kafka metrics      If JMX port is exposed      Note: If required, access to listener ports can be restricted to only those pods with specific labels by including additional configuration in the Event Streams custom resource under spec.strimziOverrides.kafka.listeners.&lt;listener&gt;.networkPolicyPeers. ZooKeeper pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Kafka, ZooKeeper, Cluster operator, Entity operator, Kafka Cruise Control pods      2181      ZooKeeper client connections      Always              TCP      Other ZooKeeper pods      2888      ZooKeeper follower connection to leader      Always              TCP      Other ZooKeeper pods      3888      ZooKeeper leader election      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      Geo-replicator pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      REST API pods      8083      Geo-replicator API traffic      Always              TCP      Cluster operator and other geo-replicator pods      8083      Geo-replicator cluster traffic      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      Schema registry pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      Administration UI pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      3000      External access to UI      Always      Administration server pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      REST producer server pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      Metrics collector pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      7888      Exported Prometheus metrics      Always              TCP      Any pod in Event Streams instance      7443      TLS inbound metrics traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS inbound metrics traffic      If internal TLS is disabled      Cluster operator pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      8080      Exported Prometheus metrics      Always              TCP      Anywhere      8081      EventStreams custom resource validator      Always      Cruise Control pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Cluster operator      9090      Access to API      Always      Kafka Connect pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Cluster operator and Kafka Connect pods      8083      Access to Kafka Connect REST API      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/security/network-policies/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "10.0",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR website  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement. Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settings. These settings are configured using the EventStreams custom resource. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemFor more information about controlling access to data stored in IBM Event Streams, see managing access. Cluster-level configuration and resources, including logs that might contain message data, are accessible through the OpenShift Container Platform web console and by using the oc CLI. Access and authorization controls can be used to control which users are able to access this cluster-level information. Data Processing Encryption of connection to IBM Event Streams Connections to IBM Event Streams are secured using TLS. If you want to use your own CA certificates instead of those generated by the operator, you can provide them in the EventStreams custom resource settings. Encryption of connections within IBM Event Streams Internal communication between Event Streams pods is encrypted by default using TLS. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "10.0",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become a destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster. You can set up geo-replication by using the IBM Event Streams UI or CLI. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. When replication is set up and working, you can switch your applications to use another cluster when needed. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "10.0",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Geo-replication requires both the origin and destination IBM Event Streams instances to have client authentication enabled on the external route listener and the internal TLS listener.  If you are using geo-replication for disaster-recovery scenarios, see the guidance about configuring your clusters and applications to ensure you can switch clusters if one becomes unavailable.  Prepare your destination cluster by creating an EventStreamsGeoReplicator instance and defining the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  The replicated topics on the destination cluster will have a prefix added to the topic name. The prefix is the name of the Event Streams instance on the origin cluster, as defined in the EventStreams custom resource, for example my_origin.&lt;topic-name&gt;.  Configuration of geo-replication is done by the UI, the CLI or by directly editing the associated KafkaMirrorMaker2 custom resource.Preparing a destination cluster Before you can set up geo-replication and start replicating topics, you must create an EventStreamsGeoReplicator custom resource at the destination. The Event Streams Operator uses the EventStreamsGeoReplicator custom resource to create a configured KafkaMirrorMaker2 custom resource. The KafkaMirrorMaker2 custom resource is used by the Event Streams Operator to create geo-replication workers, which are instances of Kafka Connect running Kafka MirrorMaker 2.0 connectors. The number of geo-replication workers running at the destination cluster is configured in the EventStreamsGeoReplicator custom resource. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of creating the EventStreamsGeoReplicator instance, or you can modify an existing EventStreamsGeoReplicator instance, even if you already have geo-replication set up and running on that cluster. Configuring a new installation If you are installing a new EventStreamsGeoReplicator instance for geo-replication to a destination cluster, you must specify the existing destination IBM Event Streams instance you are connecting to. You can also specify the number of workers as part of configuring the EventStreamsGeoReplicator instance. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI To create a new EventStreamsGeoReplicator instance for geo-replication by using the UI:   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects drop-down list, select the project that contains the existing destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  In the Operator Details &gt; Overview page, find the Geo-Replicator tile in the list of Provided APIs and click Create Instance.  In the Create EventStreamsGeoReplicator page, edit the provided YAML to set values for the following properties.          In the metadata.labels section, set the eventstreams.ibm.com/cluster property value to the name of your destination IBM Event Streams instance.      Set the metadata.name property value to the name of your destination IBM Event Streams instance.      Set the spec.replicas property value to the number of geo-replication workers you want to run.        Click Create.  The new EventStreamsGeoReplicator instance is listed in the Operator Details &gt; EventStreamsGeoReplicator page.If the EventStreamsGeoReplicator instance is configured correctly, a KafkaMirrorMaker2 custom resource is created. You can see the details for the KafkaMirrorMaker2 custom resource in the  Kafka Mirror Maker 2 tab of the Operator Details page. Using the CLI To create a new EventStreamsGeoReplicator instance for geo-replication by using the CLI:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Define an EventStreamsGeoReplicator instance in a file. For example, the following YAML defines an EventStreamsGeoReplicator instance in the my-project project that is connected to the IBM Event Streams instance named my-dest-cluster and has 3 geo-replication workers.    apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsGeoReplicatormetadata:  labels:    eventstreams.ibm.com/cluster: my-dest-cluster  name: my-dest-cluster  namespace: my-projectspec:  version: 10.0.0  replicas: 3        Note: The EventStreamsGeoReplicator metadata.name property and eventstreams.ibm.com/cluster label property must be set to the name of the destination IBM Event Streams instance that you are geo-replicating to.     Run the following command to create the EventStreamsGeoReplicator instance:oc create -f &lt;path-to-your-eventstreamsgeoreplicator-file&gt;  The new EventStreamsGeoReplicator instance is created.  Run the following command to list your EventStreamsGeoReplicator instances:oc get eventstreamsgeoreplicators  Run the following command to view the YAML for your EventStreamsGeoReplicator instance:oc get eventstreamsgeoreplicator &lt;eventstreamsgeoreplicator-instance-name&gt; -o yamlWhen the EventStreamsGeoReplicator instance is ready, a KafkaMirrorMaker2 instance will be created. Run the following command to list your KafkaMirrorMaker2 instances:  oc get kafkamirrormaker2s Run the following command to view the YAML for your KafkaMirrorMaker2 instance:  oc get kafkamirrormaker2 &lt;kafka-mirror-make-2-instance-name&gt; -o yaml Note: If you have Strimzi installed, you might need to fully-qualify the resources you are requesting. The fully-qualified name for the KafkaMirrorMaker2 instances is kafkamirrormaker2.eventstreams.ibm.com. Configuring an existing installation If you want to change the number of geo-replication workers at a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects drop-down list, select the project that contains the destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  Click the Geo-Replicator tab to see the list of EventStreamsGeoReplicator instances.  Click the EventStreamsGeoReplicator instance that you want to modify.  Click the YAML tab.  Update the spec.replicas property value to the number of geo-replication workers you want to run.  Click Save.Using the CLI To modify the number of workers by using the CLI:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Run the following command to list your EventStreamsGeoReplicator instances:oc get eventstreamsgeoreplicators  Run the following command to edit the YAML for your EventStreamsGeoReplicator instance:oc edit eventstreamsgeoreplicator &lt;eventstreamsgeoreplicator-instance-name&gt;  Update the spec.replicas property value to the number of geo-replication workers you want to run.  Save your changes and close the editor.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "10.0",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab, and click Generate connection information for this cluster.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This information includes the security credentials for your destination cluster, which is then used by your origin cluster to authenticate with the destination.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click Add destination cluster on the Destination location tab.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Alternatively, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step includes the security credentials for your destination cluster which is then used by your origin cluster to authenticate.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following command to display the connection details for your destination cluster:cloudctl es geo-cluster-connect The command returns a base64 encoded string consisting of the API URL and the security credentials required for creating a destination cluster that should be used to configure geo-replication using the CLI.  If the connection details are to be used to configure geo-replication using the UI, add the --json option to return a JSON-formatted string.  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add  --cluster-connect &lt;base64-encoded-string-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.      Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip: You can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.     Note: A prefix of the origin cluster name will be added to the name of the new replicated topic that is created on the destination cluster, resulting in replicated topics named such as &lt;origin-cluster&gt;.&lt;topic-name&gt;.     Message history is included in geo-replication. This means all available message data for the topic is copied. The amount of history is determined by the message retention options set when the topics were created on the origin cluster.     Click Create to create a geo-replicator for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicator for the selected topics is set up successfully.Note: After clicking Create, it might take up to 5 to 10 minutes before geo-replication becomes active.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row. If topics are being replicated from the cluster you are logged in to, the Geo-replication column displays the number of clusters the topic is being replicated to. Clicking the column for the topic expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. List each topic you want to replicate by using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicator for the selected topics is set up successfully.Note: A prefix of the origin cluster name will be added to the name of the new replicated topic that is created on the destination cluster, resulting in replicated topics named such as &lt;origin-cluster&gt;.&lt;topic-name&gt;. Message history is included in geo-replication. This means all available message data for the topic is copied. The amount of history is determined by the message retention option set when the topics were created on the origin cluster. Considerations IBM Event Streams geo-replication uses Kafka’s MirrorMaker 2.0 to replicate data from the origin cluster to the destination cluster. Replication Factor IBM Event Streams sets the number of replicas of geo-replicated topics to 3, or if there are fewer brokers available then to the number of brokers in the destination cluster. If a different number of replicas are required on the destination topic, edit the value of the sourceConnector configuration property replication.factor on the MirrorMaker2 instance that is created by Event Streams for the geo-replication pairing. The change will apply to all new topics created by the geo-replicator on the destination cluster after the changes is made. It will not be applied to topics already configured for geo-replication Topic configuration MirrorMaker 2.0 has a blacklist of topic properties that are not copied from the source cluster topic:   follower.replication.throttled.replicas  leader.replication.throttled.replicas  message.timestamp.difference.max.ms  message.timestamp.type  unclean.leader.election.enable  min.insync.replicasIt is not possible to override the value of these properties using MirrorMaker 2.0 configuration, instead the values are taken from the settings of the destination cluster. To query the current values set on the destination cluster:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  List the broker configuration by using cloudctl es broker 0  Update the broker configuration to set these properties to the values if required before configuring geo-replication.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "10.0",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming geo-replication, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name, which includes the release name.      The health of the geo-replication for that origin cluster: Creating, Running, Updating, Paused, Stopping, Assigning, Offline, and Error.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab for details.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab for details.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend replication of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication and restart replication of data from the origin cluster.       More options &gt; Restart failed replicators: To restart a geo-replicator that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: Awaiting creation, Pending, Running, Resume, Resuming, Pausing, Paused, Removing, and Error. When the status is Error, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Destination locations tab for details.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the name of the destination cluster for which you want to manage geo-replication.  Choose from one of the following options using the top right  More options menu:           More options &gt; Pause running geo-replicator: To pause the geo-replicator for this destination and suspend replication of data to the destination cluster for all topics.       More options &gt; Resume paused geo-replicator: To resume the paused geo-replicator for this destination and resume replication of data to the destination cluster for all topics.       More options &gt; Restart failed geo-replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove cluster as destination: To remove the cluster as a destination for geo-replication.      To stop an individual topic from being replicated and remove it from the geo-replicator, select  More options &gt; Stop replicating topic. Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters      Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination destination_byl6xThe command returns the following information:     Details of destination cluster destination_byl6x   Cluster ID          Cluster name   REST API URL                                                                     Skip SSL validation?   destination_byl6x   destination    https://destination-ibm-es-admapi-external-myproject.apps.geodest.ibm.com:443    false   Geo-replicator details   Geo-replicator name                                             Status          Origin bootstrap servers   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       RUNNING         origin_es-kafka-bootstrap-myproject.apps.geosource.ibm.com:443   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   RUNNING         origin_es-kafka-bootstrap-myproject.apps.geosource.ibm.com:443   Geo-replicated topics   Geo-replicator name                                             Origin topic    Destination topic   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       topic1          origin_es.topic1   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       topic2          origin_es.topic2   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   topic1          origin_es.topic1   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   topic2          origin_es.topic2      The MirrorSource connector replicates data from the origin to the destination cluster. You can use the MirrorCheckpoint connector during failover from the origin to the destination cluster. To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:\\      cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\" For example: cloudctl es geo-replicator-pause --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \" This will pause both the MirrorSource connector and the MirrorCheckpoint connector for this geo-replicator.  Geo-replication for all topics that are part of this geo-replicator will be paused.         cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-resume --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will resume both the MirrorSource connector and the MirrorCheckpoint connector for this geo-replicator after they have been paused. Geo-replication for all topics that are part of this geo-replicator will be resumed.         cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-restart --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will restart a failed geo-replicator.         cloudctl es geo-replicator-topics-remove --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\" --topics &lt;comma-separated-topic-list&gt;For example:cloudctl es geo-replicator-topics-remove --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \" --topics topic1,topic2This will remove the listed topics from this geo-replicator.         cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-delete --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will remove all MirrorSource and MirrorCheckpoint connectors for this geo-replicator.         cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;   For example:   cloudctl es geo-cluster-remove --destination destination_byl6x   This will permanently remove a destination cluster.   Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.   Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an unexpected error from Kafka, it might stop replicating and report a status of Error. Monitor your geo-replication cluster to confirm that your geo-replicator is replicating data. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.Using Grafana dashboards to monitor geo-replication Metrics are useful indicators of the health of geo-replication.  They can give warnings of potential problems as well as providing data that can be used to alert on outages. Monitor the health of your geo-replicator using the available metrics to ensure replication continues. Configure your IBM Event Streams geo-replicator to export metrics, and then view them using the example Grafana dashboard. Configuring metrics Enable export of metrics in Event Streams geo-replication by editing the associated KafkaMirrorMaker2 custom resource. Using the OpenShift Container Platform web console   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects dropdown list, select the project that contains the destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  Click the Kafka Mirror Maker 2 tab to see the list of KafkaMirrorMaker2 instances.  Click the KafkaMirrorMaker2 instance with the name of the instance that you are adding metrics to.  Click the YAML tab.      Add the spec.metrics property. For example:      # ... spec:   metrics: {} # ...        Click Save.Using the OpenShift Container Platform CLI To modify the number of geo-replicator workers run the following using the oc tool:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Run the following command to list your KafkaMirrorMaker2 instances:oc get kafkamirrormaker2s  Run the following command to edit the custom resource for your KafkaMirrorMaker2 instance:oc edit kafkamirrormaker2 &lt;instance-name&gt;      Add the spec.metrics property. For example:     spec:  metrics: {}        Save your changes and close the editor.Installing persistent Grafana dashboards IBM Cloud Platform Common Services does not currently have a way to configure persistent storage on Grafana. This means that when the Grafana pods get restarted, you will lose any data on Grafana. To install Event Streams Grafana dashboards that will persist, use the following steps:   Download the geo-replication MonitoringDashboard custom resource from GitHub.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the MonitoringDashboard custom resource as follows:     oc apply -f &lt;dashboard-path&gt; -n &lt;namespace&gt;   Viewing installed Grafana dashboards To view the Event Streams Grafana dashboards, follow these steps:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Navigate to the IBM Cloud Platform Common Services console homepage.  Click the hamburger icon in the top left.  Expand Monitor Health.  Click the Monitoring in the expanded menu to open the Grafana homepage.  Click the user icon in the bottom left corner to open the user profile page.  In the Organizations table, find the namespace where you installed the Event Streams geo-replication MonitoringDashboard custom resource, and switch the user profile to that namespace. If you have not installed persistent dashboards, follow the instructions for installing persistent Grafana dashboards.  Hover the Dashboards on the left and click Manage.  Click on the dashboard to view the Dashboard table.Ensure you select your namespace, cluster name, and other filters at the top of the dashboard to view the required information. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/georeplication/health/",
        "teaser":null},{
        "title": "Switching clusters",
        "collection": "10.0",
        "excerpt":"When one of your origin Event Streams clusters experiences problems and becomes unavailable, you can switch your client applications over to use the geo-replicated topics on your destination Event Streams cluster. Ensure you plan for geo-replication before setting it up. Preparing clusters and applications for switching To make switching of clusters require little intervention, consider the following guidance when preparing your Event Streams clusters and applications. Configure your applications for switching Set up your applications so that reconfiguring them to switch clusters is as easy as possible. Code your applications so that security credentials, certificates, bootstrap server addresses, and other configuration settings are not hard-coded, but can be set in configuration files, or otherwise injected into your applications. Use the same certificates Consider using the same certificates for both the origin and destination clusters, by providing your own certificates at installation. This allows applications to use a single certificate to access either cluster. Note: You must complete the process of providing your own certificates before installing an instance of Event Streams. Note: When providing your own certificates, ensure that certificate renewal processes are followed at both the origin and destination clusters, so that both clusters continue to use the same certificates. Set up the same access to both clusters Consider providing your applications the same access to both the origin and destination clusters. For example, you can duplicate the application KafkaUser credentials from the origin cluster to the destination cluster. This allows applications to use a single set of credentials to access either cluster. Use the following commands to retrieve the KafkaUser credentials and custom resource from the origin cluster, and then create a new KafkaUser with these credentials on the destination cluster:   Log in to your origin cluster. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to retrieve the name of the secret for the KafkaUser:oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'The command provides the following output:          The principal username      The name of the Kubernetes Secret, which includes the namespace, containing the SCRAM password or the TLS certificates.        Use the secret-name from the previous step to run the following command. The command retrieves the credentials from the Kubernetes Secret and and saves them to the kafkauser-secret.yaml file:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o yaml &gt; kafkauser-secret.yaml  Run the following command to retrieve the KafkaUser custom resource YAML and save it to the kafkauser.yaml file:oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o yaml &gt; kafkauser.yaml  Log in to your destination cluster.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Edit both the kafkauser-secret.yaml and kafkauser.yaml files to set the correct namespace and Event Streams cluster name for the following properties:          metadata.namespace: provide the namespace of your destination cluster.      metadata.labels[\"eventstreams.ibm.com/cluster\"]: provide the name of your destination cluster.        Run the following command to create the Kubernetes Secret containing the KafkaUser credentials on the destination cluster:oc apply -f kafkauser-secret.yamlNote: You must run this command before the creation of the KafkaUser to ensure the same credentials are available on both the origin and destination clusters.  Run the following command to create the KafkaUser on the destination cluster:oc apply -f kafkauser.yamlNote: To duplicate KafkaUser credentials that use Mutual TLS authentication, the origin and destination cluster must be configured with the same certificates for the client CA at installation. Note: When KafkaUser credentials or Access Control Lists (ACLs) are modified on the origin cluster, the changes will need to be duplicated to the destination cluster to ensure that you can still switch clusters. Use regular expressions for consumer topic subscriptions Geo-replicated topics on the destination cluster will have a prefix added to the topic name. The prefix is the name of the Event Streams instance on the origin cluster, as defined in the EventStreams custom resource, for example my_origin.&lt;topic-name&gt;. Consider using regular expressions to define the topics that consuming applications are subscribed to, for example .*&lt;topic-name&gt;.  Using a regular expression means that the topic subscription does not need to change when switching to the prefixed topic names on the destination cluster. Plan to update consumer group offsets Consider how you will update the consumer group offsets in consuming applications when switching clusters. Geo-replication includes consumer group checkpointing to store the mapping of consumer group offsets, allowing consuming applications to continue processing messages at the appropriate offset positions. Produce to the same topic name When switching clusters, produce to the same topic name on the destination cluster as was used on the origin cluster. This will ensure geo-replicated messages and directly produced messages are stored in separate topics. If consuming applications use regular expressions to subscribe to both topics, then both sets of messages will be processed. Consider message ordering If message ordering is required, configure your consuming applications to process all messages from the geo-replicated topic on the destination cluster before producing applications are restarted. Updating existing applications to use geo-replicated topics on the destination cluster If you are not using the same certificates and credentials on the origin and destination clusters, use the following instructions to retrieve the information required to update your applications so that they can use the geo-replicated topics from the destination cluster:   Log in to your destination Event Streams cluster as an administrator.  Click Connect to this cluster.  Go to the Resources tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: In the Kafka listener and credentials section, select the listener from the list.                  Click the External tab for applications connecting from outside of the OpenShift Container Platform cluster.          Click the Internal tab for applications connecting from inside the OpenShift Container Platform cluster.                    Credentials: To connect securely to Event Streams, your application needs credentials with permission to access the cluster and resources such as topics. In the Kafka listener and credentials section, click the Generate SCRAM credentials or Generate TLS credentials button next to the listener you are using, and follow the instructions to select the level of access you want to grant to your resources with the credentials.      Certificates: A certificate is required by your client applications to connect securely to the destination cluster. In the Certificates section, download either the PKCS12 certificate or PEM certificate. If you use the PKCS12 certificate, make a copy of the Certificate password to use with the certificate in your application.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Updating consumer group offsets The topic on the origin cluster and the geo-replicated topic on the destination cluster might have different offsets for the same messages, depending on when geo-replication started. This means that a consuming application that is switched to use the destination cluster cannot use the consumer group offset from the origin cluster. Updating consumer group offsets by using checkpoints Geo-replication uses the Kafka Mirror Maker 2.0 MirrorCheckpointConnector to automatically store consumer group offset checkpoints for all origin cluster consumer groups. Each checkpoint maps the last committed offset for each consumer group in the origin cluster to the equivalent offset in the destination cluster. The checkpoints are stored in the &lt;origin_cluster_name&gt;.checkpoints.internal topic on the destination cluster. Note: Consumer offset checkpoint topics are internal topics that are not displayed in the UI and CLI. Run the following CLI command to include internal topics in the topic listing:cloudctl es topics --internal. When processing messages from the destination cluster, you can use the checkpoints to start consuming from an offset that is equivalent to the last committed offset on the origin cluster. If your application is written in Java, Kafka’s RemoteClusterUtils class provides the translateOffsets() utility method to retrieve the destination cluster offsets for a consumer group from the checkpoints topic. You can then use the KafkaConsumer.seek() method to override the offsets that the consumer will use on the next poll. For example, the following Java code snippet will update the example-group consumer group offset from the origin-cluster cluster to the destination cluster equivalent: // Retrieve the mapped offsets for the destination cluster topic-partitionsMap&lt;TopicPartition, OffsetAndMetadata&gt; destinationOffsetsMap = RemoteClusterUtils.translateOffsets(properties, \"origin-cluster\",        \"example-group\", Duration.ofMillis(10000));// Update the KafkaConsumer to start at the mapped offsets for every topic-partitiondestinationOffsetsMap.forEach((topicPartition, offsetAndMetadata) -&gt; kafkaConsumer.seek(topicPartition, offsetAndMetadata));// Retrieve records from the destination cluster, starting from the mapped offsetsConsumerRecords&lt;byte[], byte[]&gt; records = kafkaConsumer.poll(Duration.ofMillis(10000))Note: To configure how often checkpoints are stored and which consumer groups are stored in the checkpoints topic, you can edit the following properties in your Kafka Mirror Maker 2 custom resource:   spec.mirror.checkpointConnector.config  spec.mirror.groupsPatternUpdating consumer group offsets manually If you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic, you can use the cloudctl es group-reset command.   To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:          Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;      Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the cloudctl es group-reset command as follows:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute        To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --executeThese methods also avoid the need to make code changes to your client application. Reverting message production and consumption back to the origin cluster When the origin Event Streams cluster becomes available again, you can switch your client applications   back to use the topics on your origin cluster. If messages have been produced directly to the destination cluster, use the following steps to replicate those messages to the origin cluster before switching back to using it.   Create an EventStreamsGeoReplicator custom resource configured to connect to the origin Event Streams cluster, and set up geo-replication in the reverse direction to the original geo-replication flow. This means there will be a geo-replicator running on the origin cluster which copies messages from non-geo-replicated topics on the destination cluster back to geo-replicated topics on the origin cluster.  The geo-replicated topic named &lt;origin-cluster&gt;.&lt;topic&gt; on the destination cluster will not have new geo-replicated messages arriving, as the producing applications have been switched to produce messages directly to the topic without a prefix on the destination cluster. Ensure that the geo-replicated topic on the destination cluster is not geo-replicated back to the origin cluster as this will result in duplicate data on the origin cluster.  Switch the producing and consuming applications back to the origin cluster again by following the previous instructions. Producing applications will continue to produce messages to the original topic name on the origin cluster, and consuming applications will read from both the geo-replicated topics and the original topics on the origin cluster. Consuming applications will need their consumer group offsets to be correctly updated for the offset positions on the origin cluster.Note: Due to the asynchronous nature of geo-replication, there might be messages in the original topics on the origin cluster that had not been geo-replicated over to the destination cluster when the origin cluster became unavailable. You will need to decide how to handle these messages. Consider setting consumer group offsets so that the messages are processed, or ignore the messages by setting consumer group offsets to the latest offset positions in the topic. For example, if the origin cluster is named my_origin, the destination cluster is named my_destination, and the topic on the my_origin cluster is named my_topic, then the geo-replicated topic on the my_destination cluster will be named my_origin.my_topic.   When the my_origin cluster becomes unavailable, producing applications are switched to the my_destination cluster. The my_destination cluster now has topics named my_topic and my_origin.my_topic. Consuming applications are also switched to the my_destination cluster and use the regular expression .*my_topic to consume from both topics.  When the my_origin cluster becomes available again, reverse geo-replication is set up between the clusters. The my_origin cluster now has the topic named my_topic and a new geo-replicated topic named my_destination.my_topic. The topic named my_destination.my_topic contains the messages that were produced directly to the my_destination cluster.  Producing applications are producing to the topic named my_topic on the my_destination cluster, so the geo-replicated topic named my_origin.my_topic on the my_destination cluster does not have any new messages arriving. Existing messages in the topic named my_origin.my_topic are consumed from the my_destination cluster until there is no more processing of the messages required. Note: The geo-replicated topic named my_origin.my_topic is not included in the reverse geo-replication back to the my_origin cluster, as that would create a geo-replicated topic named my_destination.my_origin.my_topic on the my_origin cluster containing the same messages as in the topic named my_topic.  Producing applications are now switched back to the my_origin cluster, continuing to produce to the topic named my_topic.  Consuming applications are also switched back to the my_origin cluster, with consumer group offsets updated for the offset positions at the my_origin cluster. Consuming applications continue to use the regular expression .*my_topic to consume from both the topic named my_topic and the geo-replicated topic named my_destination.my_topic.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/georeplication/failover/",
        "teaser":null},{
        "title": "Event Streams producer API",
        "collection": "10.0",
        "excerpt":"Event Streams provides a REST API to help connect your existing systems to your Event Streams Kafka cluster. Using the API, you can integrate Event Streams with any system that supports RESTful APIs. The REST producer API is a scalable REST interface for producing messages to Event Streams over a secure HTTP endpoint. Send event data to Event Streams, utilize Kafka technology to handle data feeds, and take advantage of Event Streams features to manage your data. Use the API to connect existing systems to Event Streams, such as IBM Z mainframe systems with IBM z/OS Connect, systems using IBM DataPower Gateway, and so on. About authorization By default Event Streams requires clients to be authorized to write to topics. The available authentication mechanisms for use with the REST Producer are MutualTLS (tls) and SCRAM SHA 512 (scram-sha-512). For more information about these authentication mechanisms, see the information about managing access. The REST producer API requires any authentication credentials be provided with each REST call to grant access to the requested topic. This can be done in one of the following ways:   In an HTTP authorization header: You can use this method when you have control over what HTTP headers are sent with each call to the REST producer API. For example, this is the case when the API calls are made by code you control.  Mutual TLS authentication (also referred to as SSL client authentication or SSL mutual authentication):You can use this method when you cannot control what HTTP headers are sent with each REST call. For example, this is the case when you are using third-party software or systems such as CICS events over HTTP.Note: You must have Event Streams version 2019.1.1 or later to use the REST API. In addition, you must have Event Streams version 2019.4.1 or later to use the REST API with SSL client authentication. Content types The following are supported for the value of the Content-Type header:   application/octet-stream  text/plain  application/json  text/xml  application/xmlFor each content type the message body is copied “as is” into the Kafka record value. Both the application/octet-stream and text/plain types must specify a length header to avoid accidental truncation if the HTTP connection drops prematurely. The payload of a request that uses the application/json header must parse as valid JSON. Otherwise, the request will be rejected. The Event Streams REST Producer API also supports the following vendor content types:   vnd.ibm-event-streams.json.v1 as a synonym for application/json  vnd.ibm-event-streams.binary.v1 as a synonym for application/octet-stream  vnd.ibm-event-streams.text.v1 as a synonym for text/plainThese content types can be used to pin applications at the version 1 API level. Prerequisites To be able to produce to a topic, ensure you have the following available:   The URL of the Event Streams REST Producer API endpoint.  The topic you want to produce to.  If using a REST Producer API endpoint that requires HTTPS, the Event Streams certificate.To retrieve the full URL for the Event Streams API endpoint, you can use the OpenShift Container Platform oc CLI or Event Streams UI. Using the OpenShift Container Platform oc CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to list available Event Streams REST Producer API endpoints:oc get routes -n &lt;namespace&gt; --selector app.kubernetes.io/name=rest-producer  Copy the full URL of the required endpoint from the HOST/PORT section of the response.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section.  Click Copy Producer API endpoint.By default the Event Streams REST Producer API endpoint requires a HTTPS connection. If this has not been disabled for the endpoint the Event Streams certificate must be retrieved. You can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Download the server certificate for Event Streams:cloudctl es certificates --format pem By default, the certificate is written to a file called es-cert.pem.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Certificates section.  In the PEM certificate section, click Download certificate.For information on how to create a topic to produce to, see the information about creating topics. Key and message size limits The REST producer API has a configured limit for the key size (default is 4096 bytes) and the message size (default is 65536 bytes). If the request sent has a larger key or message size than the limits set, the request will be rejected. You can configure the key and message size limits at the time of installation or later as described in modifying installation settings. The limits are configured by setting environment variables on the REST Producer component: spec:  restProducer:    env:      - name: MAX_KEY_SIZE        value: \"4096\"      - name: MAX_MESSAGE_SIZE        value: \"65536\"Important: Do not set the MAX_MESSAGE_SIZE to a higher value than the maximum message size that can be received by the Kafka broker or the individual topic (max.message.bytes). By default, the maximum message size for Kafka brokers is 1000012 bytes. If the limit is set for an individual topic, then that setting overrides the broker setting. Any message larger than the maximum limit will be rejected by Kafka. Note: Sending large requests to the REST producer increases latency, as it will take the REST producer longer to process the requests. Producing messages using REST with HTTP authorization Ensure you have gathered all the details required to use the producer API, as described in the prerequisites. Before producing you must also create authentication credentials. To create authentication credentials to use in an HTTP authorization header, you can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Use the kafka-user-create command to create a KafkaUser that can produce to your topic:    cloudctl es kafka-user-create --topic &lt;topic_name&gt; --name &lt;user_name&gt; --producer --auth-type scram-sha-512        Follow the steps in managing access to retrieve the SCRAM SHA 512 username and password.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section, then click Generate credentials.  Select SCRAM username and password, then click Next.  Fill in a Credential Name, this name must be unique.  Select Produce messages, consume messages and create topics and schemas, then click Next.  Select A specific topic and fill in the topic name, then click Next.  Click Next on the consumer group panel, then Generate credentials on the transactional IDs panel using the default settings.  Take a copy of either the username and password or Basic authentication token.You can use the usual languages for making the API call. For example, to use cURL to produce messages to a topic with the producer API using a Basic authentication header, run the curl command as follows: curl -v -X POST -H \"Authorization: Basic &lt;auth_token&gt;\" -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;auth_token&gt; is the Basic authentication token you generated earlier.  &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier. Use http instead of https if the provided Producer API endpoint has TLS disabled.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  --cacert es-cert.pem can be ommitted if the provided Producer API endpoint has TLS disabledTo use cURL to produce messages to a topic with the producer API using a SCRAM username and password, run the curl command as follows: curl -v -X POST -u &lt;user&gt;:&lt;password&gt; -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;user&gt; is the SCRAM username provided when generating credentials.  &lt;password&gt; is the SCRAM password retrieved earlier.  &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier.  Use http instead of https if the provided Producer API endpoint has TLS disabled.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  --cacert es-cert.pem can be ommitted if the provided Producer API endpoint has TLS disabledFor full details of the API, see the API reference. Producing messages using REST with Mutual TLS authentication Ensure you have gathered all the details required to use the producer API, as described in the prerequisites. Before producing you must also create TLS credentials. To create authentication credentials to use with Mutual TLS authentication, you can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Use the kafka-user-create command to create a KafkaUser that can produce to your topic:    cloudctl es kafka-user-create --topic &lt;topic_name&gt; --name &lt;user_name&gt; --producer --auth-type tls        Follow the steps in managing access to TLS certificates and keys.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section, then click Generate credentials.  Select Mutual TLS certificate, then click Next.  Fill in a Credential Name, this name must be unique.  Select Produce messages, consume messages and create topics and schemas, then click Next.  Select A specific topic and fill in the topic name, then click Next.  Click Next on the consumer group panel, then Generate credentials on the transactional IDs panel using the default settings.  Click Download certificates and unzip the downloaded ZIP archive containing the TLS certificates and keys.For some systems, for example CICS, you need to download and import the client CA certificate into your truststore. The client CA certificate can be downloaded using the OpenShift Container Platform oc and Event Streams CLI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Run the following command to view details of the KafkaUser you want the client CA certificate for: cloudctl es kafka-user &lt;user-name&gt;  Note down the name of the secret associated with the KafkaUser  Run the following oc command to get the client CA certificate from the secret found in the previous command: oc extract secret/&lt;user-secret-name&gt; --keys=ca.crt --to=- &gt; ca.crtIf you are using Java keystores, the client certificate can be imported by using the keytool -importcert ... command as described in the IBM SDK, Java Technology Edition documentation. Some systems require the client certificate and private key to be combined into one PKCS12 file (with extension .p12 or .pfx). A PKCS12 file and associated password file is included in the KafkaUser secret and the ZIP file downloaded from the Event Streams UI. You can use the usual languages for making the API call. Consult the documentation for your system to understand how to specify the client certificate and private key for the outgoing REST calls to Event Streams. For example, to use cURL to produce messages to a topic with the producer API, run the curl command as follows: curl -v -X POST -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem --key user.key --cert user.crt \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  es-cert.pem is the Event Streams server certificate downloaded as part of the prerequisites  user.key is the private key of the user downloaded from the UI or read from the KafkaUser secret  user.crt is the user certificate that contains the public key of the user downloaded from the UI or read from the KafkaUser secretFor example, the steps to configure a CICS URIMAP as an HTTP client is described in the  CICS Transaction Server documentation. In this case, load the client certificate and private key, together with the Event Streams server certificate into your RACF key ring. When defining the URIMAP:   Host is the client authentication API endpoint obtained as part of the prerequisites, without the leading https://  Path is /topics/&lt;topic-name&gt;/records  Certificate is the label given to the client certificate when it was loaded into the key ring.For full details of the API, see the API reference. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/rest-api/",
        "teaser":null},{
        "title": "Kafka Connect and connectors",
        "collection": "10.0",
        "excerpt":"You can integrate external systems with IBM Event Streams by using the Kafka Connect framework and connectors. What is Kafka Connect? When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Use Kafka Connect to reliably move large amounts of data between your Kafka cluster and external systems. For example, it can ingest data from sources such as databases and make the data available for stream processing.  Source and sink connectors Kafka Connect uses connectors for moving data into and out of Kafka. Source connectors import data from external systems into Kafka topics, and sink connectors export data from Kafka topics into external systems. A wide range of connectors exists, some of which are commercially supported. In addition, you can write your own connectors. A number of source and sink connectors are available to use with Event Streams. See the connector catalog section for more information.  Workers Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems, and all work is performed in a single process. Distributed mode is more appropriate for production use, as it benefits from additional features such as automatic balancing of work, dynamic scaling up or down, and fault tolerance.  When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to the external system (for example, IBM MQ).When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Connector catalog The connector catalog contains a list of connectors that have been verified with Event Streams. Connectors are either supported by the community or IBM. Community support means the connectors are supported through the community by the people that created them. IBM supported connectors are fully supported as part of the official Event Streams support entitlement if you are using the paid-for version of Event Streams (not Community Edition). See the connector catalog for a list of connectors that work with Event Streams.  Setting up connectors Event Streams provides help with setting up your Kafka Connect environment, adding connectors to that environment, and starting the connectors. See the instructions about setting up and running connectors. Connectors for IBM MQ Connectors are available for copying data between IBM MQ and Event Streams. There is a MQ source connector for copying data from IBM MQ into Event Streams or Apache Kafka, and a MQ sink connector for copying data from Event Streams or Apache Kafka into IBM MQ. For more information about MQ connectors, see the topic about connecting to IBM MQ. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/connectors/",
        "teaser":null},{
        "title": "Setting up and running connectors",
        "collection": "10.0",
        "excerpt":"IBM Event Streams helps you set up a Kafka Connect environment, prepare the connection to other systems by adding connectors to the environment, and start Kafka Connect with the connectors to help integrate external systems. Log in to the Event Streams UI, and click Toolbox in the primary navigation. Scroll to the Connectors section and follow the guidance for each main task. You can also find additional help on this page. Using Kafka Connect The most straightforward way to run Kafka Connect on OpenShift Container Platform is to use a custom resource called KafkaConnectS2I. An instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector. Kafka Connect topics When running in distributed mode, Kafka Connect uses three topics to store configuration, current offsets and status. Kafka Connect can create these topics automatically as it is started by the Event Streams operator. By default, the topics are:   connect-configs: This topic stores the connector and task configurations.  connect-offsets: This topic stores offsets for Kafka Connect.  connect-status: This topic stores status updates of connectors and tasks.If you want to run multiple Kafka Connect environments on the same cluster, you can override the default names of the topics in the configuration. Authentication and authorization Kafka Connect uses an Apache Kafka client just like a regular application, and the usual authentication and authorization rules apply. Kafka Connect will need authorization to:   Produce and consume to the internal Kafka Connect topics and, if you want the topics to be created automatically, to create these topics  Produce to the target topics of any source connectors you are using  Consume from the source topics of any sink connectors you are using.Note: For more information about authentication and the credentials and certificates required, see the information about managing access. Set up a Kafka Connect environment To begin using Kafka Connect, do the following. Download Kafka Connect configuration   In the Event Streams UI, click Toolbox in the primary navigation. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  Click Download Kafka Connect ZIP to download the compressed file, then extract the contents to your preferred location.You will have a Kubernetes manifest for a KafkaConnectS2I and an empty directory called my-plugins. Configure Kafka Connect Edit the downloaded kafka-connect-s2i.yaml file to enable Kafka Connect to connect to your OpenShift Container Platform cluster. You can use the snippets in the Event Streams UI as guidance to configure Kafka Connect.   Choose a name for your Kafka Connect instance.  You can run more than one worker by increasing the replicas from 1.  Set bootstrapServers to connect the bootstrap server address of a listener. If using an internal listener, this will be the address of a service. If using an external listener, this will be the address of a route.  If you have fewer than 3 brokers in your Event Streams cluster, you must set config.storage.replication.factor, offset.storage.replication.factor and status.storage.replication.factor to 1.  Unless your Event Streams cluster has authentication turned off, you must provide authentication credentials in the authentication configuration.  If clients require a certificate to connect to your Event Streams cluster (as they will if you are connecting using a route), you must provide a certificate in the tls configuration.Deploy Kafka Connect Deploy the Kafka Connect instance by applying the YAML file using the OpenShift Container Platform CLI: oc apply -f kafka-connect-s2i.yamlVerify the Kafka Connect instance has been created. It can take up to 5 minutes to become ready. oc get kafkaconnects2iOnce ready, you can see the status and which connectors are available: oc describe kafkaconnects2i &lt;kafka_connect_name&gt;Adding connectors to your Kafka Connect environment Prepare Kafka Connect for connections to your other systems by adding the required connectors. Following on from the previous step, click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Add connectors on the Add connectors to your Kafka Connect environment tile. To run a particular connector Kafka Connect must have access to a JAR file or set of JAR files for the connector. If your connector consists of just a single JAR file, you can copy it directly into the my-plugins directory. If your connector consists of multiple JAR files, create a directory for the connector inside the my-plugins directory and copy all of the connector’s JAR files into that directory. Here’s an example of how the directory structure might look with 3 connectors: +--  my-plugins|    +--  connector1.jar|    +--  connector2|    |    +-- connector2.jar|    |    +-- connector2-lib.jar|    +-- connector3.jarStarting Kafka Connect with your connectors Click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Start Kafka Connect on the Start Kafka Connect with your connectors tile. Build Kafka Connect with your connectors To add your connector JARs into your Kafka Connect environment, start a build using the directory you prepared: oc start-build &lt;kafka_connect_name&gt;-connect --from-dir ./my-plugins/Wait for the build to be marked complete and the Kafka Connect pod to become ready. It can take up to 2 minutes to complete. oc get buildsoc get podsVerify that your chosen connectors are installed in your Kafka Connect environment. oc describe kafkaconnects2i &lt;kafka_connect_name&gt;Start a connector Create a YAML file for the connector configuration. For the MQ connectors, you can use the Event Streams CLI to generate the YAML file. Alternatively, you can use the following template, taking care to replace &lt;kafka_connect_name&gt; with the name of the KafkaConnectS2I instance: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: &lt;connector_name&gt;  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: &lt;connector_class_name&gt;  tasksMax: 1  config:  # The connector configuration goes hereStart the connector build applying the YAML file: oc apply -f &lt;connector_filename&gt;.yamlYou can view the status of the connector by describing the custom resource: oc describe kafkaconnector &lt;connector_name&gt;","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/setting-up-connectors/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "10.0",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Available connectors Connectors are available for copying data in both directions.   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue. Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For convenience, it is recommended to run the Kafka Connect worker on the same OpenShift Container Platform cluster as IBM Event Streams. If the network latency between MQ and IBM Event Streams is significant, you might prefer to run the Kafka Connect worker close to the queue manager to minimize the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "10.0",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in OpenShift Container Platform. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites To follow these instructions, ensure you have the following available:   A running Kafka Connect environment on OpenShift Container Platform using a KafkaConnectS2I custom resource  IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click Generate.  In the dialog, enter the configuration of the MQ Source connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to generate a configuration file.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-source command to generate the configuration file for the MQ Source connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSOURCE and connecting to the topic TSOURCE, run the following command:    cloudctl es connector-config-mq-source --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSOURCE\" --topic=\"TSOURCE\" --file=\"mq-source\" --format yaml        Note: Omitting the --format yaml flag will generate a mq-source.properties file which can be used for standalone mode. Specifying --format json will generate a mq-source.json file which can be used for distributed mode outside OpenShift Container Platform.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: mq-source  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: com.ibm.eventstreams.connect.mqsource.MQSourceConnector  tasksMax: 1  config:    topic: TSOURCE    mq.queue.manager: QM1    mq.connection.name.list: localhost(1414)    mq.channel.name: MYSVRCONN    mq.queue: MYQSOURCE    mq.user.name: alice    mq.password: passw0rd    key.converter: org.apache.kafka.connect.storage.StringConverter    value.converter: org.apache.kafka.connect.storage.StringConverter    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilderA list of all the possible flags can be found by running the command cloudctl es connector-config-mq-source --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Source connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click Go to GitHub. Download the JAR file from the list of assets for the latest release.Configuring Kafka Connect Follow the steps in Starting Kafka Connect with your connectors. When adding connectors, add the MQ connector JAR you downloaded, and when starting the connector, use the YAML file you created earlier. Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ: $ oc logs &lt;kafka_connect_pod_name&gt;...INFO Created connector mq-source...INFO Connection to MQ established...Send a test message   To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;  Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation and select the connected topic. Messages will appear in the message browser of that topic.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "10.0",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in OpenShift Container Platform. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites To follow these instructions, ensure you have the following available:   A running Kafka Connect environment on OpenShift Container Platform using a KafkaConnectS2I custom resource  IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and put messages on a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click Generate.  In the dialog, enter the configuration of the MQ Sink connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to generate a configuration file.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-sink command to generate the configuration file for the MQ Sink connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSINK and connecting to the topics TSINK, run the following command:    cloudctl es connector-config-mq-sink --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSINK\" --topics=\"TSINK\" --file=\"mq-sink\" --format yaml        Note: Omitting the --format yaml flag will generate a mq-source.properties file which can be used for standalone mode. Specifying --format json will generate a mq-source.json file which can be used for distributed mode outside OpenShift Container Platform.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: mq-sink  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: com.ibm.eventstreams.connect.mqsink.MQSinkConnector  tasksMax: 1  config:    topics: TSINK    mq.queue.manager: QM1    mq.connection.name.list: localhost(1414)    mq.channel.name: MYSVRCONN    mq.queue: MYQSINK    mq.user.name: alice    mq.password: passw0rd    key.converter: org.apache.kafka.connect.storage.StringConverter    value.converter: org.apache.kafka.connect.storage.StringConverter    mq.message.builder: com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilderA list of all the possible flags can be found by running the command cloudctl es connector-config-mq-sink --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Sink connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click Go to GitHub. Download the JAR file from the list of assets for the latest release.Configuring Kafka Connect Follow the steps in Starting Kafka Connect with your connectors. When adding connectors, add the MQ connector JAR you downloaded, and when starting the connector, use the YAML file you created earlier. Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ: $ oc logs &lt;kafka_connect_pod_name&gt;...INFO Created connector mq-sink...INFO Connection to MQ established...Send a test message To test the connector you will need an application to produce events to your topic.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Starter application tile under Applications, and click Get started.  Click Download JAR from GitHUb. Download the JAR file from the list of assets for the latest release.  Click Generate properties.  Enter a name for the application.  Go to the Existing topic tab and select the topic you provided in the MQ connector configuration.  Click Generate and download .zip.  Follow the instructions in the UI to get the application running.Verify the message is on the queue:   Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "10.0",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. These instructions explain how to run Kafka Connect in both standalone and distributed mode. For more information and to help decide which mode to use see the explanation of Kafka Connect workers. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka Connect shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the Kafka Connect files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka Connect shell scripts and JAR files. To download Kafka Connect and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the Support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.12-2.5.0.tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.Download IBM MQ connectors and configuration Depending on the connector you want to use:\\   Download the source connector JAR and source configuration file  Download the sink connector JAR and sink configuration fileIf you want to run a standalone Kafka Connect worker, you need a .properties file. To run a distributed Kafka Connect worker, you need a .json file. Copy the connector JAR file(s) and the required configuration file to a directory on the z/OS Unix System Services. Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-standalone.sh  config/connect-standalone.properties  mq-source.properties or mq-sink.propertiesIf you want to run a distributed Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-distributed.sh  config/connect-distributed.shExtract the Apache Kafka distribution:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.12-2.5.0.tar  Change to the resulting kafka_&lt;version&gt; directory.Convert the shell scripts:   Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.shConvert the configuration files:   Copy the connect-standalone.properties file (or connect-distributed.properties for a distributed setup) into the current directory, for example:cp config/connect-standalone.properties ./connect-standalone.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.properties.orig &gt; config/connect-standalone.propertiesIf running in standalone mode:   Copy the MQ .properties file into the current directory, for example:cp ./mq-source.properties ./mq-source.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./mq-source.properties.orig &gt; ./mq-source.propertiesNote: For distributed mode the .json file must remain in ASCII format. Update the Kafka Connect configuration The connect-standalone.properties (or connect-distributed.properties for distributed mode) file must include the correct bootstrap.servers and SASL/SSL configuration for your Apache Kafka or Event Streams install. For example, if running against Event Streams, download the certificate for your install to your IBM z/OS system. Generate credentials that can produce, consume and create topics and update the connect-standalone.properties (or connect-distributed.properties) file to include: bootstrap.servers=&lt;bootstrapServers&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/opt/kafka/es-cert.p12ssl.truststore.password=&lt;truststorePassword&gt;ssl.truststore.type=PKCS12sasl.mechanism=SCRAM-SHA-512sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;userName&gt;\" password=\"&lt;password&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=/opt/kafka/es-cert.p12producer.ssl.truststore.password=&lt;truststorePassword&gt;producer.ssl.truststore.type=PKCS12producer.sasl.mechanism=SCRAM-SHA-512producer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"userName\" password=\"&lt;password&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=/opt/kafka/es-cert.p12consumer.ssl.truststore.password=&lt;truststorePassword&gt;consumer.ssl.truststore.type=PKCS12consumer.sasl.mechanism=SCRAM-SHA-512consumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"userName\" password=\"&lt;password&gt;\";plugin.path=/opt/connectorsConfiguring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.3.0-jar-with-dependencies.jar. As you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin, you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/connecting/mq/zos/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "10.0",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Checking Event Streams health Using the Event Streams UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready.If any of the components are not ready for an extended period of time, see how you can troubleshoot as described in debugging. Using the OpenShift Container Platform CLI You can check the health of your IBM Event Streams environment using the oc tool.   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:oc -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.If any of the components are not ready for an extended period of time, check the debugging topic. Debugging Using the OpenShift Container Platform UI   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Click on the Resources tab.  To filter only pods, deselect all resource types with the exception of Pod.  Click the pod not in the Running state to display information regarding the pod.  In the Overview, resource usage, such as CPU and memory, can be viewed.  Click on the Logs tab to search logs.Tip: You can also use the cluster logging provided by the OpenShift Container Platform to collect, store, and visualize logs. The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). You can download and install the pre-configured Event Streams Kibana dashboards by following the instructions in monitoring cluster health. Using the OpenShift Container Platform CLI   To retrieve further details about the pods, including events affecting them, use the following command:oc -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:oc -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about debugging, see the Kubernetes documentation. You can use the oc command instead of kubectl to perform the debugging steps. Note: After a component restarts, the oc command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the -–previous option to the oc logs command. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "10.0",
        "excerpt":"Monitoring the health of your Kafka cluster helps to verify that your operations are running smoothly. The Event Streams UI includes a preconfigured dashboard that monitors Kafka data. Event Streams also provides a number of ways to export metrics from your Kafka brokers to external monitoring and logging applications. These metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. The following sections provide an overview of the available options. For information about the health of your topics, check the producer activity dashboard. JMX Exporter You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus. For an example of how to configure the JMX exporter, see configuring the JMX Exporter Kafka Exporter You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools. This allows topic metrics such as consumer group lag to be collected. For an example of how to configure a Kafka Exporter, see configuring the Kafka Exporter. JmxTrans JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases. For more information, see configuring JmxTrans. Grafana Create dashboards in the Grafana service to monitor your Event Streams instance for health and performance of your Kafka clusters. Installing Persistent Grafana dashboards IBM Cloud Platform Common Services does not currently have a way to configure persistent storage on Grafana. This means that when the Grafana pods get restarted, you will lose any data on Grafana. To install Event Streams Grafana dashboards that will persist, use the following steps:   Download the monitoringdashboard custom resource from GitHub.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the monitoringdashboard custom resource as follows:     oc apply -f &lt;monitoringdashboard file&gt; -n &lt;namespace&gt;.   Viewing installed Grafana dashboards To view the Event Streams Grafana dashboards, follow these steps:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Navigate to the IBM Cloud Platform Common Services console homepage.  Click the hamburger icon in the top left.  Expand Monitor Health.  Click the Monitoring in the expanded menu to open the Grafana homepage.  Click the user icon in the bottom left corner to open the user profile page.  In the Organizations table, find the namespace where you installed the Event Streams monitoringdashboard custom resource, and switch the user profile to that namespace. If you have not installed persistent dashboards, follow the instructions for installing persistent Grafana dashboards.  Hover over the Dashboards on the left and click Manage.  Click on the dashboard you want to view in the Dashboard table.Ensure you select your namespace, cluster name, and other filters at the top of the dashboard to view the required information. Kibana Create dashboards in the Kibana service that is provided by the OpenShift Container Platform cluster logging, and use the dashboards to monitor for specific errors in the logs and set up alerts for when a number of errors occur over a period of time in your Event Streams instance. To install the Event Streams Kibana dashboards, follow these steps:   Ensure you have cluster logging installed.      Download the JSON file that includes the example Kibana dashboards for Event Streams from GitHub.         Navigate to the Kibana homepage on your cluster.     For IBM Cloud Platform Common Services:  Click the hamburger icon in the top left and then expand Monitor Health. Then click Logging to open the Kibana homepage.     For OpenShift Container Platform cluster logging stack: Log in to the OpenShift Container Platform web console using your login credentials. Then follow the instructions to navigate to cluster logging’s Kibana homepage.     Click the Management tab on the left.  Click the Saved Objects.  Click the Import icon and navigate to the JSON file that includes the example Kibana dashboards for Event Streams you have downloaded.  Click the Dashboard tab on the left to view the downloaded dashboards.Other Monitoring Tools You can also use external monitoring tools to monitor the deployed Event Streams Kafka cluster. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitor dashboard.   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Monitoring in the primary navigation. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Select 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/cluster-health/",
        "teaser":null},{
        "title": "Monitoring topic health",
        "collection": "10.0",
        "excerpt":"To gain an insight into the overall health of topics, and highlight potential performance issues with systems producing to Event Streams, you can use the Producers dashboard provided for each topic. The dashboard displays aggregated information about producer activity for the selected topic through metrics such as active producer count, message size, and message produce rates. The dashboard also displays information about each producer that has been producing to the topic. You can expand an individual producer identified by its Kafka producer ID to gain insight into its performance through metrics such as messages produced, message size and rates, failed produce requests and any occurrences where a producer has exceeded a broker quota. The information displayed on the dashboard can also be used to provide insight into potential causes when applications experience issues such as delays or omissions when consuming messages from the topic. For example, highlighting that a particular producer has stopped producing messages, or has a lower message production rate than expected. Important: The Producers dashboard is intended to help highlight producers that might be experiencing issues producing to the topic. You might need to investigate the producer applications themselves to identify an underlying problem. To access the dashboard:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.      Select the topic name from the list you want to view information about.The Producers tab is displayed with the dashboard and details about each producer. You can refine the time period for which information is displayed. You can expand each producer to view details about their activity.     Note: When a new client starts producing messages to a topic, it might take up to 5 to 10 minutes before information about the producer’s activity appears in the dashboard. In the meantime, you can go to the Messages tab to check whether messages are being produced.   Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For information on how to monitor consumer groups for a particular topic, see monitoring Kafka consumer group lag. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/topic-health/",
        "teaser":null},{
        "title": "Monitoring applications with distributed tracing",
        "collection": "10.0",
        "excerpt":"Event Streams 10.0.0 and later versions are built on Strimzi. Strimzi 0.14.0 and later support distributed tracing based on the open-source OpenTracing and Jaeger projects. Distributed tracing provides a way to understand how applications work across processes, services, or networks. Tracing uses the trail they leave behind based on requests made, carrying tracing information in the requests as they pass from system to system. Through tracing, you can monitor applications that work with Event Streams, helping you to understand the shape of the generated traffic, and pinpoint the causes of any potential problems. To use distributed tracing for your Kafka client applications, you add code to your applications that gets called on the client’s send and receive operations. This code adds headers to the client’s messages to carry tracing information. As such, Event Streams provides support for tracing through IBM Cloud Pak for Integration. Your applications can send tracing data into the IBM Cloud Pak for Integration Operations Dashboard runtime as “external applications”. Deployment architecture To send tracing data to the IBM Cloud Pak for Integration Operations Dashboard, the Kafka client application must be deployed into the same OpenShift Container Platform cluster as IBM Cloud Pak for Integration. The application runs in a pod into which two sidecar containers are added, one for the tracing agent and one for the tracing collector. The Kafka client OpenTracing code runs as part of your application. It forwards tracing spans over UDP to the agent. The agent decides which spans are to be sampled and forwards those over TCP to the collector. The collector forwards the data securely to the Operations Dashboard Store in the Operations Dashboard namespace.  The namespace in which the application runs must be registered with the Operations Dashboard. The registration process creates a secret which is used by the collector to communicate securely with the Operations Dashboard Store. Preparing your application to use tracing For detailed instructions about how to use Operations Dashboard with external applications, see the IBM Cloud Pak for Integration documentation. At a high level, the steps required are as follows. Step 1 - Configure the Operations Dashboard to display external applications tracing data To enable the Operations Dashboard to display tracing data from external applications:   Log into IBM Cloud Pak for Integration.  Open the Operations Dashboard Web Console by clicking Tracing in the Platform Navigator.  Navigate to System Parameters &gt; Display in the Manage section of the console.  In the Display settings, set Show external app data in the dashboards to true, and click Update.Now the Operations Dashboard is ready to receive data from external applications. Step 2 - Modify your application code to enable tracing The most convenient way to enable tracing in a Kafka application is to use the Kafka client integration which has been contributed to the OpenTracing project. Then you have a choice of configuring OpenTracing interceptors and using the regular KafkaProducer and KafkaConsumer classes, or using Tracing variants of the KafkaProducer and KafkaConsumer which wrap the real classes. The latter is more flexible but requires additional code changes to the application. There are sample applications which show you how to do this. You can clone the repository and build them yourself, or copy the techniques for your own applications. Step 3 - Deploy your application with the agent and collector sidecar containers After it is built, you can deploy your application, complete with the Operations Dashboard agent and collector containers. Without these additional containers, your application will not be able to communicate with the Operations Dashboard Store and your tracing data will not appear. The sample applications include an example of the Kubernetes deployment that includes these containers. Slightly unusually, when you deploy your application, you’ll notice that it’s running normally but the two sidecar container are not starting successfully. That’s because they depend upon a Kubernetes secret that contains credentials to connect to the OD Store. The next step creates that secret and enables the containers to start. Step 4 - Complete the registration process for the application The final stage is to use the Operations Dashboard Web Console for registering the external application to Operations Dashboard. To enable the Operations Dashboard to display tracing data from external applications:   Log into IBM Cloud Pak for Integration.  Open the Operations Dashboard Web Console by clicking Tracing in the Platform Navigator.  Navigate to System Parameters &gt; Registration requests in the Manage section of the console.  In the list of registration requests, approve the request for your application’s namespace.  Copy the command displayed by the console, and run it to create the Kubernetes secret which enables your application to send data to the Operations Dashboard Store.Using the Operations Dashboard to view tracing information You can use the IBM Cloud Pak for Integration Operations Dashboard to view tracing information and analyze spans. For more information about setting up, using, and managing the dashboard, see the IBM Cloud Pak for Integration documentation. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/tracing/",
        "teaser":null},{
        "title": "Monitoring Kafka consumer group lag",
        "collection": "10.0",
        "excerpt":"You can monitor the consumer lag for Kafka clients connecting to IBM Event Streams. This information is available through both the Event Streams UI and CLI. Consumer lag Each partition will have a consumer within a consumer group with information relating to its consumption as follows:   Client ID and Consumer ID: each partition will have exactly one consumer in the consumer group, identifiable by a Client ID and Consumer ID.  Current offset: the last committed offset of the consumer.  Log end offset: the highest offset of the partition.  Offset lag: the difference between the current consumer offset and the highest offset, which shows how far behind the consumer is.Using the Event Streams UI To access the consumer groups side panel in the Event Streams UI, do the following:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.  Locate your topic using the Name column and click the row for the topic.  Click the Consumer groups tab.  The Consumer groups dashboard will display all consumer groups for the selected topic.Click the consumer group of interest from the Consumer group ID column.The consumer group side panel should now be displayed on screen.This side panel will display a table containing consumer group information for each partition of the topic. Using the Event Streams CLI To access information about a consumer group in the Event Streams CLI, do the following:   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  To list all consumer groups on a cluster, run:cloudctl es groups  To list information about a consumer group, run:cloudctl es group --group &lt;consumer-group-id&gt;where &lt;consumer-group-id&gt; is the name of the consumer group of interest.The CLI will print a table containing consumer group information for each partition of the topic. The following example shows the information the command returns for a consumer group called my-consumer: $ cloudctl es group --group my-consumer&gt;Details for consumer group my-consumerGroup ID            Statemy-consumer         StableTopic      Partition   Current offset   End offset   Offset lag   Client        Consumer        Hostmy-topic   2           999              1001         2            some-client   some-consumer   some-hostmy-topic   0           1000             1001         1            some-client   some-consumer   some-hostmy-topic   1           1000             1000         0            some-client   some-consumer   some-hostOKTo view other Kafka-related metrics, consider configuring a Kafka Exporter. For information on how to monitor the general health of a particular topic, see monitoring topic health. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/consumer-lag/",
        "teaser":null},{
        "title": "Monitoring with external tools",
        "collection": "10.0",
        "excerpt":"You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. You must configure your installation to set up access for external monitoring tools. For examples about setting up monitoring with external tools such as Datadog, Prometheus, and Splunk, see the tutorials page. If you have a tool or service you want to use to monitor your clusters, you can contact support. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/external-monitoring/",
        "teaser":null},{
        "title": "Modifying installation settings",
        "collection": "10.0",
        "excerpt":"You can modify the configuration settings for your existing Event Streams installation by using the OpenShift Container Platform web console or the oc command line tool. The configuration changes are applied by updating the EventStreams custom resourceYou can modify existing values and introduce new properties as outlined under configuration settings.Note: Some settings might cause affected components of your Event Streams instance to restart. For examples on changes you might want to make, see scaling your Event Streams instance. Using the OpenShift Container Platform web console To modify configuration settings by using the OpenShift Container Platform web console:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Click the YAML tab to edit the custom resource.  Make the required changes on the page, or you can click Download and make the required changes in a text editor.If you clicked Download you will need to drag and drop the modified custom resource file onto the page so that it updates in the web console.  Click the Save button to apply your changes.Using the OpenShift Container Platform CLI To modify configuration settings by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to edit your EventStreams custom resource in your default editor:oc edit eventstreams &lt;instance_name&gt;  Make the required changes in your editor.  Save and quit the editor to apply your changes.Modifying Kafka broker configuration settings Kafka supports a number of key/value pair settings for broker configuration, typically provided in a properties file. In Event Streams, these settings are defined in an EventStreams custom resource under the spec.strimziOverrides.kafka.config property. For example, to set the number of I/O threads to 24 you can add the spec.strimziOverrides.kafka.config[\"num.io.threads\"] property: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    kafka:      # ...      config:         # ...         num.io.threads: 24You can specify all the broker configuration options supported by Kafka except from those managed directly by Event Streams. For further information, see the list of supported configuration options. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/modifying-installation/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "10.0",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. The pre-requisite guidance gives various examples of different production configurations on which you can base your deployment. To verify it meets your requirements, you should test the system with a workload that is representative of the expected throughput. For this purpose, IBM Event Streams provides a workload generator application to test different message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Modifying the settings These settings are defined in the EventStreams custom resource under the spec.strimziOverrides property. For more information on modifying these settings see modifying installation Increase the number of Kafka brokers in the cluster The number of Kafka brokers is defined in the EventStreams custom resource in the spec.strimziOverrides.kafka.replicas property. For example to configure Event Streams to use 6 Kafka brokers: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      replicas: 6Increase the CPU request or limit settings for the Kafka brokers The CPU settings for the Kafka brokers are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.kafka.resources. For example to configure Event Streams Kafka brokers to have a CPU request set to 2 CPUs and limit set to 4 CPUs: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      resources:        requests:          cpu: 2000m        limits:          cpu: 4000mA description of the syntax for these values can be found in the Kubernetes documentation. Increase the memory request or limit settings for the Kafka brokers and ZooKeeper nodes The memory settings for the Kafka brokers are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.kafka.resources. The memory settings for the ZooKeeper nodes are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.zookeeper.resources. For example to configure Event Streams Kafka brokers and ZooKeeper nodes to have a memory request set to 4GB and limit set to 8GB: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      resources:        requests:          memory: 4096Mi        limits:          memory: 8096Mi    zookeeper:      # ...      resources:        requests:          memory: 4096Mi        limits:          memory: 8096MiThe syntax for these values can be found in the Kubernetes documentation. Modifying the resources available to supporting components The resource settings for each supporting component are defined in the EventStreams custom resource in their corresponding component key the requests and limits properties under spec.&lt;component&gt;.resources.For example to configure the Schema Registry to have a memory request set to 4GB and limit set to 8GB: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  schemaRegistry:    # ...    resources:      requests:        memory: 4096Mi      limits:        memory: 8096MiThe syntax for these values can be found in the Kubernetes documentation. Modifying the JVM settings for Kafka brokers If you have specific requirements, you can modify the JVM settings for the Kafka brokers. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. Note: Only a selected subset of the available JVM options can be configured. JVM settings for the Kafka brokers are defined in the EventStreams custom resource in the spec.strimziOverrides.kafka.jvmOptions propety. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      jvmOptions:        -Xms: 4096m        -Xmx: 4096mIncrease the disk space available to each Kafka broker The Kafka brokers need sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods for messages, increased message sizes and additional topic partitions. The amount of storage made available to Kafka brokers is defined at the time of installation in the EventStreams custom resource in the spec.strimziOverrides.kafka.storage.size property. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      storage:        # ...        size: 100Gi","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "10.0",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For more information about quotas, see the Kafka documentation. Setting quotas To configure Kafka quotas, either update an existing KafkaUser, or manually create a new KafkaUser by using the OpenShift Container Platform web console or CLI, ensuring that the configuration has a quotas section with the relevant quotas defined. For example: spec:   #...   quotas:      producerByteRate: 1048576      consumerByteRate: 2097152      requestPercentage: 55Decide what you want to limit by defining one or more of the following quota types:             Property      Type      Description                  producerByteRate      integer      This quota limits the number of bytes that a producer application is allowed to send per second.              consumerByteRate      integer      This quota limits the number of bytes that a consumer application is allowed to receive per second.              requestPercentage      integer      This quota limits all clients based on thread utilisation.      Note: Quotas can only be applied to individual KafkaUser instances. It is advised to apply a quota to an existing KafkaUser, as described in the following sections. You can create a Kafkauser beforehand. Using the OpenShift Container Platform web console To update an existing KafkaUser by using the OpenShift Container Platform web console:   Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Kafka User tab, then select the KafkaUser instance you want to update from the list of existing users.  Expand the Actions dropdown, and select the Edit KafkaUser option.  In the YAML editor, add a quotas section with the required quotas.Using the OpenShift Container Platform CLI To update an existing KafkaUser by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run one of the following commands to add or update one or more quota types:     Setting producerByteRate:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"producerByteRate\\\":$QUOTA}}}\" \\  --type=merge        Setting consumerByteRate:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"consumerByteRate\\\":$QUOTA}}}\" \\  --type=merge        Setting requestPercentage:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"requestPercentage\\\":$QUOTA}}}\" \\  --type=merge        Setting all quotas:       PRODUCER_QUOTA=&lt;integer value&gt;; \\  CONSUMER_QUOTA=&lt;integer value&gt;; \\  PERCENTAGE_QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"producerByteRate\\\":$PRODUCER_QUOTA, \\  \\\"consumerByteRate\\\":$CONSUMER_QUOTA, \\  \\\"requestPercentage\\\":$PERCENTAGE_QUOTA}}}\" \\  --type=merge      ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/quotas/",
        "teaser":null},{
        "title": "Managing a multizone setup",
        "collection": "10.0",
        "excerpt":"If you have set up your Event Streams installation to use multiple availability zones, follow the guidance here if one of your nodes containing Kafka or ZooKeeper experiences problems. Topic configuration Only create Kafka topics where the minimum in-sync replicas configuration can be met in the event of a zone failure. This requires considering the minimum in-sync replicas value in relation to the replication factor set for the topic, and the number of availability zones being used for spreading out your Kafka brokers. For example, if you have 3 availability zones and 6 Kafka brokers, losing a zone means the loss of 2 brokers. In the event of such a zone failure, the following topic configurations will guarantee that you can continue to produce to and consume from your topics:   If the replication factor is set to 6, then the suggested minimum in-sync replica is 4.  If the replication factor is set to 5, then the suggested minimum in-sync replica is 3.Updating an existing installation If you are updating an existing installation, for example, adding a new Kafka broker node or replacing a failing node that contains a Kafka broker, you can label another node with the zone label. When the new node label is applied, Kubernetes will then be able to schedule a Kafka broker to run on that node. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/managing-multizone/",
        "teaser":null},{
        "title": "Stopping and starting Event Streams",
        "collection": "10.0",
        "excerpt":"You can stop or shut down your Event Streams instance if required.You might want to do this in cases of hardware maintenance or during scheduled power outages. Use the following instructions to gracefully shut down your Event Streams instance. The instance can be started again following the starting up Event Streams instructions. Stopping Event Streams To shut down your cluster gracefully, uninstall the operator to ensure your Event Streams instance is no longer managed and scale all components to 0 replicas as follows: Stop the operator   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Actions dropdown and click Uninstall Operator.Scale down components After the operator has uninstalled, you can safely scale down all components to 0 replicas, ensuring no pods are running. To do this for all components of type Deployment, run: kubectl get deployments -n &lt;namespace&gt; -l app.kubernetes.io/instance=&lt;instance-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; deployment.txt &amp;&amp; while read -ra deploy; do kubectl -n &lt;namespace&gt; scale --replicas=0 deployment/${deploy}; done &lt; deployment.txtWhere &lt;namespace&gt; is the namespace your Event Streams instance is installed in and &lt;instance-name&gt; is the name of your Event Streams instance. Note: This saves the state of your deployments to a file called deployment.txt in the current working directory. To do this for all components of type StatefulSet, run: kubectl get sts -n &lt;namespace&gt; -l app.kubernetes.io/instance=&lt;instance-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; sts.txt &amp;&amp; while read -ra sts; do kubectl -n &lt;namespace&gt; scale --replicas=0 sts/${sts}; done &lt; sts.txtWhere &lt;namespace&gt; is the namespace your Event Streams instance is installed in and &lt;instance-name&gt; is the name of your Event Streams instance. Note: This saves the state of your stateful sets to a file called sts.txt in the current working directory. Starting up Event Streams To scale the Event Streams instance back up to the original state, install the Event Streams operator again following the steps in the installing section.   Install the operator, ensuring it is configured with the same installation mode as used for the previous installation.  Wait for the operator to scale all resources back up to their original state. You can watch the progress by running:oc get pods --watch","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/administering/stopping-starting/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "10.0",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/intro/",
        "teaser":null},{
        "title": "Gathering logs",
        "collection": "10.0",
        "excerpt":"To help IBM Support troubleshoot any issues with your IBM Event Streams instance, use the oc adm must-gather command to capture the must gather logs. The logs are stored in a folder in the current working directory. To gather diagnostic logs, run the following commands as Cluster Administrator:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to capture the logs:     oc adm must-gather --image=ibmcom/ibm-eventstreams-must-gather -- gather -m eventstreams -n &lt;namespace&gt;   To gather system level diagnostics in addition to the Event Streams information:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to capture the logs:     oc adm must-gather --image=ibmcom/ibm-eventstreams-must-gather -- gather -m system,eventstreams -n &lt;namespace&gt;   These logs are stored in an archive file in a folder in the current working directory.For example, the must-gather archive could be located on the path: must-gather.local.6409880158745169979/docker-io-ibmcom-ibm-eventstreams-must-gather-sha256-23a438c8f46037a72b5378b29decad00e871d11f19834ba75b149326847e7c05/cloudpak-must-gather-20200624100001.tgz","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/gathering-logs/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "10.0",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible symptoms and causes. Insufficient system resources You can specify the memory and CPU requirements when the IBM Event Streams instance is installed using the EventStreams custom resource. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims - occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory - occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU - occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). Ensure that resource requests and limits do not exceed the total memory available. For example, if a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which might reside on the same node. To correct these issues, increase the amount of system resources available or re-install the IBM Event Streams instance with lower resource requirements. Problems with secrets Before installing the operator, configure secrets with your entitlement key for the IBM Container software library. This will enable container images to be pulled from the registry. See the installation section of the documentation for more information. If you do not prepare the required secrets, all pods will fail to start with ImagePullBackOff errors. In this case, configure the required secrets and allow the pod to restart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "10.0",
        "excerpt":"Symptoms After providing a list of topic names when creating a geo-replicator, only the first topic successfully replicates data. The additional topics specified are either not displayed in the destination cluster UI Topics view, or are displayed as &lt;origin-release&gt;.&lt;topic-name&gt; but are not enabled for geo-replication. Causes When using the CLI to set up replication, the list of topics to geo-replicate included spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, if you specified the topics parameter with spaces such as: --topics MyTopicName1, MyTopicName2, MyTopicName3--topics \"MyTopicName1, MyTopicName2, MyTopicName3\"Remove the spaces between the topic names and re-apply the command using a topics parameter with no spaces such as: --topics MyTopicName1,MyTopicName2,MyTopicName3","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "10.0",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutException Causes This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content, adding by uncommenting either the SCRAM or Mutual TLS authentication settings depending on how the external listener has been configured. bootstrap.servers=&lt;kafka_bootstrap_route_url&gt;# SCRAM Properties#security.protocol=SASL_SSL#sasl.mechanism=SCRAM-SHA-512#sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";# Mutual auth properties#security.protocol=SSL#ssl.keystore.location=&lt;java_keystore_file_location&gt;#ssl.keystore.password=&lt;java_keystore_password&gt;# TLS Propertiesssl.protocol=TLSv1.2ssl.truststore.location=&lt;java_truststore_file_location&gt;ssl.truststore.password=&lt;java_truststore_password&gt;Replace the &lt;kafka_bootstrap_route_url&gt; with the address of the Kafka bootstrap route. If you used SCRAM authentication for the external listener, replace &lt;username&gt; with the SCRAM user name and &lt;password&gt; with the SCRAM user’s password. If you used Mutual TLS authentication for the external listener, replace &lt;java_keystore_file_location&gt; with the location of a key store containing the client certificate and &lt;java_keystore_password&gt; with the password for the key store. Finally, replace &lt;java_truststore_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), and &lt;java_truststore_password&gt; with the password for the trust store. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "10.0",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content, adding by uncommenting either the SCRAM or Mutual TLS authentication settings depending on how the external listener has been configured. bootstrap.servers=&lt;kafka_bootstrap_route_url&gt;# SCRAM Properties#security.protocol=SASL_SSL#sasl.mechanism=SCRAM-SHA-512#sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";# Mutual auth properties#security.protocol=SSL#ssl.keystore.location=&lt;java_keystore_file_location&gt;#ssl.keystore.password=&lt;java_keystore_password&gt;# TLS Propertiesssl.protocol=TLSv1.2ssl.truststore.location=&lt;java_truststore_file_location&gt;ssl.truststore.password=&lt;java_truststore_password&gt;Replace the &lt;kafka_bootstrap_route_url&gt; with the address of the Kafka bootstrap route. If you used SCRAM authentication for the external listener, replace &lt;username&gt; with the SCRAM user name and &lt;password&gt; with the SCRAM user’s password. If you used Mutual TLS authentication for the external listener, replace &lt;java_keystore_file_location&gt; with the location of a key store containing the client certificate and &lt;java_keystore_password&gt; with the password for the key store. Finally, replace &lt;java_truststore_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), and &lt;java_truststore_password&gt; with the password for the trust store. When running the kafka-console-consumer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "10.0",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log in to the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "10.0",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the cluster and initialized the command line tool. Resolving the problem Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt; Initialize the IBM Event Streams CLI as follows: cloudctl es init Re-run the failed operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "10.0",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "10.0",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "10.0",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes To access the Event Streams UI, the user must either have the Cluster Administrator role or the Administrator role and be in a team with a namespace resource added for the namespace containing the Event Streams instance. If neither of these applies, the error will be displayed. Resolving the problem Assign access to users with an Administrator role by ensuring they are in a team with access to the correct namespace. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "The UI cannot load data",
        "collection": "10.0",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Monitor and the Topics &gt; Producers tabs do not load, displaying the following message:  Causes The IBM Cloud Pak for Integration monitoring service might not be installed. In general, the monitoring service is installed by default during the  IBM Cloud Pak for Integration installation. However, some deployment methods do not install the service. Resolving the problem Install the IBM Cloud Pak for Integration monitoring service from the Catalog or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/problem-with-piping/",
        "teaser":null},{
        "title": "UI shows black images in Firefox",
        "collection": "10.0",
        "excerpt":"Symptoms Images in the Event Streams UI are rendered as a black-filled shape instead of the correct image as shown in the following example:  Causes Mozilla Firefox 76 and earlier versions block the inline CSS styling in SVG images. This is because they incorrectly apply the page-level Content Security Policy against the SVG images. Resolving the problem Update your Firefox version to 77 or later as the issue has been resolved in Firefox 77. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/firefox-black-images/",
        "teaser":null},{
        "title": "Event Streams not installing due to Security Context Constraint (SCC) issues",
        "collection": "10.0",
        "excerpt":"Symptoms Event Streams components report that an action is forbidden, stating that it is unable to validate against any security context constraint. This could result in symptoms such as:       Installation of the operator is pending and eventually times out.           Navigating to the Conditions section for the specific operator deployment under Workloads &gt; Deployment will display a message similar to the following example:         pods \"eventstreams-cluster-operator-55d6f4cdf7-\" is forbidden: unable to validate against any security context constraint: [spec.volumes[0]: Invalid value: \"secret\": secret volumes are not allowed to be used spec.volumes[1]: Invalid value: \"secret\": secret volumes are not allowed to be used]                          Creating an instance of Event Streams is pending and eventually times out.           Navigating to the Events tab for the specific instance stateful set under Workloads &gt; Stateful Sets displays a message similar to the following example:        create Pod quickstart-zookeeper-0 in StatefulSet quickstart-zookeeper failed error: pods \"quickstart-zookeeper-0\" is forbidden: unable to validate against any security context constraint: [spec.containers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true]                          On a running instance of Event Streams, a pod that has bounced never comes back up.           Navigating to the Conditions section for the specific instance deployment under Workloads &gt; Deployment will display a message similar to the following example:        is forbidden: unable to validate against any security context constraint: [spec.initContainers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true spec.containers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true]                    Causes Event Streams has been tested with the default restricted Security Context Constraint (SCC) provided by the OpenShift Container Platform. If a user or any other operator applies a custom SCC that removes permissions required by Event Streams, then this will cause issues. Resolving the problem Apply the custom Security Context Constraint (SCC) provided by Event Streams to enable permissions required by the product. To do this, edit the eventstreams-scc.yaml file to add your namespace and apply it using oc tool as follows:       Edit the eventstreams-scc.yaml and add the namespace where your Event Streams instance is installed.         Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.         Run the following command to apply the SCC:     oc apply -f &lt;custom_scc_file_path&gt;     For example: oc apply -f eventstreams-scc.yaml   ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/default-scc-issues/",
        "teaser":null},{
        "title": "Not authorized error when building Maven schema registry project",
        "collection": "10.0",
        "excerpt":"Symptoms When building a Maven project that pulls from the IBM Event Streams schema registry, the build fails with an error similar to the following message: Could not resolve dependencies for project &lt;project&gt;: Failed to collect dependencies at com.ibm.eventstreams.schemas:&lt;schema-name&gt;:jar:&lt;version&gt;: Failed to read artifact descriptor for com.ibm.eventstreams.schemas:&lt;schema-name&gt;:jar:&lt;version&gt; Could not transfer artifact com.ibm.eventstreams.schemas:&lt;schema-name&gt;:pom:&lt;version&gt; from/to eventstreams-schemas-repository (https://&lt;schema-registry-route&gt;/files/schemas): Not authorizedCauses The Maven settings.xml code snippets provided in the Event Streams UI for both SCRAM and Mutual TLS are missing the &lt;servers&gt; XML elements. In addition, the same error can also be displayed when using SCRAM credentials, and the Authorization value does not have the following format: Basic &lt;scram-token&gt; Resolving the problem Update the Maven settings.xml file to have the following format: &lt;settings&gt;  &lt;servers&gt;    &lt;server&gt;      &lt;id&gt;eventstreams-schemas-repository&lt;/id&gt;      &lt;configuration&gt;        &lt;httpHeaders&gt;          &lt;property&gt;            &lt;name&gt;Authorization&lt;/name&gt;            &lt;value&gt;Basic &lt;scram-token&gt;&lt;/value&gt;          &lt;/property&gt;        &lt;/httpHeaders&gt;      &lt;/configuration&gt;    &lt;/server&gt;  &lt;/servers&gt;  &lt;profiles&gt;    &lt;profile&gt;      # ...Also, if using SCRAM credentials, ensure that the Authorization element has the format Basic &lt;scram-token&gt;, where the &lt;scram-token&gt; value is a Base64-encoded string in the following format: &lt;scram-username&gt;:&lt;scram-password&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/schema-registry-not-authorized-error/",
        "teaser":null},{
        "title": "Client receives AuthorizationException when communicating with brokers",
        "collection": "10.0",
        "excerpt":"Symptoms When executing operations with a Java client connected to Event Streams, the client fails with an error similar to the following message: [err] [kafka-producer-network-thread | my-client-id] ERROR org.apache.kafka.clients.producer.internals.Sender - [Producer clientId=my-client-id] Aborting producer batches due to fatal error[err] org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed.Similar messages might also be displayed when using clients written in other languages such as NodeJS. Causes The KafkaUser does not have the authorization to perform one of the operations:   If there is an authorization error with a topic resource, then a TOPIC_AUTHORIZATION_FAILED (error code: 29) will be returned.  If there is an authorization error with a group resource, then a GROUP_AUTHORIZATION_FAILED (error code: 30) will be returned.  If there is an authorization error with a cluster resource, then a CLUSTER_AUTHORIZATION_FAILED (error code: 31) will be returned.  If there is an authorization error with a transactionalId resource, then a TRANSACTIONAL_ID_AUTHORIZATION_FAILED (error code: 32) will be returned.In Java, the errors are thrown in the format &lt;resource&gt;AuthorizationException. Other clients might return the error directly or translate it into an error with a similar name. Resolving the problem Ensure the KafkaUser has the required permissions as described in managing access. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/authorization-failed-exceptions/",
        "teaser":null},{
        "title": "Required value error when installing on OpenShift 4.5.0-4.5.5",
        "collection": "10.0",
        "excerpt":"Symptoms When installing an Event Streams instance on OpenShift Container Platform 4.5.0 to 4.5.5 by using the UI, the installation stops and an error about missing values is displayed after clicking Create. The following is an example of the error: Error \"Required value\" for field \"spec.strimziOverrides.kafka.listenrers\".Causes Due to a known issue, installing on OpenShift Container Platform 4.5.0 to 4.5.5 by using the UI removes configuration details and applies an incomplete definition. Resolving the problem When installing Event Streams on OpenShift Container Platform versions 4.5.0 to 4.5.5 by using the OpenShift UI, go to the YAML View, select a sample configuration on the right, and click Try It to load a valid sample. Alternatively, upgrade your OpenShift Container Platform version to 4.5.6. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/required-value-error/",
        "teaser":null},{
        "title": "Operator is generating constant log output",
        "collection": "10.0",
        "excerpt":"Symptoms The log file for the Event Streams operator pod shows constant looping of reconciliations when installed on Red Hat OpenShift Container Platform 4.5 or later. 2020-09-22 16:01:25 INFO  AbstractOperator:173 - [lambda$reconcile$3] Reconciliation #4636(watch) EventStreams(es/example): EventStreams example should be created or updated2020-09-22 16:01:25 INFO  OperatorWatcher:35 - [eventReceived] Reconciliation #4642(watch) EventStreams(es/example): EventStreams example in namespace es was MODIFIEDCauses The Event Streams operator is notified that an instance of Event Streams has changed and needs to be reconciled. When the reconciliation is complete, the status update triggers a notification which causes a new reconciliation. Resolving the problem Contact IBM Support to request a fix, and include issue number ES-115 in your correspondence. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/operator-keeps-reconciling/",
        "teaser":null},{
        "title": "504 timeout error when viewing consumer groups in the Event Streams UI",
        "collection": "10.0",
        "excerpt":"Symptoms When viewing consumer groups in the Event Streams UI, the page displays a loading indicator while it fetches the groups. However, the groups are not displayed and a 504 timeout error is shown instead. Causes As the number of consumer groups increases it will reach a limit where the service that retrieves the groups for the UI is unable to respond before the UI considers the request to have timed out. Resolving the problem Please contact IBM Support to request a fix, and include issue number ES-135 in your correspondence. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.0/troubleshooting/504-ui-consumer-groups/",
        "teaser":null},{
        "title": "Home",
        "collection": "10.1",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "10.1",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the Apache Kafka® project and incorporates the open-source Strimzi technology. Event Streams version 10.1.0 includes Kafka release 2.6.0, and supports the use of all Kafka interfaces. IBM Event Streams uses Strimzi to deploy Apache Kafka in a resilient and manageable way, and provides a range of additional capabilities to extend the core functionality. IBM Event Streams features include:       Apache Kafka deployed by Strimzi that distributes Kafka brokers across the nodes of a Red Hat OpenShift Container Platform cluster, creating a highly-available and resilient deployment.         An administrative user interface (UI) focused on providing essential features for managing production clusters, in addition to enabling developers to get started with Apache Kafka. The Event Streams UI facilitates topic lifecycle control, message filtering and browsing, client metric analysis, schema and geo-replication management, connection details and credentials generation, and a range of tools, including a sample starter and producer application.         A command-line interface (CLI) to enable manual and scripted cluster management. For example, you can use the Event Streams CLI to inspect brokers, manage topics, deploy schemas, and manage geo-replication.         Geo-replication of topics between clusters to enable disaster recovery.         A REST API for producing messages to Event Streams topics, expanding event source possibilities.         A schema registry to support the definition and enforcement of message formats between producers and consumers. Event Streams includes the open-source Apicurio Registry for managing schemas.         Health check information to help identify issues with your clusters and brokers.         Secure by default production cluster templates with authentication, authorization and encryption included, and optional configurations to simplify proof of concept or lite development environments.         Granular security configuration through Kafka access control lists to configure authorization and quotas for connecting applications.   Operators Kubernetes Operators are designed to simplify the deployment and maintenance of complex applications. They do this by packaging up and abstracting away domain-specific knowledge of how to deploy, configure and operate the application and its component parts. An Operator can then extend Kubernetes by providing new Kubernetes constructs to support these abstractions, simplifying the initial deployment and subsequent lifecycle management of the application. Strimzi uses Operators in this manner to facilitate the deployment of Kafka clusters. Event Streams builds on top of Strimzi to deploy not only the Kafka components but also the additional features outlined earlier. A new Kubernetes resource EventStreams is provided (in Kubernetes referred to as a “kind”), to allow the definition of a complete deployment that brings together the components provided by Strimzi and Event Streams. This information is defined in a standard YAML document and deployed in a single operation. Operators provided by Event Streams The following diagram shows the operators involved in an Event Streams deployment along with the resources they manage. Event Streams builds on the Strimzi core, and adds additional components to extend the base capabilities.  The EventStreams Cluster Operator The Strimzi Cluster Operator manages the deployment of core components within the Kafka cluster such as Kafka and ZooKeeper nodes. The EventStreams Cluster Operator extends the Strimzi Cluster Operator to provision the additional components provided by  Event Streams alongside these core components. A new custom resource Type called EventStreams is provided to manage this overall deployment. The Entity Operator Kafka provides authentication and authorization through Users and Access Control Lists. These define the operations that users can perform on specific resources within the cluster. The User operator provides a new custom resource type called KafkaUser to manage these Kafka users and associated Access Control Lists and is a mandatory component of an Event Streams deployment. Kafka topics are a resource within the cluster to which a series of records can be produced. The optional Topic operator provides a new custom resource type called KafkaTopic to manage these Kafka topics. By default, the Topic operator is not part of the Event Streams cluster. Instead, to create and manage Kafka topics, use the Event Streams UI, CLI and REST API provided mechanisms. If there is a need to have Kafka topics represented as Kubernetes resources, the optional Topic operator can be included in the EventStreams definition, and will then be deployed as a container alongside the User operator within the Entity operator Pod. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "10.1",
        "excerpt":"Find out what is new in IBM Event Streams version 10.1.0. Apicurio Registry Event Streams version 10.1.0 includes the open-source Apicurio Registry for managing schemas. The Event Streams schema registry provided in earlier versions is deprecated in version 10.1.0 and later. If you are upgrading to Event Streams version 10.1.0 from an earlier version, you can migrate to the Apicurio Registry from the deprecated schema registry. Support for Linux on IBM Z In addition to Linux® 64-bit (x86_64) systems, Event Streams 10.1.0 is also supported on Linux on IBM® Z systems. Kafka version upgraded to 2.6.0 Event Streams version 10.1.0 includes Kafka release 2.6.0, and supports the use of all Kafka interfaces. Default resource requirements have changed The minimum footprint for Event Streams has been reduced. See the updated tables for the resource requirements. Support for Cruise Control Event Streams supports the deployment and use of Cruise Control.Use Cruise Control to optimize your Kafka brokers by rebalancing the Kafka cluster based on a set of defined goals.Find out more about Cruise Control with Event Streams. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "10.1",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers (Kafka brokers). The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "10.1",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the required rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "10.1",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "10.1",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "10.1",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "10.1",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "10.1",
        "excerpt":"To try out Event Streams, you have the following options:   Create a subscription for a fully managed Kafka service on IBM Cloud.      Install IBM Event Streams on the Red Hat OpenShift Container Platform for development purposes or to set up Kafka for production use, and benefit from both the support of IBM and the open-source community.     Event Streams comes with a host of useful features such as a user interface (UI) to help get started with Kafka and help operate a production cluster, geo-replication of topics between clusters, a schema registry to enforce correct and consistent message formats, a connector catalog, and more.         Use Strimzi if you want to install your own basic Kafka cluster on Kubernetes for testing and proof-of-concept purposes.     As Event Streams is based on Strimzi, you can easily move your deployment to Event Streams later, and keep your existing configurations and preferences from the Strimzi setup. Moving to Event Streams adds the benefit of full enterprise-level support from IBM.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/trying-out/",
        "teaser":null},{
        "title": "Prerequisites",
        "collection": "10.1",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. Container environment IBM Event Streams 10.1.0 is supported on the Red Hat OpenShift Container Platform. Version 10.1.0 is installed by the Event Streams operator 2.1.0, and includes Kafka version 2.6.0. For an overview of supported component and platform versions, see the support matrix. Ensure you have the following set up for your environment:   A supported version of OpenShift Container Platform installed. See the support matrix for supported versions.  The OpenShift Container Platform CLI installed.  The IBM Cloud Pak CLI (cloudctl) installed.Hardware requirements Ensure your hardware can accommodate the resource requirements for your planned deployment. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other Event Streams components which might be required to reside on the same node. For production systems, it is recommended to have Event Streams configured with at least 3 Kafka brokers, and to have one worker node for each Kafka broker. This requires a minimum of 3 worker nodes available for use by Event Streams. Ensure each worker node runs on a separate physical server. See the guidance about Kafka high availability for more information. Resource requirements Event Streams resource requirements depend on several factors. The following sections provide guidance about minimum requirements for a starter deployment, and options for initial production configurations. Minimum resource requirements are as follows.Always ensure you have sufficient resources in your environment to deploy the Event Streams operator together with a development or a production Event Streams operand configuration.             Deployment      CPU (cores)      Memory (Gi)      VPCs (see licensing)                  Operator      1.0      1.0      N/A              Development      8.2      8.2      0.5              Production      12.2      14.2      3.0      Note: Event Streams provides samples to help you get started with deployments. The resource requirements for these specific samples are detailed in the planning section. If you do not have an Event Streams installation on your system yet, always ensure you include the resource requirements for the operator together with the intended Event Streams instance requirements (development or production). Important: Licensing is based on the number of Virtual Processing Cores (VPCs) used by your Event Streams instance. See licensing considerations for more information. For a production installation of Event Streams, the ratio is 1 license required for every 1 VPC being used. For a non-production installation of Event Streams, the ratio is 1 license required for every 2 VPCs being used. Event Streams is a Kubernetes operator-based release and uses custom resources to define your Event Streams configurations.The Event Streams operator uses the declared required state of your Event Streams in the custom resources to deploy and manage the entire lifecycle of your Event Streams instances. Custom resources are presented as YAML configuration documents that define instances of the EventStreams custom resource type. The provided samples define typical configuration settings for your Event Streams instance, including broker configurations, security settings, and default values for resources such as CPU and memory defined as “request” and “limit” settings. Requests and limits are Kubernetes concepts for controlling resource types such as CPU and memory.   Requests set the minimum requirements a container requires to be scheduled. If your system does not have the required request value, then the services will not start up.  Limits set the value beyond which a container cannot consume the resource. It is the upper limit within your system for the service. Containers that exceed a CPU resource limit are throttled, and containers that exceed a memory resource limit are terminated by the system.Ensure you have sufficient CPU capacity and physical memory in your environment to service these requirements. Your Event Streams instance can be dynamically updated later through the configuration options provided in the custom resource. Installing Event Streams has two phases:   Install the Event Streams operator: this will deploy the operator that will install and manage your Event Streams instances.  Install one or more instances of Event Streams by applying configured custom resources.Operator requirements The Event Streams operator requires the following minimum resource requirements. Ensure you always include sufficient CPU capacity and physical memory in your environment to service the operator requirements.             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)                  0.2      1.0      1.0      1.0      The Event Streams operator will automatically deploy the required IBM Cloud Platform Common Services if not present. Ensure you include in your calculations the resource requirements for the following Common Services components:   Catalog UI  Certificate Manager  Common Web UI  IAM  Ingress NGINX  Installer  Licensing  Management ingress  Metering  Mongo DB  Monitoring Exporters  Monitoring Grafana  Monitoring Prometheus Ext  Platform APINote: If you are installing Event Streams in an existing IBM Cloud Pak for Integration deployment, the required Common Services might already be installed. Important: Before installing the Event Streams operator, ensure you meet the prerequisites for installing Common Services. Cluster-scoped permissions required The Event Streams operator requires the following cluster-scoped permissions:   Permission to list nodes in the cluster: When the Event Streams operator is deploying a Kafka cluster that spans multiple availability zones, it needs to label the pods with zone information. The permission to list nodes in the cluster is required to retrieve the information for these labels.  Permission to manage admission webhooks: The Event Streams operator uses admission webhooks to provide immediate validation and feedback about the creation and modification of Event Streams instances. The permission to manage webhooks is required for the operator to register these actions.  Permission to manage ConsoleYAMLSamples: ConsoleYAMLSamples are used to provide samples for Event Streams resources in the OpenShift Container Platform web console. The permission to manage ConsoleYAMLSamples is required for the operator to register the setting up of samples.  Permission to view ConfigMaps: Event Streams uses authentication services from IBM Cloud Platform Common Services. The status of these services is maintained in ConfigMaps, so the permission to view the contents of the ConfigMaps allows Event Streams to monitor the availability of the Common Services dependencies.  Permission to list specific CustomResourceDefinitions: This allows Event Streams to identify whether other optional dependencies have been installed into the cluster.  Permission to list ClusterRoles and ClusterRoleBindings: The Event Streams operator uses ClusterRoles created by the Operator Lifecycle Manager (OLM) as parents for supporting resources that the Event Streams operator creates. This is needed so that the supporting resources are correctly cleaned up when Event Streams is uninstalled. The permission to list ClusterRoles is required to allow the operator to identify the appropriate cluster role to use for this purpose.Adding Event Streams geo-replication to a deployment The Event Streams geo-replicator allows messages sent to a topic on one Event Streams cluster to be automatically replicated to another Event Streams cluster. This capability ensures messages are available on a separate system to form part of a disaster recovery plan. To use this feature, ensure you have the following additional resources available. The following table shows the prerequisites for each geo-replicator node.             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  1.0      2.0      2.0      2.0      1.0      For instructions about installing geo-replication, see configuring. Red Hat OpenShift Security Context Constraints Event Streams requires a Security Context Constraint (SCC) to be bound to the target namespace prior to installation. By default, Event Streams uses the default restricted SCC that comes with the OpenShift Container Platform. If you use a custom SCC (for example, one that is more restrictive), or have an operator that updates the default SCC, the changes might interfere with the functioning of your Event Streams deployment. To resolve any issues, apply the SCC provided by Event Streams as described in troubleshooting. Network requirements IBM Event Streams is supported for use with IPv4 networks only. Data storage requirements If you want to set up persistent storage, Event Streams requires block storage configured to use the XFS or ext4 file system. The use of file storage (for example, NFS) is not recommended. For example, you can use one of the following systems:   Kubernetes local volumes  Amazon Elastic Block Store (EBS)  Rook Ceph  Red Hat OpenShift Container StorageIBM Event Streams UI The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterSee the post-installation tasks for information about installing the CLI Kafka clients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8 or later  Oracle Java 8 or laterYou can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning your installation",
        "collection": "10.1",
        "excerpt":"Consider the following when planning your installation of Event Streams. Decide the purpose of your deployment, for example, whether you want to try a starter deployment for testing purposes, or start setting up a production deployment.   Use the sample deployments as a starting point if you need something to base your deployment on.  Size your planned deployment by considering potential throughput, the number of producers and consumers, Kafka performance tuning, and other aspects. For more details, see the performance considerations section.  For production use, and whenever you want your data to be saved in the event of a restart, set up persistent storage.  Consider the options for securing your deployment.  Plan for resilience by understanding Kafka high availability and how to support it, set up multiple availability zones for added resilience, and consider geo-replication to help with your disaster recovery planning.  Consider setting up logging for your deployment to help troubleshoot any potential issues.Sample deployments A number of sample configurations are provided when installing Event Streams on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload.   Development deployments  Production deploymentsThe sample configurations can be found in the OpenShift Container Platform web console as explained in installing or on GitHub. Development deployments If you want to try Event Streams, use one of the development samples when configuring your instance. Installing with these settings is suitable for a starter deployment intended for testing purposes and trying out Event Streams. It is not suitable for production environments. For samples appropriate for production use, see production deployments The following development samples are available:   Lightweight without security  DevelopmentExample deployment: Lightweight without security Overview: A non-production deployment suitable for basic development, and test activities. For environments where minimum resource requirements, persistent storage, access control and encryption are not required. This example provides a starter deployment that can be used if you simply want to try Event Streams with a minimum resource footprint. It installs an Event Streams instance with the following characteristics:   A small single broker Kafka cluster and a single node ZooKeeper.  As there is only 1 broker, no message replication takes place between brokers, and the system topics (message offset and transaction state) are configured accordingly for this.  There is no encryption internally between containers.  External connections use TLS encryption, but no authentication to keep the configuration to a minimum, making it easy to experiment with the platform.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  2.4      8.2      5.4      8.2      0.5      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: This deployment is not suitable for a production system even if storage configuration is applied. This is due to the number of Kafka and ZooKeeper nodes not being appropriate for data persistence and high availability. For a production system, at least three Kafka brokers and ZooKeeper nodes are required for an instance, see production sample deployments later for alternatives. Example deployment: Development Overview: A non-production deployment for experimenting with Event Streams configured for high availability, authentication, and no persistent storage. Suitable for basic development and testing activities. This example provides a starter deployment that can be used if you want to try Event Streams with a minimum resource footprint. It installs an Event Streams instance with the following settings:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  2.8      12.2      5.9      14.2      1.5      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Production deployments To start setting up a production instance, use one of the following samples.   Minimal production  Production 3 brokers  Production 6 brokers  Production 9 brokersImportant: For a production setup, the sample configuration values are for guidance only, and you might need to change them. Ensure you set your resource values as required to cope with the intended usage, and also consider important configuration options for your environment and Event Streams requirements as described in the rest of this planning section. Example deployment: Minimal production Overview: A minimal production deployment for Event Streams. This example provides the smallest possible production deployment that can be configured for Event Streams. It installs an Event Streams instance with the following settings:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.  Kafka tuning settings consistent with 3 brokers are applied as follows:num.replica.fetchers: 3num.io.threads: 24num.network.threads: 9log.cleaner.threads: 6If a storage solution has been configured, the following characteristics make this a production-ready deployment:   Messages are replicated between brokers to ensure that no single broker is a point of failure. If a broker restarts, producers and consumers of messages will not experience any loss of service.  The number of threads made available for replicating messages between brokers, is increased to 3 from the default 1. This helps to prevent bottlenecks when replicating messages between brokers, which might otherwise prevent the Kafka brokers from being fully utilized.  The number of threads made available for processing requests is increased to 24 from the default 8, and the number of threads made available for managing network traffic is increased to 9 from the default 3. This helps prevent bottlenecks for producers or consumers, which might otherwise prevent the Kafka brokers from being fully utilized.  The number of threads made available for cleaning the Kafka log is increased to 6 from the default 1. This helps to ensure that records that have exceeded their retention period are removed from the log in a timely manner, and prevents them from accumulating in a heavily loaded system.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  2.8      12.2      5.9      14.2      3.0      You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 3 brokers Overview: A small production deployment for Event Streams. This example installs a production-ready Event Streams instance similar to the Minimal production setup, but with added resource requirements:   3 Kafka brokers and 3 ZooKeeper nodes.  Internally, TLS encryption is used between containers.  External connections use TLS encryption and SCRAM SHA 512 authentication.  The memory and CPU requests and limits for the Kafka brokers are increased compared to the Minimal production sample described previously to give them the bandwidth to process a larger number of messages.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  14.5      21.2      29.3      31.9      12.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 6 brokers Overview: A medium sized production deployment for Event Streams. This sample configuration is similar to the Production 3 brokers sample described earlier, but with an increase in the following settings:   Uses 6 brokers rather than 3 to allow for additional message capacity.  The resource settings for the individual brokers are the same, but the number of threads made available for replicating messages between brokers is increased to 6 to cater for the additional brokers to manage.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  26.5      33.2      53.0      55.6      24.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. Important: This sample is not provided in the OpenShift Container Platform web console and can only be obtained through GitHub. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Example deployment: Production 9 brokers Overview: A large production deployment for Event Streams. This sample configuration is similar to the Production 6 brokers sample described earlier, but with an increase in the following settings:   Uses 9 Brokers rather than 6 to allow for additional message capacity.  The resource settings for the individual brokers are the same, but the number of threads made available for replicating messages between brokers is increased to 9 to cater for the additional brokers to manage.Resource requirements for this deployment:             CPU request (cores)      CPU limit (cores)      Memory request (Gi)      Memory limit (Gi)      VPCs (see licensing)                  38.5      45.2      76.7      79.3      36.0      Ensure you have sufficient CPU capacity and physical memory in your environment to service at least the resource request values. The resource limit values constrain the amount of resource the Event Streams instance is able to consume. Important: This sample is not provided in the OpenShift Container Platform web console and can only be acquired through GitHub. You can select this sample to use when installing your instance. You can also use it to start out with your configuration, and change settings as required for your purposes. For ways to customize your setup, see configuring. Important: For production deployments, ensure you configure a storage solution by editing the sample as described in enabling persistent storage. Planning for persistent storage If you plan to have persistent volumes, consider the disk space required for storage. Both Kafka and ZooKeeper rely on fast write access to disks. Use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation. If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. The number of Kafka brokers and ZooKeeper servers depends on your setup (for example, see the provided samples described in resource requirements). You either need to create a persistent volume for each physical volume, or specify a storage class that supports dynamic provisioning. Each component can use a different storage class to control how physical volumes are allocated. See the OpenShift Container Platform documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the Cluster Administrator role.   If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. These will then be claimed from a central pool when the IBM Event Streams instance is deployed. The installation will then claim the required number of persistent volumes from this pool.  If these persistent volumes are to be created automatically, ensure a dynamic provisioner is configured for the storage class you want to use. See data storage requirements for information about storage systems supported by Event Streams.Important: When creating persistent volumes for each component, ensure the correct Access mode is set for the volumes as described in the following table.             Component      Access mode                  Kafka      ReadWriteOnce              ZooKeeper      ReadWriteOnce      To use persistent storage, configure the storage properties in your EventStreams custom resource. Planning for security Event Streams has highly configurable security options that range from the fully secured default configuration to no security for basic development and testing. The main security vectors to consider are:   Kafka listeners  Pod-to-Pod communication  UI access  REST endpoints (REST Producer, Admin API, Apicurio Registry)Secure instances of Event Streams will make use of TLS to protect network traffic. Certificates will be generated by default, or you can use custom certificates. Note: If you want to use custom certificates, ensure you configure them before installing Event Streams. Event Streams UI Access As explained in the Managing access section, the IBM Cloud Platform Common Services Identity and Access Management (IAM) is used to bind a role to an identity. By default, the secure Event Streams instance will require an Administrator or higher role to authorize access. To setup LDAP (Lightweight Directory Access Protocol), assign roles to LDAP users, and create teams, see the instructions about configuring LDAP connections. Whilst it is highly recommended to always configure Event Streams with security enabled, it is also possible to configure the Event Streams UI to not require a login, which can be useful for proof of concept (PoC) environments. For details, see configuring Event Streams UI access. REST endpoint security Review the security and configuration settings of your development and test environments.The REST endpoints of Event Streams have a number of configuration capabilities. See configuring access for details. Securing communication between pods By default, Pod-to-Pod encryption is enabled. You can configure encryption between pods when configuring your Event Streams installation. Kafka listeners Event Streams has both internal and external configurable Kafka listeners. Optionally, each Kafka listener can be secured with TLS or SCRAM. Planning for resilience If you are looking for a more resilient setup, or want plan for disaster recovery, consider setting up multiple availability zones and creating geo-replication clusters. Also, set up your environment to support Kafka’s inherent high availability design. Kafka high availability Kafka is designed for high availability and fault tolerance. To reduce the impact of Event Streams Kafka broker failures, configure your installation with at least three brokers and spread them across several Red Hat OpenShift Container Platform worker nodes by ensuring you have at least as many worker nodes as brokers. For example, for 3 Kafka brokers, ensure you have at least 3 worker nodes running on separate physical servers. Kafka ensures that topic-partition replicas are spread across available brokers up to the replication factor specified. Usually, all of the replicas will be in-sync, meaning that they are all fully up-to-date, although some replicas can temporarily be out-of-sync, for example, when a broker has just been restarted. The replication factor controls how many replicas there are, and the minimum in-sync configuration controls how many of the replicas need to be in-sync for applications to produce and consume messages with no loss of function. For example, a typical configuration has a replication factor of 3 and minimum in-sync replicas set to 2. This configuration can tolerate 1 out-of-sync replica, or 1 worker node or broker outage with no loss of function, and 2 out-of-sync replicas, or 2 worker node or broker outages with loss of function but no loss of data. The combination of brokers spread across nodes together with the replication feature make a single Event Streams cluster highly available. Multiple availability zones To add further resilience to your Event Streams cluster, you can split your servers across multiple data centers or zones, so that even if one zone experiences a failure, you still have a working system. Multizone support provides the option to run a single Kubernetes cluster in multiple availability zones within the same region. Multizone clusters are clusters of either physical or virtual servers that are spread over different locations to achieve greater resiliency. If one location is shut down for any reason, the rest of the cluster is unaffected. Note: For Event Streams to work effectively within a multizone cluster, the network latency between zones must not be greater than 20 ms for Kafka to replicate data to the other brokers. Typically, high availability requires a minimum of 3 zones (sites or data centers) to ensure a quorum with high availability for components, such as Kafka and ZooKeeper. Without the third zone, you might end up with a third quorum member in a zone that already has a member of the quorum, consequently if that zone goes down, the majority of the quorum is lost and loss of function is inevitable. OpenShift Container Platform requires a minimum of 3 zones for high availability topologies and Event Streams supports that model. This is different from the traditional primary and backup site configuration, and is a move to support the quorum-based application paradigm. With zone awareness, Kubernetes automatically distributes pods in a replication controller across different zones. For workload-critical components, for example Kafka, ZooKeeper and REST Producer, set the number of replicas of each component to at least match the number of zones. This provides at least one replica of each component in each zone, so in the event of loss of a zone the service will continue using the other working zones. For information about how to prepare multiple zones, see preparing for multizone clusters. Geo-replication Consider configuring geo-replication to aid your disaster recovery and resilience planning. You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. No additional preparation is needed on the origin cluster, IBM Event Streams as geo-replication runs on the destination cluster. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Geo-replication is based on MirrorMaker 2.0, which uses Kafka Connect, enabling interoperability with other Kafka distributions. Use geo-replication to replicate data between Event Streams clusters.  Use MirrorMaker2 to move data between Event Streams clusters and other Kafka clusters. Cruise Control Cruise Control is an open-source system for optimizing your Kafka cluster by monitoring cluster workload, rebalancing a cluster based on predefined constraints, and detecting and fixing anomalies.You can set up Event Streams to use the following Cruise Control features:   Generating optimization proposals from multiple optimization goals.  Rebalancing a Kafka cluster based on an optimization proposal.Note: Event Streams does not support other Cruise Control features. Enable Cruise Control for Event Streams and configure optimization goals for your cluster. Note: Cruise Control stores data in Kafka topics. It does not have its own persistent storage configuration. Consider using persistent storage for your Kafka topics when using Cruise Control. Planning for log management Event Streams uses the cluster logging provided by the OpenShift Container Platform to collect, store, and visualize logs. The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). You can use this EFK stack logging capability in your environment to help resolve problems with your deployment and aid general troubleshooting. You can use log data to investigate any problems affecting your system health. Kafka static configuration properties You can set Kafka broker configuration settings in your EventStreams custom resource under the property spec.strimziOverrides.kafka. These settings will override the default Kafka configuration defined by Event Streams. You can also use this configuration property to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. Connecting clients By default, Kafka client applications connect to cluster using the Kafka bootstrap route address. Find out more about connecting external clients to your installation. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Platform Common Services monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing Licensing considerations Licensing is based on a Virtual Processing Cores (VPC) metric. To use Event Streams you must have a license for all of the virtual cores that are available to all of the following Event Streams components:   Kafka brokers  Geo-Replicator nodes  MirrorMaker2 nodes  Kafka Connect nodes hosted by Event StreamsAll other container types are pre-requisite components that are supported as part of Event Streams, and do not require additional licenses. If you are using one of the samples provided, see the sample deployments section for information about the number of VPCs required. The number of VPCs indicate the licenses required. Note: For a production installation of Event Streams, the ratio is 1 license required for every 1 VPC being used. For a non-production installation of Event Streams, the ratio is 1 license required for every 2 VPCs being used. To flag an installation of Event Streams as production or non-production, set the spec.license.use correctly during installation. See license usage for more information about selecting the correct value. If you add more Kafka replicas, geo-replicator nodes, MirrorMaker2 nodes, or Kafka Connect nodes, each one is an additional, separate chargeable unit. See license usage to learn how you can find out more about the number of virtual cores used by your deployment. License usage The license usage of IBM Event Streams is collected by the IBM Cloud Platform Common Services License Service which is automatically deployed with IBM Event Streams. This provides the service that tracks the licensed containers and their resource usage based on the product use. When creating an instance of Event Streams, ensure that you select the correct value for spec.license.use in the custom resource. This value is used for metering purposes and could result in inaccurate charging and auditing if set incorrectly. Select one of the following values based on the purpose of your deployment:   CloudPakForIntegrationNonProduction for non-production deployments suitable for basic development and test activities.  CloudPakForIntegrationProduction for production deployments.The sample deployments provided by Event Streams have the correct value set by default based on the sample deployment purposes. The license usage information can be viewed by obtaining an API token that is required to make the API calls to retrieve license usage data, and then accessing provided APIs for retrieving the license usage data. There are 3 APIs that can be viewed:   Snapshot (last 30 days) This provides audit level information in a .zip file and is a superset of the other reports.  Products report (last 30 days) This shows the VPC usage for all products that are deployed in IBM Cloud Pak for Integration, for example:    [{\"name\":\"IBM Cloud Pak for Integration\",\"id\":\"c8b82d189e7545f0892db9ef2731b90d\",\"metricPeakDate\":\"2020-06-10\",\"metricQuantity\":3,\"metricName\":\"VIRTUAL_PROCESSOR_CORE\"}]        In this example, the metricQuantity is 3 indicating that the peak VPC usage is 3.     Bundled products report (last 30 days) This shows the breakdown of bundled products that are included in IBM Cloud Paks that are deployed on a cluster with the highest VPC usage within the requested period. For Event Streams this shows the peak number of VPCs in use, the conversion ratio and the number of licenses used. For example:    [{\"productName\":\"IBM Event Streams for Non Production\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-10\",\"metricMeasuredQuantity\":6,\"metricConversion\":\"2:1\",\"metricConvertedQuantity\":3}]        In this example, the productName shows the license metrics for a IBM Event Streams for Non Production deployment. The metricMeasuredQuantity is 6 VPCs, the metricConversion is 2:1 and metricConvertedQuantity is 3 VPCs so the license usage is 3.   Note: The metricMeasuredQuantity is the peak number of VPCs used over the timeframe. If an Event Streams instance is deleted and a new instance installed, then the quantity will be the maximum used at any one time. The following examples show the number of licenses required for specific installations: Example 1 - Non-production 3 brokers This example is for an Event Streams installation configured with 3 Kafka brokers, no Mirror Maker or Kafka Connect containers. In this example installation, each Kafka container requires 2 VPCs so in total 6 VPCs are being used. For non-production deployment the metrics conversion ratio is 2:1, therefore 3 licenses are required. Example 2 - Production 3 brokers This example is for an Event Streams installation configured with 3 Kafka brokers, no Mirror Maker or Kafka Connect containers. In this example installation, each Kafka container requires 4 VPCs so in total 12 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 12 licenses are required. Example 3 - Production 6 brokers with Geo-Replication This example is for an Event Streams installation configured with 6 Kafka brokers, 1 Mirror Maker container and no Kafka Connect containers. In this example installation, each Kafka container requires 4 VPCs and each Mirror Maker container requires 1 VPC, so in total 25 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 25 licenses are required. Example 4 - Production 9 brokers with Geo-Replication and Kafka-Connect This example is for an Event Streams installation configured with 9 Kafka brokers, 1 Mirror Maker and 1 Kafka Connect container. In this example installation, each Kafka container requires 4 VPCs, each Mirror Maker container requires 1 VPC and each Kafka-Connect container requires 1 VPC, so in total 38 VPCs are being used. For production deployment the metrics conversion ratio is 1:1, therefore 38 licenses are required. If there are multiple production or non-production installations in a cluster then the API will show the total peak VPC usage for all production or non-production instances in that cluster. For example if you have 2 production instances of IBM Event Streams where each instance has 3 Kafka brokers that each use 2 VPS, then the total peak usage is 12 VPCs which converts to 12 licenses. If there are production and non-production IBM Event Streams instances installed in the cluster, then the metricConvertedQuantity under IBM Event Streams and IBM Event Streams for Non Production will need to be added to determine the total license usage. For example: [{\"productName\":\"IBM Event Streams for Non Production\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-10\",\"metricMeasuredQuantity\":6,\"metricConversion\":\"2:1\",\"metricConvertedQuantity\":3},{\"productName\":\"IBM Event Streams\",\"productId\":\"&lt;product_id&gt;\",\"cloudpakId\":\"&lt;cloudpak_id&gt;\",\"cloudpakVersion\":\"2020.2.1\",\"metricName\":\"VIRTUAL_PROCESSOR_CORE\",\"metricPeakDate\":\"2020-06-11\",\"metricMeasuredQuantity\":8,\"metricConversion\":\"1:1\",\"metricConvertedQuantity\":8}]In this example there are Event Streams installations for non-production and for production. The non-production usage is 6 VPCs which converts to 3 licenses. The production usage is 8 VPCs which converts to 8 licenses. Therefore the total license usage is 11. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/planning/",
        "teaser":null},{
        "title": "Performance and capacity planning",
        "collection": "10.1",
        "excerpt":"Guidance for production environments You can use one of the sample configurations provided by Event Streams to set up a production deployment. For information about the provided samples, see the sample deployments section. If needed, you can modify the selected sample configuration when you deploy a new instance, or make changes at a later time. Tuning Event Streams Kafka performance When preparing for your Event Streams installation, review your workload requirements and consider the configuration options available for performance tuning your Event Streams installation. Kafka offers a number of configuration settings that can be adjusted as necessary for an Event Streams deployment. These can be used to address any bottlenecks in the system as well as perform fine tuning of Kafka performance. Kafka provides a wide range of configuration properties to set, but consider the following when reviewing performance requirements:   The num.replica.fetchers property sets the number of threads available on each broker to replicate messages from topic leaders. Increasing this setting increases I/O parallelism in the follower broker, and can help reduce bottlenecks and message latency. You can start by setting this value to match the number of brokers deployed in the system.Note: Increasing this value results in brokers using more CPU resources and network bandwidth.  The num.io.threads property sets the number of threads available to a broker for processing requests. As the load on each broker increases, handling requests can become a bottleneck. Increasing this property value can help mitigate this issue. The value to set depends on the overall system load and the processing power of the worker nodes, which varies for each deployment. There is a correlation between this setting and the num.network.threads setting.  The num.network.threads property sets the number of threads available to the broker for receiving and sending requests and responses to the network. The value to set depends on the overall network load, which varies for each deployment. There is a correlation between this setting and the num.io.threads setting.  The replica.fetch.min.bytes, replica.fetch.max.bytes, and replica.fetch.response.max.bytes properties control the minimum and maximum sizes for message payloads when performing inter-broker replication. Set these values to be greater than the message.max.bytes property to ensure that all messages sent by a producer can be replicated between brokers. The value to set depends on message throughput and average size, which varies for each deployment.These properties are configured in the EventStreams custom resource for an instance when it is first created and can be modified at any time. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster-level configuration property log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster-level configuration property log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration property segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration property cleanup.policy determines the way the log segments for the topic are managed. For more information about these settings, see the Kafka documentation. The cluster-level settings are configured in the EventStreams custom resource for an instance when it is first created and can be modified at any time. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click Create a topic, and set Show all available options to On). Note: When using ephemeral storage, ensure you set retention limits for Kafka topics so that you do not run out of disk space.If message retention is set to long periods and the message volume is high, the storage requirements for the topics could impact the OpenShift nodes that host the Kafka pods, and cause the nodes to run out of allocated disk space, which could impact normal operation. Log cleanup by deletion If the topic-level configuration property cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The cluster-level configuration property log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The cluster-level configuration property log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration property cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding all but the last of each duplicate key in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration property at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/capacity-planning/",
        "teaser":null},{
        "title": "Preparing for multizone clusters",
        "collection": "10.1",
        "excerpt":"IBM Event Streams supports multiple availability zones for your clusters. Multizone clusters add resilience to your Event Streams installation. For guidance about handling outages in a multizone setup, see managing a multizone setup. Zone awareness Kubernetes uses zone-aware information to determine the zone location of each of its nodes in the cluster to enable scheduling of pod replicas in different zones. Some clusters, typically AWS, will already be zone aware. For clusters that are not zone aware, each Kubernetes node will need to be set up with a zone label. To determine if your cluster is zone aware:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command as cluster administrator:     oc get nodes --show-labels   If your Kubernetes cluster is zone aware, the following label is displayed against each node:   topology.kubernetes.io/zone if using OpenShift 4.5 or later  failure-domain.beta.kubernetes.io/zone if using an earlier version of OpenShiftThe value of the label is the zone the node is in, for example, es-zone-1. If your Kubernetes cluster is not zone aware, all cluster nodes will need to be labeled using a value that identifies the zone that each node is in. For example, run the following command to label and allocate a node to es-zone-1: oc label node &lt;node-name&gt; topology.kubernetes.io/zone=es-zone-1 The zone label is needed to set up rack awareness when installing for multizone. Kafka rack awareness In addition to zone awareness, Kafka rack awareness helps to spread the Kafka broker pods and Kafka topic replicas across different availability zones, and also sets the brokers’ broker.rack configuration property for each Kafka broker. To set up Kafka rack awareness, Kafka brokers require a cluster role to provide permission to view which Kubernetes node they are running on. Before applying Kafka rack awareness to an Event Streams installation, apply a cluster role:   Download the cluster role YAML file from GitHub.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the cluster role by using the following command and the downloaded file:     oc apply -f eventstreams-kafka-broker.yaml   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/preparing-multizone/",
        "teaser":null},{
        "title": "Installing",
        "collection": "10.1",
        "excerpt":"The following sections provide instructions about installing IBM Event Streams on the Red Hat OpenShift Container Platform. The instructions are based on using the OpenShift Container Platform web console and oc command line utility. When deploying in an air-gapped environment, ensure you have access to this documentation set, and see the instructions in the offline installation README that is provided as part of the downloaded package. Event Streams can also be installed as part of IBM Cloud Pak for Integration. Overview Event Streams is an operator-based release and uses custom resources to define your Event Streams configurations. The Event Streams operator uses the custom resources to deploy and manage the entire lifecycle of your Event Streams instances. Custom resources are presented as YAML configuration documents that define instances of the EventStreams custom resource type. Installing Event Streams has two phases:   Install the Event Streams operator: this will deploy the operator that will install and manage your Event Streams instances.  Install one or more instances of Event Streams by using the operator.Before you begin   Ensure you have set up your environment according to the prerequisites, including setting up your OpenShift Container Platform.  Ensure you have planned for your installation, such as preparing for persistent storage, considering security options, and considering adding resilience through multiple availability zones.  Obtain the connection details for your OpenShift Container Platform cluster from your administrator.Create a project (namespace) Create a namespace into which the Event Streams instance will be installed by creating a project.When you create a project, a namespace with the same name is also created. Ensure you use a namespace that is dedicated to a single instance of Event Streams. This is required because Event Streams uses network security policies to restrict network connections between its internal components. A single namespace per instance also allows for finer control of user accesses. Important: Do not use any of the default or system namespaces to install an instance of Event Streams (some examples of these are: default, kube-system, kube-public, and openshift-operators). Creating a project by using the web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Home dropdown and select Projects to open the Projects panel.  Click Create Project.  Enter a new project name in the Name field, and optionally, a display name in the Display Name field, and a description in the Description field.  Click Create.Creating a project by using the CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to create a new project:     oc new-project &lt;project_name&gt; --description=\"&lt;description&gt;\" --display-name=\"&lt;display_name&gt;\"     where description and display-name are optional flags to set a description and custom descriptive name for your project.         Ensure you are using the project you created by selecting it as follows:     oc project &lt;new-project-name&gt;     The following message is displayed if successful:     Now using project \"&lt;new-project-name&gt;\" on server \"https://&lt;OpenShift-host&gt;:6443\".      Add the Event Streams operator to the catalog Before you can install the Event Streams operator and use it to create instances of Event Streams, you must have the IBM Operator Catalog and the IBM Common Services Catalog available in your cluster. If you have other IBM products installed in your cluster, then you already have the IBM Operator Catalog available, and you can continue to installing the Event Streams operator. Ensure you also have the IBM Common Services Catalog available, as described in the following steps. If you are installing Event Streams as the first IBM product in your cluster, complete the following steps. To make the IBM Event Streams operator and related Common Services dependencies available in the OpenShift OperatorHub catalog, create the following YAML files and apply them as  follows. To add the IBM Operator Catalog:       Create a file for the IBM Operator Catalog source with the following content, and save as IBMCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: ibm-operator-catalog   namespace: openshift-marketplacespec:   displayName: \"IBM Operator Catalog\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-operator-catalog   updateStrategy:     registryPoll:       interval: 45m        Important: If you are using OpenShift Container Platform 4.3, do not include the last 3 lines in your file:     updateStrategy:  registryPoll:    interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCatalogSource.yaml   The IBM Operator Catalog source is added to the OperatorHub catalog, making the Event Streams operator available to install. To add the IBM Common Services Catalog:       Create a file for the IBM Common Services Catalog source with the following content, and save as IBMCSCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: opencloud-operators   namespace: openshift-marketplacespec:   displayName: \"IBMCS Operators\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-common-service-catalog:latest   updateStrategy:     registryPoll:       interval: 45m        Important: If you are using OpenShift Container Platform 4.3, do not include the last 3 lines in your file:     updateStrategy:  registryPoll:    interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCSCatalogSource.yaml   The IBM Common Services Catalog source is added to the OperatorHub catalog, making the IBM Cloud Platform Common Services items available to install for Event Streams. Install the Event Streams operator Ensure you have considered the Event Streams operator requirements, including resource requirements and the required cluster-scoped permissions. Choosing operator installation mode Before installing the Event Streams operator, decide if you want the operator to:       Manage instances of Event Streams in any namespace.     To use this option, select All namespaces on the cluster (default) later. The operator will be deployed into the system namespace openshift-operators, and will be able to manage instances of Event Streams in any namespace.         Only manage instances of Event Streams in a single namespace.     To use this option, select A specific namespace on the cluster later. The operator will be deployed into the specified namespace, and will not be able to manage instances of Event Streams in any other namespace.   Installing by using the web console To install the operator by using the OpenShift Container Platform web console, do the following:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select OperatorHub to open the OperatorHub dashboard.  Select the project you want to deploy the Event Streams instance in.  In the All Items search box enter IBM Event Streams to locate the operator title.  Click the IBM Event Streams tile to open the install side panel.  Click the Install button to open the Create Operator Subscription dashboard.  Select the chosen installation mode that suits your requirements.If the installation mode is A specific namespace on the cluster, select the target namespace you created previously.  Click Install to begin the installation (or click Subscribe if using an earlier version of OpenShift than 4.5).The installation can take a few minutes to complete. Checking the operator status You can see the installed operator and check its status as follows:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Scroll down to the ClusterServiceVersion Overview section of the page.  Check the Status field. After the operator is successfully installed, this will change to Succeeded.In addition to the status, information about key events that occur can be viewed under the Conditions section of the same page. After a successful installation, a condition with the following message is displayed: install strategy completed with no errors. Note: If the operator is installed into a specific namespace, then it will only appear under the associated project. If the operator is installed for all namespaces, then it will appear under any selected project. If the operator is installed for all namespaces and you select all projects from the Project drop down, the operator will be shown multiple times in the resulting list, once for each project. Note: If the required IBM Cloud Platform Common Services are not installed, they will be automatically deployed when the Event Streams operator is installed and the following additional operators will appear in the installed operator list:   Operand Deployment Lifecycle Manager.  IBM Common Service Operator.Install an Event Streams instance Instances of Event Streams can be created after the Event Streams operator is installed. If the operator was installed into a specific namespace, then it can only be used to manage instances of Event Streams in that namespace. If the operator was installed for all namespaces, then it can be used to manage instances of Event Streams in any namespace, including those created after the operator was deployed. When installing an instance of Event Streams, ensure you are using a namespace that an operator is managing. Creating an image pull secret Before installing an Event Streams instance, create an image pull secret called ibm-entitlement-key in the namespace where you want to create an instance of Event Streams. The secret enables container images to be pulled from the registry.   Obtain an entitlement key from the IBM Container software library.      Create the secret in the namespace that will be used to deploy an instance of Event Streams as follows.     Name the secret ibm-entitlement-key, use cp as the username, your entitlement key as the password, and cp.icr.io as the docker server:     oc create secret docker-registry ibm-entitlement-key --docker-username=cp --docker-password=\"&lt;your-entitlement-key&gt;\" --docker-server=\"cp.icr.io\" -n &lt;target-namespace&gt;   Note: If you do not create the required secret, pods will fail to start with ImagePullBackOff errors. In this case, ensure the secret is created and allow the pod to restart. Installing an instance by using the web console To install an Event Streams instance through the OpenShift Container Platform web console, do the following:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.      Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.     Note: If the operator is not shown, it is either not installed or not available for the selected namespace.     In the Operator Details dashboard, click the Event Streams tab.  Click the Create EventStreams button to open the Create EventStreams panel. You can use this panel to define an EventStreams custom resource.From here you can install by using the form view. For more advanced configurations or to install one of the samples, see installing by using the YAML view. Installing by using the form view To configure an EventStreams custom resource, do the following:   Enter a name for the instance in the Name field.  Click the license accept toggle to set it to True.  Ensure that the correct value is selected for the Product use from the dropdown. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  You can optionally configure other components such as Kafka, ZooKeeper, and Security to suit your requirements.  Scroll down and click the Create button at the bottom of the page to deploy the Event Streams instance.  Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.Installing by using the YAML view Alternatively, you can configure the EventStreams custom resource by editing YAML documents. To do this, click the Edit YAML tab. A number of sample configurations are provided on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload. Alternatively, a pre-configured YAML file containing the custom resource sample can be dragged and dropped onto this screen to apply the configuration. To view the samples, do the following:   Select the Samples tab to show the available sample configurations.  Click the Try it link under any of the samples to open the configuration in the Create EventStreams panel.More information about these samples is available in the planning section. You can base your deployment on the sample that most closely reflects your requirements and apply customizations on top as required. When modifying the sample configuration, the updated document can be exported from the Create EventStreams panel by clicking the Download button and re-imported by dragging the resulting file back into the window. Important: You must ensure that the spec.license.accept field in the custom resource YAML is set to true and that the correct value is selected for the spec.license.use field before deploying the Event Streams instance. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  Note: If experimenting with Event Streams for the first time, the Lightweight without security sample is the smallest and simplest example that can be used to create an experimental deployment. For the smallest production setup, use the Minimal production sample configuration. To deploy an Event Streams instance, use the following steps:   Complete any changes to the sample configuration in the Create EventStreams panel.  Click Create to begin the installation process.  Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.Installing an instance by using the CLI To install an instance of Event Streams from the command line, you must first prepare an EventStreams custom resource configuration in a YAML file. A number of sample configuration files have been provided on which you can base your deployment. These range from smaller deployments for non-production development or general experimentation to large scale clusters ready to handle a production workload. More information about these samples is available in the planning section. You can base your deployment on the sample that most closely reflects your requirements and apply customizations on top as required. Important: You must ensure that the spec.license.accept field in the configuration is set to true and that the correct value is selected for the spec.license.use field before deploying the Event Streams instance. Select CloudPakForIntegrationNonProduction for development and test deployments not intended for production use, and select CloudPakForIntegrationProduction for production deployments. See the licensing section for more details about selecting the correct value.  Note: If experimenting with Event Streams for the first time, the Lightweight without security sample is the smallest and simplest example that can be used to create an experimental deployment. For the smallest production setup, use the Minimal production sample configuration. To deploy an Event Streams instance, run the following commands:       Set the project where your EventStreams custom resource will be deployed in:     oc project &lt;project-name&gt;         Apply the configured EventStreams custom resource:     oc apply -f &lt;custom-resource-file-path&gt;     For example: oc apply -f development.yaml     Wait for the installation to complete.  You can now verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/installing/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "10.1",
        "excerpt":"Event Streams provides samples to help you get started with deployments, as described in the planning section. Choose one of the samples suited to your requirements to get started:   Lightweight without security  Development  Minimal production  Production 3 brokers  Production 6 brokers  Production 9 brokersYou can modify the samples, save them, and apply custom configuration settings as well. See the following sections for guidance about configuring your instance of Event Streams. Note: The Production 6 brokers and Production 9 brokers samples are only available on GitHub. You can configure and apply them by using the command line or by dragging and dropping them onto the OpenShift Container Platform web console, and editing them. Checking configuration settings This page gives information about many configuration options. To see further information about specific configuration options, or to see what options are available, you can use the oc explain command. To see information about a specific field, run the following: oc explain eventstreams.&lt;path-of-field&gt; Where path-of-field is the JSON path of the field of interest. For example, if you want to see more information about configuring external listeners for Kafka you can run the following command: oc explain eventstreams.spec.strimziOverrides.kafka.listeners.external Enabling persistent storage If you want your data to be preserved in the event of a restart, configure persistent storage for Kafka and ZooKeeper in your IBM Event Streams installation. Note: Ensure you have sufficient disk space for persistent storage. These settings are specified in the YAML configuration document that defines an instance of the EventStreams custom resource and can be applied when defining a new Event Streams instance under the “IBM Event Streams” operator in the OpenShift Container Platform web console.   To enable persistent storage for Kafka, add the storage property under spec.strimziOverrides.kafka  To enable persistent storage for ZooKeeper, add the storage property under spec.strimziOverrides.zookeeperComplete the configuration by adding additional fields to these storage properties as follows:       Specify the storage type in storage.type (for example, \"ephemeral\" or \"persistent-claim\").     Note: When using ephemeral storage, ensure you set retention limits for Kafka topics so that you do not run out of disk space.If message retention is set to long periods and the message volume is high, the storage requirements for the topics could impact the OpenShift nodes that host the Kafka pods, and cause the nodes to run out of allocated disk space, which could impact normal operation.     Specify the storage size in storage.size (for example, \"100Gi\").  Optionally, specify the storage class in storage.class (for example, \"rook-ceph-block-internal\").  Optionally, specify the retention setting for the storage if the cluster is deleted in storage.deleteClaim (for example, \"true\").An example of these configuration options: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    kafka:      # ...      storage:        type: \"persistent-claim\"        size: \"100Gi\"        class: \"ceph-block\"    zookeeper:      # ...      storage:        type: \"persistent-claim\"        size: \"100Gi\"        class: \"ceph-block\"# ...If present, existing persistent volumes with the specified storage class are used after installation, or if a dynamic provisioner is configured for the specified storage class, new persistent volumes are created. Where optional values are not specified:   If no storage class is specified and a default storage class has been defined in the OpenShift Container Platform settings, the default storage class will be used.  If no storage class is specified and no default storage class has been defined in the OpenShift Container Platform settings, the deployment will use any persistent volume claims that have at least the set size value.  If no retention setting is provided, the storage will be retained when the cluster is deleted.The following example YAML document shows an example EventStreams custom resource with dynamically allocated storage provided using CephFS for Kafka and ZooKeeper. To try this deployment, set the required namespace and accept the license by changing the spec.license.accept value to \"true\". apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-storage  namespace: myprojectspec:  license:    accept: false  version: 10.1.0  adminApi: {}  adminUI: {}  apicurioRegistry: {}  collector: {}  restProducer: {}  strimziOverrides:    kafka:      replicas: 1      config:        interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor        offsets.topic.replication.factor: 1        transaction.state.log.min.isr: 1        transaction.state.log.replication.factor: 1      listeners:        external:          type: route        plain: {}        tls: {}      storage:        type: persistent-claim        size: 100Gi        class: rook-ceph-block-internal        deleteClaim: true      metrics: {}    zookeeper:      replicas: 1      storage:        type: persistent-claim        size: 100Gi        class: rook-ceph-block-internal      deleteClaim: true      metrics: {}Configuring encryption between pods Pod-to-Pod encryption is enabled by default for all Event Streams pods. Unless explicitly overridden in an EventStreams custom resource, the configuration option spec.security.internalTls will be set to TLSv1.2. This value can be set to NONE which will disable Pod-to-Pod encryption. For example, the following YAML snippet disables encryption between pods: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-internal-disabled  namespace: myprojectspec:  # ...  security:    # ...    internalTls: NONE# ...Configuring UI security By default, accessing the Event Streams UI requires an IBM Cloud Platform Common Services Identity and Access Management (IAM) user that has been assigned access to Event Streams (see managing access for details). The login requirement for the UI is disabled when all Kafka authentication and authorization is disabled. This is demonstrated by the proof-of-concept lightweight without security sample. Important: When security is not configured, the Producers and the Monitoring dashboards are not available in the UI. Applying Kafka broker configuration settings Kafka supports a number of broker configuration settings, typically provided in a properties file. When creating an instance of Event Streams, these settings are defined in an EventStreams custom resource under a the spec.strimziOverrides.kafka.config property. The following example uses Kafka broker settings to configure replication for system topics: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    # ...    kafka:      # ...      config:        offsets.topic.replication.factor: 1        transaction.state.log.min.isr: 1        transaction.state.log.replication.factor: 1This custom resource can be created using the oc command or the OpenShift Container Platform web console under the IBM Event Streams operator page. You can specify all the broker configuration options supported by Kafka except from those managed directly by Event Streams. For further information, see the list of supported configuration options. After deployment, these settings can be modified by updating the EventStreams custom resource. Applying Kafka rack awareness Kafka rack awareness is configured by setting the rack property in the EventStreams custom resource using the zone label as the topology key in the spec.strimziOverrides.kafka.rack field. This key needs to match the zone label name applied to the nodes. Note: Before this is applied, ensure the Kafka cluster role for rack awareness has been applied. The following example sets the rack topologyKey to topology.kubernetes.io/zone: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    # ...    kafka:      # ...      rack:        topologyKey: topology.kubernetes.io/zone      # ...Setting geo-replication nodes You can install geo-replication in a cluster to enable messages to be automatically synchronized between local and remote topics. A cluster can be a geo-replication origin or destination. Origin clusters send messages to a remote system, while destination clusters receive messages from a remote system. A cluster can be both an origin and a destination cluster at the same time. To enable geo-replication, create an EventStreamsGeoReplicator custom resource alongside the EventStreams custom resource. This can be defined in a YAML configuration document under the IBM Event Streams operator in the OpenShift Container Platform web console. When setting up geo-replication, consider the number of geo-replication worker nodes (replicas) to deploy and configure this in the spec.replicas property. Ensure that the following properties match the name of the Event Streams instance:   metadata.name  metadata.labels[\"eventstreams.ibm.com/cluster\"]For example, to configure geo-replication with 2 replicas for an Event Streams instance called sample-three in the namespace myproject, create the following EventStreamsGeoReplicator configuration: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsGeoReplicatormetadata:  labels:    eventstreams.ibm.com/cluster: sample-three  name: sample-three  namespace: myprojectspec:  # ...  replicas: 2Note: Geo-replication can be deployed or reconfigured at any time. For more information, see Setting up geo-replication. Configuring access External access using OpenShift Container Platform routes is automatically configured for the following services if they are included in the Event Streams installation:   The Event Streams UI  The Apicurio Registry  The Admin API  The REST ProducerREST services access The REST services for Event Streams are configured with defaults for the container port, type, TLS version, certificates, and authentication mechanisms. If the Kafka listeners have been configured without authentication requirements then the authentication mechanisms are automatically removed from the REST endpoints. The schema for REST endpoint configuration is described in the following table, followed by an example of an endpoint configuration for the Admin API. In the example, the potential values for &lt;component&gt; in spec.&lt;component&gt;.endpoints are:   adminApi for the Admin API  restProducer for the REST Producer  apicurioRegistry for the Apicurio Registry            Key      Type      Description                  name      String      Name to uniquely identify the endpoint among other endpoints in the list for a component.              containerPort      Integer      A unique port to open on the container that this endpoint will serve requests on. Restricted ranges are 0-1000 and 7000-7999.              type      String [internal, route]      Event Streams REST components support internal type endpoints and OpenShift Container Platform Routes.              tlsVersion      String [TLSv1.2,NONE]      Specifies the TLS version where NONE will disable HTTPS.              authenticationMechanisms      List of Strings      List of authentication mechanisms to be supported at this endpoint. By default, all authentication mechanisms: [iam-bearer,tls,scram-sha-512] are enabled. Optionally a subset or even none ([]) can be configured.              certOverrides.certificate      String      The name of the key in the provided certOverrides.secretName secret that contains the base64 encoded certificate.              certOverrides.key      String      The name of the key in the provided certOverrides.secretName secret that contains the base64 encoded key.              certOverrides.secretName      String      The name of the secret in the instance namespace that contains the encoded certificate and key to secure the endpoint with.              host      String (DNS rules apply)      An optional override for the default host that an OpenShift Container Platform route will generate.      # ...spec:  # ...  adminApi:    # ...    endpoints:      - name: example        containerPort: 9080        type: route        tlsVersion: TLSv1.2        authenticationMechanisms:          - iam-bearer          - tls          - scram-sha-512        certOverrides:            certificate: mycert.crt            key: mykey.key            secretName: custom-endpoint-cert        host: example-host.apps.example-domain.comNote: Changing an endpoint in isolation might have adverse effects if Kafka is configured to require authentication and the configured endpoint has no authentication mechanisms specified. In such cases, a warning message might be displayed in the instance status conditions. The Event Streams REST components also allow for the default set of cipher suites to be overridden. Though not a recommended practice, it is possible to enable alternative cipher suites to facilitate connectivity of legacy systems. This capability is provided through the CIPHER_SUITES environment variable as shown in this example: # ...spec:  # ...  restProducer:    # ...    env:      - name: CIPHER_SUITES        value: &gt;-          TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256Kafka access All examples provided for Event Streams include an external listener for Kafka and varying internal listener types by default. The supported external listener is of type route. This indicates the use of an OpenShift Container Platform route, and it can have either tls or scram-sha-512 configured as the authentication mechanisms. The following example snippet defines an external listener that exposes the Kafka brokers using an OpenShift Container Platform route with SCRAM-SHA-512 authentication enabled. # ...spec:  # ...  strimziOverrides:    # ...    kafka:      listeners:        external:          type: route          authentication:            type: scram-sha-512Internal listeners for Kafka can also be configured. In addition to the external listener, there are plain and tls internal listeners. Each of these can be configured to have an authentication mechanism as shown in the following example. # ...spec:  # ...  strimziOverrides:    # ...    kafka:      listeners:        plain:          authentication:            type: scram-sha-512        tls:          authentication:            type: tlsThe Kafka listener security protocols are mapped to the internal listener configurations as shown in the following table:             Security protocol      Listener configuration                  PLAINTEXT      spec.strimziOverrides.kafka.listeners.plain: {}              SSL (no-authentication)      spec.strimziOverrides.kafka.listeners.tls: {}              SSL (mutual-authentication)      spec.strimziOverrides.kafka.listeners.tls.authentication.type: tls              SASL_PLAINTEXT      spec.strimziOverrides.kafka.listeners.plain.authentication.type: scram-sha-512              SASL_SSL      spec.strimziOverrides.kafka.listeners.tls.authentication.type: scram-sha-512      When secure listeners are configured, Event Streams will automatically generate cluster and client CA certificates, and a valid certificate for the listener endpoint. The generated CA certificates and the certificate for the endpoint can be replaced by provided certificates as described in providing certificate overrides. Configuring external monitoring through Prometheus Metrics provide information about the health and operation of the Event Streams instance. Metrics can be enabled for Kafka, ZooKeeper, geo-replicator, and Kafka Connect pods. Note: Kafka metrics can also be exposed externally through JMX by configuring external monitoring tools. Kafka metrics can be enabled by setting spec.strimziOverrides.kafka.metrics to {} in the EventStreams custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    kafka:      # ...      metrics: {}# ...ZooKeeper metrics can be enabled by setting spec.strimziOverrides.zookeeper.metrics to {} in the EventStreams custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    zookeeper:      # ...      metrics: {}# ...Geo-replicator metrics can be enabled by setting spec.metrics to {} in the KafkaMirrorMaker2 custom resource. For example: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaMirrorMaker2# ...spec:  # ...  metrics: {}# ...Note: The Event Streams operator automatically applies a KafkaMirrorMaker2 custom resource when a EventStreamsGeoReplicator custom resource is created. Metrics can then be enabled by editing the generated KafkaMirrorMaker2 custom resource. Kafka Connect metrics can be enabled by setting spec.metrics to {} in the KafkaConnectS2I custom resource. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: KafkaConnectS2I# ...spec:  # ...  metrics: {}# ...To complement the default Kafka metrics, Event Streams can be configured to publish additional information about the Event Streams instance by setting the spec.strimziOverrides.kafka.config.interceptor.class.name to com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor, for example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  strimziOverrides:    kafka:      # ...        config:          # ...          interceptor.class.names: com.ibm.eventstreams.interceptors.metrics.ProducerMetricsInterceptor# ...Note: For details about viewing metrics information, see the cluster health and topic health sections. Configuring external monitoring through JMX You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by collecting Kafka metrics. To set this up, you need to:   Have a third-party monitoring tool set up to be used within your OpenShift Container Platform cluster.  Enable access to the broker JMX port by setting spec.strimizOverrides.kafka.jmxOptions.    apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      jmxOptions: {}        Include any configuration settings for Event Streams as required by your monitoring tool. For example, Datadog’s autodiscovery requires you to annotate Kafka broker pods (strimziOverrides.kafka.template.statefulset.metadata.annotations)  Configure your monitoring applications to consume JMX metrics.Enabling and configuring Cruise Control To enable Cruise Control, set the spec.strimizOverrides.cruiseControl property to {} in the EventStreams custom resource: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    cruiseControl: {}Note: Ensure you have more than 1 Kafka broker configured to take advantage of Cruise Control. All sample configurations provided have more than 1 broker except the Lightweight without security sample. When enabled, you can use the default Cruise Control configuration to optimize your Kafka cluster. You can also specify your required configuration as described in the following sections. When configuring Cruise Control, you can define the following settings in the EventStreams custom resource:   Master optimization goals in spec.strimziOverrides.cruiseControl.config.goals      Default optimization goals in spec.strimziOverrides.cruiseControl.config[\"default.goals\"]     Note: If you do not set master optimization goals and default goals, then the Cruise Control defaults are used.     Hard goals in spec.strimziOverrides.cruiseControl.config[\"hard.goals\"]  The capacity limits for broker resources, which Cruise Control uses to determine if resource-based optimization goals are being broken. The spec.strimziOverrides.cruiseControl.brokerCapacity property defines the Kafka broker resource capacities that Cruise Control will optimize around.Cruise Control includes a number of configuration options. You can modify these configuration options for Event Streams, except the options managed directly by Strimzi. When enabled, you can use Cruise Control and the KafkaRebalance custom resources to optimize your deployed Event Streams Kafka cluster. Cruise Control defaults Event Streams supports a subset of the Cruise Control goals. If the master optimization goals and default goals (spec.strimziOverrides.cruiseControl.config.goals and spec.strimziOverrides.cruiseControl.config[\"default.goals\"], respectively) are not set, then the Cruise Control configuration defaults to the following goals (in descending order of priority):   com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.PotentialNwOutGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderBytesInDistributionGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.PreferredLeaderElectionGoalFor more information about the optimization goals, see the Cruise Control documentation. Master optimization goals The master optimization goals define the goals available to be used in Cruise Control operations. Goals not listed cannot be used. The spec.strimziOverrides.cruiseControl.config.goals property defines the list of goals Cruise Control can use. The master optimization goals have defaults if not configured. For example, if you want Cruise Control to only consider using com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal and com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, set values for spec.strimziOverrides.cruiseControl.config.goals property as follows: # ...spec:  # ...  strimziOverrides:    # ...    cruiseControl:      # ...      config:        # ...        goals: &gt;          com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal,          com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoalDefault goals The default goals define the set of goals that you want your cluster to meet most often. They are set in the spec.strimziOverrides.cruiseControl.config[\"default.goals\"] property. By default, every 15 minutes, Cruise Control will use the current state of your Kafka cluster to generate a cached optimization proposal by using the configured default.goals list. If no default goals are set, the master optimization goals are used as the default optimization goals. For example, if you want Cruise Control to always consider meeting com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, and com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal, set values for the spec.strimziOverrides.cruiseControl.config[\"default.goals\"] property as follows: # ...spec:  # ...  strimziOverrides:    # ...    cruiseControl:      # ...      config:        # ...        default.goals: &gt;          com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal,          com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal,          com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoalHard goals Hard goals define the list of goals that must be met by an optimization proposal and cannot be violated in any of the optimization functions of Cruise Control. Hard goals can be set by the spec.strimziOverrides.cruiseControl.config[\"hard.goals\"] property. In Cruise Control, the following master optimization goals are preset as hard goals:   com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal  com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoalFor example, to configure Cruise Control to always consider com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal and com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal in an optimization proposal, provide these goals as values in the spec.strimziOverrides.cruiseControl.config[\"hard.goals\"] property as follows: # ...spec:  # ...  strimziOverrides:    # ...    cruiseControl:      # ...      config:        # ...        hard.goals: &gt;          com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,          com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoalNote: The longer the list of hard goals, the less likely it is that Cruise Control will be able to find a viable optimization proposal. Consider configuring fewer hard goals and more goals for the optimization proposals in the KafkaRebalance custom resource. Cruise Control BrokerCapacity Specifies capacity limits for broker resources. Use the spec.strimziOverrides.cruiseControl.brokerCapacity property to define capacity limits for Kafka broker resources. Cruise Control will use the set limits to determine if resource-based optimization goals are being broken. The following table provides information about the resource capacity settings, the goals they affect, and the units they use:             brokerCapacity      Goal      unit                  inboundNetwork      com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal      KB/s              outboundNetwork      com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal      KB/s              disk      com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal      Bytes              cpuUtilization      com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal      Percent (0-100)      Note: Ensure you add the unit when configuring a brokerCapacity key, except for cpuUtilization where the percentage is not required. For example, to configure Cruise Control to optimize around having an inbound network byte rate of 1000 kilobytes per second and a cpu utilization of 80 percent, configure the spec.strimziOverrides.cruiseControl.brokerCapacity property as follows: # ...spec:  # ...  strimziOverrides:    # ...    cruiseControl:      # ...      brokerCapacity:        # ...        inboundNetwork: 1000KB/s        # Optimize for CPU utilization of 80%        cpuUtilization: 80Configuring the Kafka Exporter You can configure the Kafka Exporter to expose additional metrics to Prometheus on top of the default ones. For example, you can obtain the consumer group lag information for each topic. The Kafka Exporter can be configured using a regex to expose metrics for a collection of topics and consumer groups that match the expression. For example, to enable JMX metrics collection for the topic orders and the group buyers, configure the EventStreams custom resource as follows:   apiVersion: eventstreams.ibm.com/v1beta1  kind: EventStreams  # ...  spec:  # ...  strimziOverrides:    # ...    kafkaExporter:      groupRegex: orders      topicRegex: buyers      template:        pod:          metadata:            annotations:              prometheus.io/port: '9404'              prometheus.io/scheme: https              prometheus.io/scrape: 'true'For more information about configuration options, see configuring the Kafka Exporter. Configuring the JMX Exporter You can configure the JMX Exporter to expose JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes to Prometheus. To enable the collection of all JMX metrics available on the Kafka brokers and ZooKeeper nodes, configure the EventStreams custom resource as follows:   apiVersion: eventstreams.ibm.com/v1beta1  kind: EventStreams  # ...  spec:  # ...  strimziOverrides:    kafka:      metrics: {}      # ...    zookeepers:      #    # ...For more information about configuration options, see the following documentation:   Kafka and ZooKeeper JMX metrics configuration  Kafka JMX metrics configurationUsing your own certificates Event Streams offers the capability to provide your own CA certificates and private keys instead of using the ones generated by the operator. If a CA certificate and private key are provided, the listener certificate is generated automatically and signed using the CA certificate. Event Streams also offers the capability to provide your own certificates. Note: You must complete the process of providing your own certificates before installing an instance of Event Streams. You must provide your own X.509 certificates and keys in PEM format with the addition of a PKCS12-formatted certificate and the CA password. If you want to use a CA which is not a Root CA, you have to include the whole chain in the certificate file. The chain should be in the following order:   The cluster or clients CA  One or more intermediate CAs  The root CAAll CAs in the chain should be configured as a CA with the X509v3 Basic Constraints. Providing a CA certificate and key Note: In the following instructions, the CA public certificate file is denoted CA.crt and the CA private key is denoted CA.key. As Event Streams also serves the truststore in PKCS12 format, the following command can be used to generate the required .p12 file from the PEM format certificate and key: openssl pkcs12 -export -inkey CA.key -in CA.crt -out CA.p12 The cluster and/or clients certificates, and keys must be added to secrets in the namespace that the Event Streams instance is intended to be created in. The naming of the secrets and required labels must follow the conventions detailed in the following command templates. The following four commands can be used to create and label the secrets for custom certificates and keys. The templates demonstrate providing cluster certificates but the same commands can be re-used substituting cluster with clients in each secret name. For each command, provide the intended name and namespace for the Event Streams instance. oc create --namespace &lt;namespace&gt; secret generic &lt;instance-name&gt;-cluster-ca --from-file=ca.key=CA.key oc label --namespace &lt;namespace&gt; secret &lt;instance-name&gt;-cluster-ca eventstreams.ibm.com/kind=Kafka eventstreams.ibm.com/cluster=&lt;instance-name&gt; oc create --namespace &lt;namespace&gt; secret generic &lt;instance-name&gt;-cluster-ca-cert --from-file=ca.crt=CA.crt --from-file=ca.p12=CA.p12 --from-literal=ca.password='&lt;CA_PASSWORD&gt;' oc label --namespace &lt;namespace&gt; secret &lt;instance-name&gt;-cluster-ca-cert eventstreams.ibm.com/kind=Kafka eventstreams.ibm.com/cluster=&lt;instance-name&gt; To make use of the provided secrets, Event Streams will require the following overrides to be added to the custom resource. spec:  # ...  strimziOverrides:    clusterCa:      generateCertificateAuthority: false  # And/Or    clientsCa:      generateCertificateAuthority: falseIt is also possible to configure the renewalDays (default 30) and validityDays (default 365) under the spec.strimziOverrides.clusterCa and spec.strimziOverrides.clientsCa keys. Validity periods are expressed as a number of days after certificate generation. Providing listener certificates To use TLS hostname verification with your own Kafka listener certificates, ensure you use the correct Subject Alternative Names (SANs) for each listener. The certificate SANs must specify hostnames for:       All of the Kafka brokers in your cluster         The Kafka cluster bootstrap service   You can use wildcard certificates if they are supported by your CA. For internal listeners, the hostnames will be service names. For external listeners, the hostnames will be the route addresses. Create a secret containing the private key and server certificate: oc create secret generic my-secret --from-file=my-listener-key.key --from-file=my-listener-certificate.crt To make use of the secret, Event Streams will require the following overrides to be added to the custom resource. spec:  # ...  strimziOverrides:    kafka:      listeners:        external:          # ...          configuration:            brokerCertChainAndKey:              certificate: my-listener-certificate.crt              key: my-listener-key.key              secretName: my-secret","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/configuring/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "10.1",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying an installation To verify that your Event Streams installation deployed successfully, you can check the status of your instance through the OpenShift Container Platform web console or command line. Check the status of the EventStreams instance through the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  The Phase field will display the current state of the EventStreams custom resource. When the Event Streams instance is ready, the phase will display Ready, meaning the deployment has completed.Check the status of the Event Streams instance through the command line After all the components of an Event Streams instance are active and ready, the EventStreams custom resource will have a Ready phase in the status.To verify the status:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the oc get command as follows: oc get eventstreamsFor example, the installation of the instance called development is complete when the STATUS returned by the oc get command displays Ready:oc get eventstreamsAn example output: $ oc get eventstreams&gt;NAME             STATUSdevelopment      ReadyNote: It might take several minutes for all the resources to be created and the EventStreams instance to become ready. Installing the Event Streams command-line interface The Event Streams CLI is a plugin for the cloudctl CLI. Use the Event Streams CLI to manage your Event Streams instance from the command line.Examples of management activities include:   Creating, deleting, and updating Kafka topics.  Creating, deleting, and updating Kafka users.  Creating, deleting, and updating Kafka message schemas.  Managing geo-replication.  Displaying the cluster configuration and credentials.To install the Event Streams CLI:   Ensure you have the IBM Cloud Pak CLI (cloudctl) installed either by retrieving the binary from your cluster or downloading the binary from a release on the GitHub project.Note: Ensure you download the correct binary for your architecture and operating system.  Log in to your Event Streams instance as an administrator.  Click Toolbox in the primary navigation.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;path-to-plugin&gt;To start the Event Streams CLI and check all available command options in the CLI, use the cloudctl es command.For an exhaustive list of commands, you can run: cloudctl es --help To get help for a specific command, run: cloudctl es &lt;command&gt; --help To use the Event Streams CLI against an OpenShift Container Platform cluster, do the following: Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt; To configure the CLI to connect to a specific Event Streams instance running a namespace: cloudctl es init -n &lt;namespace&gt; Firewall and load balancer settings Consider the following guidance about firewall and load balancer settings for your deployment. Using OpenShift Container Platform routes Event Streams uses OpenShift routes. Ensure your OpenShift router is set up as required. Connecting clients For instructions about connecting a client to your Event Streams instance, see connecting clients. Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Platform Common Services teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment. Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/post-installation/",
        "teaser":null},{
        "title": "Migrating from open-source Apache Kafka to Event Streams",
        "collection": "10.1",
        "excerpt":"If you are using open-source Apache Kafka as your event-streaming platform, you can move to IBM Event Streams and benefit from its features and enterprise-level support. Prerequisites Ensure you have an Event Streams deployment available. See the instructions for installing on OpenShift Container Platform. For many of the tasks, you can use the Kafka console tools. Many of the console tools work with Event Streams, as described in the using console tools topic. Create the required topics in Event Streams In Event Streams, create the same set of topics that have been deployed in the open-source Kafka cluster. To list these topics, run the following Kafka console tool: ./kafka-topics.sh --bootstrap-server &lt;host&gt;:&lt;port&gt; --describe For each existing topic, create a new topic in Event Streams with the same name. Ensure you use the same partition and replica settings, as well as any other non-default settings that were applied to the existing topic in your open-source Kafka instance. Change producer configuration Change the configuration for applications that produce messages to your open-source Kafka cluster to connect to Event Streams instead as described in connecting clients. If you are using the Kafka console tools, see the instructions for the example console producer in using the console tools to change where the messages are produced to. Change consumer configuration Change the configuration for applications that consume messages from your open-source Kafka cluster to connect to Event Streams instead as described in connecting clients. If you are using the Kafka console tools, see the instructions for the example console consumer in using the console tools to change where messages are consumed from. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/moving-from-oss-kafka/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "10.1",
        "excerpt":"You can remove an Event Streams instance by using the oc utility or the OpenShift Container Platform web console.You can also remove the Event Streams operator itself. Uninstalling an Event Streams instance Uninstalling using the OpenShift Container Platform web console To delete an Event Streams instance:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  In the Operator Details panel, select the Event Streams tab to show the  Event Streams instances in the selected namespace.  Click  More options next to the instance to be deleted to open the actions menu.  Click the Delete EventStreams menu option to open the confirmation panel.  Check the namespace and instance name and click Delete to shutdown the associated pods and delete the instance.Check uninstallation progress   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Workloads dropdown and select Pods to open the Pods dashboard.  Click Select All Filters to display pods in any state.  Enter the name of the Event Streams instance being deleted in the Filter by name box.  Wait for all the Event Streams pods to be displayed as Terminating and then be removed from the list.Removing persistence resources If you had enabled persistence for the Event Streams instance but set the deleteClaim storage property to false, you will need to manually remove the associated Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) that were created at installation time. The deleteClaim property is configured in the EventStreams custom resource and can be set to true during installation to ensure the PVs and PVCs are automatically removed when the instance is deleted. For Kafka and ZooKeeper, this property can be found as follows:   spec.strimziOverrides.kafka.storage.deleteClaim  spec.strimziOverrides.zookeeper.storage.deleteClaimFor other components, this property can be found as follows:   spec.&lt;component_name&gt;.storage.deleteClaimImportant: This change will cause data to be removed during an upgrade. For example, to configure automatic deletion for the Kafka storage when uninstalling: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams...spec:  ...  strimziOverrides:    ...    kafka:      ...      storage:        type: persistent-claim        ...        deleteClaim: trueTo remove any remaining storage, delete the PVCs first then delete any remaining PVs. Delete the Persistent Volume Claims (PVCs):   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Storage dropdown and select Persistent Volume Claims to open the Persistent Volume Claims page.  In the Project dropdown select the required namespace.  Click Select All Filters to display PVCs in any state.  Enter the name of the Event Streams instance in the Filter by name box.  For each PVC to be deleted, make a note of the Persistent Volume listed for that PVC and then click  More options to open the actions menu.  Click the Delete Persistent Volume Claim menu option to open the confirmation panel.  Check the PVC name and namespace, then click Delete to remove the PVC.Delete remaining Persistent Volumes (PVs):   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Storage dropdown and select Persistent Volumes to open the Persistent Volumes page.  In the Project dropdown select the required namespace.  For each PV you made a note of when deleting PVCs, click  More options to open the actions menu.  Click the Delete Persistent Volume menu option to open the confirmation panel.  Check the PV name and click Delete to remove the PV.Uninstalling using the CLI You can delete an Event Streams installation using the oc command line tool:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Ensure you are using the project where your Event Streams instance is located:oc project &lt;project_name&gt;  Run the following command to display the Event Streams instances:oc get es  Run the following command to delete your instance:oc delete es --selector app.kubernetes.io/instance=&lt;instance_name&gt;Check uninstallation progress Run the following command to check the progress:oc get pods --selector app.kubernetes.io/instance=&lt;instance_name&gt; Pods will initially display a STATUS Terminating and then be removed from the output as they are deleted. $ oc get pods --selector app.kubernetes.io/instance=minimal-prod&gt;NAME                                            READY     STATUS        RESTARTS   AGEminimal-prod-entity-operator-77dfff7c79-cnrx5   0/2       Terminating   0          5h35mminimal-prod-ibm-es-admapi-b49f976c9-xhsrv      0/1       Terminating   0          5h35mminimal-prod-ibm-es-recapi-6f6bd784fc-jvf9z     0/1       Terminating   0          5h35mminimal-prod-ibm-es-ac-reg-6dffdb54f9-dfdpl     0/3       Terminating   0          5h35mminimal-prod-ibm-es-ui-5dd7496dbc-qks7m         0/2       Terminating   0          5h35mminimal-prod-kafka-0                            2/2       Terminating   0          5h36mminimal-prod-zookeeper-0                        0/2       Terminating   0          5h37mRemoving persistence resources If you had enabled persistence for the Event Streams instance but set the deleteClaim storage property to false, you will need to manually remove the associated Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) that were created at installation time. The deleteClaim property is configured in the EventStreams custom resource and can be set to true during installation to ensure the PVs and PVCs are automatically removed when the instance is deleted. For Kafka and ZooKeeper, this property can be found as follows:   spec.strimziOverrides.kafka.storage.deleteClaim  spec.strimziOverrides.zookeeper.storage.deleteClaimFor other components, this property can be found as follows:   spec.&lt;component_name&gt;.storage.deleteClaimImportant: This change will cause data to be removed during an upgrade. For example, to configure automatic deletion for the Kafka storage when uninstalling: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams...spec:  ...  strimziOverrides:    ...    kafka:      ...      storage:        type: persistent-claim        ...        deleteClaim: trueTo remove any remaining storage, delete the PVCs first then delete any remaining PVs. Delete the PVCs:       Run the following command to list the remaining PVCs associated with the deleted instance:oc get pvc --selector app.kubernetes.io/instance=&lt;instance_name&gt;         Run the following to delete a PVC:oc delete pvc &lt;pvc_name&gt;   Delete remaining PVs:   Run the following command to list the remaining PVs:oc get pv  Run the following command to delete any PVs that were listed in the Volume column of the deleted PVCs.oc delete pv &lt;pv_name&gt;Note: Take extreme care to select the correct PV name to ensure you do not delete storage associated with a different application instance. Uninstalling an Event Streams operator To delete an Event Streams operator:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Operators and click Installed Operators.  In the Project dropdown select the required namespace. For cluster-wide operators, select the openshift-operators project.  Click  More options next to the IBM Event Streams operator to be deleted to open the actions menu.  Click the Uninstall Operator menu option to open the confirmation panel.  Check the namespace and operator name, then click Remove to uninstall the operator.Removing Event Streams Custom Resoure Definitions The Event Streams Custom Resource Definitions (CRDs) are not deleted automatically. You must manually delete any CRDs that you do not want:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Administration and click Custom Resource Definitions.  Enter eventstreams in the Filter by name box to filter the CRDs associated with Event Streams.  Click  More options next to the CRD to be deleted to open the actions menu.  Click the Delete Custom Resource Definition menu option to open the confirmation panel.  Check the name of the CRD and click Delete to remove the CRD.Uninstalling IBM Cloud Platform Common Services For information about uninstalling IBM Cloud Platform Common Services see the IBM Cloud Platform Common Services documentation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading and migrating",
        "collection": "10.1",
        "excerpt":"Upgrade your installation of IBM Event Streams operator to 2.1.0 and operand to 10.1.0 as follows. Upgrade paths The following upgrade paths are available:   You can upgrade the Event Streams operator to version 2.1.0 directly from versions 2.0.0 and 2.0.1.  You can upgrade the Event Streams operand to version 10.1.0 directly from version 10.0.0.  If you have an earlier version than 10.0.0, you must first upgrade your Event Streams version to 10.0.0, before upgrading to 10.1.0.Prerequisites If you are upgrading Event Streams in an existing IBM Cloud Pak for Integration deployment, ensure that the IBM Cloud Pak for Integration operator has been upgraded from 2020.2.1 to v2020.3.1. This adds the  IBM Event Streams operator version 2.1.0 to the OpenShift Container Platform OperatorHub catalog. Upgrade process The upgrade process requires the upgrade of the Event Streams operator, and then the upgrade of your Event Streams instances. Complete the steps in the following sections to upgrade your Event Streams installation. Upgrade the Event Streams operator to 2.1.0   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Operators in the navigation on the left, and click Installed Operators.  Locate the operator that manages your Event Streams instance in the namespace. It is called IBM Event Streams in the NAME column.  Click the IBM Event Streams link in the row and click the Subscription tab. This shows the Subscription Overview for the Event Streams operator.  Select v2.0 in the Channel section. The Change Subscription Update Channel dialog is displayed, showing the channels that are available to upgrade to.  Select v2.1 and click the Save button on the Change Subscription Update Channel dialog.All Event Streams pods that need to be updated as part of the upgrade will be gracefully rolled. Where required ZooKeeper pods will roll one at a time, followed by Kafka brokers rolling one at a time. Note: The number of containers in each Kafka broker will reduce from 2 to 1 as the TLS-sidecar container will be removed from each broker during the upgrade process. Upgrade the Event Streams operand (instance) to 10.1.0   Click Installed Operators from the navigation on the left to show the list of installed operators that includes the upgraded IBM Event Streams operator.  Select the IBM Event Streams operator from the list of Installed Operators.  Select the Event Streams tab. This lists the Event Streams operands.  Find your instance in the Name column and click the link for the instance.   Select the YAML tab. The Event Streams instance custom resource is shown.  In the YAML, change the version field from 10.0.0 to 10.1.0.  Click the Save button.All Event Streams pods will gracefully roll again. Verify the upgrade   Wait for all Event Streams pods to complete the upgrade process. This is indicated by the Running state.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  To retrieve a list of Event Streams instances, run the following command:oc get es -n &lt;namespace&gt;  For the instance of Event Streams that you upgraded, check that the status returned by the command in the previous step is Ready.Post-upgrade tasks Upgrade the Kafka broker protocol version When all previous steps are complete and the cluster’s behavior and performance have been verified, change the inter.broker.protocol.version value in the YAML and set it to 2.6. This will restart the brokers one at a time. Upgrade to use the Apicurio Registry If you want to migrate from the deprecated Event Streams schema registry to Apicurio Registry, you will need to move your schemas to the new registry and reconfigure any applications that use those schemas to connect to the new registry, as described in migrating. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/upgrading/",
        "teaser":null},{
        "title": "Migrating to Apicurio Registry",
        "collection": "10.1",
        "excerpt":"Overview The Event Streams schema registry for managing schemas is deprecated in version 10.1.0 and later. It is replaced by the Apicurio Registry, an open-source schema registry. This means that the registry for schemas is set by using the Apicurio Registry configuration option spec.apicurioRegistry in the EventStreams custom resource (instead of the deprecated spec.schemaRegistry setting). To migrate from the deprecated schema registry to the Apicurio Registry, you will need to move your schemas to the new registry and reconfigure any applications that use those schemas to connect to the new registry, as described in the following sections. Note: You only need to migrate if you have an existing Event Streams installation where you are using the schema registry with existing schemas. Migrating To migrate your schemas to the Apicurio Registry, use the following steps:   Ensure that you have upgraded your Event Streams version to 10.1.0.      Update the EventStreams custom resource to use the new Apicurio Registry configuration, ensuring not to delete the spec.schemaRegistry field. Deleting this field  will disrupt any clients currently using the schema registry and may result in schema data loss.     The following is an example snippet showing how to add Apicurio in the custom resource:     # ...spec:  # ...  apicurioRegistry: {}        Note: This will trigger a warning to appear in the status conditions of the EventStreams custom resource, informing a user they have both registries deployed. Ignore this for now, as we are currently migrating them and once finished with this migration the warning will be removed.         Wait until the operator has updated the Event Streams instance to include the newly deployed Apicurio Registry. You can check if Apicurio has been included by running the following command:     oc get eventstreams &lt;instance_name&gt; -ojsonpath={.status.routes.ac-reg-external}     Where &lt;instance_name&gt; is the name of your Event Streams instance. This will return a route name when the instance has been updated.     When the Apicurio Registry is up we can start migrating the schemas.         Migrate the schemas to the Apicurio Registry. Ensure you have logged into the CLI. If prompted to choose a schema registry on login, pick the deprecated registry:     cloudctl es init -n &lt;namespace&gt;Select a schema registry service:1. Apicurio Registry2. Event Streams Schema Registry (deprecated)Enter a number&gt;        Enter number 2, picking the deprecated schema registry.         Export your schemas from the deprecated registry as follows:     cloudctl es schemas-export --file my-schemas.zip     A message similar to this should appear:     Schemas to export: XExported Y versions from schema my-schema-1Exported Z versions from schema my-schema-2...Exported schemas successfully written to /tmp/my-schemas.zipOK        Note: This should normally take less than a minute but may take longer for schema registries with many large schemas.         Switch over to the Apicurio Registry by running the init command again, this time selecting Apicurio Registry:     cloudctl es init -n &lt;namespace&gt;Select a schema registry service:1. Apicurio Registry2. Event Streams Schema Registry (deprecated)Enter a number&gt;        Enter number 1 to select the Apicurio Registry.         Import your schemas into the new Apicurio Registry by running:     cloudctl es schemas-import my-schemas.zip     Note: Subsequent runs of this command will not add pre-existing schemas again.         Validate that all schemas are migrated into the Apicurio Registry by running:     cloudctl es schemas     Check that all previously used schemas are present and listed.     Switch clients over to use the schemas in the Apicurio Registry by changing their configuration to use the new Apicurio Registry route.dIf using the Event Streams Java serdes, update the com.ibm.eventstreams.serdes.SchemaRegistryConfig.PROPERTY_API_URL property value as follows:    props.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://&lt;route_address&gt;\");        Where &lt;route_address&gt; is the output of oc get eventstreams &lt;instance_name&gt; -ojsonpath={.status.routes.ac-reg-external}     When all schemas and clients have been migrated over to use the Apicurio Registry, you can safely remove the spec.schemaRegistry key and any configuration you applied to it. Run the following command:   oc edit eventstreams &lt;instance_name&gt;   Where &lt;instance_name&gt; is the name of your Event Streams instance.   Alternatively, the same edit can be made through the OpenShift Container Platform web console.Cleaning up persistence Apicurio Registry persists its data in Event Streams Kafka topics, and not in persistent storage. If you used persistence for the previous schema registry, and you have validated that all required schema registry data has been successfully migrated to the Apicurio Registry as described in the previous section, then you can delete the PersistentVolumeClaim (PVC) and PersistentVolume (PV) used by the previous registry. These are not automatically deleted to ensure that if the schema registry is accidentally removed, data loss does not occur. To delete the previous PVCs and PVs that are no longer required:       Identify the previous schema registry PVC by running the following command, replacing &lt;instance_name&gt; with the name of your Event Streams installation:     oc get pvc -l app.kubernetes.io/instance=&lt;instance_name&gt;,app.kubernetes.io/name=schema-registry     This command displays an output similar to the following example:     NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE&lt;instance_name&gt;-ibm-es-schema   Bound    pvc-7bb4d042-7fb3-4334-9c64-12d9c4806e4a   100Mi      RWO            rook-ceph-block   100d        Make a note of the value in the VOLUME field, this is the persistent volume that the claim is bound to.         Delete the PVC by running the following command:     oc delete pvc &lt;instance_name&gt;-ibm-es-schema     Where &lt;instance_name&gt; is the name of your Event Streams installation.         Deletion of the PVC will also delete the underlying PV for many types of storage. However, some storage types might leave the PV in place without deleting it. In such cases, manually delete the related PV.                   Check if the PV has been deleted:oc get pv &lt;volume_name&gt;Where &lt;volume_name&gt; is the previously noted value from the VOLUME field, defining the volume name that the claim was bound to.                     If this command returns no PV, then all relevant PVs have already been deleted automatically. If it has not been deleted, the PV can be used for other applications, or it can be deleted by running the follwing command:oc delete pv &lt;volume_name&gt;             ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/installing/migrating-to-apicurio/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "10.1",
        "excerpt":"Log in to your IBM Event Streams UI from a supported web browser. Determining the URL depends on your platform. Using the OpenShift Container Platform CLI Event Streams uses OpenShift routes. To retrieve the URL for your Event Streams UI, use the following command:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command:     oc get routes -n &lt;namespace&gt; -l app.kubernetes.io/name=admin-ui        The following is an example output, and you use the value from the HOST/PORT column to log in to your UI in a web browser:     NAME                        HOST/PORT                                                           PATH   SERVICES                    PORT   TERMINATION   WILDCARDmy-eventstreams-ibm-es-ui   my-eventstreams-ibm-es-ui-myproject.apps.my-cluster.my-domain.com          my-eventstreams-ibm-es-ui   3000   reencrypt     None        Enter the address in a web browser. Add https:// in front of the HOST/PORT value. For example:    https://my-eventstreams-ibm-es-ui-myproject.apps.my-cluster.my-domain.com        Use your credentials provided to you by your cluster administrator.A cluster administrator can manage access rights by following the instructions in managing access.Enter your username and password to access the Event Streams UI.Using OpenShift Container Platform UI Event Streams uses OpenShift routes. To retrieve the URL for your Event Streams UI, you can find it in the OpenShift UI:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand Operators in the navigation on the left, and click Installed Operators.  Locate the operator that manages your Event Streams instance in the namespace. It is called IBM Event Streams in the NAME column.  Click the IBM Event Streams link in the row and click the Event Streams tab. This lists the Event Streams operands related to this operator.  Find your instance in the Name column and click the link for the instance.   A link to the IBM Event Streams UI is displayed under the label Admin UI. Click the link to open the IBM Event Streams UI login page in your browser tab.  Use your credentials provided to you by your cluster administrator.A cluster administrator can manage access rights by following the instructions in managing access.Enter your username and password to access the Event Streams UI.Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Platform Common Services session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/logging-in/",
        "teaser":null},{
        "title": "Creating a Kafka topic",
        "collection": "10.1",
        "excerpt":"To use Kafka topics to store events in IBM Event Streams, create and configure a Kafka topic. Using the UI   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Home in the primary navigation.  Click the Create a topic tile.      Enter a topic name in the Topic name field, for example, my-topic.This is the name of the topic that an application will be producing to or consuming from.     Click Next.         Enter the number of Partitions, for example, 1.Partitions are used for scaling and distributing topic data across the Apache Kafka brokers.For the purposes of a basic starter application, using only 1 partition is sufficient.     Click Next.         Select a Message retention,  for example,  A day.This is how long messages are retained before they are deleted.     Click Next.         Select a replication factor in Replicas,  for example, Replication factor: 1.This is how many copies of a topic will be made for high availability. For production environments, select Replication factor: 3 as a minimum.     Click Create topic. The topic is created and can be viewed from the Topics tab located in the primary navigation.Note: To view all configuration options you can set for topics, set Show all available options to On. Note: Kafka supports additional topic configuration settings. Enable Show all available options to access more detailed configuration settings if required. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init      Run the following command to create a topic:     cloudctl es topic-create --name &lt;topic-name&gt; --partitions &lt;number-of-partitions&gt; --replication-factor &lt;replication-factor&gt;     For example, to create a topic called my-topic that has 1 partition, a replication factor of 1, and 1 day set for message retention time (provided in milliseconds):     cloudctl es topic-create --name my-topic --partitions 1 --replication-factor 1 --config retention.ms=86400000     Important: Do not set &lt;replication-factor&gt; to a greater value than the number of available brokers.   Note: To view all configuration options you can set for topics, use the help option as follows: cloudctl es topic-create --help Kafka supports additional topic configuration settings. Extend the topic creation command with one or more --config &lt;property&gt;=&lt;value&gt; properties to apply additional configuration settings. The following additinal properties are currently supported:   cleanup.policy  compression.type  delete.retention.ms  file.delete.delay.ms  flush.messages  flush.ms  follower.replication.throttled.replicas  index.interval.bytes  leader.replication.throttled.replicas  max.message.bytes  message.format.version  message.timestamp.difference.max.ms  message.timestamp.type  min.cleanable.dirty.ratio  min.compaction.lag.ms  min.insync.replicas  preallocate  retention.bytes  retention.ms  segment.bytes  segment.index.bytes  segment.jitter.ms  segment.ms  unclean.leader.election.enable","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/creating-topics/",
        "teaser":null},{
        "title": "Running a starter application",
        "collection": "10.1",
        "excerpt":"To learn more about how to create applications that can take advantage of IBM Event Streams capabilities, you can use the starter application. The starter application can produce and consume messages, and you can specify the topic to send messages to and the contents of the message. About the application The starter application provides a demonstration of a Java application that uses the Vert.x Kafka Client to send events to, and receive events from, Event Streams. It also includes a user interface to easily view message propagation. The source code is provided in GitHub to allow you to understand the elements required to create your own Kafka application. Downloading the application If you do not already have the application, download the JAR file from the Event Streams UI.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Try the starter application tile, or click Toolbox in the primary navigation, go to the Starter application section, and click Get started.  Click Download JAR from GitHub.  Download the JAR file for the latest release.Generate security and configuration files Before you can run the application, generate security and configuration files to connect to your Event Streams and the target topic. Some of the following steps depend on your access permissions. If you are not permitted to generate credentials you will not see the Generate properties button and will have to obtain security and configuration files from your administrator before running the application. If you are not permitted to create topics, you will not be able to create a topic as part of this lesson and will have to use a pre-existing topic.   From the Starter application menu opened in the previous step, click Generate properties to open the side panel.      Enter an application name in the Starter application name field. This value is used by Event Streams to create a KafkaUser, which will provide your application with credentials to connect to Event Streams securely.Note: This name must be unique to avoid potential clashes with pre-existing KafkaUser resources.     Select a new or existing topic to connect to.Note: When creating a new topic the name must be unique to avoid potential clashes with pre-existing topics.  Click the Generate and download .zip button to download the compressed file, then extract the contents to your preferred location.Running the application Before running the application, ensure you have the following available:   The application JAR file.  A directory containing security and configuration filesRun the following command to start the application: java -Dproperties_path=&lt;configuration_properties_path&gt; -jar &lt;jar_path&gt;/demo-all.jarWhere:   configuration_properties_path is the path to the directory containing the extracted security and configuration files.  jar_path is the path to the downloaded application JAR file.Wait for the application to be ready. It will print out the following message: Application started in XmsWhen the application is ready, access the UI by using the following URL: http://localhost:8080. Use the start button in the UI to produce messages and see them consumed. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads",
        "collection": "10.1",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory of the event-streams-sample-producer project.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the bootstrap.servers and any required security configuration details in the configuration file. Obtaining configuration details The bootstrap servers address can be obtained from the Event Streams UI as described in the following steps. Other methods to obtain the bootstrap servers address are described in connecting client.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Click the Resources tab.  Go to the Kafka listener and credentials section.  Copy the address from one of the External listeners.The producer application might require credentials for the listener chosen in the previous step. For more information about these credentials, see the information about managing access. Obtain the required credentials as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Kafka listener and credentials section.  Click the button next to the listener chosen as the bootrap.servers configuration. If present, the button will either be labelled Generate SCRAM credentials or Generate TLS credentials.  Select Produce messages, consume messages and create topics and schemas and click Next.  Select A specific topic, enter the name of a topic to produce to and click Next.  Select All consumer groups and click Next.  Select No transactional IDs and click Generate credentials.  Retrieve the generated credentials:          If using SCRAM note down the Username and password.      If using TLS click Download certificates and extract the contents of the resulting .zip file to a preferred location.      Obtain the Event Streams certificate as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Certificates section.  In the PKCS12 certificate section click Download certificate.  Note down the generated password displayed in the Certificate password section.Updating the configuration file Before updating the file, obtain the credentials by following the steps in Obtaining configuration details. Update the producer.config file with your configuration details using the following table as guidance.             Attribute      Description                         bootstrap.servers      The bootstrap address for the chosen external listener.                     security.protocol      Set to SSL. Can be ommitted if the chosen external listener has TLS disabled.                     ssl.truststore.location      The full path and name of the Event Streams PKCS12 certificate file. Can be ommitted if the chosen external listener has TLS disabled.                     ssl.truststore.password      The password for the Event Streams PKCS12 certificate file. Can be ommitted if the chosen external listener has TLS disabled.                     sasl.mechanism      Set to SCRAM-SHA-512 if using SCRAM credentials, otherwise ommitted.                     sasl.jaas.config      Set to org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";, where &lt;username&gt; and &lt;password&gt; are replaced with the SCRAM credentials. Omitted if not using SCRAM credentials.                     ssl.keystore.location      Set to the full path and name of the user.p12 keystore file downloaded from the Event Streams UI. Ommitted if not using TLS credentials.                     ssl.keystore.password      Set to the password listed in the user.password file downloaded from the Event Streams UI. Ommitted if not using TLS credentials.             Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t my-topic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called my-topic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Specifying a load You can generate a custom message load using your own settings. For example, to test the load to the topic called my-topic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t my-topic -T 1000 -n 60000 -r 1024The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      integer      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      integer      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      boolean      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      integer      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      integer      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "10.1",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:    &lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;    &lt;version&gt;1.7.26&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;    &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;    &lt;version&gt;1.7.26&lt;/version&gt;&lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:    implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.6.0'implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.26'implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.26'        Ensure you set up security.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/client/",
        "teaser":null},{
        "title": "Connecting clients",
        "collection": "10.1",
        "excerpt":"Learn how to discover connection details to connect your clients to your Event Streams instance. Obtaining the bootstrap address Use one of the following methods to obtain the bootstrap address for your connection to your Event Streams instance, choosing the listener type appropriate for your client. More information on configuring listener types can be found in the configuring Kafka access section. Using the Event Streams UI   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile.      Go to the Kafka listener and credentials section, and select the listener from the list.           Click the External tab for applications connecting from outside of the OpenShift Container Platform cluster.      Click the Internal tab for applications connecting from inside the OpenShift Container Platform cluster.        Note: The list reflects the listeners configured in spec.strimziOverrides.kafka.listeners. For example, you will have external listeners displayed if you have spec.strimziOverrides.kafka.listeners.external configured. If spec.strimziOverrides.kafka.listeners is empty for your instance (not configured), then no address is displayed here.   Using the Event Streams CLI Note: You can only use the Event Streams CLI to retrieve the address if your Event Streams instance has an external listener configured in spec.strimziOverrides.kafka.listeners.external.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.      Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es initMake note of the Event Streams bootstrap address value. This is the Kafka bootstrap address that your application will use.     Note: If you have multiple listeners defined in spec.strimziOverrides.kafka.listeners, only the external listener is displayed. If you only have internal listeners defined, nothing is displayed.   Using the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Select the YAML tab.  Scroll down and look for status.kafkaListeners.  The kafkaListeners field will contain one or more listeners each with a bootstrapServers property.Find the type of listener you want to connect to and use the bootstrapServers value from the entry.Note: if using the external Kafka listener, the OpenShift route is a HTTPS address so the port in use is 443. Using the OpenShift Container Platform CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  To find the type and address for the Kafka bootstrap route for each listener run the following command:oc get eventstreams &lt;instance-name&gt; -o=jsonpath='{range .status.kafkaListeners[*]}{.type} {.bootstrapServers}{\"\\n\"}{end}'Where &lt;instance-name&gt; is the name of your Event Streams instance.Note: if using the external Kafka listener, the OpenShift route is a HTTPS address so the port in use is 443. Securing the connection To connect client applications to a secured IBM Event Streams, you must obtain the following:   A copy of the server-side public certificate to add to your client-side trusted certificates.  SCRAM-SHA-512 (username and password) or Mutual TLS (user certificates) Kafka credentials.Obtaining the server-side public certificate from the Event Streams UI   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  From the Certificates section, download the server certificate. If you are using a Java client, use the PKCS12 certificate, remembering to copy the truststore password presented during download. Otherwise, use the PEM certificate.Obtaining the server-side public certificate from the Event Streams CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init      Use the certificates command to download the cluster’s public certificate in the required format:cloudctl es certificates --format p12The truststore password will be displayed in the output for the command. The following example has a truststore password of mypassword:     $ cloudctl es certificates --format p12Trustore password is mypasswordCertificate successfully written to /es-cert.p12.OK        Note: You can optionally change the format to download a PEM Certificate if required.   Obtaining the server-side public certificate from the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Select the Resources tab.  To filter only secrets, deselect all resource types with the exception of Secret.  Locate and select the &lt;instance-name&gt;-cluster-ca-cert secret. Where &lt;instance-name&gt; is the name of your Event Streams instance.  In the Secret Overview panel scroll down to the Data section. Then, click the copy button to transfer the ca.p12 certificate to the clipboard. The password can be found under ca.password.Note: For a PEM certificate, click the copy button for ca.crt instead. Obtaining the server-side public certificate from the OpenShift Container Platform CLI To extract the server-side public certificate to a ca.p12 file, run the following command: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.p12 Where &lt;instance-name&gt; is the name of your Event Streams instance. To extract the password for the certificate to a ca.password file, run the following command: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.password Note: If a PEM certificate is required, run the following command to extract the certificate to a ca.crt file: oc extract secret/&lt;instance-name&gt;-cluster-ca-cert --keys=ca.crt Generating or Retrieving Client Credentials See the assigning access to applications section to learn how to create new application credentials or retrieve existing credentials. Configuring your SCRAM client Add the truststore certificate details and the SCRAM credentials to your Kafka client application to set up a secure connection from your application to your Event Streams instance. You can configure a Java application as follows: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;bootstrap-address&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.p12-file-location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore-password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.scram.ScramLoginModule required \"    + \"username=\\\"&lt;scram-username&gt;\\\" password=\\\"&lt;scram-password&gt;\\\";\");            Property Placeholder      Description                  &lt;bootstrap-address&gt;      Bootstrap servers address              &lt;certs.p12-file-location&gt;      Path to your truststore certificate. This must be a fully qualified path. As this is a Java application, the PKCS12 certificate is used.              &lt;truststore-password&gt;      Truststore password.              &lt;scram-username&gt;      SCRAM username.              &lt;scram-password&gt;      SCRAM password.      Configuring your Mutual TLS client Add the truststore and keystore certificate details to your Kafka client application to set up a secure connection from your application to your Event Streams instance. You can configure a Java application as follows: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;bootstrap-address&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.p12-file-location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore-password&gt;\");properties.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user.p12-file-location&gt;\");properties.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user.p12-password&gt;\");            Property Placeholder      Description                  &lt;bootstrap-address&gt;      Bootstrap servers address.              &lt;certs.p12-file-location&gt;      Path to your truststore certificate. This must be a fully qualified path. As this is a Java application, the PKCS12 certificate is used.              &lt;truststore-password&gt;      Truststore password.              &lt;user.p12-file-location&gt;      Path to user.p12 keystore file from credentials zip archive.              &lt;user.p12-password&gt;      The user.p12 keystore password found in the user.password file in the credentials zip archive.      Obtaining Java code samples from the Event Streams UI For a Java application, you can copy the connection code snippet from the Event Streams UI by doing the following:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile.  Click the Sample code tab.  Copy the snippet from the Sample connection code section into your Java Kafka client application. Uncomment the relevant sections and replace the property placeholders with the values from the relevant table for SCRAM or Mutual TLS.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/connecting/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "10.1",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      Yes                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No      cloudctl es entity-config              kafka-configs.sh --entity-type users      No      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      Yes      cloudctl es topics              kafka-topics.sh --describe      Yes      cloudctl es topic              kafka-topics.sh --create      Yes      cloudctl es topic-create              kafka-topics.sh --delete      Yes      cloudctl es topic-delete              kafka-topics.sh --alter --config      Yes      cloudctl es topic-update              kafka-topics.sh --alter --partitions      Yes      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      Yes      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Schemas overview",
        "collection": "10.1",
        "excerpt":"Apache Kafka can handle any data, but it does not validate the information in the messages. However, efficient handling of data often requires that it includes specific information in a certain format. Using schemas, you can define the structure of the data in a message, ensuring that both producers and consumers use the correct structure. Schemas help producers create data that conforms to a predefined structure, defining the fields that need to be present together with the type of each field. This definition then helps consumers parse that data and interpret it correctly. Event Streams supports schemas and includes a schema registry for using and managing schemas. It is common for all of the messages on a topic to use the same schema. The key and value of a message can each be described by a schema.  Schema registry Schemas are stored in internal Kafka topics by the Apicurio Registry, an open-source schema registry. In addition to storing a versioned history of schemas, Apicurio Registry provides an interface for retrieving them. Each Event Streams cluster has its own instance of Apicurio Registry providing schema registry functionality. Your producers and consumers validate the data against the specified schema stored in the schema registry. This is in addition to going through Kafka brokers. The schemas do not need to be transferred in the messages this way, meaning the messages are smaller than without using a schema registry.  If you are migrating to use Event Streams as your Kafka solution, and have been using a schema registry from a different provider, you can migrate to using Event Streams and the Apicurio Registry. The Event Streams schema registry provided in earlier versions is deprecated in version 10.1.0 and later. If you are upgrading to Event Streams version 10.1.0 from an earlier version, you can migrate to the Apicurio Registry from the deprecated schema registry. Apache Avro data format Schemas are defined using Apache Avro, an open-source data serialization technology commonly used with Apache Kafka. It provides an efficient data encoding format, either by using the compact binary format or a more verbose, but human-readable JSON format. The schema registry in Event Streams uses Apache Avro data formats. When messages are sent in the Avro format, they contain the data and the unique identifier for the schema used. The identifier specifies which schema in the registry is to be used for the message. Avro has support for a wide range of data types, including primitive types (null, boolean, integer, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Learn more about how you can create schemas in Event Streams.  Serialization and deserialization A producing application uses a serializer to produce messages conforming to a specific schema. As mentioned earlier, the message contains the data in Avro format, together with the schema identifier. A consuming application then uses a deserializer to consume messages that have been serialized using the same schema. When a consumer reads a message sent in Avro format, the deserializer finds the identifier of the schema in the message, and retrieves the schema from the schema registry to deserialize the data. This process provides an efficient way of ensuring that data in messages conform to the required structure. Serializers and deserializers that automatically retrieve the schemas from the schema registry as required are provided by IBM Event Streams. If you need to use schemas in an environment for which serializers or deserializers are not provided, you can use the command line or UI directly to retrieve the schemas.  Versions and compatibility Whenever you add a schema, and any subsequent versions of the same schema, Apicurio Registry validates the format automatically and warns of any issues. You can evolve your schemas over time to accommodate changing requirements. You simply create a new version of an existing schema, and the registry ensures that the new version is compatible with the existing version, meaning that producers and consumers using the existing version are not broken by the new version. When you create a new version of the schema, you simply add it to the registry and version it. You can then set your producers and consumers that use the schema to start using the new version. Until they do, both producers and consumers are warned that a new version of the schema is available.  Lifecycle When a new version is used, you can deprecate the previous version. Deprecating means that producing and consuming applications still using the deprecated version are warned that a new version is available to upgrade to. When you upgrade your producers to use the new version, you can disable the older version so it can no longer be used, or you can remove it entirely from the schema registry. You can use the Event Streams UI or CLI to manage the lifecycle of schemas, including registering, versioning, and deprecating.  How to get started with schemas   Create schemas  Add schemas to schema registry  Set your Java or non-Java applications to use schemas  Manage schema lifecycle","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/overview/",
        "teaser":null},{
        "title": "Creating and adding schemas",
        "collection": "10.1",
        "excerpt":"You can create schemas in Avro format and then use the Event Streams UI or CLI to add them to the Apicurio Registry. Creating schemas Event Streams supports Apache Avro schemas. Avro schemas are written in JSON to define the format of the messages. For more information about Avro schemas, see the Avro documentation. Apicurio Registry in Event Streams imports, stores, and uses Avro schemas to serialize and deserialize Kafka messages. Apicurio Registry supports Avro schemas using the record complex type. The record type can include multiple fields of any data type, primitive or complex. Define your Avro schema files and save them by using the .avsc or .json file extension. For example, the following Avro schema defines a Book record in the org.example namespace, and contains the Title, Author, and Format fields with different data types: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        }    ]}Adding schemas to the registry To use schemas in Kafka applications, import your schema definitions into the schema registry. Your applications can then retrieve the schemas from the registry as required. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation, and then click Add schema.  Click Upload definition and select your Avro schema file. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Optional: Edit the Schema name and Version fields.          The name of the record defined in the Avro schema file is added to the Schema name field. You can edit this field to add a different name for the schema. Changing the Schema name field does not update the Avro schema definition itself.      The value 1.0.0 is automatically added to the Version field as the initial version of the schema. You can edit this field to set a different version number for the schema.        Click Add schema. The schema is added to the list of schemas in the Event Streams schema registry.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to add a schema to the schema registry:cloudctl es schema-add --name &lt;schema-name&gt; --version &lt;schema-version&gt; --file &lt;path-to-schema-file&gt;Adding new schema versions Apicurio Registry can store multiple versions of the same schema. As your applications and environments evolve, your schemas need to change to accommodate the requirements. You can import, manage, and use different versions of a schema. As your schemas change, consider the options for managing their lifecycle. Note: A new version of a schema must be compatible with previous versions. This means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. For example, the following Avro schema defines a new version of the Book record, adding a PageCount field. By including a default value for this field, messages that were serialized with the previous version of this schema (which would not have a PageCount value) can still be deserialized using this version. {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },        {\"name\": \"PageCount\",  \"type\": \"int\", \"default\": 0}    ]}Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Locate your schema in the list of registered schemas and click its name. The list of versions for the schema is displayed.  Click Add new version to add a new version of the schema.  Click Upload definition and select the file that contains the new version of your schema. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Set a value in the Version field to be the version number for this iteration of the schema. For the current list of all versions, click View all versions.  Click Add schema. The schema version is added to the list of all versions for the schema.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Install the Event Streams CLI plugin if not already installed.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to list all schemas in the schema registry, and find the schema name you want to add a new version to:cloudctl es schemas  Run the following command to add a new version of the schema to the registry:cloudctl es schema-add --name &lt;schema-name-from-previous-step&gt; --version &lt;new-schema-version&gt; --file &lt;path-to-new-schema-file&gt;","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/creating/",
        "teaser":null},{
        "title": "Managing schema lifecycle",
        "collection": "10.1",
        "excerpt":"Multiple versions of each schema can be stored in the Apicurio Registry. Kafka producers and consumers retrieve the right schema version they use from the registry based on a unique identifier and version. When a new schema version is added, you can set both the producer and consumer applications to use that version. You then have the following options to handle earlier versions. The lifecycle is as follows:   Add schema  Add new schema version  Deprecate version or entire schema  Disable version or entire schema  Remove version or entire schemaDeprecating If you want your applications to use a new version of a schema, you can set the earlier version to Deprecated. When a version is deprecated, the applications using that version receive a message to warn them to stop using it. Applications can continue to use the old version of the schema, but warnings will be written to application logs about the schema version being deprecated. You can customize the message to be provided in the logs, such as providing information for what schema or version to use instead. Deprecated versions are still available in the registry and can be used again. Note: You can deprecate an entire schema, not just the versions of that schema. If the entire schema is set to deprecated, then all of its versions are reported as deprecated (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to deprecate from the list.  Set the entire schema or a selected version of the schema to be deprecated:          If you want to deprecate the entire schema and all its versions, click the Manage schema tab, and set Mark schema as deprecated to on.      To deprecate a specific version, select it from the list, and click the Manage version tab for that version. Then set Mark schema as deprecated to on.      Deprecated schemas and versions are marked with a Deprecated flag on the UI. You can re-activate a schema or its version by setting Mark schema as deprecated to off. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to deprecate a schema version:cloudctl es schema-modify --deprecate --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To deprecate an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     To re-activate a schema version:cloudctl es schema-modify --activate --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To re-activate an entire schema, do not specify the --version &lt;schema-version-id&gt; option.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Disabling If you want your applications to stop using a specific schema, you can set the schema version to Disabled. If you disable a version, applications will be prevented from producing and consuming messages using it. After being disabled, a schema can be enabled again to allow applications to use the schema. When a schema is disabled, applications that want to use the schema receive an error response. Java producers using the Event Streams schema registry serdes library will throw a SchemaDisabledException when attempting to produce messages using a disabled schema version. For example, the message and stack trace for a disabled schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaDisabledException: Schema \"Test_Schema\" version \"1.0.0\" is disabled.    at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:174)    at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:41)    at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:884)    at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:846)    at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:733)    at Producer.main(Producer.java:92)Note: You can disable a entire schema, not just the versions of that schema. If the entire schema is disabled, then all of its versions are disabled as well, which means no version of the schema can be used by applications (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to disable from the list.  Set the entire schema or a selected version of the schema to be disabled:          If you want to disable the entire schema and all its versions, click the Manage schema tab, and click Disable schema, then click Disable.      To disable a specific version, select it from the list, and click the Manage version tab for that version. Then click Disable version, then click Disable.You can re-enable a schema by clicking Enable schema, and re-enable a schema version by clicking  Re-enable version.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to disable a schema version:cloudctl es schema-modify --disable --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To disable an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     To re-enable a schema version:cloudctl es schema-modify --enable --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To re-enable an entire schema, do not specify the --version &lt;schema-version-id&gt; option.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Removing If a schema version has not been used for a period of time, you can remove it from the schema registry. Removing a schema version means it will be permanently deleted from the schema registry of your Event Streams instance, and applications will be prevented from producing and consuming messages using it. If a schema is no longer available in the registry, Java applications using the Event Streams schema registry serdes library will receive a SchemaNotFoundException message when they attempt to use the schema. For example, the message and stack trace when producing a message with a missing schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaNotFoundException: Schema \"Test_Schema\" not found    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.handleErrorResponse(SchemaRegistryRestAPIClient.java:145)    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.get(SchemaRegistryRestAPIClient.java:120)    at com.ibm.eventstreams.serdes.SchemaRegistry.downloadSchema(SchemaRegistry.java:253)    at com.ibm.eventstreams.serdes.SchemaRegistry.getSchema(SchemaRegistry.java:239)Important: You cannot reverse the removal of a schema. This action is permanent. Note: You can remove an entire schema, including all of its versions. If the entire schema is removed, then all of its versions are permanently deleted from the schema registry of your Event Streams instance. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation.  Select the schema you want to remove from the list.  Remove the entire schema or a selected version of the schema:          If you want to remove the entire schema and all its versions, click the Manage schema tab, and click Remove schema, then click Remove.      To remove a specific version, select it from the list, and click the Manage version tab for that version. Then click Remove version, then click Remove.        Important: This action is permanent and cannot be reversed.   Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to remove a schema version:cloudctl es schema-remove --name &lt;schema-name&gt; --version &lt;schema-version-id&gt;     To remove an entire schema, do not specify the --version &lt;schema-version-id&gt; option.     Important: This action is permanent and cannot be reversed.   Note: &lt;schema-version-id&gt; is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/manage-lifecycle/",
        "teaser":null},{
        "title": "Setting Java applications to use schemas",
        "collection": "10.1",
        "excerpt":"If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas. Note: If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas. Preparing the setup To use schemas stored in the Apicurio Registry in Event Streams, your client applications need to be able to serialize and deserialize messages based on schemas.   Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.  Consuming applications then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.The Event Streams UI provides help with setting up your Java applications to use schemas. Note: Apicurio Registry in Event Streams works with multiple schema registry serdes libraries, including the Event Streams schema registry serdes library and the Apicurio Registry serdes library. You can use either serdes library in your applications by adding Maven dependencies to your project pom.xml files. The following instructions and code snippets use the Event Streams schema registry serdes library. To use the Apicurio Registry serdes library, see the guidance about setting up Java applications to use schemas with the Apicurio Registry serdes library. To set up your Java applications to use the Event Streams schema registry serdes library and Apicurio Registry, prepare the connection for your application as follows:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Ensure you have added schemas to the registry.  Click Schema registry in the primary navigation.  Select a schema from the list and click the row for the schema.  Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.      Set the preferences for your connection in the Configure the schema connection section. Use the defaults or change them by clicking Change configuration.                   For producers, set the method for Message encoding.                   Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.          JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.                            For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.                   Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.          Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and will allow a Kafka consumer to continue to process further messages.                      If any configuration was changed, click Save.  Click Generate credentials to generate SCRAM or Mutual TLS credentials and follow the instructions.Important: Ensure you make note of the information provided upon completion as you will need this later.Alternatively you can generate credentials later by using the Event Streams UI or CLI.  Click Generate connection details.  Click Download certificate to download the cluster PKCS12 certificate. This is the Java truststore file which contains the server certificate. Take a copy of the Certificate password for use with the certificate in your application.      If your project uses Maven, add the following dependency to your project pom.xml file to use the Event Streams schema registry serdes library:     &lt;dependency&gt;    &lt;groupId&gt;com.ibm.eventstreams.schemaregistry&lt;/groupId&gt;    &lt;artifactId&gt;event-streams-serdes&lt;/artifactId&gt;    &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt;        Alternatively, if your project does not use Maven, select the Use JARs tab and click Java dependencies to download the java dependencies JAR files to use for your application in its code.     Depending on your application, click the Producer or Consumer tab, and copy the sample Java code snippets displayed. The sample code snippets include the settings you configured to set up your applications to use the schema.  Add the required snippets into your application code as described in the following sections.          Setting up producers to use schemas      Setting up consumers to use schemas      Setting up producers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies if not using Maven, and copying code snippets for a producing application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your producer Kafka application.  Use the code snippets you copied from the UI for a producer and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets up the application to use the Event Streams schema registry’s serdes library, for example: import java.util.Properties;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericRecord;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import com.ibm.eventstreams.serdes.SchemaInfo;import com.ibm.eventstreams.serdes.SchemaRegistry;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;The code snippet from the Connection properties section specifies connection and access permission details for your Event Streams cluster, for example: Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the SchemaRegistry and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//Schema Registry connectionprops.put(SchemaRegistryConfig.PROPERTY_API_URL, \"&lt;Schema registry endpoint&gt;\");//Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Follow the instructions in the code snippet to uncomment lines. Replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address), and &lt;Schema registry endpoint&gt; with the endpoint address for Apicurio Registry in Event Streams. For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. For more information about the configuration keys and values to use with the Event Streams serdes library, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Producer code section defines properties for the producer application that set it to use the schema registry and the correct schema, for example: // Set the value serializer for produced messages to use the Event Streams serializerprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"com.ibm.eventstreams.serdes.EventStreamsSerializer\");// Set the encoding type used by the message serializerprops.put(SchemaRegistryConfig.PROPERTY_ENCODING_TYPE, SchemaRegistryConfig.ENCODING_BINARY);// Get a new connection to the Schema RegistrySchemaRegistry schemaRegistry = new SchemaRegistry(props);// Get the schema from the registrySchemaInfo schema = schemaRegistry.getSchema(\"ABC_Assets_Schema\", \"1.0.0\");// Get a new Generic KafkaProducerKafkaProducer&lt;String, GenericRecord&gt; producer = new KafkaProducer&lt;&gt;(props);// Get a new Generic record based on the schemaGenericRecord genericRecord = new GenericData.Record(schema.getSchema());// Add fields and values to the genericRecord, for example:// genericRecord.put(\"title\", \"this is the value for a title field\");// Prepare the record, adding the Schema Registry headersProducerRecord&lt;String, GenericRecord&gt; producerRecord =    new ProducerRecord&lt;String, GenericRecord&gt;(\"&lt;my_topic&gt;\", genericRecord);producerRecord.headers().add(SchemaRegistryConfig.HEADER_MSG_SCHEMA_ID,    schema.getIdAsBytes());producerRecord.headers().add(SchemaRegistryConfig.HEADER_MSG_SCHEMA_VERSION,    schema.getVersionAsBytes());// Send the record to Kafkaproducer.send(producerRecord);// Close the producerproducer.close();The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsSerializer, telling Kafka to use the Event Streams serializer for message values when producing messages. You can also use the Event Streams serializer for message keys. For more information about the configuration keys and values to use with the Event Streams serdes library, see the SchemaRegistryConfig class in the schema API reference. Note: Use the put method in the GenericRecord class to set field names and values in your message. Note: Replace &lt;my_topic&gt; with the name of the topic to produce messages to. Setting up consumers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies if not using Maven, and copying code snippets for a consuming application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your consumer Kafka application.  Use the code snippets you copied from the UI for a consumer and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets up the application to use the Event Streams schema registry’s serdes library, for example: import java.time.Duration;import java.util.Arrays;import java.util.Properties;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericRecord;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.KafkaConsumer;The code snippet from the Connection properties section specifies connection and access permission details for your Event Streams cluster, for example: Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the SchemaRegistry and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//Schema Registry connectionprops.put(SchemaRegistryConfig.PROPERTY_API_URL, \"&lt;Schema registry endpoint&gt;\");//Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Follow the instructions in the code snippet to uncomment lines. Replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address), and &lt;Schema registry endpoint&gt; with the endpoint address for Apicurio Registry in Event Streams. For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. For more information about the configuration keys and values to use with the Event Streams serdes library, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Consumer code section defines properties for the consumer application that set it to use the schema registry and the correct schema, for example: // Set the value deserializer for consumed messages to use the Event Streams deserializerprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"com.ibm.eventstreams.serdes.EventStreamsDeserializer\");// Set the behavior of the deserializer when a record cannot be deserializedprops.put(SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE, SchemaRegistryConfig.BEHAVIOR_STRICT);// Set the consumer group ID in the propertiesprops.put(\"group.id\", \"&lt;my_consumer_group&gt;\");KafkaConsumer&lt;String, GenericRecord&gt; consumer = new KafkaConsumer&lt;&gt;(props);// Subscribe to the topicconsumer.subscribe(Arrays.asList(\"&lt;my_topic&gt;\"));// Poll the topic to retrieve recordswhile(true) {    ConsumerRecords&lt;String, GenericRecord&gt; records = consumer.poll(Duration.ofSeconds(5));    for (ConsumerRecord&lt;String, GenericRecord&gt; record : records) {        GenericRecord genericRecord = record.value();        // Get fields and values from the genericRecord, for example:        // String titleValue = genericRecord.get(\"title\").toString();    }}The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsDeserializer, telling Kafka to use the Event Streams deserializer for message values when consuming messages. You can also use the Event Streams deserializer for message keys. For more information about the configuration keys and values to use with the Event Streams serdes library, see the SchemaRegistryConfig class in the schema API reference. Note: Use the get method in the GenericRecord class to get field names and values. Note: Replace &lt;my_consumer_group&gt; with the name of the consumer group to use and &lt;my_topic&gt; with the name of the topic to consume messages from. Setting up Kafka Streams applications Kafka Streams applications can also use the Event Streams serdes library to serialize and deserialize messages. For example: // Set the Event Streams serdes properties, including the override option to set the schema// and version used for serializing produced messages.Map&lt;String, Object&gt; serdesProps = new HashMap&lt;String, Object&gt;();serdesProps.put(SchemaRegistryConfig.PROPERTY_API_URL, \"&lt;Schema registry endpoint&gt;\");serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE, \"ABC_Assets_Schema\");serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE, \"1.0.0\");// TLS PropertiesserdesProps.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");// SCRAM authentication properties - uncomment to connect using Scram//serdesProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//serdesProps.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//serdesProps.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);// Mutual authentication properties - uncomment to connect using Mutual authentication//serdesProps.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//serdesProps.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//serdesProps.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");// Set up the Kafka StreamsBuilderStreamsBuilder builder = new StreamsBuilder();// Configure a Kafka Serde instance to use the Event Streams schema registry// serializer and deserializer for message valuesSerde&lt;IndexedRecord&gt; valueSerde = new EventStreamsSerdes();valueSerde.configure(serdesProps, false);// Get the stream of messages from the source topic, deserializing each message value with the// Event Streams deserializer, using the schema and version specified in the message headers.builder.stream(\"&lt;my_source_topic&gt;\", Consumed.with(Serdes.String(), valueSerde))    // Get the 'nextcount' int field from the record.    // The Event Streams deserializer constructs instances of the generated schema-specific    // ABC_Assets_Schema_Count class based on the values in the message headers.    .mapValues(new ValueMapper&lt;IndexedRecord, Integer&gt;() {        @Override        public Integer apply(IndexedRecord val) {            return ((ABC_Assets_Schema_Count) val).getNextcount();        }    })    // Get all the records    .selectKey((k, v) -&gt; 0).groupByKey()    // Sum the values    .reduce(new Reducer&lt;Integer&gt;() {        @Override        public Integer apply(Integer arg0, Integer arg1) {            return arg0 + arg1;        }    })    .toStream()    // Map the summed value to a field in the schema-specific generated ABC_Assets_Schema class    .mapValues(        new ValueMapper&lt;Integer, IndexedRecord&gt;() {            @Override            public IndexedRecord apply(Integer val) {                ABC_Assets_Schema record = new ABC_Assets_Schema();                record.setSum(val);                return record;            }     })     // Finally, put the result to the destination topic, serializing the message value     // with the Event Streams serializer, using the overridden schema and version from the     // configuration.    .to(\"&lt;my_destination_topic&gt;\", Produced.with(Serdes.Integer(), valueSerde));// Create and start the streamfinal KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig);streams.start();In this example, the Kafka StreamsBuilder is configured to use the com.ibm.eventstreams.serdes.EventStreamsSerdes class, telling Kafka to use the Event Streams deserializer for message values when consuming messages and the Event Streams serializer for message values when producing messages. Note: The Kafka Streams org.apache.kafka.streams.kstream API does not provide access to message headers, so to produce messages with the Event Streams schema registry headers, use the SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE and SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE configuration properties. Setting these configuration properties will mean produced messages are serialized using the provided schema version and the Event Streams schema registry message headers will be set. For more information about the configuration keys and values to use with the Event Streams serdes library, see the SchemaRegistryConfig class in the schema API reference. Note: To re-use this example, replace &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application, &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address), and &lt;Schema registry endpoint&gt; with the endpoint address for Apicurio Registry in Event Streams. For SCRAM, replace the &lt;username&gt; and &lt;password&gt; with the SCRAM username and password. For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier, and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/setting-java-apps/",
        "teaser":null},{
        "title": "Setting non-Java applications to use schemas",
        "collection": "10.1",
        "excerpt":"If you have producer or consumer applications created in languages other than Java, use the following guidance to set them up to use schemas. You can also use the REST producer API to send messages that are encoded with a schema. For a producer application:   Retrieve the schema definition that you will be using from Apicurio Registry in Event Streams and save it in a local file.  Use an Apache Avro library for your programming language to read the schema definition from the local file and encode a Kafka message with it.  Set the schema registry headers in the Kafka message, so that consumer applications can understand which schema and version was used to encode the message, and which encoding format was used.  Send the message to Kafka.For a consumer application:   Retrieve the schema definition that you will be using from Apicurio Registry in Event Streams and save it in a local file.  Consume a message from Kafka.  Check the headers for the Kafka message to ensure they match the expected schema ID and schema version ID.  Use the Apache Avro library for your programming language to read the schema definition from the local file and decode the Kafka message with it.Retrieving the schema definition from the schema registry Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema registry in the primary navigation and find your schema in the list.  Copy the schema definition into a new local file.          For the latest version of the schema, expand the row. Copy and paste the schema definition into a new local file.      For a different version of the schema, click on the row and then select the version to use from the list of schema versions. Click the Schema definition tab and then copy and paste the schema definition into a new local file.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster: cloudctl es init  Run the following command to list all the schemas in the schema registry: cloudctl es schemas  Select your schema from the list and run the following command to list all the versions of the schema: cloudctl es schema &lt;schema-name&gt;  Select your version of the schema from the list and run the following command to retrieve the schema definition for the version and copy it into a new local file: cloudctl es schema &lt;schema-name&gt; --version &lt;schema-version-id&gt; &gt; &lt;schema-definition-file&gt;.avscNote: &lt;schema-version-id&gt;is the integer ID that is displayed when listing schema versions using the following command:cloudctl es schema &lt;schema-name&gt;. Setting headers in the messages you send to Event Streams Kafka Set the following headers in the message to enable applications that use the Event Streams serdes Java library to consume and deserialize the messages automatically. Setting these headers also enables the Event Streams UI to display additional details about the message. The required message header keys and values are listed in the following table.             Header name      Header key      Header value                  Schema ID      com.ibm.eventstreams.schemaregistry.schema.id      The schema ID as a string.              Schema version ID      com.ibm.eventstreams.schemaregistry.schema.version      The schema version ID as a string.              Message encoding      com.ibm.eventstreams.schemaregistry.encoding      Either JSON for Avro JSON encoding, or BINARY for Avro binary encoding.      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/setting-nonjava-apps/",
        "teaser":null},{
        "title": "Migrating existing applications to the Event Streams schema registry",
        "collection": "10.1",
        "excerpt":"If you are using the Confluent Platform schema registry, Event Streams provides a migration path for moving your Kafka consumers and producers over to use the Apicurio Registry in Event Streams. Migrating schemas to Apicurio Registry in Event Streams To migrate schemas, you can use schema auto-registration in your Kafka producer, or you can manually migrate schemas by downloading the schema definitions from the Confluent Platform schema registry and adding them to the Apicurio Registry in Event Streams. Migrating schemas with auto-registration When using auto-registration, the schema will be automatically uploaded to the Apicurio Registry in Event Streams, and named with the subject ID (which is based on the subject name strategy in use) and a random suffix. Auto-registration is enabled by default in the Confluent Platform schema registry client library. To disable it, set the auto.register.schemas property to false. Note: To auto-register schemas in the Event Streams schema registry, you need credentials that have producer permissions and permission to create schemas. You can generate credentials by using the Event Streams UI or CLI. Migrating schemas manually To manually migrate the schemas, download the schema definitions from the Confluent Platform schema registry, and add them to the Apicurio Registry in Event Streams. When manually adding schemas to the Apicurio Registry in Event Streams, the provided schema name must match the subject ID used by the Confluent Platform schema registry subject name strategy. If you are using the default TopicNameStrategy, the schema name must be &lt;TOPIC_NAME&gt;-&lt;'value'|'key'&gt; If you are using the RecordNameStrategy, the schema name must be &lt;SCHEMA_DEFINITION_NAMESPACE&gt;.&lt;SCHEMA_DEFINITION_NAME&gt; For example, if you are using the default TopicNameStrategy as your subject name strategy, and you are serializing your data into the message value and producing to the MyTopic topic, then the schema name you must provide when adding the schema in the UI must be MyTopic-value For example, if you are using the RecordNameStrategy as your subject name strategy, and the schema definition file begins with the following, then the schema name you must provide when adding the schema in the UI must be org.example.Book: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [...If you are using the CLI, run the following command when adding the schema: cloudctl es schema-add --create --name org.example.Book --version 1.0.0 --file /path/to/Book.avsc Migrating a Kafka producer application To migrate a Kafka producer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your producer application to secure the connection between the producer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the following command:  cloudctl es init      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");            Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:\\     export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\       -Djavax.net.ssl.trustStorePassword=password\"      Migrating a Kafka consumer application To migrate a Kafka consumer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Apicurio Registry in Event Streams.   Configure your consumer application to secure the connection between the consumer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the following command:  cloudctl es init      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");            Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Apicurio Registry in Event Streams. For example:\\     export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\        -Djavax.net.ssl.trustStorePassword=password\"      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/migrating/",
        "teaser":null},{
        "title": "Using schemas with the REST producer API",
        "collection": "10.1",
        "excerpt":"You can use schemas when producing messages with the Event Streams REST producer API. You simply add the following parameters to the API call:   schemaname: The name of the schema you want to use when producing messages.  schemaversion: The schema version you want to use when producing messages.For example, to use cURL to produce messages to a topic with the producer API, and specify the schema to be used, run the curl command as follows: Using SCRAM Basic Authentication: curl -H \"Authorization: &lt;basic-auth&gt;\" -H \"Accept: application/json\" -H \"Content-Type: text/plain\" -d '&lt;avro-encoded-message&gt;' --cacert es-cert.pem \"https://&lt;producer-endpoint&gt;/topics/&lt;my-topic&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" Using TLS Mutual Authentication: curl -H \"Accept: application/json\" -H \"Content-Type: text/plain\" -d '&lt;avro-encoded-message&gt;' --cacert es-cert.pem --key user.key --cert user.crt \"https://&lt;producer-endpoint&gt;/topics/&lt;my-topic&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" Note: Replace the values in brackets as follows:   &lt;basic-auth&gt; with the SCRAM Basic Authentication token which is generated using the Event Streams UI.  &lt;producer-endpoint&gt; with the producer URL which is available from Connect to this cluster or Connect to this topic and copy the URL in the Producer endpoint and credentials section.  &lt;my_topic&gt; with your topic name.  &lt;schema-name&gt; with the name of your schema that is stored in Apicurio Registry in Event Streams.  &lt;schema-version-name&gt; with the version of your schema that is stored in Apicurio Registry in Event Streams.  &lt;avro-encoded-message&gt; with your avro encoded message.The es-cert.pem certificate is downloaded by running the following command:cloudctl es certificates --format pem The user.key and user.crt files are downloaded in a .zip file by clicking Generate credentials in the Producer endpoint and credentials section of Connect to this cluster or Connect to this topic, selecting Mutual TLS certificate, following the instructions in the wizard and clicking Download certificates. By adding these parameters to the API call, a lookup is done on the specified schema and its version to check if it is valid. If valid, the correct message headers are set for the produced message. Important: When using the producer API, the lookup does not validate the data in the request to see if it matches the schema. Ensure the message conforms to the schema, and that it has been encoded in the Apache Avro binary or JSON encoding format. If the message does not conform and is not encoded with either of those formats, consumers will not be able to deserialize the data. If the message has been encoded in the Apache Avro binary format, ensure the HTTP Content-Type header is set to application/octet-stream. If the message has been encoded in the Apache Avro JSON format, ensure the HTTP Content-Type header is set to application/json. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/using-with-rest-producer/",
        "teaser":null},{
        "title": "Using the schema API",
        "collection": "10.1",
        "excerpt":"Event Streams provides a serdes Java library to enable Kafka applications to serialize and deserialize messages using schemas stored in Apicurio Registry in Event Streams. Using the Event Streams serdes library API, schema versions are automatically downloaded from your Apicurio Registry in Event Streams, checked to see if they are in a disabled or deprecated state, and cached. The schemas are used to serialize messages produced to Kafka and deserialize messages consumed from Kafka. Schemas downloaded by the Event Streams serdes library API are cached in memory with a 10 minute expiration period. This means that if a schema is deprecated or disabled, it might take 10 minutes before consuming or producing applications will see the change. To change the expiration period, set the SchemaRegistryConfig.PROPERTY_SCHEMA_CACHE_REFRESH_RATE configuration property to a new milliseconds value. For more details, including code snippets that use the schema registry serdes API, see setting Java applications to use schemas. For full details of the Event Streams schema registry serdes API, see the Schema API Javadoc. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/schema-api/",
        "teaser":null},{
        "title": "Setting Java applications to use schemas with the Apicurio Registry serdes library",
        "collection": "10.1",
        "excerpt":"If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas and the Apicurio Registry serdes library. Note: If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas. Preparing the setup To use schemas stored in the Apicurio Registry in Event Streams, your client applications need to be able to serialize and deserialize messages based on schemas.   Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.  Consuming applications then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.The Event Streams UI provides help with setting up your Java applications to use schemas. Note: Apicurio Registry in Event Streams works with multiple schema registry serdes libraries, including the Event Streams schema registry serdes library and the Apicurio Registry serdes library. You can use either serdes library in your applications by adding Maven dependencies to your project pom.xml files. The following instructions and code snippets use the Apicurio Registry serdes library. To use the Event Streams schema registry serdes library, see the guidance about setting up Java applications to use schemas. To set up your Java applications to use the Apicurio Registry serdes library with Apicurio Registry in Event Streams, prepare the connection for your application as follows:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Ensure you have added schemas to the registry.  Click Schema registry in the primary navigation.  Select a schema from the list and click the row for the schema.  Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.      Set the preferences for your connection in the Configure the schema connection section. Use the defaults or change them by clicking Change configuration.                   For producers, set the method for Message encoding.                   Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.          JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.                            For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.                   Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.          Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and will allow a Kafka consumer to continue to process further messages.                      If any configuration was changed, click Save.  Click Generate credentials to generate SCRAM or Mutual TLS credentials and follow the instructions.Important: Ensure you make note of the information provided upon completion as you will need this later.Alternatively you can generate credentials later by using the Event Streams UI or CLI.  Click Generate connection details.  Click Download certificate to download the cluster PKCS12 certificate. This is the Java truststore file which contains the server certificate. Take a copy of the Certificate password for use with the certificate in your application.      Add the following dependency to your project Maven pom.xml file to use the Apicurio Registry  serdes library:     &lt;dependency&gt;    &lt;groupId&gt;io.apicurio&lt;/groupId&gt;    &lt;artifactId&gt;apicurio-registry-utils-serde&lt;/artifactId&gt;    &lt;version&gt;1.3.1.Final&lt;/version&gt;&lt;/dependency&gt;            If you want to generate specific schema classes from your project Avro schema files, add the following Avro plugin to your project Maven pom.xml file, replacing SCHEMA-FILE-NAME with the name of your schema file.     &lt;profile&gt;  &lt;id&gt;avro&lt;/id&gt;  &lt;build&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;        &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt;        &lt;version&gt;1.9.2&lt;/version&gt;        &lt;executions&gt;          &lt;execution&gt;            &lt;phase&gt;generate-sources&lt;/phase&gt;            &lt;goals&gt;              &lt;goal&gt;schema&lt;/goal&gt;            &lt;/goals&gt;            &lt;configuration&gt;                &lt;sourceDirectory&gt;${project.basedir}/src/main/resources/avro-schemas/&lt;/sourceDirectory&gt;                &lt;includes&gt;                    &lt;include&gt;SCHEMA-FILE-NAME.avsc&lt;/include&gt;                &lt;/includes&gt;                &lt;outputDirectory&gt;${project.build.directory}/generated-sources&lt;/outputDirectory&gt;            &lt;/configuration&gt;          &lt;/execution&gt;        &lt;/executions&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/profile&gt;        Add snippets into your application code as described in the following sections.          Setting up producers to use schemas      Setting up consumers to use schemas      Additional configuration options for Apicurio serdes library The Apicurio serdes library supports the following configuration options that can be specified to change how data is serialized.             Key      Value      Description              apicurio.registry.use.headers      true      Use this option to store the schema ID in the header of the message rather than within the payload.              apicurio.avro.encoding      JSON or BINARY      Specify whether to use BINARY (default) or JSON encoding within the Avro serializer      Note: If you are using headers to store the schema ID, you can override the keys used in the header with the following values:   apicurio.key.artifactId.name  apicurio.value.artifactId.name  apicurio.key.version.name  apicurio.value.version.name  apicurio.key.globalId.name  apicurio.value.globalId.nameBy setting these values you can change the name of the header that the Apicurio serdes library uses when adding headers for the artifactId, version, or globalId in the Kafka message. Setting up producers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies if not using Maven, and copying code snippets for a producing application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your producer Kafka application.  Add the following code snippets to your application code.Imports import java.io.File;import java.util.Properties;import io.apicurio.registry.utils.serde.AbstractKafkaStrategyAwareSerDe;import io.apicurio.registry.utils.serde.AvroKafkaSerializer;import org.apache.avro.Schema;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericRecord;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;Connection properties Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the Serializer and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// TLS Properties for Apicurio Registry connectionprops.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_REQUEST_TRUSTSTORE_LOCATION, \"&lt;ca_p12_file_location&gt;\");props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_REQUEST_TRUSTSTORE_PASSWORD, \"&lt;ca_p12_password&gt;\");// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);//// URL for Apicurio Registry connection including basic auth parameters//props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_URL_CONFIG_PARAM, \"https://&lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;\");// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//// Mutual authentication properties for Apicurio Registry connection//props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_REQUEST_KEYSTORE_LOCATION, \"&lt;user_p12_file_location&gt;\");//props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_REQUEST_KEYSTORE_LOCATION, \"&lt;user_p12_file_location&gt;\");//// URL for Apicurio Registry connection//props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_URL_CONFIG_PARAM, \"https://&lt;Schema registry endpoint&gt;\");// Set the ID strategy to use the fully-qualified schema name (including namespace)props.put(AbstractKafkaStrategyAwareSerDe.REGISTRY_ARTIFACT_ID_STRATEGY_CONFIG_PARAM, \"io.apicurio.registry.utils.serde.strategy.RecordIdStrategy\");// Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Uncomment lines depending on your authentication settings. Replace:   &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier.  &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application.  &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address)  &lt;Schema registry endpoint&gt; with the endpoint address for Apicurio Registry in Event Streams.  For SCRAM, replace &lt;username&gt; and &lt;password&gt; with the SCRAM username and password.  For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file.Producer code example // Set the value serializer for produced messages to use the Apicurio Registry serializerprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"io.apicurio.registry.utils.serde.AvroKafkaSerializer\");// Get a new Generic KafkaProducerKafkaProducer&lt;String, GenericRecord&gt; producer = new KafkaProducer&lt;&gt;(props);// Read in the local schema fileSchema.Parser schemaDefinitionParser = new Schema.Parser();Schema schema = schemaDefinitionParser.parse(new File(\"&lt;path_to_schema_file.avsc&gt;\"));// Get a new Generic record based on the schemaGenericRecord genericRecord = new GenericData.Record(schema);// Add fields and values to the genericRecord, for example:// genericRecord.put(\"title\", \"this is the value for a title field\");// Prepare the recordProducerRecord&lt;String, GenericRecord&gt; producerRecord =    new ProducerRecord&lt;String, GenericRecord&gt;(\"&lt;my_topic&gt;\", genericRecord);// Send the record to Kafkaproducer.send(producerRecord);// Close the producerproducer.close();The Kafka configuration property value.serializer is set to io.apicurio.registry.utils.serde.AvroKafkaSerializer, telling Kafka to use the Apicurio Registry Avro serializer for message values when producing messages. You can also use the Apicurio Registry Avro serializer for message keys. Note: Use the put method in the GenericRecord class to set field names and values in your message. Note: Replace:   &lt;my_topic&gt; with the name of the topic to produce messages to.  &lt;path_to_schema_file.avsc&gt; with the path to the Avro schema file.Note: If you want to retrieve the schema from the registry instead of loading the file locally, you can use the following code: // Convert kafka connection properties to a MapMap&lt;String, Object&gt; config = (Map) props;// Create the clientRegistryRestClient client = RegistryRestClientFactory.create(REGISTRY_URL, config);try {    // Get the schema from apicurio and convert to an avro schema    Schema schema = new Schema.Parser().parse(client.getLatestArtifact(artifactId));} catch (Exception e) {    e.printStackTrace();}Setting up consumers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies if not using Maven, and copying code snippets for a consuming application.  If your project does not use Maven, ensure you add the location of the JAR files to the build path of your consumer Kafka application.  Add the following code snippets to your application code.Imports import java.time.Duration;import java.util.Arrays;import java.util.Properties;import io.apicurio.registry.utils.serde.AbstractKafkaSerDe;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericRecord;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.KafkaConsumer;Connection properties Properties props = new Properties();// TLS Propertiesprops.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;ca_p12_file_location&gt;\");props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;ca_p12_password&gt;\");//If your Kafka and Schema registry endpoints do not use the same authentication method, you will need//to duplicate the properties object - and add the Schema Registry authentication and connection properties//to 'props', and the Kafka authentication and connection properties to 'kafkaProps'. The different properties objects//are then supplied to the Serializer and Producer/Consumer respectively.//Uncomment the next two lines.//Properties kafkaProps = new Properties();//kafkaProps.putAll(props);// TLS Properties for Apicurio Registry connectionprops.put(AbstractKafkaSerDe.REGISTRY_REQUEST_TRUSTSTORE_LOCATION, \"&lt;ca_p12_file_location&gt;\");props.put(AbstractKafkaSerDe.REGISTRY_REQUEST_TRUSTSTORE_PASSWORD, \"&lt;ca_p12_password&gt;\");// SCRAM authentication properties - uncomment to connect using Scram//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");//props.put(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");//String saslJaasConfig = \"org.apache.kafka.common.security.scram.ScramLoginModule required \"//+ \"username=\\\"&lt;username&gt;\\\" password=\\\"&lt;password&gt;\\\";\";//props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);//// URL for Apicurio Registry connection including basic auth parameters//props.put(AbstractKafkaSerDe.REGISTRY_URL_CONFIG_PARAM, \"https://&lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;\");// Mutual authentication properties - uncomment to connect using Mutual authentication//props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SSL\");//props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, \"&lt;user_p12_file_location&gt;\");//props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, \"&lt;user_p12_password&gt;\");//// Mutual authentication properties for Apicurio Registry connection//props.put(AbstractKafkaSerDe.REGISTRY_REQUEST_KEYSTORE_LOCATION, \"&lt;user_p12_file_location&gt;\");//props.put(AbstractKafkaSerDe.REGISTRY_REQUEST_KEYSTORE_LOCATION, \"&lt;user_p12_file_location&gt;\");//// URL for Apicurio Registry connection//props.put(AbstractKafkaSerDe.REGISTRY_URL_CONFIG_PARAM, \"https://&lt;Schema registry endpoint&gt;\");// Kafka connectionprops.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;Kafka listener&gt;\");Note: Uncomment lines depending on your authentication settings. Replace:   &lt;ca_p12_file_location&gt; with the path to the Java truststore file you downloaded earlier.  &lt;ca_p12_password&gt; with the truststore password which has the permissions needed for your application.  &lt;Kafka listener&gt; with the bootstrap address (find out how to obtain the address)  &lt;Schema registry endpoint&gt; with the endpoint address for Apicurio Registry in Event Streams.  For SCRAM, replace &lt;username&gt; and &lt;password&gt; with the SCRAM username and password.  For Mutual authentication, replace &lt;user_p12_file_location&gt; with the path to the user.p12 file extracted from the .zip file downloaded earlier and &lt;user_p12_password&gt; with the contents of the user.password file in the same .zip file.Consumer code example // Set the value deserializer for consumed messages to use the Apicurio Registry deserializerprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"io.apicurio.registry.utils.serde.AvroKafkaDeserializer\");// Set the consumer group ID in the propertiesprops.put(\"group.id\", \"&lt;my_consumer_group&gt;\");KafkaConsumer&lt;String, GenericRecord&gt; consumer = new KafkaConsumer&lt;&gt;(props);// Subscribe to the topicconsumer.subscribe(Arrays.asList(\"&lt;my_topic&gt;\"));// Poll the topic to retrieve recordswhile(true) {    ConsumerRecords&lt;String, GenericRecord&gt; records = consumer.poll(Duration.ofSeconds(5));    for (ConsumerRecord&lt;String, GenericRecord&gt; record : records) {        GenericRecord genericRecord = record.value();        // Get fields and values from the genericRecord, for example:        // String titleValue = genericRecord.get(\"title\").toString();    }}The Kafka configuration property value.serializer is set to io.apicurio.registry.utils.serde.AvroKafkaDeserializer, telling Kafka to use the Apicurio Registry Avro deserializer for message values when consuming messages. You can also use the Apicurio Registry Avro deserializer for message keys. Note: Use the get method in the GenericRecord class to get field names and values. Note: Replace:   &lt;my_consumer_group&gt; with the name of the consumer group to use.  &lt;my_topic&gt; with the name of the topic to consume messages from.Setting up Kafka Streams applications Kafka Streams applications can also use the Apicurio Registry serdes library to serialize and deserialize messages. In particular, the io.apicurio.registry.utils.serde.AvroSerde class can be used to provide the Apicurio Avro serializer and deserializer for the \"default.value.serde\" or \"default.key.serdes\" properties. Additionally, setting the \"apicurio.registry.use-specific-avro-reader\" property to \"true\" tells the Apicurio Registry serdes library to use specific schema classes that have been generated from your project Avro schema files. For example: import io.apicurio.registry.utils.serde.AbstractKafkaSerDe;import io.apicurio.registry.utils.serde.AvroSerde;import io.apicurio.registry.utils.serde.avro.AvroDatumProvider;import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.StreamsConfig;import java.util.Properties;...final Properties streamsConfiguration = new Properties();// URL for Apicurio Registry connection (including basic auth parameters)streamsConfiguration.put(AbstractKafkaSerDe.REGISTRY_URL_CONFIG_PARAM, \"https://&lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;\");// Specify default serializer and deserializer for record keys and for record values.streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, AvroSerde.class);// Specify using specific (generated) Avro schema classesstreamsConfiguration.put(AvroDatumProvider.REGISTRY_USE_SPECIFIC_AVRO_READER_CONFIG_PARAM, \"true\");Setting up Kafka Connect connectors The Apicurio Registry converter library can be used in Kafka Connect to provide the converters that are required to convert between Kafka Connect’s format and the Avro serialized format. Source connectors can use the converter to write Avro-formatted values, keys, and headers to Kafka. Sink connectors can use the converter to read Avro-formatted values, keys, and headers from Kafka. In particular, the io.apicurio.registry.utils.converter.AvroConverter class can be used to provide the Apicurio Avro converter for the \"key.converter\", \"value.converter\", or \"header.converter\" properties. For example: key.converter=io.apicurio.registry.utils.converter.AvroConverterkey.converter.apicurio.registry.url=https://&lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;value.converter=io.apicurio.registry.utils.converter.AvroConvertervalue.converter.apicurio.registry.url=https://&lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;To use the Apicurio Registry  converter library, add the following dependency to your project Maven pom.xml file: &lt;dependency&gt;    &lt;groupId&gt;io.apicurio&lt;/groupId&gt;    &lt;artifactId&gt;apicurio-registry-utils-converter&lt;/artifactId&gt;    &lt;version&gt;1.3.1.Final&lt;/version&gt;&lt;/dependency&gt;Alternatively, if you are not building your connector, you can download the Apicurio converter artifacts from Maven. After downloading, extract the tar.gz file and place the folder with all the JARs into a subdirectory within the folder where you are building your KafkaConnectS2I image. To enable Kafka properties to be pulled from a file, add the following to your KafkaConnector or KafkaConnectS2I custom resource definition: config.providers: fileconfig.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProviderThen reference the Kafka connection details in the KafkaConnector custom resource of your connector. For example, the following shows a value converter with SCRAM credentials specified in the custom resource: value.converter.apicurio.registry.url: &lt;username&gt;:&lt;password&gt;@&lt;Schema registry endpoint&gt;value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategyvalue.converter.apicurio.registry.request.ssl.truststore.location: \"${file:/tmp/strimzi-connect.properties:ssl.truststore.location}\"value.converter.apicurio.registry.request.ssl.truststore.password: \"${file:/tmp/strimzi-connect.properties:ssl.truststore.password}\"value.converter.apicurio.registry.request.ssl.truststore.type: \"${file:/tmp/strimzi-connect.properties:ssl.truststore.type}\"","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/schemas/setting-java-apps-apicurio-serdes/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "10.1",
        "excerpt":"You can secure your IBM Event Streams resources by managing the access each user and application has to each resource. An Event Streams cluster can be configured to expose up to 2 internal and 1 external Kafka listeners. These listeners provide the mechanism for Kafka client applications to communicate with the Kafka brokers. The bootstrap address is used for the initial connection to the cluster. The address will resolve to one of the brokers in the cluster and respond with metadata describing all the relevant connection information for the remaining brokers. Each Kafka listener providing a connection to Event Streams can be configured to authenticate connections with either Mutual TLS or SCRAM SHA 512 authentication mechanisms. Additionally, the Event Streams cluster can be configured to authorize operations sent via an authenticated listener. Note: Schemas in the Event Streams schema registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. Accessing the Event Streams UI and CLI When Kafka authentication is enabled, the Event Streams UI requires you to log in by using an IBM Cloud Platform Common Services Identity and Access Management (IAM) user. Access to the Event Streams CLI always requires a user to be supplied to the cloudctl login command, so the CLI is not affected by enabling Kafka authentication. The default cluster administrator (admin) IAM user credentials are stored in a secret within the ibm-common-services namespace. To retrieve the username and password:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Extract and decode the current IBM Cloud Platform Common Services admin username:oc --namespace ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_username}' | base64 --decode  Extract and decode the current IBM Cloud Platform Common Services admin password:oc --namespace ibm-common-services get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 --decodeNote: The password is auto-generated in accordance with the default password rules for IBM Cloud Platform Common Services. To change the username or password of the admin user, see the instructions about changing the cluster administrator access credentials. The admin user has the Cluster Administrator role granting full access to all resources within the cluster, including Event Streams. Adding additional groups and users Access for groups and users is managed through IAM teams. If you have not previously created any teams, the admin user credentials can be used to set up a team. Managing access to the UI and CLI Access to the Event Streams UI and CLI requires an IAM user with a role of Cluster Administrator or Administrator. The role can be set for the user or for the group the user is part of. Any groups or users added to an IAM team with the Cluster Administrator role can log in to the Event Streams UI and CLI. Any groups or users with the Adminstrator role will not be able to log in until the namespace that contains the Event Streams cluster is added as a resource for the IAM team. If Kafka authorization is enabled by setting spec.strimziOverrides.kafka.authorization to type: runas, operations by IAM users are are automatically mapped to a Kafka principal with authorization for the required Kafka resources. Managing access to Kafka resources Each Kafka listener exposing an authenticated connection to Event Streams requires credentials to be presented when connecting. Credentials are created by using a KafkaUser custom resource, where the spec.authentication.type field has a value that matches the Kafka listener authentication type. You can create a KafkaUser by using the Event Streams UI or CLI. It is also possible to create a KafkaUser by using the OpenShift Container Platform UI or CLI, or the underlying Kubernetes API by applying a KafkaUser operand request. To assist in generating compatible KafkaUser credentials, the Event Streams UI indicates which authentication mechanism is being configured for each Kafka listener. Warning: Do not use or modify the internal Event Streams KafkaUsers named &lt;cluster&gt;-ibm-es-kafka-user and &lt;cluster&gt;-ibm-es-georep-source-user. These are reserved to be used internally within the Event Streams instance. Creating a KafkaUser in the IBM Event Streams UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Connect to this cluster tile to view the Cluster connection panel.  Go to the Kafka listener and Credentials section.  To generate credentials for external clients, click External, or to generate credentials for internal clients, click Internal.  Click the Generate SCRAM credential or Generate TLS credential button next to the required listener to view the credential generation dialog.  Follow the instructions to generate credentials with desired permissions.Note: If your cluster does not have authorization enabled, the permission choices will not have any effect.The generated credential appears after the listener bootstrap address:   For SCRAM credentials, two tabs are displayed: Username and password and Basic Authentication. The SCRAM username and password combination is used by Kafka applications, while the Basic Authentication credential is for use as an HTTP authorization header.  For TLS credentials, a download button is displayed, providing a zip archive with the required certificates and keys.A KafkaUser will be created with the entered credential name. The cluster truststore is not part of the above credentials zip archive. This certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. Creating a KafkaUser in the IBM Event Streams CLI   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Initialize the Event Streams plugin specifying your namespace and choose your instance from the numbered list:  cloudctl es init -n &lt;namespace&gt;  Use the kafka-user-create command to create a KafkaUser with the accompanying permissions.Note: If your cluster does not have authorization enabled, the permission choices will not have any effect.For example, to create SCRAM credentials with authorization to create topics &amp; schemas and produce (including transactions) &amp; consume from every topic: cloudctl es kafka-user-create \\  --name my-user \\  --consumer \\  --producer \\  --schema-topic-create \\  --all-topics \\  --all-groups \\  --all-txnids \\  --auth-type scram-sha-512For information about all options provided by the command, use the --help flag:cloudctl es kafka-user-create --help UI and CLI KafkaUser authorization KafkaUsers created by using the UI and CLI can only be configured with permissions for the most common operations against Kafka resources. You can later modify the created KafkaUser to add additional ACL rules. Creating a KafkaUser in the  UI Navigate to the IBM Event Streams installed operator menu and select the KafkaUser tab. Click Create KafkaUser. The YAML view contains sample KafkaUser definitions to consume, produce, or modify every resource. Retrieving credentials When a KafkaUser custom resource is created, the Entity Operator within Event Streams will create the principal in ZooKeeper with appropriate ACL entries. It will also create a Kubernetes Secret that contains the Base64-encoded SCRAM password for the scram-sha-512 authentication type, or the Base64-encoded certificates and keys for the tls authentication type. You can retrieve the credentials later in the OpenShift Container Platform by using the name of the KafkaUser. For example, to retrieve the credentials by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Use the following command retrieve the required KafkaUser data, adding the KafkaUser name and your chosen namespace:     oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'   The command provides the following output:          The principal username      The name of the Kubernetes Secret, which includes the namespace, containing the SCRAM password or the TLS certificates.        Decode the credentials.For SCRAM, use the secret-name from step 2 to get the password and decode it:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.password}' | base64 --decodeFor TLS, get the credentials, decode them, and write each certificates and keys to files:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.ca\\.crt}' | base64 --decode &gt; ca.crtoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.crt}' | base64 --decode &gt; user.crtoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.key}' | base64 --decode &gt; user.keyoc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.p12}' | base64 --decode &gt; user.p12oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o jsonpath='{.data.user\\.password}' | base64 --decodeThe cluster truststore certificate is required for all external connections and is available to download from the Cluster connection panel under the Certificates heading. Upon downloading the PKCS12 certificate, the certificate password will also be displayed. Similarly, these KafkaUser and Secret resources can be inspected by using the OpenShift Container Platform web console. Warning: Do not use or modify the internal Event Streams KafkaUsers named &lt;cluster&gt;-ibm-es-kafka-user and &lt;cluster&gt;-ibm-es-georep-source-user. These are reserved to be used internally within the Event Streams instance. Authorization What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in Access Control List (ACL) rules:   Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (transactionalId): you can control the ability to use the transaction capability in Kafka.  Cluster (cluster): you can control operations that affect the whole cluster, including idempotent writes.Note: Schemas in the Event Streams Schema Registry are a special case and are secured using the resource type of topic combined with a prefix of __schema_. You can control the ability of users and applications to create, delete, read, and update schemas. What are the available Kafka operations? Access control in Apache Kafka is defined in terms of operations and resources. Operations are actions performed on a resource, and each operation maps to one or more APIs or requests.             Resource type      Operation      Kafka API                  topic      Alter      CreatePartitions                     AlterConfigs      AlterConfigs                            IncrementalAlterConfigs                     Create      CreateTopics                            Metadata                     Delete      DeleteRecords                            DeleteTopics                     Describe      ListOffsets                            Metadata                            OffsetFetch                            OffsetForLeaderEpoch                     DescribeConfigs      DescribeConfigs                     Read      Fetch                            OffsetCommit                            TxnOffsetCommit                     Write      AddPartitionsToTxn                            Produce                     All      All topic APIs                                           group      Delete      DeleteGroups                     Describe      DescribeGroup                            FindCoordinator                            ListGroups                     Read      AddOffsetsToTxn                            Heartbeat                            JoinGroup                            LeaveGroup                            OffsetCommit                            OffsetFetch                            SyncGroup                            TxnOffsetCommit                     All      All group APIs                                           transactionalId      Describe      FindCoordinator                     Write      AddOffsetsToTxn                            AddPartitionsToTxn                            EndTxn                            InitProducerId                            Produce                            TxnOffsetCommit                     All      All txnid APIs                                           cluster      Alter      AlterReplicaLogDirs                            CreateAcls                            DeleteAcls                     AlterConfigs      AlterConfigs                            IncrementalAlterConfigs                     ClusterAction      ControlledShutdown                            ElectPreferredLeaders                            Fetch                            LeaderAndISR                            OffsetForLeaderEpoch                            StopReplica                            UpdateMetadata                            WriteTxnMarkers                     Create      CreateTopics                            Metadata                     Describe      DescribeAcls                            DescribeLogDirs                            ListGroups                            ListPartitionReassignments                     DescribeConfigs      DescribeConfigs                     IdempotentWrite      InitProducerId                            Produce                     All      All cluster APIs      Implicitly-derived operations Certain operations provide additional implicit operation access to applications. When granted Read, Write, or Delete, applications implicitly derive the Describe operation.When granted AlterConfigs, applications implicitly derive the DescribeConfigs operation. For example, to Produce to a topic, the Write operation for the topic resource is required, which will implicitly derive the Describe operation required to get the topic metadata. Access Control List (ACL) rules Access to resources is assigned to applications through an Access Control List (ACL), which consists of rules. An ACL rule includes an operation on a resource together with the additional fields listed in the following tables. A KafkaUser custom resource contains the binding of an ACL to a principal, which is an entity that can be authenticated by the Event Streams instance. An ACL rule adheres to the following schema:             Property      Type      Description                  host      string      The host from which the action described in the ACL rule is allowed.              operation      string      The operation which will be allowed on the chosen resource.              resource      object      Indicates the resource for which the ACL rule applies.      The resource objects used in ACL rules adhere to the following schema:             Property      Type      Description                  type      string      Can be one of cluster, group, topic or transactionalId.              name      string      Name of the resource for which the ACL rule applies. Can be combined with the patternType field to use the prefix pattern.              patternType      string      Describes the pattern used in the resource field. The supported types are literal and prefix. With literal pattern type, the resource field will be used as a definition of a full topic name. With prefix pattern type, the resource name will be used only as a prefix.  The default value is literal.      Using the information about schemas and resource-operations described in the previous tables, the spec.authorization.acls list for a KafkaUser can be created as follows: # ...spec:# ...  authorization:    # ...    acls:      - host: '*'        resource:          type: topic          name: 'client-'          patternType: prefix        operation: WriteIn this example, an application using this KafkaUser would be allowed to write to any topic beginning with client- (for example, client-records or client-billing) from any host machine. Note: The write operation also implicitly derives the required describe operation that Kafka clients require to understand the data model of a topic. The following is an example ACL rule that provides access to read Schemas:     - host: '*'      resource:        type: topic        name: '__schema_'        patternType: prefix      operation: ReadRevoking access for an application As each application will be using credentials provided through a KafkaUser instance, deleting the instance will revoke all access for that application. Individual ACL rules can be deleted from a KafkaUser instance to remove the associated control on a resource operation. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "10.1",
        "excerpt":"The following encryption is always provided in IBM Event Streams:   Network connections into the Event Streams deployment from external clients are secured using TLS.  Kafka replication between brokers is also TLS encrypted.Consider the following for encryption as well:   Internal Kafka listeners can be configured with or without encryption as described in configuring access.  The REST producer endpoint can be configured with or without encryption as described in configuring access.In addition, you can supplement the existing data encryption with disk encryption where supported by your chosen storage provider. You can also encrypt messages within your applications before producing them. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/security/encrypting-data/",
        "teaser":null},{
        "title": "Configuring secure JMX connections",
        "collection": "10.1",
        "excerpt":"Java Management Extensions (JMX) Java Management Extensions (JMX) is a way of retrieving metrics from your specific processes dynamically at runtime. This can be used to get metrics that are specific to Java operations on Kafka To manage resources, management beans (MBeans) are used. MBeans represent a resource in the JVM. There are specific MBean attributes you can use with Kafka. Metrics can be retrieved from applications running inside your OpenShift Container Platform cluster by connecting to an exposed JMX port. The metrics can also be pushed in various formats to remote sinks inside or outside of the cluster by using JmxTrans. Exposing a JMX port on Kafka You can expose the JMX port (9999) of each Kafka broker to be accessible to secure connections from within the OpenShift Container Platform cluster. This grants applications deployed inside the cluster (including JmxTrans) read-only access to Kafka metrics. To expose the JMX port, set the spec.strimziOverrides.kafka.jmxOptions value to {}. This will create an open JMX port allowing any pod to read from it. The JMX port can be password-protected to prevent unauthorised pods from accessing it. It is good practice to secure the JMX port, as an unprotected port could allow a user to invoke an MBean operation on the Java JVM. To enable security for the JMX port, set the spec.strimiziOverrrides.kafka.jmxOptions.authentication.type field to password. For example: #...spec:  #...  strimziOverrides:    #...    kafka:      #...      jmxOptions:        authentication:          type: \"password\"    #...This will cause the JMX port to be secured using a generated username and password. When the JMX port is password protected, a Kubernetes secret named &lt;releasename&gt;ibm-es-jmx-secret is created inside the Event Streams namespace. The secret contains the following content:             Name      Description                  jmx_username      The user that is authenticated to connect to the JMX port.              jmx_password      The password for the authenticated user.      Connecting internal applications To connect your application to the Kafka JMX port, it must be deployed running inside the OpenShift Container Platform cluster. After your application is deployed, you can connect to each Kafka broker with the following URL pattern:&lt;cluster-name&gt;-kafka-&lt;kafka-ordinal&gt;.&lt;cluster-name&gt;-kafka-brokers.svc:9999 To connect to the JMX port, clients must use the following Java options:   javax.net.ssl.trustStore=&lt;path to trustStore&gt;  javax.net.ssl.trustStorePassword=&lt;password for trustStore&gt;In addition, when initiating the JMX connection, if the port is secured then clients must provide the username and password from the JMX secret. For example, the JConsole UI provide a username/password box to enter the credentials. Retrieving the truststore Using the Event Streams UI:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.  Click Connect to this cluster.  In the certificates section, click download certificate.You will now have the required certificate and the password will be displayed in the UI.Using the Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the command cloudctl es certificates to download the certificate. The password is displayed in the CLI.Retrieving the JMX username and password Using the oc CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following commands:     oc get secret &lt;cluster-name&gt;-jmx -o jsonpath='{.data.jmx\\-username}' -namespace &lt;name_of_your_namespace&gt; | base64 -decode &gt; jmx_username.txt     oc get secret &lt;cluster-name&gt;-jmx -o jsonpath='{.data.jmx\\-password}' -namespace &lt;name_of_your_namespace&gt; | base64 -decode &gt; jmx_password.txt   These will output the jmx_username and jmx_password values into the respective .txt files. Mounting the JMX secret directly into a pod Mounting the secret will project the jmx_username and jmx_password values as files under the mount path folder. apiVersion: v1kind: Podmetadata:  name: example-podspec:  volumes:      - name: jmx-secret        secret:          secretName: &lt;cluster-name&gt;-jmx  containers:    - name: example-container      image: example-image    volumeMounts:    - name: jmx-secret      mountPath: /path/to/jmx-secret      readOnly: trueFor more information, see Kubernetes Secrets If the connecting application is not installed inside the Event Streams namespace, it must be copied to the application namespace using the following command: oc -n &lt;instance_namespace&gt; get secret &lt;releasename&gt;ibm-es-jmx-secret -o yaml --export | oc -n &lt;application_namespace&gt; apply -f -JmxTrans The Kafka broker JMX ports are not exposed to applications outside of the cluster. However, a JmxTrans pod can be deployed that provides a mechanism to read JMX metrics from the Kafka brokers and push them to applications inside or outside the cluster. JmxTrans reads JMX metric data from the Kafka brokers and sends the data to applications in various data formats. Configuring a JmxTrans deployment To configure a JmxTrans deployment, you will need to use the spec.strimziOverrides.jmxTrans field to define the outputDefinitions and kafkaQueries. outputDefinitions: This specifies where the metric data will be pushed to and in what data format it will be provided in.             Attribute      Description                  outputType      The specified format you want to transform your query to. For a list of supported data formats, see the OutputWriters documentation              host      The target host address the data is pushed to.              port      The target port the data is pushed to.              flushDelay      Number of seconds JmxTrans agent waits before pushing new data.              name      Name of the property which is later referenced by spec.strimiziOverrides.jmxTrans.queries.      The following is an example configuration pushing JMX data to standardOut in the JmxTrans logs and another pushing JMX data every 10 seconds in the Graphite format to a Logstash database at the address mylogstash.com:31028: # ...spec:  # ...  strimziOverrides:    # ...    jmxTrans:      # ...      outputDefinitions:        - outputType: \"com.googlecode.jmxtrans.model.output.StdOutWriter\"          name: \"standardOut\"        - outputType: \"com.googlecode.jmxtrans.model.output.GraphiteOutputWriter\"          host: \"mylogstash.com\"          port: 31028          flushDelayInSeconds: 5          name: \"logstash\"kafkaQueries: This specifies what JMX metrics are read from the Kafka brokers. Note: Metrics are read from all Kafka brokers. There is no configuration option to obtain metrics from selected brokers only.             Attribute      Description                  targetMBean      Specifies what metrics you want to get from the JVM.              attributes      Specifies which MBean metric is read from the targetMBean as JMX metrics.              output      Defines where the metrics are pushed to, by choosing an output type.      The following is an example JmxTrans deployment that reads from all MBeans that match the pattern kafka.server:type=BrokerTopicMetrics,name=* and have name in the target MBeans name. From those MBeans, it obtains JMX metrics about the Count attribute, and pushes the metrics to a standard output as defined by the outputs attribute. #...spec:  #...  strimziOverrides:    #...    jmxTrans:      #...      kafkaQueries:        - targetMBean: \"kafka.server:type=BrokerTopicMetrics,name=*\"          attributes: [\"Count\"]          outputs: [\"standardOut\"]","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/security/secure-jmx-connections/",
        "teaser":null},{
        "title": "Network policies",
        "collection": "10.1",
        "excerpt":"Inbound network connections (ingress) Network policies are used to control inbound connections into pods. These connections can be from pods within the cluster, or from external sources. When you install an instance of Event Streams, the required network policies will be automatically created. To review the network policies that have been applied:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to display the installed network policies for a specific namespace:oc get netpol -n &lt;namespace&gt;The following tables provide information about the network policies that are applicable to each pod within the Event Streams instance. If a particular pod is not required by a given Event Streams configuration, the associated network policy will not be applied. Note: Where a network policy exposes a port to the Event Streams Cluster operator, it is configured to allow connections from any namespace. Kafka pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      REST API, REST Producer and Schema Registry pods      8091      Broker communication      Always              TCP      Kafka, Cluster operator, Entity operator, Kafka Cruise Control and Kafka Exporter pods      9091      Broker communication      Always              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9092      Kafka access for Plain listener      If the Plain listener is enabled              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9093      Kafka access for TLS listener      If the TLS listener is enabled              TCP      Anywhere (can be restricted by including networkPolicyPeers in the listener configuration)      9094      Kafka access for External listener      If the External listener is enabled              TCP      Anywhere      9404      Prometheus access to Kafka metrics      If metrics are enabled              TCP      Anywhere      9999      JMX access to Kafka metrics      If JMX port is exposed      Note: If required, access to listener ports can be restricted to only those pods with specific labels by including additional configuration in the Event Streams custom resource under spec.strimziOverrides.kafka.listeners.&lt;listener&gt;.networkPolicyPeers. ZooKeeper pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Kafka, ZooKeeper, Cluster operator, Entity operator, Kafka Cruise Control pods      2181      ZooKeeper client connections      Always              TCP      Other ZooKeeper pods      2888      ZooKeeper follower connection to leader      Always              TCP      Other ZooKeeper pods      3888      ZooKeeper leader election      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      Geo-replicator pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      REST API pods      8083      Geo-replicator API traffic      Always              TCP      Cluster operator and other geo-replicator pods      8083      Geo-replicator cluster traffic      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      Schema registry pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      Administration UI pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      3000      External access to UI      Always      Administration server pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      REST producer server pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      9443      External access to API      Always              TCP      Any pod in Event Streams instance      7443      TLS cluster traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS cluster traffic      If internal TLS is disabled      Metrics collector pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      7888      Exported Prometheus metrics      Always              TCP      Any pod in Event Streams instance      7443      TLS inbound metrics traffic      If internal TLS is enabled              TCP      Any pod in Event Streams instance      7080      Non-TLS inbound metrics traffic      If internal TLS is disabled      Cluster operator pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Anywhere      8080      Exported Prometheus metrics      Always              TCP      Anywhere      8081      EventStreams custom resource validator      Always      Cruise Control pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Cluster operator      9090      Access to API      Always      Kafka Connect pod             Type      Origin      Port      Reason      Enabled in policy                  TCP      Cluster operator and Kafka Connect pods      8083      Access to Kafka Connect REST API      Always              TCP      Anywhere      9404      Exported Prometheus metrics      If metrics are enabled      Outbound network connections (egress) The following tables provide information about the outbound network connections (egress) initiated by pods in an Event Streams installation. If a particular pod is not required by an Event Streams configuration, the associated outbound connection is not applicable. Kafka pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      9091      Broker replication              TCP      ZooKeeper      app.kubernetes.io/name=zookeeper      2181      ZooKeeper communication      ZooKeeper pod             Type      Destination      Pod Label      Port      Reason                  TCP      ZooKeeper      app.kubernetes.io/name=zookeeper      2888      ZooKeeper clustering              TCP      ZooKeeper      app.kubernetes.io/name=zookeeper      3888      ZooKeeper leader elections      Geo-replicator pod             Type      Destination      Pod Label      Port      Reason                  TCP      Geo-replicator      app.kubernetes.io/name=kafka-mirror-maker-2      8083      Geo-replicator cluster traffic      Note: Geo-replicator destination is external to the cluster. Schema registry pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      8091      Broker communication      Administration server pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      8091      Broker API traffic              TCP      Geo-replicator      app.kubernetes.io/name=kafka-mirror-maker-2      8083      Geo-replicator API traffic              TCP      Kafka Connect      app.kubernetes.io/name=kafka-connect      8083      Kafka Connect API traffic      REST producer server pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      8091      Producer messages      Cluster operator pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      9091      Configuration              TCP      ZooKeeper      app.kubernetes.io/name=zookeeper      2181      Configuration              TCP      Geo-replicator      app.kubernetes.io/name=kafka-mirror-maker-2      8083      Configuration              TCP      Kafka Connect      app.kubernetes.io/name=kafka-connect      8083      Configuration              TCP      Admin API      app.kubernetes.io/name=admin-api      7443      Configuration              TCP      Metrics collector      app.kubernetes.io/name=metrics      7443      Configuration              TCP      Rest producer      app.kubernetes.io/name=rest-producer      7443      Configuration      Cruise Control pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka      app.kubernetes.io/name=kafka      9091      Broker communication              TCP      ZooKeeper      app.kubernetes.io/name=zookeeper      2181      ZooKeeper client connections      Kafka Connect pod             Type      Destination      Pod Label      Port      Reason                  TCP      Kafka Connect      app.kubernetes.io/name=kafka-connect      8083      Access to Kafka Connect REST API      ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/security/network-policies/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "10.1",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR website  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement. Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settings. These settings are configured using the EventStreams custom resource. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemFor more information about controlling access to data stored in IBM Event Streams, see managing access. Cluster-level configuration and resources, including logs that might contain message data, are accessible through the OpenShift Container Platform web console and by using the oc CLI. Access and authorization controls can be used to control which users are able to access this cluster-level information. Data Processing Encryption of connection to IBM Event Streams Connections to IBM Event Streams are secured using TLS. If you want to use your own CA certificates instead of those generated by the operator, you can provide them in the EventStreams custom resource settings. Encryption of connections within IBM Event Streams Internal communication between Event Streams pods is encrypted by default using TLS. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "10.1",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become a destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster. You can set up geo-replication by using the IBM Event Streams UI or CLI. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. When replication is set up and working, you can switch your applications to use another cluster when needed. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "10.1",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Geo-replication requires both the origin and destination IBM Event Streams instances to have client authentication enabled on the external route listener and the internal TLS listener.  If you are using geo-replication for disaster-recovery scenarios, see the guidance about configuring your clusters and applications to ensure you can switch clusters if one becomes unavailable.  Prepare your destination cluster by creating an EventStreamsGeoReplicator instance and defining the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  The replicated topics on the destination cluster will have a prefix added to the topic name. The prefix is the name of the Event Streams instance on the origin cluster, as defined in the EventStreams custom resource, for example my_origin.&lt;topic-name&gt;.  Configuration of geo-replication is done by the UI, the CLI or by directly editing the associated KafkaMirrorMaker2 custom resource.Preparing a destination cluster Before you can set up geo-replication and start replicating topics, you must create an EventStreamsGeoReplicator custom resource at the destination. The Event Streams Operator uses the EventStreamsGeoReplicator custom resource to create a configured KafkaMirrorMaker2 custom resource. The KafkaMirrorMaker2 custom resource is used by the Event Streams Operator to create geo-replication workers, which are instances of Kafka Connect running Kafka MirrorMaker 2.0 connectors. The number of geo-replication workers running at the destination cluster is configured in the EventStreamsGeoReplicator custom resource. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of creating the EventStreamsGeoReplicator instance, or you can modify an existing EventStreamsGeoReplicator instance, even if you already have geo-replication set up and running on that cluster. Configuring a new installation If you are installing a new EventStreamsGeoReplicator instance for geo-replication to a destination cluster, you must specify the existing destination IBM Event Streams instance you are connecting to. You can also specify the number of workers as part of configuring the EventStreamsGeoReplicator instance. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI To create a new EventStreamsGeoReplicator instance for geo-replication by using the UI:   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects drop-down list, select the project that contains the existing destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  In the Operator Details &gt; Overview page, find the Geo-Replicator tile in the list of Provided APIs and click Create Instance.  In the Create EventStreamsGeoReplicator page, edit the provided YAML to set values for the following properties.          In the metadata.labels section, set the eventstreams.ibm.com/cluster property value to the name of your destination IBM Event Streams instance.      Set the metadata.name property value to the name of your destination IBM Event Streams instance.      Set the spec.replicas property value to the number of geo-replication workers you want to run.        Click Create.  The new EventStreamsGeoReplicator instance is listed in the Operator Details &gt; EventStreamsGeoReplicator page.If the EventStreamsGeoReplicator instance is configured correctly, a KafkaMirrorMaker2 custom resource is created. You can see the details for the KafkaMirrorMaker2 custom resource in the  Kafka Mirror Maker 2 tab of the Operator Details page. Using the CLI To create a new EventStreamsGeoReplicator instance for geo-replication by using the CLI:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Define an EventStreamsGeoReplicator instance in a file. For example, the following YAML defines an EventStreamsGeoReplicator instance in the my-project project that is connected to the IBM Event Streams instance named my-dest-cluster and has 3 geo-replication workers.    apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsGeoReplicatormetadata:  labels:    eventstreams.ibm.com/cluster: my-dest-cluster  name: my-dest-cluster  namespace: my-projectspec:  version: 10.1.0  replicas: 3        Note: The EventStreamsGeoReplicator metadata.name property and eventstreams.ibm.com/cluster label property must be set to the name of the destination IBM Event Streams instance that you are geo-replicating to.     Run the following command to create the EventStreamsGeoReplicator instance:oc create -f &lt;path-to-your-eventstreamsgeoreplicator-file&gt;  The new EventStreamsGeoReplicator instance is created.  Run the following command to list your EventStreamsGeoReplicator instances:oc get eventstreamsgeoreplicators  Run the following command to view the YAML for your EventStreamsGeoReplicator instance:oc get eventstreamsgeoreplicator &lt;eventstreamsgeoreplicator-instance-name&gt; -o yamlWhen the EventStreamsGeoReplicator instance is ready, a KafkaMirrorMaker2 instance will be created. Run the following command to list your KafkaMirrorMaker2 instances:  oc get kafkamirrormaker2s Run the following command to view the YAML for your KafkaMirrorMaker2 instance:  oc get kafkamirrormaker2 &lt;kafka-mirror-make-2-instance-name&gt; -o yaml Note: If you have Strimzi installed, you might need to fully-qualify the resources you are requesting. The fully-qualified name for the KafkaMirrorMaker2 instances is kafkamirrormaker2.eventstreams.ibm.com. Configuring an existing installation If you want to change the number of geo-replication workers at a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects drop-down list, select the project that contains the destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  Click the Geo-Replicator tab to see the list of EventStreamsGeoReplicator instances.  Click the EventStreamsGeoReplicator instance that you want to modify.  Click the YAML tab.  Update the spec.replicas property value to the number of geo-replication workers you want to run.  Click Save.Using the CLI To modify the number of workers by using the CLI:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Run the following command to list your EventStreamsGeoReplicator instances:oc get eventstreamsgeoreplicators  Run the following command to edit the YAML for your EventStreamsGeoReplicator instance:oc edit eventstreamsgeoreplicator &lt;eventstreamsgeoreplicator-instance-name&gt;  Update the spec.replicas property value to the number of geo-replication workers you want to run.  Save your changes and close the editor.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "10.1",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab, and click Generate connection information for this cluster.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This information includes the security credentials for your destination cluster, which is then used by your origin cluster to authenticate with the destination.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click Add destination cluster on the Destination location tab.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Alternatively, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step includes the security credentials for your destination cluster which is then used by your origin cluster to authenticate.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following command to display the connection details for your destination cluster:cloudctl es geo-cluster-connect The command returns a base64 encoded string consisting of the API URL and the security credentials required for creating a destination cluster that should be used to configure geo-replication using the CLI.  If the connection details are to be used to configure geo-replication using the UI, add the --json option to return a JSON-formatted string.  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add  --cluster-connect &lt;base64-encoded-string-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.      Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip: You can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.     Note: A prefix of the origin cluster name will be added to the name of the new replicated topic that is created on the destination cluster, resulting in replicated topics named such as &lt;origin-cluster&gt;.&lt;topic-name&gt;.     Message history is included in geo-replication. This means all available message data for the topic is copied. The amount of history is determined by the message retention options set when the topics were created on the origin cluster.     Click Create to create a geo-replicator for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicator for the selected topics is set up successfully.Note: After clicking Create, it might take up to 5 to 10 minutes before geo-replication becomes active.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row. If topics are being replicated from the cluster you are logged in to, the Geo-replication column displays the number of clusters the topic is being replicated to. Clicking the column for the topic expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. List each topic you want to replicate by using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicator for the selected topics is set up successfully.Note: A prefix of the origin cluster name will be added to the name of the new replicated topic that is created on the destination cluster, resulting in replicated topics named such as &lt;origin-cluster&gt;.&lt;topic-name&gt;. Message history is included in geo-replication. This means all available message data for the topic is copied. The amount of history is determined by the message retention option set when the topics were created on the origin cluster. Considerations IBM Event Streams geo-replication uses Kafka’s MirrorMaker 2.0 to replicate data from the origin cluster to the destination cluster. Replication Factor IBM Event Streams sets the number of replicas of geo-replicated topics to 3, or if there are fewer brokers available then to the number of brokers in the destination cluster. If a different number of replicas are required on the destination topic, edit the value of the sourceConnector configuration property replication.factor on the MirrorMaker2 instance that is created by Event Streams for the geo-replication pairing. The change will apply to all new topics created by the geo-replicator on the destination cluster after the changes is made. It will not be applied to topics already configured for geo-replication Topic configuration MirrorMaker 2.0 has a blacklist of topic properties that are not copied from the source cluster topic:   follower.replication.throttled.replicas  leader.replication.throttled.replicas  message.timestamp.difference.max.ms  message.timestamp.type  unclean.leader.election.enable  min.insync.replicasIt is not possible to override the value of these properties using MirrorMaker 2.0 configuration, instead the values are taken from the settings of the destination cluster. To query the current values set on the destination cluster:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  List the broker configuration by using cloudctl es broker 0  Update the broker configuration to set these properties to the values if required before configuring geo-replication.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "10.1",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming geo-replication, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name, which includes the release name.      The health of the geo-replication for that origin cluster: Creating, Running, Updating, Paused, Stopping, Assigning, Offline, and Error.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab for details.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab for details.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend replication of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication and restart replication of data from the origin cluster.       More options &gt; Restart failed replicators: To restart a geo-replicator that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: Awaiting creation, Pending, Running, Resume, Resuming, Pausing, Paused, Removing, and Error. When the status is Error, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Destination locations tab for details.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the name of the destination cluster for which you want to manage geo-replication.  Choose from one of the following options using the top right  More options menu:           More options &gt; Pause running geo-replicator: To pause the geo-replicator for this destination and suspend replication of data to the destination cluster for all topics.       More options &gt; Resume paused geo-replicator: To resume the paused geo-replicator for this destination and resume replication of data to the destination cluster for all topics.       More options &gt; Restart failed geo-replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove cluster as destination: To remove the cluster as a destination for geo-replication.      To stop an individual topic from being replicated and remove it from the geo-replicator, select  More options &gt; Stop replicating topic. Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters      Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination destination_byl6xThe command returns the following information:     Details of destination cluster destination_byl6x   Cluster ID          Cluster name   REST API URL                                                                     Skip SSL validation?   destination_byl6x   destination    https://destination-ibm-es-admapi-external-myproject.apps.geodest.ibm.com:443    false   Geo-replicator details   Geo-replicator name                                             Status          Origin bootstrap servers   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       RUNNING         origin_es-kafka-bootstrap-myproject.apps.geosource.ibm.com:443   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   RUNNING         origin_es-kafka-bootstrap-myproject.apps.geosource.ibm.com:443   Geo-replicated topics   Geo-replicator name                                             Origin topic    Destination topic   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       topic1          origin_es.topic1   origin_es-&gt;destination-mm2connector.MirrorSourceConnector       topic2          origin_es.topic2   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   topic1          origin_es.topic1   origin_es-&gt;destination-mm2connector.MirrorCheckpointConnector   topic2          origin_es.topic2      The MirrorSource connector replicates data from the origin to the destination cluster. You can use the MirrorCheckpoint connector during failover from the origin to the destination cluster. To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:\\      cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\" For example: cloudctl es geo-replicator-pause --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \" This will pause both the MirrorSource connector and the MirrorCheckpoint connector for this geo-replicator.  Geo-replication for all topics that are part of this geo-replicator will be paused.         cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-resume --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will resume both the MirrorSource connector and the MirrorCheckpoint connector for this geo-replicator after they have been paused. Geo-replication for all topics that are part of this geo-replicator will be resumed.         cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-restart --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will restart a failed geo-replicator.         cloudctl es geo-replicator-topics-remove --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\" --topics &lt;comma-separated-topic-list&gt;For example:cloudctl es geo-replicator-topics-remove --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \" --topics topic1,topic2This will remove the listed topics from this geo-replicator.         cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name \"&lt;replicator-name&gt;\"   For example:   cloudctl es geo-replicator-delete --destination destination_byl6x  --name \"origin_es-&gt;destination-mm2connector.MirrorSourceConnector \"   This will remove all MirrorSource and MirrorCheckpoint connectors for this geo-replicator.         cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;   For example:   cloudctl es geo-cluster-remove --destination destination_byl6x   This will permanently remove a destination cluster.   Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.   Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an unexpected error from Kafka, it might stop replicating and report a status of Error. Monitor your geo-replication cluster to confirm that your geo-replicator is replicating data. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.Using Grafana dashboards to monitor geo-replication Metrics are useful indicators of the health of geo-replication.  They can give warnings of potential problems as well as providing data that can be used to alert on outages. Monitor the health of your geo-replicator using the available metrics to ensure replication continues. Configure your IBM Event Streams geo-replicator to export metrics, and then view them using the example Grafana dashboard. Configuring metrics Enable export of metrics in Event Streams geo-replication by editing the associated KafkaMirrorMaker2 custom resource. Using the OpenShift Container Platform web console   Go to where your destination cluster is installed. Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects dropdown list, select the project that contains the destination IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  Click the Kafka Mirror Maker 2 tab to see the list of KafkaMirrorMaker2 instances.  Click the KafkaMirrorMaker2 instance with the name of the instance that you are adding metrics to.  Click the YAML tab.      Add the spec.metrics property. For example:      # ... spec:   metrics: {} # ...        Click Save.Using the OpenShift Container Platform CLI To modify the number of geo-replicator workers run the following using the oc tool:   Go to where your destination cluster is installed. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing destination cluster:oc project &lt;project-name&gt;  Run the following command to list your KafkaMirrorMaker2 instances:oc get kafkamirrormaker2s  Run the following command to edit the custom resource for your KafkaMirrorMaker2 instance:oc edit kafkamirrormaker2 &lt;instance-name&gt;      Add the spec.metrics property. For example:     spec:  metrics: {}        Save your changes and close the editor.Installing persistent Grafana dashboards IBM Cloud Platform Common Services does not currently have a way to configure persistent storage on Grafana. This means that when the Grafana pods get restarted, you will lose any data on Grafana. To install Event Streams Grafana dashboards that will persist, use the following steps:   Download the geo-replication MonitoringDashboard custom resource from GitHub.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the MonitoringDashboard custom resource as follows:     oc apply -f &lt;dashboard-path&gt; -n &lt;namespace&gt;   Viewing installed Grafana dashboards To view the Event Streams Grafana dashboards, follow these steps:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Navigate to the IBM Cloud Platform Common Services console homepage.  Click the hamburger icon in the top left.  Expand Monitor Health.  Click the Monitoring in the expanded menu to open the Grafana homepage.  Click the user icon in the bottom left corner to open the user profile page.  In the Organizations table, find the namespace where you installed the Event Streams geo-replication MonitoringDashboard custom resource, and switch the user profile to that namespace. If you have not installed persistent dashboards, follow the instructions for installing persistent Grafana dashboards.  Hover the Dashboards on the left and click Manage.  Click on the dashboard to view the Dashboard table.Ensure you select your namespace, cluster name, and other filters at the top of the dashboard to view the required information. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/georeplication/health/",
        "teaser":null},{
        "title": "Switching clusters",
        "collection": "10.1",
        "excerpt":"When one of your origin Event Streams clusters experiences problems and becomes unavailable, you can switch your client applications over to use the geo-replicated topics on your destination Event Streams cluster. Ensure you plan for geo-replication before setting it up. Preparing clusters and applications for switching To make switching of clusters require little intervention, consider the following guidance when preparing your Event Streams clusters and applications. Configure your applications for switching Set up your applications so that reconfiguring them to switch clusters is as easy as possible. Code your applications so that security credentials, certificates, bootstrap server addresses, and other configuration settings are not hard-coded, but can be set in configuration files, or otherwise injected into your applications. Use the same certificates Consider using the same certificates for both the origin and destination clusters, by providing your own certificates at installation. This allows applications to use a single certificate to access either cluster. Note: You must complete the process of providing your own certificates before installing an instance of Event Streams. Note: When providing your own certificates, ensure that certificate renewal processes are followed at both the origin and destination clusters, so that both clusters continue to use the same certificates. Set up the same access to both clusters Consider providing your applications the same access to both the origin and destination clusters. For example, you can duplicate the application KafkaUser credentials from the origin cluster to the destination cluster. This allows applications to use a single set of credentials to access either cluster. Use the following commands to retrieve the KafkaUser credentials and custom resource from the origin cluster, and then create a new KafkaUser with these credentials on the destination cluster:   Log in to your origin cluster. Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to retrieve the name of the secret for the KafkaUser:oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o jsonpath='{\"username: \"}{.status.username}{\"\\nsecret-name: \"}{.status.secret}{\"\\n\"}'The command provides the following output:          The principal username      The name of the Kubernetes Secret, which includes the namespace, containing the SCRAM password or the TLS certificates.        Use the secret-name from the previous step to run the following command. The command retrieves the credentials from the Kubernetes Secret and and saves them to the kafkauser-secret.yaml file:oc get secret &lt;secret-name&gt; --namespace &lt;namespace&gt; -o yaml &gt; kafkauser-secret.yaml  Run the following command to retrieve the KafkaUser custom resource YAML and save it to the kafkauser.yaml file:oc get kafkauser &lt;name&gt; --namespace &lt;namespace&gt; -o yaml &gt; kafkauser.yaml  Log in to your destination cluster.  Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Edit both the kafkauser-secret.yaml and kafkauser.yaml files to set the correct namespace and Event Streams cluster name for the following properties:          metadata.namespace: provide the namespace of your destination cluster.      metadata.labels[\"eventstreams.ibm.com/cluster\"]: provide the name of your destination cluster.        Run the following command to create the Kubernetes Secret containing the KafkaUser credentials on the destination cluster:oc apply -f kafkauser-secret.yamlNote: You must run this command before the creation of the KafkaUser to ensure the same credentials are available on both the origin and destination clusters.  Run the following command to create the KafkaUser on the destination cluster:oc apply -f kafkauser.yamlNote: To duplicate KafkaUser credentials that use Mutual TLS authentication, the origin and destination cluster must be configured with the same certificates for the client CA at installation. Note: When KafkaUser credentials or Access Control Lists (ACLs) are modified on the origin cluster, the changes will need to be duplicated to the destination cluster to ensure that you can still switch clusters. Use regular expressions for consumer topic subscriptions Geo-replicated topics on the destination cluster will have a prefix added to the topic name. The prefix is the name of the Event Streams instance on the origin cluster, as defined in the EventStreams custom resource, for example my_origin.&lt;topic-name&gt;. Consider using regular expressions to define the topics that consuming applications are subscribed to, for example .*&lt;topic-name&gt;.  Using a regular expression means that the topic subscription does not need to change when switching to the prefixed topic names on the destination cluster. Plan to update consumer group offsets Consider how you will update the consumer group offsets in consuming applications when switching clusters. Geo-replication includes consumer group checkpointing to store the mapping of consumer group offsets, allowing consuming applications to continue processing messages at the appropriate offset positions. Produce to the same topic name When switching clusters, produce to the same topic name on the destination cluster as was used on the origin cluster. This will ensure geo-replicated messages and directly produced messages are stored in separate topics. If consuming applications use regular expressions to subscribe to both topics, then both sets of messages will be processed. Consider message ordering If message ordering is required, configure your consuming applications to process all messages from the geo-replicated topic on the destination cluster before producing applications are restarted. Updating existing applications to use geo-replicated topics on the destination cluster If you are not using the same certificates and credentials on the origin and destination clusters, use the following instructions to retrieve the information required to update your applications so that they can use the geo-replicated topics from the destination cluster:   Log in to your destination Event Streams cluster as an administrator.  Click Connect to this cluster.  Go to the Resources tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: In the Kafka listener and credentials section, select the listener from the list.                  Click the External tab for applications connecting from outside of the OpenShift Container Platform cluster.          Click the Internal tab for applications connecting from inside the OpenShift Container Platform cluster.                    Credentials: To connect securely to Event Streams, your application needs credentials with permission to access the cluster and resources such as topics. In the Kafka listener and credentials section, click the Generate SCRAM credentials or Generate TLS credentials button next to the listener you are using, and follow the instructions to select the level of access you want to grant to your resources with the credentials.      Certificates: A certificate is required by your client applications to connect securely to the destination cluster. In the Certificates section, download either the PKCS12 certificate or PEM certificate. If you use the PKCS12 certificate, make a copy of the Certificate password to use with the certificate in your application.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Updating consumer group offsets The topic on the origin cluster and the geo-replicated topic on the destination cluster might have different offsets for the same messages, depending on when geo-replication started. This means that a consuming application that is switched to use the destination cluster cannot use the consumer group offset from the origin cluster. Updating consumer group offsets by using checkpoints Geo-replication uses the Kafka Mirror Maker 2.0 MirrorCheckpointConnector to automatically store consumer group offset checkpoints for all origin cluster consumer groups. Each checkpoint maps the last committed offset for each consumer group in the origin cluster to the equivalent offset in the destination cluster. The checkpoints are stored in the &lt;origin_cluster_name&gt;.checkpoints.internal topic on the destination cluster. Note: Consumer offset checkpoint topics are internal topics that are not displayed in the UI and CLI. Run the following CLI command to include internal topics in the topic listing:cloudctl es topics --internal. When processing messages from the destination cluster, you can use the checkpoints to start consuming from an offset that is equivalent to the last committed offset on the origin cluster. If your application is written in Java, Kafka’s RemoteClusterUtils class provides the translateOffsets() utility method to retrieve the destination cluster offsets for a consumer group from the checkpoints topic. You can then use the KafkaConsumer.seek() method to override the offsets that the consumer will use on the next poll. For example, the following Java code snippet will update the example-group consumer group offset from the origin-cluster cluster to the destination cluster equivalent: // Retrieve the mapped offsets for the destination cluster topic-partitionsMap&lt;TopicPartition, OffsetAndMetadata&gt; destinationOffsetsMap = RemoteClusterUtils.translateOffsets(properties, \"origin-cluster\",        \"example-group\", Duration.ofMillis(10000));// Update the KafkaConsumer to start at the mapped offsets for every topic-partitiondestinationOffsetsMap.forEach((topicPartition, offsetAndMetadata) -&gt; kafkaConsumer.seek(topicPartition, offsetAndMetadata));// Retrieve records from the destination cluster, starting from the mapped offsetsConsumerRecords&lt;byte[], byte[]&gt; records = kafkaConsumer.poll(Duration.ofMillis(10000))Note: To configure how often checkpoints are stored and which consumer groups are stored in the checkpoints topic, you can edit the following properties in your Kafka Mirror Maker 2 custom resource:   spec.mirror.checkpointConnector.config  spec.mirror.groupsPatternUpdating consumer group offsets manually If you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic, you can use the cloudctl es group-reset command.   To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:          Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;      Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the cloudctl es group-reset command as follows:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute        To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --executeThese methods also avoid the need to make code changes to your client application. Reverting message production and consumption back to the origin cluster When the origin Event Streams cluster becomes available again, you can switch your client applications   back to use the topics on your origin cluster. If messages have been produced directly to the destination cluster, use the following steps to replicate those messages to the origin cluster before switching back to using it.   Create an EventStreamsGeoReplicator custom resource configured to connect to the origin Event Streams cluster, and set up geo-replication in the reverse direction to the original geo-replication flow. This means there will be a geo-replicator running on the origin cluster which copies messages from non-geo-replicated topics on the destination cluster back to geo-replicated topics on the origin cluster.  The geo-replicated topic named &lt;origin-cluster&gt;.&lt;topic&gt; on the destination cluster will not have new geo-replicated messages arriving, as the producing applications have been switched to produce messages directly to the topic without a prefix on the destination cluster. Ensure that the geo-replicated topic on the destination cluster is not geo-replicated back to the origin cluster as this will result in duplicate data on the origin cluster.  Switch the producing and consuming applications back to the origin cluster again by following the previous instructions. Producing applications will continue to produce messages to the original topic name on the origin cluster, and consuming applications will read from both the geo-replicated topics and the original topics on the origin cluster. Consuming applications will need their consumer group offsets to be correctly updated for the offset positions on the origin cluster.Note: Due to the asynchronous nature of geo-replication, there might be messages in the original topics on the origin cluster that had not been geo-replicated over to the destination cluster when the origin cluster became unavailable. You will need to decide how to handle these messages. Consider setting consumer group offsets so that the messages are processed, or ignore the messages by setting consumer group offsets to the latest offset positions in the topic. For example, if the origin cluster is named my_origin, the destination cluster is named my_destination, and the topic on the my_origin cluster is named my_topic, then the geo-replicated topic on the my_destination cluster will be named my_origin.my_topic.   When the my_origin cluster becomes unavailable, producing applications are switched to the my_destination cluster. The my_destination cluster now has topics named my_topic and my_origin.my_topic. Consuming applications are also switched to the my_destination cluster and use the regular expression .*my_topic to consume from both topics.  When the my_origin cluster becomes available again, reverse geo-replication is set up between the clusters. The my_origin cluster now has the topic named my_topic and a new geo-replicated topic named my_destination.my_topic. The topic named my_destination.my_topic contains the messages that were produced directly to the my_destination cluster.  Producing applications are producing to the topic named my_topic on the my_destination cluster, so the geo-replicated topic named my_origin.my_topic on the my_destination cluster does not have any new messages arriving. Existing messages in the topic named my_origin.my_topic are consumed from the my_destination cluster until there is no more processing of the messages required. Note: The geo-replicated topic named my_origin.my_topic is not included in the reverse geo-replication back to the my_origin cluster, as that would create a geo-replicated topic named my_destination.my_origin.my_topic on the my_origin cluster containing the same messages as in the topic named my_topic.  Producing applications are now switched back to the my_origin cluster, continuing to produce to the topic named my_topic.  Consuming applications are also switched back to the my_origin cluster, with consumer group offsets updated for the offset positions at the my_origin cluster. Consuming applications continue to use the regular expression .*my_topic to consume from both the topic named my_topic and the geo-replicated topic named my_destination.my_topic.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/georeplication/failover/",
        "teaser":null},{
        "title": "Event Streams producer API",
        "collection": "10.1",
        "excerpt":"Event Streams provides a REST API to help connect your existing systems to your Event Streams Kafka cluster. Using the API, you can integrate Event Streams with any system that supports RESTful APIs. The REST producer API is a scalable REST interface for producing messages to Event Streams over a secure HTTP endpoint. Send event data to Event Streams, utilize Kafka technology to handle data feeds, and take advantage of Event Streams features to manage your data. Use the API to connect existing systems to Event Streams, such as IBM Z mainframe systems with IBM z/OS Connect, systems using IBM DataPower Gateway, and so on. About authorization By default Event Streams requires clients to be authorized to write to topics. The available authentication mechanisms for use with the REST Producer are MutualTLS (tls) and SCRAM SHA 512 (scram-sha-512). For more information about these authentication mechanisms, see the information about managing access. The REST producer API requires any authentication credentials be provided with each REST call to grant access to the requested topic. This can be done in one of the following ways:   In an HTTP authorization header: You can use this method when you have control over what HTTP headers are sent with each call to the REST producer API. For example, this is the case when the API calls are made by code you control.  Mutual TLS authentication (also referred to as SSL client authentication or SSL mutual authentication):You can use this method when you cannot control what HTTP headers are sent with each REST call. For example, this is the case when you are using third-party software or systems such as CICS events over HTTP.Note: You must have Event Streams version 2019.1.1 or later to use the REST API. In addition, you must have Event Streams version 2019.4.1 or later to use the REST API with SSL client authentication. Content types The following are supported for the value of the Content-Type header:   application/octet-stream  text/plain  application/json  text/xml  application/xmlFor each content type the message body is copied “as is” into the Kafka record value. Both the application/octet-stream and text/plain types must specify a length header to avoid accidental truncation if the HTTP connection drops prematurely. The payload of a request that uses the application/json header must parse as valid JSON. Otherwise, the request will be rejected. The Event Streams REST Producer API also supports the following vendor content types:   vnd.ibm-event-streams.json.v1 as a synonym for application/json  vnd.ibm-event-streams.binary.v1 as a synonym for application/octet-stream  vnd.ibm-event-streams.text.v1 as a synonym for text/plainThese content types can be used to pin applications at the version 1 API level. Prerequisites To be able to produce to a topic, ensure you have the following available:   The URL of the Event Streams REST Producer API endpoint.  The topic you want to produce to.  If using a REST Producer API endpoint that requires HTTPS, the Event Streams certificate.To retrieve the full URL for the Event Streams API endpoint, you can use the OpenShift Container Platform oc CLI or Event Streams UI. Using the OpenShift Container Platform oc CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to list available Event Streams REST Producer API endpoints:oc get routes -n &lt;namespace&gt; --selector app.kubernetes.io/name=rest-producer  Copy the full URL of the required endpoint from the HOST/PORT section of the response.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section.  Click Copy Producer API endpoint.By default the Event Streams REST Producer API endpoint requires a HTTPS connection. If this has not been disabled for the endpoint the Event Streams certificate must be retrieved. You can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Download the server certificate for Event Streams:cloudctl es certificates --format pem By default, the certificate is written to a file called es-cert.pem.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Certificates section.  In the PEM certificate section, click Download certificate.For information on how to create a topic to produce to, see the information about creating topics. Key and message size limits The REST producer API has a configured limit for the key size (default is 4096 bytes) and the message size (default is 65536 bytes). If the request sent has a larger key or message size than the limits set, the request will be rejected. You can configure the key and message size limits at the time of installation or later as described in modifying installation settings. The limits are configured by setting environment variables on the REST Producer component: spec:  restProducer:    env:      - name: MAX_KEY_SIZE        value: \"4096\"      - name: MAX_MESSAGE_SIZE        value: \"65536\"Important: Do not set the MAX_MESSAGE_SIZE to a higher value than the maximum message size that can be received by the Kafka broker or the individual topic (max.message.bytes). By default, the maximum message size for Kafka brokers is 1000012 bytes. If the limit is set for an individual topic, then that setting overrides the broker setting. Any message larger than the maximum limit will be rejected by Kafka. Note: Sending large requests to the REST producer increases latency, as it will take the REST producer longer to process the requests. Producing messages using REST with HTTP authorization Ensure you have gathered all the details required to use the producer API, as described in the prerequisites. Before producing you must also create authentication credentials. To create authentication credentials to use in an HTTP authorization header, you can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Use the kafka-user-create command to create a KafkaUser that can produce to your topic:    cloudctl es kafka-user-create --topic &lt;topic_name&gt; --name &lt;user_name&gt; --producer --auth-type scram-sha-512        Follow the steps in managing access to retrieve the SCRAM SHA 512 username and password.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section, then click Generate credentials.  Select SCRAM username and password, then click Next.  Fill in a Credential Name, this name must be unique.  Select Produce messages, consume messages and create topics and schemas, then click Next.  Select A specific topic and fill in the topic name, then click Next.  Click Next on the consumer group panel, then Generate credentials on the transactional IDs panel using the default settings.  Take a copy of either the username and password or Basic authentication token.You can use the usual languages for making the API call. For example, to use cURL to produce messages to a topic with the producer API using a Basic authentication header, run the curl command as follows: curl -v -X POST -H \"Authorization: Basic &lt;auth_token&gt;\" -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;auth_token&gt; is the Basic authentication token you generated earlier.  &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier. Use http instead of https if the provided Producer API endpoint has TLS disabled.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  --cacert es-cert.pem can be ommitted if the provided Producer API endpoint has TLS disabledTo use cURL to produce messages to a topic with the producer API using a SCRAM username and password, run the curl command as follows: curl -v -X POST -u &lt;user&gt;:&lt;password&gt; -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;user&gt; is the SCRAM username provided when generating credentials.  &lt;password&gt; is the SCRAM password retrieved earlier.  &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier.  Use http instead of https if the provided Producer API endpoint has TLS disabled.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  --cacert es-cert.pem can be ommitted if the provided Producer API endpoint has TLS disabledFor full details of the API, see the API reference. Producing messages using REST with Mutual TLS authentication Ensure you have gathered all the details required to use the producer API, as described in the prerequisites. Before producing you must also create TLS credentials. To create authentication credentials to use with Mutual TLS authentication, you can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Use the kafka-user-create command to create a KafkaUser that can produce to your topic:    cloudctl es kafka-user-create --topic &lt;topic_name&gt; --name &lt;user_name&gt; --producer --auth-type tls        Follow the steps in managing access to TLS certificates and keys.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the Producer endpoint and credentials section, then click Generate credentials.  Select Mutual TLS certificate, then click Next.  Fill in a Credential Name, this name must be unique.  Select Produce messages, consume messages and create topics and schemas, then click Next.  Select A specific topic and fill in the topic name, then click Next.  Click Next on the consumer group panel, then Generate credentials on the transactional IDs panel using the default settings.  Click Download certificates and unzip the downloaded ZIP archive containing the TLS certificates and keys.For some systems, for example CICS, you need to download and import the client CA certificate into your truststore. The client CA certificate can be downloaded using the OpenShift Container Platform oc and Event Streams CLI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Run the following command to view details of the KafkaUser you want the client CA certificate for: cloudctl es kafka-user &lt;user-name&gt;  Note down the name of the secret associated with the KafkaUser  Run the following oc command to get the client CA certificate from the secret found in the previous command: oc extract secret/&lt;user-secret-name&gt; --keys=ca.crt --to=- &gt; ca.crtIf you are using Java keystores, the client certificate can be imported by using the keytool -importcert ... command as described in the IBM SDK, Java Technology Edition documentation. Some systems require the client certificate and private key to be combined into one PKCS12 file (with extension .p12 or .pfx). A PKCS12 file and associated password file is included in the KafkaUser secret and the ZIP file downloaded from the Event Streams UI. You can use the usual languages for making the API call. Consult the documentation for your system to understand how to specify the client certificate and private key for the outgoing REST calls to Event Streams. For example, to use cURL to produce messages to a topic with the producer API, run the curl command as follows: curl -v -X POST -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem --key user.key --cert user.crt \"https://&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;api_endpoint&gt; is the full URL copied from the Producer API endpoint field earlier.  &lt;topic_name&gt; is the name of the topic you want to produce messages to.  es-cert.pem is the Event Streams server certificate downloaded as part of the prerequisites  user.key is the private key of the user downloaded from the UI or read from the KafkaUser secret  user.crt is the user certificate that contains the public key of the user downloaded from the UI or read from the KafkaUser secretFor example, the steps to configure a CICS URIMAP as an HTTP client is described in the  CICS Transaction Server documentation. In this case, load the client certificate and private key, together with the Event Streams server certificate into your RACF key ring. When defining the URIMAP:   Host is the client authentication API endpoint obtained as part of the prerequisites, without the leading https://  Path is /topics/&lt;topic-name&gt;/records  Certificate is the label given to the client certificate when it was loaded into the key ring.For full details of the API, see the API reference. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/rest-api/",
        "teaser":null},{
        "title": "Kafka Connect and connectors",
        "collection": "10.1",
        "excerpt":"You can integrate external systems with IBM Event Streams by using the Kafka Connect framework and connectors. What is Kafka Connect? When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Use Kafka Connect to reliably move large amounts of data between your Kafka cluster and external systems. For example, it can ingest data from sources such as databases and make the data available for stream processing.  Source and sink connectors Kafka Connect uses connectors for moving data into and out of Kafka. Source connectors import data from external systems into Kafka topics, and sink connectors export data from Kafka topics into external systems. A wide range of connectors exists, some of which are commercially supported. In addition, you can write your own connectors. A number of source and sink connectors are available to use with Event Streams. See the connector catalog section for more information.  Workers Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems, and all work is performed in a single process. Distributed mode is more appropriate for production use, as it benefits from additional features such as automatic balancing of work, dynamic scaling up or down, and fault tolerance.  When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to the external system (for example, IBM MQ).When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Connector catalog The connector catalog contains a list of connectors that have been verified with Event Streams. Connectors are either supported by the community or IBM. Community support means the connectors are supported through the community by the people that created them. IBM supported connectors are fully supported as part of the official Event Streams support entitlement if you are using the paid-for version of Event Streams (not Community Edition). See the connector catalog for a list of connectors that work with Event Streams.  Setting up connectors Event Streams provides help with setting up your Kafka Connect environment, adding connectors to that environment, and starting the connectors. See the instructions about setting up and running connectors. Connectors for IBM MQ Connectors are available for copying data between IBM MQ and Event Streams. There is a MQ source connector for copying data from IBM MQ into Event Streams or Apache Kafka, and a MQ sink connector for copying data from Event Streams or Apache Kafka into IBM MQ. For more information about MQ connectors, see the topic about connecting to IBM MQ. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/connectors/",
        "teaser":null},{
        "title": "Setting up and running connectors",
        "collection": "10.1",
        "excerpt":"IBM Event Streams helps you set up a Kafka Connect environment, prepare the connection to other systems by adding connectors to the environment, and start Kafka Connect with the connectors to help integrate external systems. Log in to the Event Streams UI, and click Toolbox in the primary navigation. Scroll to the Connectors section and follow the guidance for each main task. You can also find additional help on this page. Using Kafka Connect The most straightforward way to run Kafka Connect on OpenShift Container Platform is to use a custom resource called KafkaConnectS2I. An instance of this custom resource represents a Kafka Connect distributed worker cluster. In this mode, workload balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. Each connector is represented by another custom resource called KafkaConnector. Kafka Connect topics When running in distributed mode, Kafka Connect uses three topics to store configuration, current offsets and status. Kafka Connect can create these topics automatically as it is started by the Event Streams operator. By default, the topics are:   connect-configs: This topic stores the connector and task configurations.  connect-offsets: This topic stores offsets for Kafka Connect.  connect-status: This topic stores status updates of connectors and tasks.If you want to run multiple Kafka Connect environments on the same cluster, you can override the default names of the topics in the configuration. Authentication and authorization Kafka Connect uses an Apache Kafka client just like a regular application, and the usual authentication and authorization rules apply. Kafka Connect will need authorization to:   Produce and consume to the internal Kafka Connect topics and, if you want the topics to be created automatically, to create these topics  Produce to the target topics of any source connectors you are using  Consume from the source topics of any sink connectors you are using.Note: For more information about authentication and the credentials and certificates required, see the information about managing access. Set up a Kafka Connect environment To begin using Kafka Connect, do the following. Download Kafka Connect configuration   In the Event Streams UI, click Toolbox in the primary navigation. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  Click Download Kafka Connect ZIP to download the compressed file, then extract the contents to your preferred location.You will have a Kubernetes manifest for a KafkaConnectS2I and an empty directory called my-plugins. Configure Kafka Connect Edit the downloaded kafka-connect-s2i.yaml file to enable Kafka Connect to connect to your OpenShift Container Platform cluster. You can use the snippets in the Event Streams UI as guidance to configure Kafka Connect.   Choose a name for your Kafka Connect instance.  You can run more than one worker by increasing the replicas from 1.  Set bootstrapServers to connect the bootstrap server address of a listener. If using an internal listener, this will be the address of a service. If using an external listener, this will be the address of a route.  If you have fewer than 3 brokers in your Event Streams cluster, you must set config.storage.replication.factor, offset.storage.replication.factor and status.storage.replication.factor to 1.  Unless your Event Streams cluster has authentication turned off, you must provide authentication credentials in the authentication configuration.  If clients require a certificate to connect to your Event Streams cluster (as they will if you are connecting using a route), you must provide a certificate in the tls configuration.Deploy Kafka Connect Deploy the Kafka Connect instance by applying the YAML file using the OpenShift Container Platform CLI: oc apply -f kafka-connect-s2i.yamlVerify the Kafka Connect instance has been created. It can take up to 5 minutes to become ready. oc get kafkaconnects2iOnce ready, you can see the status and which connectors are available: oc describe kafkaconnects2i &lt;kafka_connect_name&gt;Adding connectors to your Kafka Connect environment Prepare Kafka Connect for connections to your other systems by adding the required connectors. Following on from the previous step, click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Add connectors on the Add connectors to your Kafka Connect environment tile. To run a particular connector Kafka Connect must have access to a JAR file or set of JAR files for the connector. If your connector consists of just a single JAR file, you can copy it directly into the my-plugins directory. If your connector consists of multiple JAR files, create a directory for the connector inside the my-plugins directory and copy all of the connector’s JAR files into that directory. Here’s an example of how the directory structure might look with 3 connectors: +--  my-plugins|    +--  connector1.jar|    +--  connector2|    |    +-- connector2.jar|    |    +-- connector2-lib.jar|    +-- connector3.jarStarting Kafka Connect with your connectors Click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Start Kafka Connect on the Start Kafka Connect with your connectors tile. Build Kafka Connect with your connectors To add your connector JARs into your Kafka Connect environment, start a build using the directory you prepared: oc start-build &lt;kafka_connect_name&gt;-connect --from-dir ./my-plugins/Wait for the build to be marked complete and the Kafka Connect pod to become ready. It can take up to 2 minutes to complete. oc get buildsoc get podsVerify that your chosen connectors are installed in your Kafka Connect environment. oc describe kafkaconnects2i &lt;kafka_connect_name&gt;Start a connector Create a YAML file for the connector configuration. For the MQ connectors, you can use the Event Streams CLI to generate the YAML file. Alternatively, you can use the following template, taking care to replace &lt;kafka_connect_name&gt; with the name of the KafkaConnectS2I instance: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: &lt;connector_name&gt;  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: &lt;connector_class_name&gt;  tasksMax: 1  config:  # The connector configuration goes hereStart the connector build applying the YAML file: oc apply -f &lt;connector_filename&gt;.yamlYou can view the status of the connector by describing the custom resource: oc describe kafkaconnector &lt;connector_name&gt;","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/setting-up-connectors/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "10.1",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Available connectors Connectors are available for copying data in both directions.   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue. Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For convenience, it is recommended to run the Kafka Connect worker on the same OpenShift Container Platform cluster as IBM Event Streams. If the network latency between MQ and IBM Event Streams is significant, you might prefer to run the Kafka Connect worker close to the queue manager to minimize the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "10.1",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in OpenShift Container Platform. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites To follow these instructions, ensure you have the following available:   A running Kafka Connect environment on OpenShift Container Platform using a KafkaConnectS2I custom resource  IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click Generate.  In the dialog, enter the configuration of the MQ Source connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to generate a configuration file.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-source command to generate the configuration file for the MQ Source connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSOURCE and connecting to the topic TSOURCE, run the following command:    cloudctl es connector-config-mq-source --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSOURCE\" --topic=\"TSOURCE\" --file=\"mq-source\" --format yaml        Note: Omitting the --format yaml flag will generate a mq-source.properties file which can be used for standalone mode. Specifying --format json will generate a mq-source.json file which can be used for distributed mode outside OpenShift Container Platform.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: mq-source  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: com.ibm.eventstreams.connect.mqsource.MQSourceConnector  tasksMax: 1  config:    topic: TSOURCE    mq.queue.manager: QM1    mq.connection.name.list: localhost(1414)    mq.channel.name: MYSVRCONN    mq.queue: MYQSOURCE    mq.user.name: alice    mq.password: passw0rd    key.converter: org.apache.kafka.connect.storage.StringConverter    value.converter: org.apache.kafka.connect.storage.StringConverter    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilderA list of all the possible flags can be found by running the command cloudctl es connector-config-mq-source --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Source connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click Go to GitHub. Download the JAR file from the list of assets for the latest release.Configuring Kafka Connect Follow the steps in Starting Kafka Connect with your connectors. When adding connectors, add the MQ connector JAR you downloaded, and when starting the connector, use the YAML file you created earlier. Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ: $ oc logs &lt;kafka_connect_pod_name&gt;...INFO Created connector mq-source...INFO Connection to MQ established...Send a test message   To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;  Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation and select the connected topic. Messages will appear in the message browser of that topic.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "10.1",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in OpenShift Container Platform. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites To follow these instructions, ensure you have the following available:   A running Kafka Connect environment on OpenShift Container Platform using a KafkaConnectS2I custom resource  IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and put messages on a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click Generate.  In the dialog, enter the configuration of the MQ Sink connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to generate a configuration file.   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-sink command to generate the configuration file for the MQ Sink connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSINK and connecting to the topics TSINK, run the following command:    cloudctl es connector-config-mq-sink --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSINK\" --topics=\"TSINK\" --file=\"mq-sink\" --format yaml        Note: Omitting the --format yaml flag will generate a mq-source.properties file which can be used for standalone mode. Specifying --format json will generate a mq-source.json file which can be used for distributed mode outside OpenShift Container Platform.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaConnectormetadata:  name: mq-sink  labels:    # The eventstreams.ibm.com/cluster label identifies the KafkaConnect instance    # in which to create this connector. That KafkaConnect instance    # must have the eventstreams.ibm.com/use-connector-resources annotation    # set to true.    eventstreams.ibm.com/cluster: &lt;kafka_connect_name&gt;spec:  class: com.ibm.eventstreams.connect.mqsink.MQSinkConnector  tasksMax: 1  config:    topics: TSINK    mq.queue.manager: QM1    mq.connection.name.list: localhost(1414)    mq.channel.name: MYSVRCONN    mq.queue: MYQSINK    mq.user.name: alice    mq.password: passw0rd    key.converter: org.apache.kafka.connect.storage.StringConverter    value.converter: org.apache.kafka.connect.storage.StringConverter    mq.message.builder: com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilderA list of all the possible flags can be found by running the command cloudctl es connector-config-mq-sink --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Sink connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click Go to GitHub. Download the JAR file from the list of assets for the latest release.Configuring Kafka Connect Follow the steps in Starting Kafka Connect with your connectors. When adding connectors, add the MQ connector JAR you downloaded, and when starting the connector, use the YAML file you created earlier. Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ: $ oc logs &lt;kafka_connect_pod_name&gt;...INFO Created connector mq-sink...INFO Connection to MQ established...Send a test message To test the connector you will need an application to produce events to your topic.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Starter application tile under Applications, and click Get started.  Click Download JAR from GitHUb. Download the JAR file from the list of assets for the latest release.  Click Generate properties.  Enter a name for the application.  Go to the Existing topic tab and select the topic you provided in the MQ connector configuration.  Click Generate and download .zip.  Follow the instructions in the UI to get the application running.Verify the message is on the queue:   Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "10.1",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. These instructions explain how to run Kafka Connect in both standalone and distributed mode. For more information and to help decide which mode to use see the explanation of Kafka Connect workers. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka Connect shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the Kafka Connect files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka Connect shell scripts and JAR files. To download Kafka Connect and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.12-2.6.0.tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.Download IBM MQ connectors and configuration Depending on the connector you want to use:\\   Download the source connector JAR and source configuration file  Download the sink connector JAR and sink configuration fileIf you want to run a standalone Kafka Connect worker, you need a .properties file. To run a distributed Kafka Connect worker, you need a .json file. Copy the connector JAR file(s) and the required configuration file to a directory on the z/OS Unix System Services. Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-standalone.sh  config/connect-standalone.properties  mq-source.properties or mq-sink.propertiesIf you want to run a distributed Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-distributed.sh  config/connect-distributed.shExtract the Apache Kafka distribution:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.12-2.6.0.tar  Change to the resulting kafka_&lt;version&gt; directory.Convert the shell scripts:   Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.shConvert the configuration files:   Copy the connect-standalone.properties file (or connect-distributed.properties for a distributed setup) into the current directory, for example:cp config/connect-standalone.properties ./connect-standalone.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.properties.orig &gt; config/connect-standalone.propertiesIf running in standalone mode:   Copy the MQ .properties file into the current directory, for example:cp ./mq-source.properties ./mq-source.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./mq-source.properties.orig &gt; ./mq-source.propertiesNote: For distributed mode the .json file must remain in ASCII format. Update the Kafka Connect configuration The connect-standalone.properties (or connect-distributed.properties for distributed mode) file must include the correct bootstrap.servers and SASL/SSL configuration for your Apache Kafka or Event Streams install. For example, if running against Event Streams, download the certificate for your install to your IBM z/OS system. Generate credentials that can produce, consume and create topics and update the connect-standalone.properties (or connect-distributed.properties) file to include: bootstrap.servers=&lt;bootstrapServers&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/opt/kafka/es-cert.p12ssl.truststore.password=&lt;truststorePassword&gt;ssl.truststore.type=PKCS12sasl.mechanism=SCRAM-SHA-512sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;userName&gt;\" password=\"&lt;password&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=/opt/kafka/es-cert.p12producer.ssl.truststore.password=&lt;truststorePassword&gt;producer.ssl.truststore.type=PKCS12producer.sasl.mechanism=SCRAM-SHA-512producer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"userName\" password=\"&lt;password&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=/opt/kafka/es-cert.p12consumer.ssl.truststore.password=&lt;truststorePassword&gt;consumer.ssl.truststore.type=PKCS12consumer.sasl.mechanism=SCRAM-SHA-512consumer.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"userName\" password=\"&lt;password&gt;\";plugin.path=/opt/connectorsConfiguring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.3.0-jar-with-dependencies.jar. As you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin, you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/connecting/mq/zos/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "10.1",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Checking Event Streams health Using the Event Streams UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready.If any of the components are not ready for an extended period of time, see how you can troubleshoot as described in debugging. Using the OpenShift Container Platform CLI You can check the health of your IBM Event Streams environment using the oc tool.   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:oc -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.If any of the components are not ready for an extended period of time, check the debugging topic. Debugging Using the OpenShift Container Platform UI   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Click on the Resources tab.  To filter only pods, deselect all resource types with the exception of Pod.  Click the pod not in the Running state to display information regarding the pod.  In the Overview, resource usage, such as CPU and memory, can be viewed.  Click on the Logs tab to search logs.Tip: You can also use the cluster logging provided by the OpenShift Container Platform to collect, store, and visualize logs. The cluster logging components are based upon Elasticsearch, Fluentd, and Kibana (EFK). You can download and install the pre-configured Event Streams Kibana dashboards by following the instructions in monitoring cluster health. Using the OpenShift Container Platform CLI   To retrieve further details about the pods, including events affecting them, use the following command:oc -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:oc -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about debugging, see the Kubernetes documentation. You can use the oc command instead of kubectl to perform the debugging steps. Note: After a component restarts, the oc command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the -–previous option to the oc logs command. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "10.1",
        "excerpt":"Monitoring the health of your Kafka cluster helps to verify that your operations are running smoothly. The Event Streams UI includes a preconfigured dashboard that monitors Kafka data. Event Streams also provides a number of ways to export metrics from your Kafka brokers to external monitoring and logging applications. These metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. The following sections provide an overview of the available options. For information about the health of your topics, check the producer activity dashboard. JMX Exporter You can use Event Streams to collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus. For an example of how to configure the JMX exporter, see configuring the JMX Exporter Kafka Exporter You can use Event Streams to export metrics to Prometheus. These metrics are otherwise only accessible through the Kafka command line tools. This allows topic metrics such as consumer group lag to be collected. For an example of how to configure a Kafka Exporter, see configuring the Kafka Exporter. JmxTrans JmxTrans can be used to push JMX metrics from Kafka brokers to external applications or databases. For more information, see configuring JmxTrans. Grafana You can use dashboards in the Grafana service to monitor your Event Streams instance for health and performance of your Kafka clusters. Viewing installed Grafana dashboards To view the Event Streams Grafana dashboards, follow these steps:   Log in to your IBM Cloud Platform Common Services management console as an administrator. For more information, see the IBM Cloud Platform Common Services documentation.  Navigate to the IBM Cloud Platform Common Services console homepage.  Click the hamburger icon in the top left.  Expand Monitor Health.  Click the Monitoring in the expanded menu to open the Grafana homepage.  Click the user icon in the bottom left corner to open the user profile page.  In the Organizations table, find the namespace where you installed the Event Streams monitoringdashboard custom resource, and switch the user profile to that namespace. If you have not installed persistent dashboards, follow the instructions for installing persistent Grafana dashboards.  Hover over the Dashboards on the left and click Manage.  Click on the dashboard you want to view in the Dashboard table.Ensure you select your namespace, cluster name, and other filters at the top of the dashboard to view the required information. Kibana Create dashboards in the Kibana service that is provided by the OpenShift Container Platform cluster logging, and use the dashboards to monitor for specific errors in the logs and set up alerts for when a number of errors occur over a period of time in your Event Streams instance. To install the Event Streams Kibana dashboards, follow these steps:   Ensure you have cluster logging installed.      Download the JSON file that includes the example Kibana dashboards for Event Streams from GitHub.         Navigate to the Kibana homepage on your cluster.     For IBM Cloud Platform Common Services:  Click the hamburger icon in the top left and then expand Monitor Health. Then click Logging to open the Kibana homepage.     For OpenShift Container Platform cluster logging stack: Log in to the OpenShift Container Platform web console using your login credentials. Then follow the instructions to navigate to cluster logging’s Kibana homepage.     Click the Management tab on the left.  Click the Saved Objects.  Click the Import icon and navigate to the JSON file that includes the example Kibana dashboards for Event Streams you have downloaded.  Click the Dashboard tab on the left to view the downloaded dashboards.Other Monitoring Tools You can also use external monitoring tools to monitor the deployed Event Streams Kafka cluster. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitoring dashboard.   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Monitoring in the primary navigation. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Select 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/cluster-health/",
        "teaser":null},{
        "title": "Monitoring topic health",
        "collection": "10.1",
        "excerpt":"To gain an insight into the overall health of topics, and highlight potential performance issues with systems producing to Event Streams, you can use the Producers dashboard provided for each topic. The dashboard displays aggregated information about producer activity for the selected topic through metrics such as active producer count, message size, and message produce rates. The dashboard also displays information about each producer that has been producing to the topic. You can expand an individual producer identified by its Kafka producer ID to gain insight into its performance through metrics such as messages produced, message size and rates, failed produce requests and any occurrences where a producer has exceeded a broker quota. The information displayed on the dashboard can also be used to provide insight into potential causes when applications experience issues such as delays or omissions when consuming messages from the topic. For example, highlighting that a particular producer has stopped producing messages, or has a lower message production rate than expected. Important: The Producers dashboard is intended to help highlight producers that might be experiencing issues producing to the topic. You might need to investigate the producer applications themselves to identify an underlying problem. To access the dashboard:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.      Select the topic name from the list you want to view information about.The Producers tab is displayed with the dashboard and details about each producer. You can refine the time period for which information is displayed. You can expand each producer to view details about their activity.     Note: When a new client starts producing messages to a topic, it might take up to 5 to 10 minutes before information about the producer’s activity appears in the dashboard. In the meantime, you can go to the Messages tab to check whether messages are being produced.   Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For information on how to monitor consumer groups for a particular topic, see monitoring Kafka consumer group lag. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/topic-health/",
        "teaser":null},{
        "title": "Monitoring applications with distributed tracing",
        "collection": "10.1",
        "excerpt":"Event Streams 10.0.0 and later versions are built on Strimzi. Strimzi 0.14.0 and later support distributed tracing based on the open-source OpenTracing and Jaeger projects. Distributed tracing provides a way to understand how applications work across processes, services, or networks. Tracing uses the trail they leave behind based on requests made, carrying tracing information in the requests as they pass from system to system. Through tracing, you can monitor applications that work with Event Streams, helping you to understand the shape of the generated traffic, and pinpoint the causes of any potential problems. To use distributed tracing for your Kafka client applications, you add code to your applications that gets called on the client’s send and receive operations. This code adds headers to the client’s messages to carry tracing information. As such, Event Streams provides support for tracing through IBM Cloud Pak for Integration. Your applications can send tracing data into the IBM Cloud Pak for Integration Operations Dashboard runtime as “external applications”. Deployment architecture To send tracing data to the IBM Cloud Pak for Integration Operations Dashboard, the Kafka client application must be deployed into the same OpenShift Container Platform cluster as IBM Cloud Pak for Integration. The application runs in a pod into which two sidecar containers are added, one for the tracing agent and one for the tracing collector. The Kafka client OpenTracing code runs as part of your application. It forwards tracing spans over UDP to the agent. The agent decides which spans are to be sampled and forwards those over TCP to the collector. The collector forwards the data securely to the Operations Dashboard Store in the Operations Dashboard namespace.  The namespace in which the application runs must be registered with the Operations Dashboard. The registration process creates a secret which is used by the collector to communicate securely with the Operations Dashboard Store. Preparing your application to use tracing For detailed instructions about how to use Operations Dashboard with external applications, see the IBM Cloud Pak for Integration documentation. At a high level, the steps required are as follows. Step 1 - Configure the Operations Dashboard to display external applications tracing data To enable the Operations Dashboard to display tracing data from external applications:   Log into IBM Cloud Pak for Integration.  Open the Operations Dashboard Web Console by clicking Tracing in the Platform Navigator.  Navigate to System Parameters &gt; Display in the Manage section of the console.  In the Display settings, set Show external app data in the dashboards to true, and click Update.Now the Operations Dashboard is ready to receive data from external applications. Step 2 - Modify your application code to enable tracing The most convenient way to enable tracing in a Kafka application is to use the Kafka client integration which has been contributed to the OpenTracing project. Then you have a choice of configuring OpenTracing interceptors and using the regular KafkaProducer and KafkaConsumer classes, or using Tracing variants of the KafkaProducer and KafkaConsumer which wrap the real classes. The latter is more flexible but requires additional code changes to the application. There are sample applications which show you how to do this. You can clone the repository and build them yourself, or copy the techniques for your own applications. Step 3 - Deploy your application with the agent and collector sidecar containers After it is built, you can deploy your application, complete with the Operations Dashboard agent and collector containers. Without these additional containers, your application will not be able to communicate with the Operations Dashboard Store and your tracing data will not appear. The sample applications include an example of the Kubernetes deployment that includes these containers. Slightly unusually, when you deploy your application, you’ll notice that it’s running normally but the two sidecar container are not starting successfully. That’s because they depend upon a Kubernetes secret that contains credentials to connect to the OD Store. The next step creates that secret and enables the containers to start. Step 4 - Complete the registration process for the application The final stage is to use the Operations Dashboard Web Console for registering the external application to Operations Dashboard. To enable the Operations Dashboard to display tracing data from external applications:   Log into IBM Cloud Pak for Integration.  Open the Operations Dashboard Web Console by clicking Tracing in the Platform Navigator.  Navigate to System Parameters &gt; Registration requests in the Manage section of the console.  In the list of registration requests, approve the request for your application’s namespace.  Copy the command displayed by the console, and run it to create the Kubernetes secret which enables your application to send data to the Operations Dashboard Store.Using the Operations Dashboard to view tracing information You can use the IBM Cloud Pak for Integration Operations Dashboard to view tracing information and analyze spans. For more information about setting up, using, and managing the dashboard, see the IBM Cloud Pak for Integration documentation. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/tracing/",
        "teaser":null},{
        "title": "Monitoring Kafka consumer group lag",
        "collection": "10.1",
        "excerpt":"You can monitor the consumer lag for Kafka clients connecting to IBM Event Streams. This information is available through both the Event Streams UI and CLI. Consumer lag Each partition will have a consumer within a consumer group with information relating to its consumption as follows:   Client ID and Consumer ID: each partition will have exactly one consumer in the consumer group, identifiable by a Client ID and Consumer ID.  Current offset: the last committed offset of the consumer.  Log end offset: the highest offset of the partition.  Offset lag: the difference between the current consumer offset and the highest offset, which shows how far behind the consumer is.Using the Event Streams UI To access the consumer groups side panel in the Event Streams UI, do the following:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.  Locate your topic using the Name column and click the row for the topic.  Click the Consumer groups tab.  The Consumer groups dashboard will display all consumer groups for the selected topic.Click the consumer group of interest from the Consumer group ID column.The consumer group side panel should now be displayed on screen.This side panel will display a table containing consumer group information for each partition of the topic. Using the Event Streams CLI To access information about a consumer group in the Event Streams CLI, do the following:   Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  To list all consumer groups on a cluster, run:cloudctl es groups  To list information about a consumer group, run:cloudctl es group --group &lt;consumer-group-id&gt;where &lt;consumer-group-id&gt; is the name of the consumer group of interest.The CLI will print a table containing consumer group information for each partition of the topic. The following example shows the information the command returns for a consumer group called my-consumer: $ cloudctl es group --group my-consumer&gt;Details for consumer group my-consumerGroup ID            Statemy-consumer         StableTopic      Partition   Current offset   End offset   Offset lag   Client        Consumer        Hostmy-topic   2           999              1001         2            some-client   some-consumer   some-hostmy-topic   0           1000             1001         1            some-client   some-consumer   some-hostmy-topic   1           1000             1000         0            some-client   some-consumer   some-hostOKTo view other Kafka-related metrics, consider configuring a Kafka Exporter. For information on how to monitor the general health of a particular topic, see monitoring topic health. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/consumer-lag/",
        "teaser":null},{
        "title": "Monitoring with external tools",
        "collection": "10.1",
        "excerpt":"You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. You must configure your installation to set up access for external monitoring tools. For examples about setting up monitoring with external tools such as Datadog, Prometheus, and Splunk, see the tutorials page. If you have a tool or service you want to use to monitor your clusters, you can contact support. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/external-monitoring/",
        "teaser":null},{
        "title": "Modifying installation settings",
        "collection": "10.1",
        "excerpt":"You can modify the configuration settings for your existing Event Streams installation by using the OpenShift Container Platform web console or the oc command line tool. The configuration changes are applied by updating the EventStreams custom resourceYou can modify existing values and introduce new properties as outlined under configuration settings.Note: Some settings might cause affected components of your Event Streams instance to restart. For examples on changes you might want to make, see scaling your Event Streams instance. Using the OpenShift Container Platform web console To modify configuration settings by using the OpenShift Container Platform web console:   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Event Streams tab and search the Name column for the installed instance and click it.  Click the YAML tab to edit the custom resource.  Make the required changes on the page, or you can click Download and make the required changes in a text editor.If you clicked Download you will need to drag and drop the modified custom resource file onto the page so that it updates in the web console.  Click the Save button to apply your changes.Using the OpenShift Container Platform CLI To modify configuration settings by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to edit your EventStreams custom resource in your default editor:oc edit eventstreams &lt;instance_name&gt;  Make the required changes in your editor.  Save and quit the editor to apply your changes.Modifying Kafka broker configuration settings Kafka supports a number of key/value pair settings for broker configuration, typically provided in a properties file. In Event Streams, these settings are defined in an EventStreams custom resource under the spec.strimziOverrides.kafka.config property. For example, to set the number of I/O threads to 24 you can add the spec.strimziOverrides.kafka.config[\"num.io.threads\"] property: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreamsmetadata:  name: example-broker-config  namespace: myprojectspec:  # ...  strimziOverrides:    kafka:      # ...      config:         # ...         num.io.threads: 24You can specify all the broker configuration options supported by Kafka except from those managed directly by Event Streams. For further information, see the list of supported configuration options. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/modifying-installation/",
        "teaser":null},{
        "title": "Optimizing Kafka cluster with Cruise Control",
        "collection": "10.1",
        "excerpt":"Overview Cruise Control is an open-source system for optimizing your Kafka cluster by monitoring cluster workload, rebalancing a cluster based on predefined constraints, and detecting and fixing anomalies. You can set up Event Streams to use the following Cruise Control features:   Generating optimization proposals from multiple optimization goals.  Rebalancing a Kafka cluster based on an optimization proposal.Note: Event Streams does not support other Cruise Control features. Cruise Control can be used to dynamically optimize the distribution of the partitions on your brokers so that resources are used more efficiently. Cruise Control reduces the time and effort involved in running an efficient and balanced Kafka cluster. A typical cluster can become unevenly loaded over time. Partitions that handle large amounts of message traffic might be unevenly distributed across the available brokers. To rebalance the cluster, administrators must monitor the load on brokers and manually reassign busy partitions to brokers with spare capacity. Cruise Control automates the cluster rebalancing process. It constructs a workload model of resource utilization for the cluster based on CPU, disk, and network load, ​and generates optimization proposals (that you can approve or reject) for more balanced partition assignments. A set of configurable optimization goals is used to calculate these proposals. When you approve an optimization proposal, Cruise Control applies it to your Kafka cluster. When the cluster rebalancing operation is complete, the broker pods are used more effectively and the Kafka cluster is more evenly balanced. Steps for rebalancing a cluster Follow these steps to rebalance a cluster by using Cruise Control:   Ensure you have an Event Streams installation that has Cruise Control enabled in the EventStreams custom resource.      In your EventStreams custom resource, configure the optimization goals that are available for rebalancing, and set capacity limits for broker resources. You can use Cruise Control defaults or define your own goals and capacity limits, as described in enabling and configuring Cruise Control.     Important: The configuration settings in the EventStreams custom resource are used by all optimization proposals defined in the KafkaRebalance custom resource, and can constrain the possible proposals a user can make.         Set up the optimization proposal by using the KafkaRebalance custom resource.     Important: Deploying Cruise Control allows for rebalancing a cluster based on the predefined constraints defined in the KafkaRebalance custom resource. If Cruise Control is not enabled, the KafkaRebalance custom resource has no effect on a cluster, and similarly, without a KafkaRebalance custom resource Cruise Control will make no changes to a cluster even if it is enabled.     Wait for an optimization proposal to appear in the status of the KafkaRebalance custom resource.  Accept the optimization proposal in the KafkaRebalance custom resource by annotating it.  Check for the KafkaRebalance proposal to be finished processed.  Depending on the outcome of the KafkaRebalance proposal, perform the following steps:          If status.condtions[0].status = Ready: Cruise Control has successfully optimized the Kafka cluster and no further action is needed.      If status.condtions[0].status = NotReady: Cruise Control was unable to optimize the Kafka cluster and you need to address the error      Setting up optimization To rebalance a Kafka cluster, Cruise Control uses the available optimization goals to generate optimization proposals, which you can approve or reject. Creating an optimization proposal To define the goals to use when rebalancing the cluster, use the KafkaRebalance custom resource. The goals defined in the custom resource determine how the cluster will be optimized during the calculation of the KafkaRebalance proposal. Important: The hard goals that are configured in spec.strimziOverrides.config[\"hard.goals\"] in the EventStreams custom resource must be satisfied, whereas the goals defined in the KafkaRebalance custom resource will be optimized if possible, but not at the cost of violating any of the hard goals. To create a KafkaRebalance custom resource that creates a proposal that does not consider the hard goals set in spec.strimziOverrides.cruiseControl.config[\"hard.goals\"] in the EventStreams custom resource, set spec.skipHardGoalsCheck to true in the KafkaRebalance custom resource. Hard goals include settings that must be met by an optimization proposal and cannot be ignored. Other goals in the KafkaRebalance custom resource are optimized only if possible. To create a KafkaRebalance custom resource for your Event Streams instance, add the eventstreams.ibm.com/cluster=&lt;instance-name&gt; label to metadata.labels in your KafkaRebalance custom resource as follows, where &lt;instance-name&gt; is the name of your Event Streams cluster. # ...metadata:  # ...  labels:    eventstreams.ibm.com/cluster: &lt;instance-name&gt;spec:  # ...  goals:    - NetworkInboundCapacityGoal    - NetworkOutboundCapacityGoal  skipHardGoalCheck: trueTo add a list of goals to your KafkaRebalance custom resource, add the subset of defined goals to the spec.goals property. The previous example has the NetworkInboundCapacityGoal and NetworkOutboundCapacityGoal goals added. To configure a KafkaRebalance custom resource, use the OpenShift Container Platform web console or the OpenShift Container Platform CLI as follows. Using the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  From the navigation menu, click Operators &gt; Installed Operators.  In the Projects dropdown list, select the project that contains the IBM Event Streams instance.  Select the IBM Event Streams Operator in the list of Installed Operators.  In the Operator Details &gt; Overview page, find the KafkaRebalance tile in the list of Provided APIs and click Create Instance.  In the Create KafkaRebalance page, edit the provided YAML to set values for the following properties.          In the metadata.labels section, set the eventstreams.ibm.com/cluster property value to the name of your IBM Event Streams instance that you want to rebalance.      Set the metadata.name property value to what you want to call the KafkaRebalance custom resource.      Set the spec.goals to provide a list of goals you want to optimize.      Set the spec.skipHardGoalCheck to provide true if you want to skip the hard goals.        See the previous YAML snippet as an example.     Click Create.  The new KafkaRebalance custom resource is listed in the Operator Details &gt; KafkaRebalance page.Using the OpenShift Container Platform CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing Event Streams cluster:oc project &lt;project-name&gt;      Define a KafkaRebalance custom resource in a file. For example, the following YAML defines a KafkaRebalance custom resource that will rebalance the Kafka cluster associated with the IBM Event Streams instance named my-cluster and will create a proposal to satisfy the NetworkInboundCapacityGoal goal.     apiVersion: eventstreams.ibm.com/v1alpha1kind: KafkaRebalancemetadata:  labels:    eventstreams.ibm.com/cluster: my-cluster  name: my-test-kafka-rebalancespec:  goals:    - NetworkInboundCapacityGoal        Note: The KafkaRebalance must have a label under metadata.labels with the key eventstreams.ibm.com/cluster, and the value must be set to the name of the IBM Event Streams instance that you are rebalancing.     Run the following command to create the KafkaRebalance custom resource:oc create -f &lt;path-to-your-KafkaRebalance-file&gt;  Verify the KafkaRebalance custom resource has been created by running:oc get kafkarebalancesEnsure the KafkaRebalance custom resource you are trying to create is listed.  To view the KafkaRebalance proposal by running:oc get KafkaRebalance &lt;KafkaRebalance-instance-name&gt; -o yamlReceiving an optimization proposal After you have created the KafkaRebalance custom resource, Cruise Control creates an optimization proposal if it can, and will add the ProposalReady status to the status.conditions property and display an overview of the proposal in the status.optimizationResult field. The following is an example of a successful proposal: # ...status:  # ...  conditions:    - lastTransitionTime: '2020-07-07T08:36:09.193Z'      status: ProposalReady      type: State  observedGeneration: 5  optimizationResult:    intraBrokerDataToMoveMB: 0    onDemandBalancednessScoreBefore: 0    recentWindows: 1    dataToMoveMB: 3    excludedTopics: []    excludedBrokersForReplicaMove: []    numReplicaMovements: 7    onDemandBalancednessScoreAfter: 100    numLeaderMovements: 0    monitoredPartitionsPercentage: 100    numIntraBrokerReplicaMovements: 0    excludedBrokersForLeadership: []  sessionId: 03974f67-b208-4133-9f54-305d268a1a22For more information about optimization proposals, see the Strimzi documentation. Refreshing an optimization proposal A KafkaRebalance custom resource does not automatically refresh the optimization proposal, it requires a manual refresh to be triggered. This is to ensure that a proposal does not change without your permission, and also to ensure the proposal does not change before it is approved. To refresh the optimization proposal, you will need to add eventstreams.ibm.com/rebalance: refresh to the KafkaRebalance custom resource as described in adding an annotation. If a valid optimization exists for the configuration and the goals specified, then the KafkaRebalance custom resource will get updated, and status.optimizationResult shows the updated proposal. Approving an optimization proposal Cruise Control is used for generating and implementing Kafka rebalance proposals. The KafkaRebalance custom resource is the way a user interacts with Cruise Control via the operator. The operator has automated the process of generating a proposal request, but to approve the proposal a user needs to annotate their KafkaRebalance custom resource. If the KafkaRebalance custom resource contains a rebalance proposal that you want to accept, then you will need to approve the proposal by adding the eventstreams.ibm.com/rebalance: approve annotation to your KafkaRebalance custom resource by adding an annotation When approved, the KafkaRebalance custom resource is updated to show the status Rebalancing in status.conditions. This means that Cruise Control is currently rebalancing the Kafka cluster. The amount of time required for a rebalance depends on various factors such as the amount of data on a partition and the goals being accomplished. If the rebalance is successful, then the KafkaRebalance custom resource will have the status updated to Ready in the status.conditions field. Dealing with rebalancing errors If Cruise Control is unsuccessful in rebalancing the Kafka brokers, then the KafkaRebalance custom resource will be updated to show the NotReady status in the status.conditions field, and status.conditions will contain the error and how to remediate. After you have followed the guidance to fix the error, trigger a KafkaRebalance refresh and then approve the KafkaRebalance proposal. Stopping an inflight optimization proposal If you want to stop the process of rebalancing a Kafka cluster, you can add the eventstreams.ibm.com/rebalance: stop annotation by adding an annotation. Note: Cruise Control will not restore the state of the topics and partitions that existed before rebalancing. This means the state of the Kafka brokers might not be the same as before rebalancing started. Adding annotations to a KafkaRebalance custom resource To add annotations to the KafkaRebalance custom resource, use the OpenShift Container Platform web console or the OpenShift Container Platform CLI as follows. Using the OpenShift Container Platform web console   Log in to the OpenShift Container Platform web console using your login credentials.  The new KafkaRebalance custom resource is listed in the Operator Details &gt; KafkaRebalance page.  Click the name of the KafkaRebalance custom resource you want to edit.  Click the YAML tab.  Add the eventstreams.ibm.com/rebalance annotation to the metadata.annotations field.    # ...metadata:  # ...  annotations: eventstreams.ibm.com/rebalance: &lt;annotation-value&gt;        Click Save.Using the OpenShift Container Platform CLI   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.  Run the following command to select the project that contains the existing KafkaRebalance custom resource:oc project &lt;project-name&gt;  Run the following command to see a list of all the KafkaRebalance custom resources:oc get kafkarebalances  Find the name of the KafkaRebalance custom resource you want to annotate.  Run the following command to view the YAML for your KafkaRebalance instance:oc annotate kafkarebalances &lt;rebalance-cr-name&gt; &lt;annotation-key&gt;=&lt;annotation-value&gt;Optimization goals Optimization goals determine what the KafkaRebalance proposal chooses to optimize when rebalancing the Kafka cluster. Note: The more goals you use, the harder it will be for Cruise Control to create an optimization proposal that will satisfy all of the goals. Consider creating a KafkaRebalance custom resource with fewer goals. Important: Most goals might result in the moving of partitions across brokers. This might impact performance during the operation, and cause issues with connecting clients. You can configure the following optimization goals. the goals are listed in order of decreasing priority. RackAwareGoal Ensures that all replicas of each partition are assigned in a rack-aware manner. This means no more than one replica of each partition resides on the same rack. Using it in a multi-zone environment ensures that partitions are spread across different zones. This will improve the availability of topic data. ReplicaCapacityGoal Ensures that the maximum number of replicas per broker is under the specified maximum limit. This goal does not ensure that the replicas are evenly distributed across brokers. It ensures that the total replicas on a single broker remains under a specified maximum limit. For even distribution of replicas amongst brokers, see ReplicaDistributionGoal. DiskCapacityGoal Ensures that disk space usage of each broker is below the threshold set in the spec.strimziOverrides.cruiseControl.brokerCapacity.disk property in the EventStreams custom resource. Use this goal to optimize the distribution of partitions amongst brokers based on the disk usage of a partition. This might not lead to an even distribution of partitions amongst the brokers. For even distribution of disk space usage on brokers, see DiskUsageDistributionGoal. NetworkInboundCapacityGoal Ensures that inbound network utilization of each broker is below the threshold set in the spec.strimziOverrides.cruiseControl.brokerCapacity.inboundNetwork property in the EventStreams custom resource. This is mainly affected by the number of producers producing and the size of produced messages. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the inbound network traffic. This might not result in the inbound network being evenly distributed amongst brokers, it only ensures that the inbound network of each broker is under a given threshold. For even distribution of inbound network traffic amongst brokers, see NetworkInboundDistributionUsageGoal NetworkOutboundCapacityGoal Ensures that outbound network utilization of each broker is below a given threshold set in the spec.strimziOverrides.cruiseControl.brokerCapacity.inboundNetwork property in the EventStreams custom resource. This is mainly affected by the number of consumers consuming and communication between the Kafka Brokers. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the outbound network traffic. This might not result in the outbound network being evenly distributed amongst brokers, it only ensures that the outbound network of each broker is under a given threshold. For even distribution of outbound network traffic amongst brokers, see NetworkOutboundDistributionUsageGoal. ReplicaDistributionGoal Attempts to make all the brokers in a cluster have a similar number of replicas. Use this goal to ensure that the total number of partition replicas are distributed evenly amongst brokers. This does not mean that partitions will be optimally distributed for other metrics such as network traffic and disk usage, so it might still cause brokers to be unevenly loaded. PotentialNwOutGoal Ensures that the potential network output (when all the replicas in the broker become leaders) on each of the broker do not exceed the broker’s network outbound bandwidth capacity. This is mainly affected by the number of consumers consuming and the size of the consumed messages. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the potential outbound network traffic. This will ensure that the partitions with the most outbound traffic are moved to the most idle brokers. The difference between this goal and NetworkOutboundCapacityGoal is that this goal will rebalance the cluster based on the worst-case network outbound usage around the outbound network capacity specified, and the NetworkOutboundCapacityGoal will rebalance the outbound network usage around the outbound network capacity specified. DiskUsageDistributionGoal Attempts to keep the disk space usage variance among brokers within a certain range relative to the average disk utilization. Use this goal to optimize the distribution of partitions amongst brokers to ensure that the disk utilization amongst brokers are similar. NetworkInboundUsageDistributionGoal Attempts to keep the inbound network utilization variance among brokers within a certain range relative to the average inbound network utilization. This is mainly affected by the number of producers producing and the size of produced messages. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the inbound network traffic. This will ensure that the partitions with the most inbound traffic are moved to the most idle brokers and the inbound network traffic amongst brokers are similar. NetworkOutboundUsageDistributionGoal Attempts to keep the outbound network utilization variance among brokers within a certain range relative to the average outbound network utilization. This is mainly affected by the number of consumers consuming and the size of the consumed messages. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the outbound network traffic. This will ensure that the partitions with the most outbound traffic are moved to the most idle brokers and the outbound network traffic amongst brokers are similar. CpuUsageDistributionGoal Attempts to keep the CPU usage variance among brokers within a certain range relative to the average CPU utilization. Use this goal to rebalance the partitions to optimize the distribution of the partitions on the CPU usage of brokers. This will ensure that the CPU usage of brokers are similar. LeaderReplicaDistributionGoal Attempts to make all the brokers in a cluster have a similar number of leader replicas. Use this goal to ensure the total number of partitions leaders are similar amongst brokers. LeaderBytesInDistributionGoal Attempts to equalize the leader bytes in rate on each host. This goal may not result in an even distribution of replicas across brokers as it is optimizing the leader bytes on each broker. TopicReplicaDistributionGoal Attempts to maintain an even distribution of any topic’s partitions across the entire cluster. Use this goal to ensure that the total number of partition replicas are distributed evenly amongst brokers. This does not mean that partitions will be optimally distributed for other metrics such as network traffic and disk usage, so might cause brokers to be unevenly loaded. This might also cause a partition on a topic to be present on a subset of the total brokers. PreferredLeaderElectionGoal Simply moves the leaders to the first replica of each partition. Cruise Control does not try to optimize the distribution of replicas. Before using this goal, check the state of the partitions to see where the leaders will end up to ensure the required outcome. IntraBrokerDiskCapacityGoal Ensures that disk space usage of each disk is below a given threshold. Use this goal to rebalance the distribution of the partitions on disk space usage under a given threshold. This may not result in the disk space usage being evenly distributed amongst brokers, it only ensures that the disk space usage of each broker is under a given threshold. For even distribution of disk space usage amongst brokers, see IntraBrokerDiskUsageDistributionGoal. IntraBrokerDiskUsageDistributionGoal Attempts to keep the disk space usage variance among disks within a certain range relative to the average broker disk utilization. Use this goal to rebalance the distribution of the partitions on disk space usage. This goal will ensure that the disk space usage amongst brokers are similar. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/cruise-control/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "10.1",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. The pre-requisite guidance gives various examples of different production configurations on which you can base your deployment. To verify it meets your requirements, you should test the system with a workload that is representative of the expected throughput. For this purpose, IBM Event Streams provides a workload generator application to test different message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Modifying the settings These settings are defined in the EventStreams custom resource under the spec.strimziOverrides property. For more information on modifying these settings see modifying installation Increase the number of Kafka brokers in the cluster The number of Kafka brokers is defined in the EventStreams custom resource in the spec.strimziOverrides.kafka.replicas property. For example to configure Event Streams to use 6 Kafka brokers: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      replicas: 6Increase the CPU request or limit settings for the Kafka brokers The CPU settings for the Kafka brokers are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.kafka.resources. For example to configure Event Streams Kafka brokers to have a CPU request set to 2 CPUs and limit set to 4 CPUs: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      resources:        requests:          cpu: 2000m        limits:          cpu: 4000mA description of the syntax for these values can be found in the Kubernetes documentation. Increase the memory request or limit settings for the Kafka brokers and ZooKeeper nodes The memory settings for the Kafka brokers are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.kafka.resources. The memory settings for the ZooKeeper nodes are defined in the EventStreams custom resource in the requests and limits properties under spec.strimziOverrides.zookeeper.resources. For example to configure Event Streams Kafka brokers and ZooKeeper nodes to have a memory request set to 4GB and limit set to 8GB: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      resources:        requests:          memory: 4096Mi        limits:          memory: 8096Mi    zookeeper:      # ...      resources:        requests:          memory: 4096Mi        limits:          memory: 8096MiThe syntax for these values can be found in the Kubernetes documentation. Modifying the resources available to supporting components The resource settings for each supporting component are defined in the EventStreams custom resource in their corresponding component key the requests and limits properties under spec.&lt;component&gt;.resources.For example, to configure the Apicurio Registry to have a memory request set to 4GB and limit set to 8GB: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  apicurioRegistry:    # ...    resources:      requests:        memory: 4096Mi      limits:        memory: 8096MiThe syntax for these values can be found in the Kubernetes documentation. Modifying the JVM settings for Kafka brokers If you have specific requirements, you can modify the JVM settings for the Kafka brokers. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. Note: Only a selected subset of the available JVM options can be configured. JVM settings for the Kafka brokers are defined in the EventStreams custom resource in the spec.strimziOverrides.kafka.jvmOptions propety. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      jvmOptions:        -Xms: 4096m        -Xmx: 4096mIncrease the disk space available to each Kafka broker The Kafka brokers need sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods for messages, increased message sizes and additional topic partitions. The amount of storage made available to Kafka brokers is defined at the time of installation in the EventStreams custom resource in the spec.strimziOverrides.kafka.storage.size property. For example: apiVersion: eventstreams.ibm.com/v1beta1kind: EventStreams# ...spec:  # ...  strimziOverrides:    # ...    kafka:      # ...      storage:        # ...        size: 100Gi","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "10.1",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For more information about quotas, see the Kafka documentation. Setting quotas To configure Kafka quotas, either update an existing KafkaUser, or manually create a new KafkaUser by using the OpenShift Container Platform web console or CLI, ensuring that the configuration has a quotas section with the relevant quotas defined. For example: spec:   #...   quotas:      producerByteRate: 1048576      consumerByteRate: 2097152      requestPercentage: 55Decide what you want to limit by defining one or more of the following quota types:             Property      Type      Description                  producerByteRate      integer      This quota limits the number of bytes that a producer application is allowed to send per second.              consumerByteRate      integer      This quota limits the number of bytes that a consumer application is allowed to receive per second.              requestPercentage      integer      This quota limits all clients based on thread utilisation.      Note: Quotas can only be applied to individual KafkaUser instances. It is advised to apply a quota to an existing KafkaUser, as described in the following sections. You can create a Kafkauser beforehand. Using the OpenShift Container Platform web console To update an existing KafkaUser by using the OpenShift Container Platform web console:   Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Kafka User tab, then select the KafkaUser instance you want to update from the list of existing users.  Expand the Actions dropdown, and select the Edit KafkaUser option.  In the YAML editor, add a quotas section with the required quotas.Using the OpenShift Container Platform CLI To update an existing KafkaUser by using the OpenShift Container Platform CLI:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run one of the following commands to add or update one or more quota types:     Setting producerByteRate:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"producerByteRate\\\":$QUOTA}}}\" \\  --type=merge        Setting consumerByteRate:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"consumerByteRate\\\":$QUOTA}}}\" \\  --type=merge        Setting requestPercentage:       QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"requestPercentage\\\":$QUOTA}}}\" \\  --type=merge        Setting all quotas:       PRODUCER_QUOTA=&lt;integer value&gt;; \\  CONSUMER_QUOTA=&lt;integer value&gt;; \\  PERCENTAGE_QUOTA=&lt;integer value&gt;; \\  oc patch kafkauser --namespace &lt;namespace&gt; &lt;kafka-user-name&gt; \\  -p \"{\\\"spec\\\":{\\\"quotas\\\": \\  {\\\"producerByteRate\\\":$PRODUCER_QUOTA, \\  \\\"consumerByteRate\\\":$CONSUMER_QUOTA, \\  \\\"requestPercentage\\\":$PERCENTAGE_QUOTA}}}\" \\  --type=merge      ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/quotas/",
        "teaser":null},{
        "title": "Managing a multizone setup",
        "collection": "10.1",
        "excerpt":"If you have set up your Event Streams installation to use multiple availability zones, follow the guidance here if one of your nodes containing Kafka or ZooKeeper experiences problems. Topic configuration Only create Kafka topics where the minimum in-sync replicas configuration can be met in the event of a zone failure. This requires considering the minimum in-sync replicas value in relation to the replication factor set for the topic, and the number of availability zones being used for spreading out your Kafka brokers. For example, if you have 3 availability zones and 6 Kafka brokers, losing a zone means the loss of 2 brokers. In the event of such a zone failure, the following topic configurations will guarantee that you can continue to produce to and consume from your topics:   If the replication factor is set to 6, then the suggested minimum in-sync replica is 4.  If the replication factor is set to 5, then the suggested minimum in-sync replica is 3.Updating an existing installation If you are updating an existing installation, for example, adding a new Kafka broker node or replacing a failing node that contains a Kafka broker, you can label another node with the zone label. When the new node label is applied, Kubernetes will then be able to schedule a Kafka broker to run on that node. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/managing-multizone/",
        "teaser":null},{
        "title": "Stopping and starting Event Streams",
        "collection": "10.1",
        "excerpt":"You can stop or shut down your Event Streams instance if required.You might want to do this in cases of hardware maintenance or during scheduled power outages. Use the following instructions to gracefully shut down your Event Streams instance. The instance can be started again following the starting up Event Streams instructions. Stopping Event Streams To shut down your cluster gracefully, uninstall the operator to ensure your Event Streams instance is no longer managed and scale all components to 0 replicas as follows: Stop the operator   Log in to the OpenShift Container Platform web console using your login credentials.  Expand the Operators dropdown and select Installed Operators to open the Installed Operators page.  Expand the Project dropdown and select the project the instance is installed in. Click the operator called IBM Event Streams managing the project.  Select the Actions dropdown and click Uninstall Operator.Scale down components After the operator has uninstalled, you can safely scale down all components to 0 replicas, ensuring no pods are running. To do this for all components of type Deployment, run: kubectl get deployments -n &lt;namespace&gt; -l app.kubernetes.io/instance=&lt;instance-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; deployment.txt &amp;&amp; while read -ra deploy; do kubectl -n &lt;namespace&gt; scale --replicas=0 deployment/${deploy}; done &lt; deployment.txtWhere &lt;namespace&gt; is the namespace your Event Streams instance is installed in and &lt;instance-name&gt; is the name of your Event Streams instance. Note: This saves the state of your deployments to a file called deployment.txt in the current working directory. To do this for all components of type StatefulSet, run: kubectl get sts -n &lt;namespace&gt; -l app.kubernetes.io/instance=&lt;instance-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; sts.txt &amp;&amp; while read -ra sts; do kubectl -n &lt;namespace&gt; scale --replicas=0 sts/${sts}; done &lt; sts.txtWhere &lt;namespace&gt; is the namespace your Event Streams instance is installed in and &lt;instance-name&gt; is the name of your Event Streams instance. Note: This saves the state of your stateful sets to a file called sts.txt in the current working directory. Starting up Event Streams To scale the Event Streams instance back up to the original state, install the Event Streams operator again following the steps in the installing section.   Install the operator, ensuring it is configured with the same installation mode as used for the previous installation.  Wait for the operator to scale all resources back up to their original state. You can watch the progress by running:oc get pods --watch","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/administering/stopping-starting/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "10.1",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/intro/",
        "teaser":null},{
        "title": "Gathering logs",
        "collection": "10.1",
        "excerpt":"To help IBM Support troubleshoot any issues with your IBM Event Streams instance, use the oc adm must-gather command to capture the must gather logs. The logs are stored in a folder in the current working directory. To gather diagnostic logs, run the following commands as Cluster Administrator:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to capture the logs:     oc adm must-gather --image=ibmcom/ibm-eventstreams-must-gather -- gather -m eventstreams -n &lt;namespace&gt;   To gather system level diagnostics in addition to the Event Streams information:   Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Run the following command to capture the logs:     oc adm must-gather --image=ibmcom/ibm-eventstreams-must-gather -- gather -m system,eventstreams -n &lt;namespace&gt;   These logs are stored in an archive file in a folder in the current working directory.For example, the must-gather archive could be located on the path: must-gather.local.6409880158745169979/docker-io-ibmcom-ibm-eventstreams-must-gather-sha256-23a438c8f46037a72b5378b29decad00e871d11f19834ba75b149326847e7c05/cloudpak-must-gather-20200624100001.tgz","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/gathering-logs/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "10.1",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible symptoms and causes. Insufficient system resources You can specify the memory and CPU requirements when the IBM Event Streams instance is installed using the EventStreams custom resource. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims - occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory - occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU - occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). Ensure that resource requests and limits do not exceed the total memory available. For example, if a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which might reside on the same node. To correct these issues, increase the amount of system resources available or re-install the IBM Event Streams instance with lower resource requirements. Problems with secrets Before installing the operator, configure secrets with your entitlement key for the IBM Container software library. This will enable container images to be pulled from the registry. See the installation section of the documentation for more information. If you do not prepare the required secrets, all pods will fail to start with ImagePullBackOff errors. In this case, configure the required secrets and allow the pod to restart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "10.1",
        "excerpt":"Symptoms After providing a list of topic names when creating a geo-replicator, only the first topic successfully replicates data. The additional topics specified are either not displayed in the destination cluster UI Topics view, or are displayed as &lt;origin-release&gt;.&lt;topic-name&gt; but are not enabled for geo-replication. Causes When using the CLI to set up replication, the list of topics to geo-replicate included spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, if you specified the topics parameter with spaces such as: --topics MyTopicName1, MyTopicName2, MyTopicName3--topics \"MyTopicName1, MyTopicName2, MyTopicName3\"Remove the spaces between the topic names and re-apply the command using a topics parameter with no spaces such as: --topics MyTopicName1,MyTopicName2,MyTopicName3","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "10.1",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutException Causes This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content, adding by uncommenting either the SCRAM or Mutual TLS authentication settings depending on how the external listener has been configured. bootstrap.servers=&lt;kafka_bootstrap_route_url&gt;# SCRAM Properties#security.protocol=SASL_SSL#sasl.mechanism=SCRAM-SHA-512#sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";# Mutual auth properties#security.protocol=SSL#ssl.keystore.location=&lt;java_keystore_file_location&gt;#ssl.keystore.password=&lt;java_keystore_password&gt;# TLS Propertiesssl.protocol=TLSv1.2ssl.truststore.location=&lt;java_truststore_file_location&gt;ssl.truststore.password=&lt;java_truststore_password&gt;Replace the &lt;kafka_bootstrap_route_url&gt; with the address of the Kafka bootstrap route. If you used SCRAM authentication for the external listener, replace &lt;username&gt; with the SCRAM user name and &lt;password&gt; with the SCRAM user’s password. If you used Mutual TLS authentication for the external listener, replace &lt;java_keystore_file_location&gt; with the location of a key store containing the client certificate and &lt;java_keystore_password&gt; with the password for the key store. Finally, replace &lt;java_truststore_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), and &lt;java_truststore_password&gt; with the password for the trust store. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "10.1",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content, adding by uncommenting either the SCRAM or Mutual TLS authentication settings depending on how the external listener has been configured. bootstrap.servers=&lt;kafka_bootstrap_route_url&gt;# SCRAM Properties#security.protocol=SASL_SSL#sasl.mechanism=SCRAM-SHA-512#sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"&lt;username&gt;\" password=\"&lt;password&gt;\";# Mutual auth properties#security.protocol=SSL#ssl.keystore.location=&lt;java_keystore_file_location&gt;#ssl.keystore.password=&lt;java_keystore_password&gt;# TLS Propertiesssl.protocol=TLSv1.2ssl.truststore.location=&lt;java_truststore_file_location&gt;ssl.truststore.password=&lt;java_truststore_password&gt;Replace the &lt;kafka_bootstrap_route_url&gt; with the address of the Kafka bootstrap route. If you used SCRAM authentication for the external listener, replace &lt;username&gt; with the SCRAM user name and &lt;password&gt; with the SCRAM user’s password. If you used Mutual TLS authentication for the external listener, replace &lt;java_keystore_file_location&gt; with the location of a key store containing the client certificate and &lt;java_keystore_password&gt; with the password for the key store. Finally, replace &lt;java_truststore_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), and &lt;java_truststore_password&gt; with the password for the trust store. When running the kafka-console-consumer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "10.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log in to the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "10.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the cluster and initialized the command line tool. Resolving the problem Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt; Initialize the IBM Event Streams CLI as follows: cloudctl es init Re-run the failed operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "10.1",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "10.1",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Pak CLI: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "10.1",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes To access the Event Streams UI, the user must either have the Cluster Administrator role or the Administrator role and be in a team with a namespace resource added for the namespace containing the Event Streams instance. If neither of these applies, the error will be displayed. Resolving the problem Assign access to users with an Administrator role by ensuring they are in a team with access to the correct namespace. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "The UI cannot load data",
        "collection": "10.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Monitor and the Topics &gt; Producers tabs do not load, displaying the following message:  Causes The IBM Cloud Pak for Integration monitoring service might not be installed. In general, the monitoring service is installed by default during the  IBM Cloud Pak for Integration installation. However, some deployment methods do not install the service. Resolving the problem Install the IBM Cloud Pak for Integration monitoring service from the Catalog or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/problem-with-piping/",
        "teaser":null},{
        "title": "UI shows black images in Firefox",
        "collection": "10.1",
        "excerpt":"Symptoms Images in the Event Streams UI are rendered as a black-filled shape instead of the correct image as shown in the following example:  Causes Mozilla Firefox 76 and earlier versions block the inline CSS styling in SVG images. This is because they incorrectly apply the page-level Content Security Policy against the SVG images. Resolving the problem Update your Firefox version to 77 or later as the issue has been resolved in Firefox 77. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/firefox-black-images/",
        "teaser":null},{
        "title": "Event Streams not installing due to Security Context Constraint (SCC) issues",
        "collection": "10.1",
        "excerpt":"Symptoms Event Streams components report that an action is forbidden, stating that it is unable to validate against any security context constraint. This could result in symptoms such as:       Installation of the operator is pending and eventually times out.           Navigating to the Conditions section for the specific operator deployment under Workloads &gt; Deployment will display a message similar to the following example:         pods \"eventstreams-cluster-operator-55d6f4cdf7-\" is forbidden: unable to validate against any security context constraint: [spec.volumes[0]: Invalid value: \"secret\": secret volumes are not allowed to be used spec.volumes[1]: Invalid value: \"secret\": secret volumes are not allowed to be used]                          Creating an instance of Event Streams is pending and eventually times out.           Navigating to the Events tab for the specific instance stateful set under Workloads &gt; Stateful Sets displays a message similar to the following example:        create Pod quickstart-zookeeper-0 in StatefulSet quickstart-zookeeper failed error: pods \"quickstart-zookeeper-0\" is forbidden: unable to validate against any security context constraint: [spec.containers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true]                          On a running instance of Event Streams, a pod that has bounced never comes back up.           Navigating to the Conditions section for the specific instance deployment under Workloads &gt; Deployment will display a message similar to the following example:        is forbidden: unable to validate against any security context constraint: [spec.initContainers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true spec.containers[0].securityContext.readOnlyRootFilesystem: Invalid value: false: ReadOnlyRootFilesystem must be set to true]                    Causes Event Streams has been tested with the default restricted Security Context Constraint (SCC) provided by the OpenShift Container Platform. If a user or any other operator applies a custom SCC that removes permissions required by Event Streams, then this will cause issues. Resolving the problem Apply the custom Security Context Constraint (SCC) provided by Event Streams to enable permissions required by the product. To do this, edit the eventstreams-scc.yaml file to add your namespace and apply it using oc tool as follows:       Edit the eventstreams-scc.yaml and add the namespace where your Event Streams instance is installed.         Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.         Run the following command to apply the SCC:     oc apply -f &lt;custom_scc_file_path&gt;     For example: oc apply -f eventstreams-scc.yaml   ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/default-scc-issues/",
        "teaser":null},{
        "title": "Not authorized error when building Maven schema registry project",
        "collection": "10.1",
        "excerpt":"Symptoms When building a Maven project that pulls from the IBM Event Streams schema registry, the build fails with an error similar to the following message: Could not resolve dependencies for project &lt;project&gt;: Failed to collect dependencies at com.ibm.eventstreams.schemas:&lt;schema-name&gt;:jar:&lt;version&gt;: Failed to read artifact descriptor for com.ibm.eventstreams.schemas:&lt;schema-name&gt;:jar:&lt;version&gt; Could not transfer artifact com.ibm.eventstreams.schemas:&lt;schema-name&gt;:pom:&lt;version&gt; from/to eventstreams-schemas-repository (https://&lt;schema-registry-route&gt;/files/schemas): Not authorizedCauses The Maven settings.xml code snippets provided in the Event Streams UI for both SCRAM and Mutual TLS are missing the &lt;servers&gt; XML elements. In addition, the same error can also be displayed when using SCRAM credentials, and the Authorization value does not have the following format: Basic &lt;scram-token&gt; Resolving the problem Update the Maven settings.xml file to have the following format: &lt;settings&gt;  &lt;servers&gt;    &lt;server&gt;      &lt;id&gt;eventstreams-schemas-repository&lt;/id&gt;      &lt;configuration&gt;        &lt;httpHeaders&gt;          &lt;property&gt;            &lt;name&gt;Authorization&lt;/name&gt;            &lt;value&gt;Basic &lt;scram-token&gt;&lt;/value&gt;          &lt;/property&gt;        &lt;/httpHeaders&gt;      &lt;/configuration&gt;    &lt;/server&gt;  &lt;/servers&gt;  &lt;profiles&gt;    &lt;profile&gt;      # ...Also, if using SCRAM credentials, ensure that the Authorization element has the format Basic &lt;scram-token&gt;, where the &lt;scram-token&gt; value is a Base64-encoded string in the following format: &lt;scram-username&gt;:&lt;scram-password&gt; ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/schema-registry-not-authorized-error/",
        "teaser":null},{
        "title": "Client receives AuthorizationException when communicating with brokers",
        "collection": "10.1",
        "excerpt":"Symptoms When executing operations with a Java client connected to Event Streams, the client fails with an error similar to the following message: [err] [kafka-producer-network-thread | my-client-id] ERROR org.apache.kafka.clients.producer.internals.Sender - [Producer clientId=my-client-id] Aborting producer batches due to fatal error[err] org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed.Similar messages might also be displayed when using clients written in other languages such as NodeJS. Causes The KafkaUser does not have the authorization to perform one of the operations:   If there is an authorization error with a topic resource, then a TOPIC_AUTHORIZATION_FAILED (error code: 29) will be returned.  If there is an authorization error with a group resource, then a GROUP_AUTHORIZATION_FAILED (error code: 30) will be returned.  If there is an authorization error with a cluster resource, then a CLUSTER_AUTHORIZATION_FAILED (error code: 31) will be returned.  If there is an authorization error with a transactionalId resource, then a TRANSACTIONAL_ID_AUTHORIZATION_FAILED (error code: 32) will be returned.In Java, the errors are thrown in the format &lt;resource&gt;AuthorizationException. Other clients might return the error directly or translate it into an error with a similar name. Resolving the problem Ensure the KafkaUser has the required permissions as described in managing access. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/authorization-failed-exceptions/",
        "teaser":null},{
        "title": "Operator is generating constant log output",
        "collection": "10.1",
        "excerpt":"Symptoms The log file for the Event Streams operator pod shows constant looping of reconciliations when installed on Red Hat OpenShift Container Platform 4.5 or later. 2020-09-22 16:01:25 INFO  AbstractOperator:173 - [lambda$reconcile$3] Reconciliation #4636(watch) EventStreams(es/example): EventStreams example should be created or updated2020-09-22 16:01:25 INFO  OperatorWatcher:35 - [eventReceived] Reconciliation #4642(watch) EventStreams(es/example): EventStreams example in namespace es was MODIFIEDCauses The Event Streams operator is notified that an instance of Event Streams has changed and needs to be reconciled. When the reconciliation is complete, the status update triggers a notification which causes a new reconciliation. Resolving the problem Contact IBM Support to request a fix, and include issue number ES-115 in your correspondence. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/operator-keeps-reconciling/",
        "teaser":null},{
        "title": "Operator upgrade causes Kafka pod errors",
        "collection": "10.1",
        "excerpt":"Symptoms If you have an existing Event Streams 10.0.0 installation created by the Event Streams operator version 2.0.1, and you upgrade your operator to 2.1.0, Kafka pods show the error status CrashLoopBackOff, for example: NAME                                             READY   STATUS             RESTARTS   AGE&lt;instance&gt;-es-entity-operator-67fc545f67-wngkw   2/2     Running            0          7d7h&lt;instance&gt;-es-ibm-es-admapi-7d6dd5bf96-5f29k     1/1     Running            0          28h&lt;instance&gt;-es-ibm-es-metrics-df9dff954-87zwr     1/1     Running            0          28h&lt;instance&gt;-es-ibm-es-recapi-5467dc886b-8k57j     1/1     Running            0          28h&lt;instance&gt;-es-ibm-es-schema-0                    3/3     Running            0          28h&lt;instance&gt;-es-ibm-es-ui-7bfdf7c8d4-7hvnf         2/2     Running            0          28h&lt;instance&gt;-es-kafka-0                            1/2     CrashLoopBackOff   5          5m31s&lt;instance&gt;-es-kafka-1                            0/2     CrashLoopBackOff   1          2m30s&lt;instance&gt;-es-kafka-2                            2/2     Running            6          7d8h&lt;instance&gt;-es-zookeeper-0                        1/1     Running            0          33m&lt;instance&gt;-es-zookeeper-1                        1/1     Running            0          31m&lt;instance&gt;-es-zookeeper-2                        1/1     Running            0          28hThe logs for the crashed Kafka pods show an error message similar to the following: 2020-10-20 20:39:20,683 ERROR Exiting Kafka due to fatal exception (kafka.Kafka$) [main]org.apache.kafka.common.config.ConfigException: Missing required configuration\"zookeeper.connect\" which has no default value.Causes Due to a sequencing issue, some entries are missing from the Kafka ConfigMap (&lt;name&gt;-es-kafka-config) that are required at startup time. Resolving the problem Contact IBM Support to request help with fixing this issue. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/operator-upgrade-issue/",
        "teaser":null},{
        "title": "504 timeout error when viewing consumer groups in the Event Streams UI",
        "collection": "10.1",
        "excerpt":"Symptoms When viewing consumer groups in the Event Streams UI, the page displays a loading indicator while it fetches the groups. However, the groups are not displayed and a 504 timeout error is shown instead. Causes As the number of consumer groups increases it will reach a limit where the service that retrieves the groups for the UI is unable to respond before the UI considers the request to have timed out. Resolving the problem Please contact IBM Support to request a fix, and include issue number ES-135 in your correspondence. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/10.1/troubleshooting/504-ui-consumer-groups/",
        "teaser":null},{
        "title": "Home",
        "collection": "10.2",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "10.2",
        "excerpt":"IBM FHIR Server Introduction This CASE provides an Operator to manage IBM FHIR Server instances. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/about/overview/",
        "teaser":null},{
        "title": "Details",
        "collection": "10.2",
        "excerpt":"Details This CASE contains two inventory items:   ibmFhirServerOperatorSetup - Installs the IBM FHIR Server Operator.  ibmFhirServerOperator - Creates instances of IBM FHIR Server. Requires the IBM FHIR Server Operator to already be installed.","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/about/whats-new/",
        "teaser":null},{
        "title": "Installing IBM FHIR Server",
        "collection": "10.2",
        "excerpt":"  The IBM FHIR Server operator can be installed in an on-line cluster through the OpenShift CLI. (Installing via OLM, or in an air-gapped cluster, is not yet supported.)  Multiple instances of the IBM FHIR Server operator may be deployed into different namespaces, one per namespace.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/trying-out/",
        "teaser":null},{
        "title": "Prerequisites",
        "collection": "10.2",
        "excerpt":"Prerequisites Red Hat OpenShift Container Platform 4.4 or later installed on one of the following platforms:   Linux x86_64Connectivity to any of the following database systems:   IBM Db2 11.5 or later  PostgreSQL 12.1 or laterConnectivity to any of the following event streaming platforms (optional):   Kafka 1.0 or higher","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/prerequisites/",
        "teaser":null},{
        "title": "Limitations",
        "collection": "10.2",
        "excerpt":"Limitations   The Operator may be deployed into different namespaces, one per namespace.  The Operator has limited support for IBM FHIR Server configuration.  Bulk Data Access operations $export, $import, and $status are not supported.Schema upgrades require downtime: The IBM FHIR Server requires downtime to complete upgrades of the IBM FHIR Server’s relational data. During the upgrade Values tables are refreshed, updated and optimized for the workloads that the FHIR specification supports. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/planning/",
        "teaser":null},{
        "title": "Installing",
        "collection": "10.2",
        "excerpt":"The following sections provide instructions about installing IBM FHIR Server on the Red Hat OpenShift Container Platform. The instructions are based on using the OpenShift Container Platform web console and oc command line utility. When deploying in an air-gapped environment, link to airgap instructions. Overview IBM FHIR Server is an operator-based release and uses custom resources to define your IBM FHIR Server configurations. The IBM FHIR Server operator uses the custom resources to deploy and manage the entire lifecycle of your IBM FHIR Server instances. Custom resources are presented as YAML configuration documents that define instances of the IBMFHIRServer custom resource type. Installing IBM FHIR Server has two phases:   Install the IBM FHIR Server operator: this will deploy the operator that will install and manage your IBM FHIR Server instances.  Install one or more instances of IBM FHIR Server by using the operator.Before you begin   Ensure you have set up your environment , including setting up your OpenShift Container Platform.  Obtain the connection details for your OpenShift Container Platform cluster from your administrator.Create a project (namespace) Create a namespace into which the Event Streams instance will be installed by creating a project.When you create a project, a namespace with the same name is also created. Ensure you use a namespace that is dedicated to a single instance of IBM FHIR Server. A single namespace per instance also allows for finer control of user accesses. Important: Do not use any of the default or system namespaces to install an instance of Event Streams (some examples of these are: default, kube-system, kube-public, and openshift-operators). Add the IBM FHIR Server operator to the catalog Before you can install the IBM FHIR Server operator and use it to create instances of IBM FHIR Server, you must have the IBM Operator Catalog available in your cluster. If you have other IBM products installed in your cluster, then you already have the IBM Operator Catalog available, and you can continue to installing the IBM FHIR Server operator. If you are installing IBM FHIR Server as the first IBM product in your cluster, complete the following steps. To make the IBM FHIR Server operator available in the OpenShift OperatorHub catalog, create the following YAML files and apply them as  follows. To add the IBM Operator Catalog:       Create a file for the IBM Operator Catalog source with the following content, and save as IBMCatalogSource.yaml:     apiVersion: operators.coreos.com/v1alpha1kind: CatalogSourcemetadata:   name: ibm-operator-catalog   namespace: openshift-marketplacespec:   displayName: \"IBM Operator Catalog\"   publisher: IBM   sourceType: grpc   image: docker.io/ibmcom/ibm-operator-catalog   updateStrategy:     registryPoll:       interval: 45m        Log in to your Red Hat OpenShift Container Platform as a cluster administrator by using the oc CLI.      Apply the source by using the following command:     oc apply -f IBMCatalogSource.yaml   The IBM Operator Catalog source is added to the OperatorHub catalog, making the Event Streams operator available to install. Installing the IBM FHIR Server Operator See Installing in the IBM FHIR Server Operator Setup README. Creating an instance of IBM FHIR Server See Creating an instance in the IBM FHIR Server Operator README. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/installing/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "10.2",
        "excerpt":"See Define IBM FHIR Server configuration in the IBM FHIR Server Operator README. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/configuring/",
        "teaser":null},{
        "title": "Storage",
        "collection": "10.2",
        "excerpt":"  Storage for the database instance that the IBM FHIR Server connects to is outside the scope of this Operator.cts to is outside the scope of this operator","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/post-installation/",
        "teaser":null},{
        "title": "Air Gap Installation",
        "collection": "10.2",
        "excerpt":"Air Gap Installation Since air gap environments do not have access to the public internet, and therefore no access to DockerHub, the following preparation steps are necessary to make the required images accessable to the Red Hat OpenShift Container Platform cluster. If the Red Hat OpenShift Container Platform cluster has a Bastion host, ensure that the Bastion host can access:   The public internet to download the CASE and images.  The target (air gap) image registry where all the images will be mirrored to.  The Red Hat OpenShift Container Platform cluster to install the Operator on.In the absence of a Bastion host, a portable host with access to the public internet may used. By downloading the CASE and images onto the portable host, and then transporting the portable host into the air gap environment, the images can then be mirrored to the target (air gap) image registry. If using a Bastion host, refer to Using a Bastion host.If using a portable host, refer to Using a portable host. Using a Bastion host 1. Prepare the Bastion host Ensure you have the following installed on the Bastion host:   Docker CLI (docker)  IBM Cloud Pak CLI (cloudctl)  Red Hat OpenShift Container Platform CLI (oc)  Skopeo (skopeo)2. Download the CASE   Create a local directory in which to save the CASE.$ mkdir -p $HOME/offline  Save the CASE.$ cloudctl case save --case &lt;case-path&gt; --outputdir $HOME/offline  &lt;case-path&gt; is the path or URL to the CASE to save.The following output is displayed: Downloading and extracting the CASE ...- SuccessRetrieving CASE version ...- SuccessValidating the CASE ...- SuccessCreating inventory ...- SuccessFinding inventory items- SuccessResolving inventory items ...Parsing inventory items- Success  Verify the CASE (.tgz) file and images (.csv) file have been downloaded.$ ls $HOME/offlinechartsibm-fhir-server-case-&lt;version&gt;-charts.csvibm-fhir-server-case-&lt;version&gt;-images.csvibm-fhir-server-case-&lt;version&gt;.tgz  &lt;version&gt; is the CASE version.3. Log into cluster Log into the Red Hat OpenShift Container Platform cluster as a cluster administrator using the oc login command. 4. Configure target registry authentication secret For IBM FHIR Server, all images are available publicly in DockerHub, so no authentication secret for the source (public) registry is needed.   Create the authentication secret for the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry &lt;target-registry&gt; --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.  &lt;registry-user&gt; is the username for the target registry.  &lt;registry-password&gt; is the password for the target registry.The credentials are saved to $HOME/.airgap/secrets/&lt;target-registry&gt;.json 5. Mirror images to target registry   Copy the images in the CASE from the source (public) registry to the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.6. Configure cluster to access target registry   Configure a global image pull secret and ImageContentSourcePolicy resource.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --namespace openshift-marketplace \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-cluster-airgap \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.WARNING: This step may restart all cluster nodes. The cluster resources might be unavailable until the time the new pull secret is applied.   Optional: If you are using an insecure target registry, you must add the target registry to the cluster insecureRegistries list.$ oc patch image.config.openshift.io/cluster --type=merge \\    -p '{\"spec\":{\"registrySources\":{\"insecureRegistries\":[\"'&lt;target-registry&gt;'\"]}}}'  &lt;target-registry&gt; is the target registry.7. Proceed with installation Now that the air gap installation preparation steps are complete, you may continue with the IBM FHIR Server Operator installation. Using a portable host 1. Prepare the portable host Ensure you have the following installed on the portable host:   Docker CLI (docker)  IBM Cloud Pak CLI (cloudctl)  Red Hat OpenShift Container Platform CLI (oc)  Skopeo (skopeo)2. Download the CASE   Create a local directory in which to save the CASE.$ mkdir -p $HOME/offline  Save the CASE.$ cloudctl case save --case &lt;case-path&gt; --outputdir $HOME/offline  &lt;case-path&gt; is the path or URL to the CASE to save.The following output is displayed: Downloading and extracting the CASE ...- SuccessRetrieving CASE version ...- SuccessValidating the CASE ...- SuccessCreating inventory ...- SuccessFinding inventory items- SuccessResolving inventory items ...Parsing inventory items- Success3. Configure portable registry and authentication secret For IBM FHIR Server, all images are available publicly in DockerHub, so no authentication secret for the source (public) registry is needed.   Initialize the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action init-registry \\    --args \"--registry localhost --user &lt;registry-user&gt; --pass &lt;registry-password&gt; \\        --dir $HOME/offline/imageregistry\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry, which is initialized to this value.  &lt;registry-password&gt; is the password for the registry, which is initialized to this value.  Start the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action start-registry \\    --args \"--registry localhost --port 443 --user &lt;registry-user&gt; --pass &lt;registry-password&gt; \\        --dir $HOME/offline/imageregistry\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry.  &lt;registry-password&gt; is the password for the registry.  Create the authentication secret for the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry localhost:443 --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;registry-user&gt; is the username for the registry.  &lt;registry-password&gt; is the password for the registry.The credentials are saved to $HOME/.airgap/secrets/localhost:443.json 4. Mirror images to portable registry   The following step copies the images in the CASE from the source (public) registry to the portable registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--registry localhost:443 --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.5. Transport portable device Now that the images are in the portable registry, transport the portable host into the air gap environment. 6. Log into the cluster Log into the Red Hat OpenShift Container Platform cluster as a cluster administrator using the oc login command. 7. Configure target registry authentication secret   Create the authentication secret for the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-creds-airgap \\    --args \"--registry &lt;target-registry&gt; --user &lt;registry-user&gt; --pass &lt;registry-password&gt;\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.  &lt;registry-user&gt; is the username for the target registry.  &lt;registry-password&gt; is the password for the target registry.The credentials are saved to $HOME/.airgap/secrets/$TARGET_REGISTRY.json 8. Mirror images to target registry   The following step copies the images in the CASE from the portable registry to the target (air gap) registry.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --inventory ibmFhirServerOperatorSetup \\    --action mirror-images \\    --args \"--fromRegistry localhost:443 --registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.9. Configure cluster to access target registry   Configure a global image pull secret and ImageContentSourcePolicy resource.$ cloudctl case launch \\    --case $HOME/offline/&lt;case-file&gt; \\    --namespace openshift-marketplace \\    --inventory ibmFhirServerOperatorSetup \\    --action configure-cluster-airgap \\    --args \"--registry &lt;target-registry&gt; --inputDir $HOME/offline\"  &lt;case-file&gt; is the CASE file.  &lt;target-registry&gt; is the target registry.WARNING: This step may restart all cluster nodes. The cluster resources might be unavailable until the time the new pull secret is applied.   Optional: If you are using an insecure target registry, you must add the target registry to the cluster insecureRegistries list.$ oc patch image.config.openshift.io/cluster --type=merge \\    -p '{\"spec\":{\"registrySources\":{\"insecureRegistries\":[\"'&lt;target-registry&gt;'\"]}}}'  &lt;target-registry&gt; is the target registry.10. Proceed with installation Now that the air gap installation preparation steps are complete, you may continue with the IBM FHIR Server Operator installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/moving-from-oss-kafka/",
        "teaser":null},{
        "title": "Resources Required",
        "collection": "10.2",
        "excerpt":"Resources Required       Describe Minimum System Resources Required     Minimum scheduling capacity:                             Software          Memory (GB)          CPU (cores)          Disk (GB)          Nodes                                      IBM FHIR Server          6          2          N/A          2                          Total          6          2          N/A          2                      Recommended scheduling capacity:                             Software          Memory (GB)          CPU (cores)          Disk (GB)          Nodes                                      IBM FHIR Server          64          16          N/A          3                          Total          64          16          N/A          3                      Note: There is an initContainer with the IBM FHIR Server called the IBM FHIR Server Schema Tool. This tool has a small memory footprint used on initialization of a pod and is accounted for in the above capacities.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/uninstalling/",
        "teaser":null},{
        "title": "Tracking license consumption of IBM FHIR Server",
        "collection": "10.2",
        "excerpt":"Tracking license consumption of IBM FHIR Server License Service is required for monitoring and measuring license usage of IBM FHIR Server in accord with the pricing rule for containerized environments. Manual license measurements are not allowed. Deploy License Service on all clusters where IBM FHIR Server is installed. The IBM FHIR Server Operator contains an integrated service for measuring the license usage at the cluster level for license evidence purposes. Overview The integrated licensing solution collects and stores the license usage information which can be used for audit purposes and for tracking license consumption in cloud environments. The solution works in the background and does not require any configuration. Only one instance of the License Service is deployed per cluster regardless of the number of Cloud Paks and containerized products that you have installed on the cluster. Deploying License Service Deploy License Service on each cluster where IBM FHIR Server is installed. License Service can be deployed on any Kubernetes cluster. For more information about License Service, how to install and use it, see the License Service documentation. Validating if License Service is deployed on the cluster To ensure license reporting continuity for license compliance purposes make sure that License Service is successfully deployed. It is recommended to periodically verify whether it is active. To validate whether License Service is deployed and running on the cluster, you can, for example, log into the Red Hat OpenShift Container Platform cluster and run the following command: $ oc get pods --all-namespaces | grep ibm-licensing | grep -v operatorThe following response is a confirmation of successful deployment: 1/1     RunningArchiving license usage data Remember to archive the license usage evidence before you decommission the cluster where IBM FHIR Server was deployed. Retrieve the audit snapshot for the period when IBM FHIR Server was on the cluster and store it in case of audit. For more information about the licensing solution, see License Service documentation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/installing/upgrading/",
        "teaser":null},{
        "title": "Security",
        "collection": "10.2",
        "excerpt":"Security   The IBM FHIR Server is a stateless offering. It is the responsibility of the user to ensure that the proper security measures are established when using the server.Data in motion   All transports used to interact with IBM FHIR Server must be encrypted. TLS 1.2 is recommended.  Users are expected to use TLS when configuring their IBM FHIR Server to connect with their database instance.Data at rest   The prerequisite database must have data encryption enabled.  Each instance is responsible for Backup and Recovery of the Database and must backup solution specific configurations.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/security/managing-access/",
        "teaser":null},{
        "title": "SecurityContextConstraints Requirements",
        "collection": "10.2",
        "excerpt":"SecurityContextConstraints Requirements By default, the IBM FHIR Server Operator uses the restricted SecurityContextConstraints resource. If desired, the following custom SecurityContextConstraints resource can be applied and used instead. apiVersion: security.openshift.io/v1kind: SecurityContextConstraintsmetadata:  name: ibm-fhir-server-operator-scc  annotations:    kubernetes.io/description: ibm-fhir-server-operator-scc denies access to all      host features and requires pods to be run with a UID, and SELinux context      that are allocated to the namespace, enforces readOnlyRootFilesystem, and      drops all capabilities.allowHostDirVolumePlugin: falseallowHostIPC: falseallowHostNetwork: falseallowHostPID: falseallowHostPorts: falseallowPrivilegeEscalation: falseallowPrivilegedContainer: falseallowedCapabilities: []defaultAddCapabilities: []groups: []fsGroup:  type: MustRunAspriority: nullreadOnlyRootFilesystem: truerequiredDropCapabilities:  - ALLrunAsUser:  type: MustRunAsRangeseLinuxContext:  type: MustRunAssupplementalGroups:  type: RunAsAnyusers: []volumes:  - configMap  - downwardAPI  - emptyDir  - persistentVolumeClaim  - projected  - secretTo cause the IBM FHIR Server Operator to use the custom SecurityContextConstraints resource.       Find the ibm-fhir-server-operator-sa ServiceAccount resource in the same namespace as the Operator.         Add the following to the rules in the ClusterRole resource that the ServiceAccount resource is bound to, and apply.   - apiGroups:    - security.openshift.io  resourceNames:    - ibm-fhir-server-operator-scc  resources:    - securitycontextconstraints  verbs:    - use  The IBM FHIR Server Operator also creates custom ClusterRole, ClusterRoleBinding, Role, RoleBinding, SecurityContextConstraints, and ServiceAccount resources to ensure separation of duties.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/security/encrypting-data/",
        "teaser":null},{
        "title": "Home",
        "collection": "2018.3.1",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "2018.3.1",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka® project.  Event Streams release 2018.3.0 uses the Kafka 2.0 release and supports the use of all Kafka interfaces.  Event Streams release 2018.3.1 uses the Kafka 2.0.1 release. IBM Event Streams builds upon the IBM Cloud Private platform to deploy Apache Kafka in a resilient and manageable way. It includes a UI design aimed at application developers getting started with Apache Kafka, as well as users operating a production cluster. IBM Event Streams is available in two editions:   IBM Event Streams Community Edition is a free version intended for trials and demonstration purposes. It can be installed and used without charge.  IBM Event Streams is a paid-for version intended for enterprise use, and includes additional features such as geo-replication.IBM Event Streams features include:   Apache Kafka deployment that maximizes the spread of Kafka brokers across the nodes of the IBM Cloud Private cluster. This creates a highly-available configuration making the deployment resilient to many classes of failure with automatic restart of brokers included.  Health check information and options to resolve issues with your clusters and brokers.  Geo-replication of your topics between clusters to enable disaster recovery and scalability.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition).  UI for browsing messages to help view and filter hundreds of thousands of messages, including options to drill in and see message details from a set time.  Encrypted communication between internal components and encrypted storage by using features available in IBM Cloud Private.  Security with authentication and authorization using IBM Cloud Private.","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "2018.3.1",
        "excerpt":"Find out what is new in IBM Event Streams. Release 2018.3.1 Support for IBM Cloud Private on Linux on IBM Z In addition to Linux® 64-bit (x86_64) systems, Event Streams 2018.3.1 and later is also supported on Linux on IBM® Z systems. Support for IBM Cloud Private version 3.1.1 In addition to IBM Cloud Private 3.1.0, Event Streams 2018.3.1 and later is also supported on IBM Cloud Private 3.1.1. Kafka version upgraded to 2.0.1 Event Streams 2018.3.1 uses the Kafka 2.0.1 release. Kafka Connect sink connector for IBM MQ In addition to the Kafka Connect source connector for IBM MQ, the Kafka Connect sink connector for IBM MQ is also availble to use with IBM Event Streams 2018.3.1 and later. Support for Kafka quotas to allow clients to be throttled You can set Kafka quotas to control the broker resources used by clients. New design for sample application The UI for the starter application has been redesigned to have both the producer and the consumer on a single page. New Cluster Connection view with API Key generation and updated geo-replication flow A new UI component helps obtain connection details (including certificates and API keys), access sample code snippets, and set up geo-replication (geo-replication is not available in Community Edition). Log in to your Event Streams UI, and click Connect to this cluster on the right to access the new UI. Default resource requirements have changed See the updated tables for the Event Streams resource requirements. Documentation: Highlighting differences between versions Any difference in features or behaviour between Event Streams releases is highlighted in the documentation using the following graphics:    Applicable to IBM Event Streams 2018.3.1.   Applicable to IBM Event Streams 2018.3.0.Release 2018.3.0 First GA release. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "2018.3.1",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers. The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "2018.3.1",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the desired rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");  props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "2018.3.1",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "2018.3.1",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "2018.3.1",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "2018.3.1",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "2018.3.1",
        "excerpt":"To try out Event Streams, you can install a basic deployment of the Community Edition. IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. Note: These instructions do not include setting up persistent storage, so your data and configuration settings are not retained in the event of a restart. For more features and full IBM support, install IBM Event Streams.   If you do not have IBM Cloud Private installed already, you can download and install IBM Cloud Private-CE.  Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.The default user name is admin, and the default password is admin.  Create a namespace where you will install your Event Streams instance.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-dev and select it from the result. This is the Helm chart for the IBM Event Streams Community Edition. The README is displayed.  Click Configure.  Enter a release name, select the target namespace you created previously, and accept the terms of the license agreement.You can leave all other settings at their default values.  Click Install.  Verify your installation and log in to start using Event Streams.These steps install a basic instance of IBM Event Streams Community Edition that you can try out. You can also configure your installation to change the default settings as required, for example, to set up persistent storage. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/trying-out/",
        "teaser":null},{
        "title": "Pre-requisites",
        "collection": "2018.3.1",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. IBM Cloud Private environment             Event Streams version      Container platform      Systems                        IBM Cloud Private 3.1.1 and 3.1.2      - Linux® 64-bit (x86_64) systems - Linux on IBM® Z systems                     IBM Cloud Private 3.1.0      Linux® 64-bit (x86_64) systems                    IBM Cloud Private 3.1.0      Linux® 64-bit (x86_64) systems       has Helm chart version 1.1.0 and includes Kafka version 2.0.1.  has Helm chart version 1.0.0 and includes Kafka version 2.0. For an overview of supported component and platform versions, see the support matrix. Ensure you have the following set up for your IBM Cloud Private environment:   Install IBM Cloud Private. Note: IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage.  Install the Kubernetes command line tool, and configure access to your cluster.  Install the IBM Cloud Private Command Line Interface (CLI).  Install the Helm CLI required for your version of IBM Cloud Private, and add the IBM Cloud Private internal Helm repository called local-charts to the Helm CLI as an external repository.  For message indexing capabilities (enabled by default), ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node: sudo sysctl -w vm.max_map_count=262144echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required.Hardware requirements The Helm chart for IBM Event Streams specifies default values for the CPU and memory usage of the Apache Kafka brokers and Apache ZooKeeper servers. See the following table for memory requirements of each Helm chart component. Ensure you have sufficient physical memory to service these requirements. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other IBM Event Streams components which might be required to reside on the same node. Ensure you have one IBM Cloud Private worker node per Kafka broker, and a minimum of 3 worker nodes available for use by IBM Event Streams. Helm resource requirements The following table lists the resource requirements of the IBM Event Streams Helm chart. For details about the requirements for each pod and their containers, see the tables in the following sections.             Pod      Number of replicas      Total CPU per pod      Total memory per pod (Gi)                  Kafka      3*      1*      3.5*              ZooKeeper      3      0.1*      1*              Geo-replicator      0*      1 per replica      1 per replica              Administration UI      1      1      1              Administration server      1      4.5      2.5              Network proxy      2      unlimited      unlimited              Access controller      1      0.1      0.25              Index manager      1      unlimited      unlimited              Elasticsearch      2      unlimited      4      Important: You can configure the settings marked with an asterisk (*). Note: Before installing IBM Event Streams (not Community Edition), consider the number of Kafka replicas and geo-replicator nodes you plan to use. Each Kafka replica and geo-replicator node is a separate chargeable unit. The CPU and memory limits for some components are not limited by the chart, so they inherit the resource limits for the namespace that the chart is being installed into. If there are no resource limits set for the namespace, the containers run with unbounded CPU and memory limits. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Kafka pod             Container      CPU per container      Memory per container (Gi)                  Kafka      1*      2*              Metrics reporter      unlimited      1.5*              Metrics proxy      unlimited      unlimited              Healthcheck      unlimited      unlimited      ZooKeeper pod             Container      CPU per container      Memory per container (Gi)                  ZooKeeper      0.1*      1*      Geo-replicator pod             Container      CPU per container      Memory per container (Gi)                  Replicator      1      1              Metrics reporter      unlimited      unlimited      Administration UI pod             Container      CPU per container      Memory per container (Gi)                  UI      1      1              Redis      unlimited      unlimited              Proxy      unlimited      unlimited      Administration server pod             Container      CPU per container      Memory per container (Gi)                  Rest      4      2              Codegen      0.5      0.5              Proxy      unlimited      unlimited      Network proxy pod             Container      CPU per container      Memory per container (Gi)                  Proxy      unlimited      unlimited      Access controller pod             Container      CPU per container      Memory per container (Gi)                  Access controller      0.1      0.25              Redis      unlimited      unlimited      Index manager pod             Container      CPU per container      Memory per container (Gi)                  Index manager      unlimited      unlimited      Elasticsearch pod             Container      CPU per container      Memory per container (Gi)                  Elastic      unlimited      4      PodSecurityPolicy requirements To install the Event Streams chart, you must have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace. You can define the PodSecurityPolicy when creating the namespace for your installation. For more information about PodSecurityPolicy definitions, see the IBM Cloud Private documentation. Network requirements IBM Event Streams is supported for use with IPv4 networks only. File systems for storage If you want to set up persistent storage, you must have physical volumes available, backed by one of the following file systems:   NFS version 4  GlusterFS version 3.10.1IBM Event Streams user interface The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterClients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8  Oracle Java 8You can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning for installation",
        "collection": "2018.3.1",
        "excerpt":"Consider the following when planning your installation. IBM Event Streams Community Edition The IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. You can install the Community Edition from the catalog included with IBM Cloud Private. IBM Event Streams IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can install IBM Event Streams by downloading the image from IBM Passport Advantage, and making it available in the IBM Cloud Private catalog. Note: If you do not have IBM Cloud Private already, IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage as well, and install as a prerequisite. IBM Cloud Private Foundation can only be used to deploy IBM Event Streams. No other service can be deployed without upgrading IBM Cloud Private. Persistent storage Persistence is not enabled by default, so no persistent volumes are required. Enable persistence if you want messages in topics and configuration settings to be retained in the event of a restart. You should enable persistence for production use and whenever you want your data to survive a restart. If you plan to have persistent volumes, consider the disk space required for storage. Also, as both Kafka and ZooKeeper rely on fast write access to disks, ensure you use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation. If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. You either need to create a persistent volume for each Kafka broker and ZooKeeper server, or specify a storage class that supports dynamic provisioning. Kafka and ZooKeeper can use different storage classes to control how physical volumes are allocated. See the IBM Cloud Private documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the IBM Cloud Private Cluster administrator role. Important: When creating persistent volumes to use with IBM Event Streams, ensure you set Access mode to ReadWriteOnce. More information about persistent volumes and the system administration steps required before installing IBM Event Streams can be found in the Kubernetes documentation. If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. The administrator will add these to a central pool before the Helm chart can be installed. The installation will then claim the required number of persistent volumes from this pool. For manual creation, dynamic provisioning must be disabled when configuring your installation. It is up to the administrator to provide appropriate storage to contain these physical volumes. If these persistent volumes are to be created automatically at the time of installation, the system administrator must enable support for this prior to installing IBM Event Streams. For automatic creation, enable dynamic provisioning when configuring your installation, and provide the storage class names to define the persistent volumes that get allocated to the deployment. Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. ConfigMap for Kafka static configuration You can choose to create a ConfigMap to specify Kafka configuration settings for your IBM Event Streams installation. This is optional. You can use a ConfigMap to override default Kafka configuration settings when installing IBM Event Streams. You can also use a ConfigMap to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. To create a ConfigMap:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Note: To create a ConfigMap, you must have the Operator, Administrator, or Cluster administrator role in IBM Cloud Private.  To create a ConfigMap from an existing Kafka server.properties file, use the following command (where namespace is where you install Event Streams):  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  To create a blank ConfigMap for future configuration updates, use the following command:  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt;Geo-replication You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. Find out more about geo-replication. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Connecting clients By default, Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. If you want clients to connect through a different route, specify the target endpoint host name or IP address when configuring your installation. Sizing considerations Consider the capacity requirements of your deployment before installing IBM Event Streams. See the information about scaling for guidance. You can modify the capacity settings for existing installations as well. Logging IBM Cloud Private uses the Elastic Stack for managing logs (Elasticsearch, Logstash, and Kibana products). IBM Event Streams logs are written to stdout and are picked up by the default Elastic Stack setup. Consider setting up the IBM Cloud Private logging for your environment to help resolve problems with your deployment and aid general troubleshooting. See the IBM Cloud Private documentation about logging for information about the built-in Elastic Stack. As part of setting up the IBM Cloud Private logging for IBM Event Streams, ensure you consider the following:   Capacity planning guidance: set up your system to have sufficient resources towards the capture, storage, and management of logs.  Log retention: The logs captured using the Elastic Stack persist during restarts. However, logs older than a day are deleted at midnight by default to prevent log data from filling up available storage space. Consider changing the log data retention in line with your capacity planning. Longer retention of logs provides access to older data that might help troubleshoot problems.You can use log data to investigate any problems affecting your system health. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Private monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing You require a license to use IBM Event Streams. Licensing is based on a Virtual Processing Cores (VPC) metric. An IBM Event Streams deployment consists of a number of different types of containers, as described in the components of the helm chart. To use IBM Event Streams you must have a license for all of the virtual cores that are available to all Kafka and Geo-replicator containers deployed. All other container types are pre-requisite components that are supported as part of IBM Event Streams, and do not require additional licenses. The number of virtual cores available to each Kafka and geo-replicator container can be specified during installation or modified later. To check the number of cores, use the IBM Cloud Private metering report as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Platform &gt; Metering.  Select your namespace, and select IBM Event Streams (Chargeable).  Click Containers.  Go to the Containers section on the right, and ensure you select the Usage tab.  Select Capped Processors from the first drop-down list, and select 1 Month from the second drop-down list.A page similar to the following is displayed:  Click Download Report, and save the CSV file to a location of your choice.  Open the downloaded report file.  Look for the month in Period, for example, 2018/9, then in the rows underneath look for IBM Event Streams (Chargeable), and check the CCores/max Cores column. The value is the maximum aggregate number of cores provided to all Kafka and geo-replicator containers. You are charged based on this number.For example, the following excerpt from a downloaded report shows that for the period 2018/9 the chargeable IBM Event Streams containers had a total of 4 cores available (see the highlighted fields):","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/planning/",
        "teaser":null},{
        "title": "Capacity planning",
        "collection": "2018.3.1",
        "excerpt":"When preparing for your IBM Event Streams installation, consider the capacity requirements for your system. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster configuration log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster configuration log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration cleanup.policy determines the way the log segments for the topic are managed. For more information about the broker configurations and topic-level configurations, see the Kafka documentation. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click the Topics tab, then click Create topic, and click Advanced). Log cleanup by deletion If the topic-level configuration cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The broker configuration log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The broker configuration log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second.  The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding keys which have later messages in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/capacity-planning/",
        "teaser":null},{
        "title": "Installing",
        "collection": "2018.3.1",
        "excerpt":"IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can also install a basic deployment of Event Streams to try it out. Before you begin   Ensure you have set up your environment according to the prerequisites, including your IBM Cloud Private environment.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.Preparing the platform Prepare your platform for installing Event Streams as follows. Create a namespace You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. You must have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace. To create a namespace, you must have the Cluster administrator role. See the IBM Cloud Private documentation for more information about creating namespaces. Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site and make it available in your catalog.   Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform (for example, the Event Streams package for the Red Hat OpenShift Container Platform includes rhel in the package name).  Ensure you configure your Docker CLI to access your cluster.  Log in to your cluster from the IBM Cloud Private CLI and log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443 --skip-ssl-validationdocker login &lt;cluster_CA_domain&gt;:8500        Note: The default value for the cluster_CA_domain parameter is mycluster.icp. If necessary add an entry to your system’s host file to allow it to be resolved. For more information, see the IBM Cloud Private documentation.     Install the Event Streams Helm chart by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-archive --archive &lt;PPA-image-name.tar.gz&gt;When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.Preparing the repository Prepare your repository for the installation as follows. Note: Before running kubectl commands as instructed in any the following steps, log in to your IBM Cloud Private cluster as an administrator by using cloudctl login. Create an image pull secret Create an image pull secret for the namespace where you intend to install Event Streams (this is the namespace created earlier). The secret enables access to the internal Docker repository provided by IBM Cloud Private. To create a secret, use the following command: kubectl create secret docker-registry regcred --docker-server=&lt;cluster_CA_domain&gt;:8500 --docker-username=&lt;user-name&gt; --docker-password=&lt;password&gt; --docker-email=&lt;your-email&gt; -n &lt;namespace_for_event_streams&gt; For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=john.smith@ibm.com -n event-streams For more information about creating image pull secrets, see the IBM Cloud Private documentation. Create an image policy Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation.To create an image policy:   Create a .yaml file with the following content, then replace &lt;cluster_CA_domain&gt; with the correct value for your IBM Cloud Private environment, and replace the &lt;namespace_for_event_streams&gt; value with the name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;cluster_CA_domain&gt;:8500/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-prod and select it from the result. The IBM Event Streams README is displayed.  If you are installing Event Streams on IBM Cloud Private 3.1.1 running on Red Hat Enterprise Linux, remove AppArmor settings in the PodSecurityPolicy to avoid installation issues.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.Important: As part of the configuration process, enter the name of the secret you created previously in the Image pull secret field.Note: Ensure the Docker image registry field value does not have a trailing slash, for example: mycluster.icp:8500/ibmcom  Click Install.  Verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/installing/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "2018.3.1",
        "excerpt":"Enabling persistent storage Set persistent storage for Kafka and ZooKeeper in your IBM Event Streams installation. To enable persistent storage for Kafka:   Go to the Kafka persistent storage settings section.  Select the Enable persistent storage for Apache Kafka check box.  Optional: Select the Use dynamic provisioning for Apache Kafka check box and provide a storage class name if the Persistent Volumes will be created dynamically.To enable persistent storage for ZooKeeper:   Go to the ZooKeeper settings section.  Select the Enable persistent storage for ZooKeeper servers check box.  Optional: Select the Use dynamic provisioning for ZooKeeper servers check box and provide a storage class name if the Persistent Volumes will be created dynamically.Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Specifying a ConfigMap for Kafka configuration If you have a ConfigMap for Kafka configuration settings, you can provide it to your IBM Event Streams installation to use. Enter the name in the Cluster configuration ConfigMap field of the Kafka broker settings section. Important: The ConfigMap must be in the same namespace as where you intend to install the IBM Event Streams release. Setting geo-replication nodes When installing IBM Event Streams as an instance intended for geo-replication, configure the number of geo-replication worker nodes in the Geo-replication settings section by setting the number of nodes required in the Geo-replicator workers field. Note: If you want to set up a cluster as a destination for geo-replication, ensure you set a minimum of 2 nodes for high availability reasons. Consider the number of geo-replication nodes to run on a destination cluster. You can also set up destination clusters and configure the number of geo-replication worker nodes for an existing installation later. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Configuring external access By default, external Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. You simply leave the External hostname/IP address field of the External access settings section blank. If you want clients to connect through a different route such as a load balancer, use the field to specify the host name or IP address of the endpoint. Also ensure you configure security for your cluster by setting certificate details in the Secure connection settings section. By default, a self-signed certificate is created during installation and the Private key, TLS certificate, and CA certificate fields can be left blank. If you want to use an existing certificate, select provided under Certificate type, and provide these additional keys and certificate values as base 64-encoded strings. Alternatively, you can generate your own certificates. After installation, set up external access by checking the port number to use for external connections and ensuring the necessary certificates are configured within your client environment. Configuration reference Configure your IBM Event Streams installation by setting the following parameters as needed. Global install settings The following table describes the parameters for setting global installation options.             Field      Description      Default                  Docker image registry      Docker images are fetched from this registry. The format is &lt;cluster_name&gt;:&lt;port&gt;/&lt;namespace&gt;.Note: Ensure the Docker image registry value does not have a trailing slash, for example: mycluster.icp:8500/ibmcom      ibmcom              Image pull secret      If using a registry that requires authentication, the name of the secret containing credentials.      None              Image pull policy      Controls when Docker images are fetched from the registry.      IfNotPresent              File system group ID      Specify the ID of the group that owns the file system intended to be used for persistent volumes. Volumes that support ownership management must be owned and writable by this group ID.      None              Architecture scheduling preferences      Select the platform you want to install Event Streams on.      AMD64 platforms      Insights - help us improve our product The following table describes the options for product improvement analytics.             Field      Description      Default                  Share my product usage data      Select to enable product usage analytics to be transmitted to IBM for business reporting and product usage understanding.      Not selected (false)      Note: The data gathered helps IBM understand how IBM Event Streams is used, and can help build knowledge about typical deployment scenarios and common user preferences. The aim is to improve the overall user experience, and the data could influence decisions about future enhancements. For example, information about the configuration options used the most often could help IBM provide better default values for making the installation process easier. The data is only used by IBM and is not shared outside of IBM.If you enable analytics, but want to opt out later, or want more information, contact us. Kafka broker settings The following table describes the options for configuring Kafka brokers.             Field      Description      Default                  CPU limit for Kafka brokers      The maximum CPU resource that is allowed for each Kafka broker when the broker is heavily loaded expressed in CPU units.      1000m              Memory limit for Kafka brokers      The maximum amount of memory that will be allocated for each Kafka broker when the broker is heavily loaded. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi              CPU request for Kafka brokers      The expected CPU resource that will be required for each Kafka broker expressed in CPU units.      1000m              Memory request for Kafka brokers      The base amount of memory allocated for each Kafka broker. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi              Kafka brokers      Number of brokers in the Kafka cluster.      3              Cluster configuration ConfigMap      Provide the name of a ConfigMap containing Kafka configuration to apply changes to Kafka’s server.properties. See how to create a ConfigMap for your installation.      None      Kafka persistent storage settings The following table describes the options for configuring persistent storage.             Field      Description      Default                  Enable persistent storage for Apache Kafka      Set whether to store Apache Kafka broker data on Persistent Volumes.      Not selected (false)              Use dynamic provisioning for Apache Kafka      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache Kafka. Selecting will dynamically create Persistent Volume Claims for the Kafka brokers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for the Apache Kafka brokers.      datadir              Storage class name      Storage Class to use for Kafka brokers if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Kafka nodes.      4Gi      ZooKeeper settings The following table describes the options for configuring ZooKeeper.             Field      Description      Default                  CPU limit for ZooKeeper servers      The maximum CPU resource that is allowed for each ZooKeeper server when the server is heavily loaded, expressed in CPU units.      100m              CPU request for ZooKeeper servers      The expected CPU resource that will be required for each ZooKeeeper server, expressed in CPU units.      100m              Enable persistent storage for ZooKeeper servers      Set whether to store Apache ZooKeeper data on Persistent Volumes.      Not selected (false)              Use dynamic provisioning for ZooKeeper servers      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache ZooKeeper. Selecting will dynamically create Persistent Volume Claims for the ZooKeeper servers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for Apache ZooKeeper.      datadir              Storage class name      Storage Class to use for Apache ZooKeeper if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Apache ZooKeeper.      2Gi      External access settings The following table describes the options for configuring external access to Kafka.             Field      Description      Default                  External hostname/IP address      The external hostname or IP address to be used by external clients. Leave blank to default to the IP address of the cluster master node.      None      Secure connection settings The following table describes the options for configuring secure connections.             Field      Description      Default                  Certificate type      Select whether you want to have a self-signed certificate generated during installation, or if you will provide your own certificate details.      selfsigned              Private key      If you set Certificate type to provided, this is the base64-encoded TLS key or private key.      None              TLS certificate      If you set Certificate type to provided, this is the base64-encoded TLS certificate or public key certificate.      None              CA certificate      If you set Certificate type to provided, this is the base64-encoded TLS cacert or Certificate Authority Root Certificate.      None      Message indexing settings The following table describes the options for configuring message indexing.             Field      Description      Default                  Enable message indexing      Set whether to enable message indexing to enhance browsing the messages on topics.      Selected (true)              Memory limits for Index Manager nodes      The maximum amount of memory allocated for index manager nodes. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi      Geo-replication settings The following table describes the options for configuring geo-replicating topics between clusters.             Field      Description      Default                  Geo-replicator workers      Number of workers to support geo-replication.      0      Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Generating your own certificates You can create your own certificates for configuring external access. When prompted, answer all questions with the appropriate information.   Create the certificate to use for the Certificate Authority (CA):openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 365 -out ca.pem      Generate a RSA 2048-bit private key:  openssl genrsa -out es.key 2048  Other key lengths and algorithms are also supported. See the following lists for supported cipher suites.  Note: In the following lists, the string “TLS” is interchangeable with “SSL” and vice versa. For example, where TLS_RSA_WITH_AES_128_CBC_SHA is specified, SSL_RSA_WITH_AES_128_CBC_SHA also applies. For more information about each cipher suite, go to the  Internet Assigned Numbers Authority (IANA) site and search for the selected cipher suite ID.      In IBM Event Streams 2018.3.1, the following cipher suites are supported, using TLS 1.2 and later only:           TLS_RSA_WITH_AES_128_GCM_SHA256      TLS_RSA_WITH_AES_256_GCM_SHA384      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384         In IBM Event Streams 2018.3.0, the following cipher suites are supported:           TLS_RSA_WITH_3DES_EDE_CBC_SHA      TLS_RSA_WITH_AES_128_CBC_SHA      TLS_RSA_WITH_AES_256_CBC_SHA      TLS_RSA_WITH_AES_128_GCM_SHA256      TLS_RSA_WITH_AES_256_GCM_SHA384      TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA      TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA      TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384        Create a certificate signing request for the key generated in the previous step:openssl req -new -key es.key -out es.csr  Sign the request with the CA certificate created in step 1:openssl x509 -req -in es.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out es.pem  Encode your generated file to a base64 string. This can be done using command line tools such as base64, for example, to encode the file created in step 1:cat ca.pem | base64 &gt; ca.b64Completing these steps creates the following files which, after being encoded to a base64 string, can be used to configure your installation:   ca.pem : CA public certificate  es.pem : Release public certificate  es.key : Release private key","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/configuring/",
        "teaser":null},{
        "title": "Migrating from Community Edition",
        "collection": "2018.3.1",
        "excerpt":"You can migrate from the IBM Event Streams Community Edition to IBM Event Streams. Migrating involves removing your previous Community Edition installation and installing IBM Event Streams in the same namespace and using the same release name. Using this procedure,your settings and data are also migrated to the new installation if you had persistent volumes enabled previously.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Delete the Community Edition installation, making a note of the namespace and release name:helm delete --purge &lt;release_name&gt;This command does not delete the PersistentVolumeClaim (PVC) instances. Your PVCs are reused in your new IBM Event Streams installation with the same release name.  Install IBM Event Streams in the same namespace and using the same release name as used for your previous  Community Edition installation. Ensure you select the ibm-eventstreams-prod chart, and apart from the namespace and release name, also ensure you retain the same configuration settings you used for your previous installation, such as persistent volume settings.IBM Event Streams is installed with the configuration settings and data migrated from the Community Edition to your new IBM Event Streams installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/migrating/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "2018.3.1",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying your installation To verify that your IBM Event Streams installation deployed successfully, check the status of your release as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate your installation in the NAME column, and ensure the STATUS column for that row states Deployed.  Optional: Click the name of your installation to check further details of your IBM Event Streams installation. For example, you can check the ConfigMaps used, or check the logs for your pods.  Log in to your IBM Event Streams UI to get started.Installing the command-line interface The IBM Event Streams command-line interface (CLI) is a plugin for the IBM Cloud Private CLI. You can use the IBM Event Streams CLI to manage your IBM Event Streams instance from the command line, such as creating, deleting, and updating topics. To install the IBM Event Streams CLI:   Ensure you have the IBM Cloud Private CLI installed.  Log in to the IBM Event Streams as an administrator.  Click the Toolbox tab.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the IBM Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;full_path&gt;/es-pluginTo start the IBM Event Streams CLI and check all available command options in the CLI, use the cloudctl es command. To get help on each command, use the --help option. To use the IBM Event Streams CLI against a deployed IBM Cloud Private cluster, run the following commands, replacing &lt;master_ip_address&gt; with your master node IP address, &lt;master_port_number&gt; with the master node port number, and &lt;my_cluster&gt; with your cluster name: cloudctl login -a https://&lt;master_ip_address&gt;:&lt;master_port_number&gt; -c &lt;my_cluster&gt;cloudctl es initFirewall settings In your firewall settings, ensure you enable communication for the node ports that IBM Event Streams services use. To find out what ports need to be opened, follow these steps:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column, and click the name.  Scroll down to the Service table. The table lists information about the IBM Event Streams services.  In the Service table, look for NodePort in the TYPE column.In each row that has NodePort as type, look in the PORT(S) column to find the port numbers you need to ensure are open to communication.The port numbers are paired as &lt;internal_number:external_number&gt; (for example, 32000:30553). For your firewall settings, ensure the external numbers are open.The following image provides an example of the table:Connecting clients You can set up external client access during installation. After installation, clients can connect to the Kafka cluster by using the externally visible IP address for the Kubernetes cluster. The port number for the connection is allocated automatically and varies between installations. To look up this port number after the installation is complete:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  In the NAME column, locate and click the release name used during installation.  Scroll down through the sections and locate the Service section.  In the NAME column, locate and click the &lt;releasename&gt;-ibm-es-proxy-svc NodePort entry.  In the Type column, locate the list of Node port links.  Locate the top entry in the list named bootstrap &lt;bootstrap port&gt;/TCP.  If no external hostname was specified when IBM Event Streams was installed, this is the IP address and port number that external clients should connect to.  If an external hostname was specified when IBM Event Streams was installed, clients should connect to that external hostname using this bootstrap port number.Before connecting a client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or export the self-signed public certificate from the browser.  In IBM Event Streams 2018.3.1, use the following steps:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate. In IBM Event Streams 2018.3.0, use the following steps:   Log in to your IBM Event Streams UI.  Click the Topics tab to view the topic list.  Click the topic you want to connect the client to.  Click the Connection information tab.  Copy the broker information. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the JKS truststore. Otherwise, use the PEM certificate.Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Private teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/post-installation/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "2018.3.1",
        "excerpt":"You can uninstall IBM Event Streams by using the UI or the CLI. Using the UI To delete the Event Streams installation by using the UI:   Log in to the Event Streams UI as an administrator.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Delete in the corresponding row.  Optional: If you enabled persistence during installation, you also need to manually remove  PersistentVolumes and PersistentVolumeClaims.Using the CLI Delete the Event Streams installation by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command:helm delete --purge &lt;release_name&gt;  Optional: If you enabled persistence during installation, you also need to manually remove PersistentVolumeClaims and PersistentVolumes. Use the Kubernetes command line tool as follows:          To delete PersistentVolumeClaims:kubectl delete pvc &lt;PVC_name&gt; -n &lt;namespace&gt;      To delete PersistentVolumes:kubectl delete pv &lt;PV_name&gt;      Cleaning up after uninstallation The uninstallation process might leave behind artifacts that you have to clear manually. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading to 2018.3.1",
        "collection": "2018.3.1",
        "excerpt":"Upgrade your installation to the latest version of IBM Event Streams as follows. Use the CLI to upgrade Event Streams. You cannot use the UI to upgrade to Event Streams 2018.3.1.   Ensure you have the latest helm chart version available on your local file system.          You can retrieve the charts from the UI.      Alternatively, if you downloaded the archive from IBM Passport Advantage, the chart file is included in the archive. Ensure it is available to your IBM Cloud Private instance.        Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.Important: You must have the Cluster Administrator role to install the chart.  Optional: Due to a known defect in the Kafka health check, the upgrade process creates a short outage period whilst the upgrade takes place where messages cannot be sent to topics. If you want to avoid an outage, follow these steps to upgrade the health check before upgrading Event Streams:          Run the following command:kubectl edit sts &lt;release-name&gt;-ibm-es-kafka-stsThis command opens the stateful set configuration in the default editor (for example, the vi editor).      There are four image: tags in the statefulset definition, search for the tag containing the word healthcheck.An example of the string for a Community Edition installation is as follows:        image: ibmcom/eventstreams-healthcheck-ce:2018-09-18-11.23.15-1a8f35a71d2b2edc91bf1eccf664d821e04f8420                If you installed from an image downloaded from IBM Passport Advantage, your IBM Cloud Private Docker registry name would be displayed instead of ibmcom, and the string would not have ce included.             Update the tag with the unique identifier after ibmcom/.Note: The ibmcom/ can also be docker/ or a different &lt;repository-name&gt;/.                  For Community Edition installations, the unique identifier is as follows:             eventstreams-healthcheck-ce-icp-linux-amd64:2018-11-21-16.21.30-9947d87                                For Event Streams downloaded from IBM Passport Advantage, the unique identifier is as follows:             eventstreams-healthcheck-icp-linux-amd64:2018-11-21-16.21.30-9947d87                         The following example shows the changing of the tag for a Community Edition installation after ibmcom/:From             ibmcom/eventstreams-healthcheck-ce:2018-09-18-11.23.15-1a8f35a71d2b2edc91bf1eccf664d821e04f8420                        To             ibmcom/eventstreams-healthcheck-ce-icp-linux-amd64:2018-11-21-16.21.30-9947d87                                          Save your changes and exit the editor.        Optional: The helm resource requirements changed for release 2018.3.1. If you modified the default settings for your existing installation, ensure you have the ZooKeeper memory limit set to at least 1 GB, the ZooKeeper request memory set to at least 750 MB, and the Elastic Search memory limit set to at least 4 GB.To set the values, run the helm upgrade command as follows:          Set the ZooKeeper memory limit and request memory to at least 1 GB and 750 MB, respectively: helm upgrade --reuse-values  --set zookeeper.resources.limits.memory=1Gi --set zookeeper.resources.requests.memory=750Mi &lt;release-name&gt; &lt;chart-file&gt;.tgz For example: helm upgrade --reuse-values  --set zookeeper.resources.limits.memory=1Gi --set zookeeper.resources.requests.memory=750Mi event-streams-1 ibm-eventstreams-dev-1.0.0.tgz      Set the Elastic Search memory limit to at least 4 GB: helm upgrade --reuse-values  --set messageIndexing.resources.limits.memory=4Gi &lt;release-name&gt; &lt;chart-file&gt;.tgz For example: helm upgrade --reuse-values  --set messageIndexing.resources.limits.memory=4Gi event-streams-1 ibm-eventstreams-dev-1.0.0.tgz        Save the configuration settings of your existing installation to a file as follows:helm get values &lt;release-name&gt; &gt; &lt;saved-configuration-settings&gt;.yamlFor example:helm get values eventstreams1 &gt; config-settings.yaml  Run the helm upgrade command as follows, referencing the file where you saved your configuration settings and the helm chart you want to upgrade to:helm upgrade -f &lt;saved-configuration-settings&gt;.yaml &lt;release-name&gt; &lt;latest-chart-version&gt; --set global.arch=amd64For example:helm upgrade -f config-settings.yaml eventstreams1 /Users/admin/upgrade/ibm-eventstreams-dev-1.1.0.tgz --set global.arch=amd64The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting. When the upgrade is complete, refresh your browser, and continue to use the Event Streams UI. Depending on your set-up, consider completing the post-upgrade tasks. Post-upgrade tasks Depending on your set-up, you might need to complete additional steps as described in the following sections. Access management If you have IBM Cloud Private teams set up for access management, you must associate the teams again with your IBM Event Streams instance after successfully completing the upgrade. To use your upgraded Event Streams instance with existing IBM Cloud Private teams, re-apply the security resources to any teams you have defined as follows:   Check the teams you use:          Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Enter an IBM Cloud Private administrator user name and password.      From the navigation menu, click Manage &gt; Identity &amp; Access &gt; Teams. Look for the teams you use with your Event Streams instance.        Ensure you have installed the latest version of the Event Streams CLI.  Run the following command for each team that references your instance of Event Streams:  cloudctl es iam-add-release-to-team --namespace &lt;namespace&gt; --helm-release &lt;helm-release&gt; --team &lt;team-name&gt;Browser certificates If you trusted certificates in your browser for using the Event Streams UI, you might not be able to access the UI after upgrading. To resolve this issue, you must delete previous certificates and trust new ones. Check the browser help for instructions, the process for deleting and accepting certificates varies depending on the type of browser you have. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/installing/upgrading/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "2018.3.1",
        "excerpt":"Log in to your IBM Event Streams installation. To determine the IBM Event Streams UI URL:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column.  Expand the Launch link in the row and click admin-ui-https.The IBM Event Streams log in page is displayed.Note: You can also determine the IBM Event Streams UI URL by using the CLI. Click the release name and scroll to the Notes section at the bottom of the page and follow the instructions. You can then use the URL to log in.  Use your IBM Cloud Private administrator user name and password to access the UI. Use the same username and password as you use to log in to IBM Cloud Private.From the Getting started page, you can start exploring Event Streams through a simulated topic, or through learning about the concepts of the underlying technology. You can also generate a starter application that lets you learn more about writing applications. For more useful applications, tools, and connectors, go to the Toolbox tab. Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Private session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/getting-started/logging-in/",
        "teaser":null},{
        "title": "Generating a starter application",
        "collection": "2018.3.1",
        "excerpt":"To learn more about how you can create applications that can take advantage of IBM Event Streams capabilities, generate a starter application. The starter application can produce and consume messages, and you can specify the topic where you want to send messages to. About the application The starter application provides a demonstration of a Java application running on WebSphere Liberty that sends events, receives events, or both, by using IBM Event Streams. Project contents The Java classes in the application.kafka package are designed to be independent of the specific use case for Kafka. The application.kafka sample code is used by the application.demo package and can also be used to understand the elements required to create your own Kafka application. The src/main/java/application/demo folder contains the framework for running the sample in a user interface, providing an easy way to view message propagation. Security The starter application is generated with the correct security configurations to connect to IBM Event Streams. These security configurations include a .jks file for the certificate and API keys for producing and consuming messages. For more information, see the  instructions for connecting clients. Note: The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application. Generating and running To generate the application:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Log in to your IBM Event Streams UI.  Click the Generate a starter application tile, or click the Toolbox tab, and go to the Starter application section and click Generate application.  Provide a name for the application.  Decide whether you want the application to produce or consume messages, or both.  Specify a target topic for the messages from the application.  Click Generate. The application is created.  Download the compressed file and extract the file to a preferred location.  Navigate to the extracted file, and run the following command to build and deploy the application:   mvn install liberty:run-server  Access the successfully deployed sample application using the following URL: http://localhost:9080/&lt;your_application_name&gt;  Click Close and go to topics to see your topic and check the messages generated by the application.Note: Some of these options depend on your access permissions. If you are not permitted to create topics, you will not be able to create a topic as part of building the starter application. If you are not permitted to write to topics, you will not be able to create a starter application that produces messages, only consumes them. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads ",
        "collection": "2018.3.1",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the following details in the configuration file.             Attribute      Description                         bootstrap.servers      The URL used for bootstrapping knowledge about the rest of the cluster. You can find this address in the Event Streams UI as described later.                     ssl.truststore.location      The location of the JKS keystore used to securley communicate with your IBM Event Streams instance. You can downloaded the JKS keystore file from the Event Streams UI as described later.                     sasl.jaas.config      Set to org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;password&gt;\";, where &lt;password&gt; is replaced by an API key. This is needed to authorize production to your topic. To generate API keys, go to the Event Streams UI as described later.             Obtaining configuration details  In IBM Event Streams 2018.3.1, obtain the required configuration details as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  Go to the Connect a client tab.  Locate the details:          For the bootstrap.servers, copy the address from the Bootstrap server section.      To downloaded the JKS keystore file, go to the Certificates section, and download the server certificate from the Java truststore section. Set the ssl.truststore.location to the full path and name of the downloaded file.      To generate API keys, go to the API key section and follow the instructions.       In IBM Event Streams 2018.3.0, obtain the required configuration details as follows:   Log in to your IBM Event Streams UI.  Click the Topics tab to view the topic list, click the name of a topic.  Click the Connection information tab.  Locate the details:          For the bootstrap.servers, copy the address from the Broker URL field.      To downloaded the JKS keystore file, go to the Certificates section and click Download Java truststore. Set the ssl.truststore.location to the full path and name of the downloaded file.      To generate API keys, go to the Credentials and access control section and follow the instructions.      Important: To have access to the Connection information tab in the UI, you must have at least one topic. For example, if you are just starting out, use the starter application to generate topics. You can secure access to your topics as described in managing access. Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t myTopic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called myTopic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Using user-defined loads You can generate a custom message load using your own settings. For example, to test the load to the topic called myTopic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t myTopic -T 1000 -n 60000 -r 1024 The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      int      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      int      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      bool      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      int      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      int      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "2018.3.1",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:     &lt;dependency&gt;     &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;     &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;     &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:     implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.1.0' implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25' implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.25'        Ensure you set up security.Securing the connection You must secure the connection from your client applications to IBM Event Streams. To secure the connection, you must obtain the following:   A copy of the server-side public certificate added to your client-side trusted certificates.  An API key generated from the IBM Cloud Private UI.Before connecting an external client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or use the following instructions to retrieve a copy.  In IBM Event Streams 2018.3.1, copy the server-side public certificate and generate an API key as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.  To generate API keys, go to the API key section and follow the instructions. In IBM Event Streams 2018.3.0, copy the server-side public certificate and generate an API key as follows: To copy the certificate and related details:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Select any topic in the list of topics.  Click the Connection information tab.  Copy the Broker URL. This is the Kafka bootstrap server.  In the Certificates section, download the Java truststore or PEM certificate and provide it to your client application.To generate an API key:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Manage &gt; Identity &amp; Access-&gt; Service IDs.  Click Create a Service ID.  Provide a name, a description, and select your namespace. Then click Create.  Click the service id you created.  Click Create Service Policy.  Select a role, select eventstreams as your service type, select the Event Streams release instance you want to apply the policy to, and provide a Resource Type (for example, topic) and a Resource Identifier (for example, the name of the topic).If you do not specify a resource type or identifier, the policy applies its role to all resources in the Event Streams instance.  Click Add.  Click the API keys tab.  Click Create API key.  Provide a name and a description. Then click Create.  Click Download to download a file containing the API key.Important: To have access to the Connection information tab in the UI, you must have at least one topic. For example, if you are just starting out, use the starter application to generate topics. Configuring your client Add the certificate details and the API key to your Kafka client application to set up a secure connection from your application to your Event Streams instance. For example, for Java: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;broker_url&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");Replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key copied from its file. Note:  In IBM Event Streams 2018.3.1, you can copy the connection code snippet from the Event Streams UI with the broker URL already filled in for you. After logging in, click Connect to this cluster on the right, and click the Sample code tab. Copy the snippet from the Sample connection code section into your Kafka client application. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/getting-started/client/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "2018.3.1",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      No, see managing access                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No, requires ZooKeeper access      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No, requires ZooKeeper access      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No, requires ZooKeeper access      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No, requires ZooKeeper access      cloudctl es entity-config              kafka-configs.sh --entity-type users      No, requires ZooKeeper access      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      No, requires ZooKeeper access      cloudctl es topics              kafka-topics.sh --describe      No, requires ZooKeeper access      cloudctl es topic              kafka-topics.sh --create      No, requires ZooKeeper access      cloudctl es topic-create              kafka-topics.sh --delete      No, requires ZooKeeper access      cloudctl es topic-delete              kafka-topics.sh --alter --config      No, requires ZooKeeper access      cloudctl es topic-update              kafka-topics.sh --alter --partitions      No, requires ZooKeeper access      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      No, requires ZooKeeper access      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "2018.3.1",
        "excerpt":"You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:   Cluster (cluster): you can control which users and applications can connect to the cluster.  Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.What roles can I assign? Roles define the levels of access a user or application has to resources. The following table describes the roles you can assign in IBM Cloud Private.             Role      Permitted actions      Example actions                  Viewer      Viewers have permissions to perform read-only actions within IBM Event Streams such as viewing resources.      Allow an application to connect to a cluster by assigning read access to the cluster resource.              Editor      Editors have permissions beyond the Viewer role, including writing to IBM Event Streams resources such as topics.      Allow an application to produce to topics by assigning editor access to topic resources.              Operator      Operators have permissions beyond the Editor role, including creating and editing IBM Event Streams resources.      Allow access to create resources by assigning operator access to the IBM Event Streams instance.              Auditor      No actions are currently assigned to this role.                     Administrator      Administrators have permissions beyond the Operator role to complete privileged actions.      Allow full access to all resources by assigning administrator access to the IBM Event Streams instance.      Mapping service actions to roles Access control in Apache Kafka is defined in terms of operations and resources. In IBM Event Streams, the operations are grouped into a smaller set of service actions, and the service actions are then assigned to roles. The mapping between Kafka operations and service actions is described in the following table. If you understand the Kafka authorization model, this tells you how IBM Event Streams maps operations into service actions.             Resource type      Kafka operation      Service action                  Cluster      Describe      -                     Describe Configs      -                     Idempotent Write      -                     Create      cluster.manage                     Alter      RESERVED                     Alter Configs      cluster.manage                     Cluster Action      RESERVED              Topic      Describe      -                     Describe Configs      topic.read                     Read      topic.read                     Write      topic.write                     Create      topic.manage                     Delete      topic.manage                     Alter      topic.manage                     Alter Configs      topic.manage              Group      Describe      -                     Read      group.read                     Delete      group.manage              Transactional ID      Describe      -                     Write      txnid.write      In addition, IBM Event Streams adds another service action called cluster.read. This service action is used to control connection access to the cluster. Note: Where the service action for an operation is shown in the previous table as a dash -, the operation is permitted to all roles. The mapping between service actions and IBM Event Streams roles is described in the following table.             Resource type      Administrator      Operator      Editor      Viewer                  Cluster      cluster.read      cluster.read      cluster.read      cluster.read                     cluster.manage      cluster.manage                            Topic      topic.read      topic.read      topic.read      topic.read                     topic.write      topic.write      topic.write                            topic.manage      topic.manage                            Group      group.read      group.read      group.read      group.read                     group.manage      group.manage                            Transactional ID      txnid.write      txnid.write      txnid.write             Assigning access to users If you have not set up IBM Cloud Private teams, the default  admin user has unlimited access to all resources. The default admin user is defined at the time of installation in the IBM Cloud Private config.yaml file by using the default_admin_user parameter. If you are using IBM Cloud Private teams, you must associate the team with the IBM Event Streams instance to apply the team members’ roles to the resources within the instance, including any users that have the Cluster Administrator role. You can do this by using the cloudctl es iam-add-release-to-team command. This command creates policies that grant access to resources based on the roles in the team. It is possible to refine user access to specific resources further and limit actions they can take against resources by using the IBM Cloud Private APIs. If you require such granular settings for security, contact us. Note: It can take up to 10 minutes after assigning access before users can perform tasks associated with their permissions. Common scenarios for users The following table summarizes common IBM Event Streams scenarios and the roles you need to assign.             Permission      Role required                  Allow full access to all resources      Administrator              Create and delete topics      Operator or higher              Generate the starter application to produce messages      Editor or higher              View the messages on a topic      Viewer or higher      Assigning access to applications Each application that connects to IBM Event Streams provides credentials associated with an IBM Cloud Private service ID. You assign access to a service ID by creating service policies. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Select the ServiceID you are interested in or create one.Warning: Do not use the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. This service ID is reserved to be used within the Events Streams cluster.Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information:   The role assigned to the policy. For example, Viewer, Editor, or Operator.  The type of service the policy applies to. For example, IBM Event Streams.  The instance of the service to be secured.  The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance.  The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance.You can create a single policy that does not specify either the resource type or the resource identifier. This kind of policy applies its role to all resources in the IBM Event Streams instance. If you want more precise access control, you can create a separate policy for each specific resource that the service ID will use. Note: It can take up to 10 minutes after assigning access before applications can perform tasks associated with their permissions. Common scenarios for applications If you choose to use a single policy to grant access to all resources in the IBM Event Streams instance, the following table summarizes the roles required for common scenarios.             Permission      Policies required                  Connect to the cluster      1. Role: Viewer or higher              Consume from a topic      1. Role: Viewer or higher              Produce to a topic      1. Role: Editor or higher              Use all features of the Kafka Streams API      1. Role: Operator or higher      Alternatively, you can assign specific service policies for the individual resources. The following table summarizes common IBM Event Streams scenarios and the service policies you need to assign.             Permission      Policies required                  Connect to the cluster      1. Resource type: cluster Role: Viewer or higher              Produce to a topic      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher              Produce to a topic using a transactional ID      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher  3. Resource type: txnid  Resource identifier: transactional_id Role: Editor or higher              Consume from a topic (no consumer group)      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher              Consume from a topic in a consumer group      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher  3. Resource type: group  Resource identifier: name_of_consumer_group Role: Viewer or higher      Revoking access for an application You can revoke access to IBM Event Streams by deleting the IBM Cloud Private service ID or API key that the application is using. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Find the Service ID being used by the application in the Service IDs list.  Remove either the service ID or the API key that the application is using. Removing the service ID also removes all API keys that are owned by the service ID.Warning: Do not remove the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. Removing this service ID corrupts your deployment, which can only be resolved by reinstalling Event Streams.  Remove the service ID by clicking  Menu overflow &gt; Remove in the row of the service ID. Click Remove Service ID on the confirmation dialog.  Remove the API key by clicking the service ID. On the service ID page, click API keys. Locate the API key being used by the application in the API keys list. CLick  Menu overflow &gt; Remove in the row of the API key. Click Remove API key on the confirmation dialog.Note: Revoking a service ID or API key in use by any Kafka client might not disable access for the application immediately. The API key is stored in a token cache in Kafka which has a 23 hour expiration period. When the token cache expires, it is refreshed from IBM Cloud Private and any revoked service IDs or API keys are reflected in the new token cache, causing application access be be disabled. To immediately disable application access, you can force a refresh of the Kafka token cache by restarting each Kafka broker. To do this without causing downtime, you can patch the stateful set by using the following command: kubectl -n &lt;namespace&gt; patch sts &lt;release_name&gt;-ibm-es-kafka-sts -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"restarted\":\"123\"}}}}}' This does not make changes to the broker configuration, but it still causes the Kafka brokers to restart one at a time, meaning no downtime is experienced. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "2018.3.1",
        "excerpt":"Network connections into the IBM Event Streams deployment are secured using TLS. By default, data within the IBM Event Streams deployment is not encrypted. To secure this data, you must ensure that any storage and communication channels are encrypted as follows:   Encrypt data at rest by using disk encryption or encrypting volumes using dm-crypt.  Encrypt internal network traffic within the cluster with IPSec:          On AMD64 platforms (x86-64), you must use IBM Cloud Private version 3.1.1 or later to encrypt traffic with IPsec.      On s390x platforms, you must use IBM Cloud Private version 3.1.2 or later to encrypt traffic with IPsec.        Encrypt messages in applications.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/security/encrypting-data/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "2018.3.1",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR Information Portal  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settingsusing the IBM Event Streams CLI. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemUser roles can be used to control access to data stored in IBM Event Streams accessed over these APIs. In addition, the Kubernetes APIs can be used to access cluster configuration and resources, including but not limited to logs that may contain message data. Access and autorization controls can be used to control which users are able to access this cluster level information. Data Processing Encryption of connection to IBM Event Streams: Connections to IBM Event Streams are secured using TLS. When deploying IBM Event Streams, the default setting for the charts .Values.global.tls.type is “selfsigned”. In this case, a self signed certificate is generated for use creating secure connections. Alternatively, .Values.global.tls.type can be set to “provided” and the TLS certificate (.Values.global.tls.cert), TLS private key (.Values.global.tls.key) and CA certificate (.Values.global.tls.cacert) can be specified to use an existing configuration. If a self signed certificate is used, a certificate and key are generated for each installation of IBM Event Streams and stored securely within a Kubernetes secret. Clients can access the public key via any web browser in the usual manner.If the certificate is provided, you are responsible for provisioning this certificate, for ensuring it is trusted by the clients you will use and for protecting the key. Encryption of connections within IBM Event Streams: Data in motion between pods the IBM Cloud Private deployment should be encrypted using IPSec. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "2018.3.1",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster to maintain the correct message order on the geo-replicated topic. This behavior continues even when geo-replication is paused. You can set up geo-replication by using the IBM Event Streams UI or CLI. When replication is set up and working, you can switch to another cluster when needed. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "2018.3.1",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Prepare your destination cluster by setting the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Decide whether you want to include message history in the geo-replication, or only copy messages from the time of setting up geo-replication. By default, the message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  Decide whether the replicated topics on the destination cluster should have the same name as their corresponding topics on the origin cluster, or if a prefix should be added to the topic name. The prefix is the release name of the origin cluster. By default, the replicated topics on the destination cluster have the same name.Preparing destination clusters Before you can set up geo-replication and start replicating topics, you must configure the number of geo-replication workers on the destination cluster. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. You can use the same approach to determine the number as used when setting the number of brokers for your installation. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of installing IBM Event Streams, or you can modify an existing installation, even if you already have geo-replication set up and running on that installation. Configuring a new installation If you are installing a new IBM Event Streams instance for use as a destination cluster, you can specify the number of workers when configuring the installation. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI You have the option to specify the number of workers during the installation process on the Configure page. Go to the Geo-replication section and specify the number of workers in the Geo-replicator workers field. Using the CLI You have the option to specify the number of workers during the installation process by adding the --set replicator.replicas=&lt;number-of-workers&gt; to your helm install command. Configuring an existing installation If you decide to use an existing IBM Event Streams instance as a destination cluster, or want to change the number of workers on an existing instance used as a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing IBM Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Go to the Geo-replication settings section and modify the Geo-replicator workers field to the required number of workers.Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems.  Click Upgrade.Using the CLI To modify the number of workers by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Use the following helm command to modify the number of workers:helm upgrade --reuse-values --set replicator.replicas=&lt;number-of-workers&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.1.0.tgz --tls","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "2018.3.1",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Go to the Origin locations section, and click Generate connection information for this cluster under Want to replicate topics to this cluster?  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Click Add destination cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Add destination. In IBM Event Streams 2018.3.1, click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created. In IBM Event Streams 2018.3.1, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the destination cluster: cloudctl es init  Run the following command to create an API key for your destination cluster:cloudctl es geo-cluster-apikey The command provides the API URL and the API key required for creating a destination cluster, for example:--rest-api-url https://192.0.2.24:32046 --rest-api-key H4C2S6Moq7KuDcYRJaM4Ye_6-XShEnB6JHnATaDaBFQZ  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the origin cluster: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add --rest-api-url &lt;api-url-from-step-3&gt; --rest-api-key &lt;api-key-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.  Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip:  In IBM Event Streams 2018.3.1, you can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.  Optional: Select whether to add a prefix to the name of the new replicated topic that is created on the destination cluster. Click Add prefix to destination topic names to add the release name of the origin cluster as a prefix to the replicated topics.  Optional: Select whether you want to include the message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Click Include message history if you want to include history.  Click Create to create geo-replicators for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row as follows:       If topics are being replicated from the cluster you are logged into, the  icon is displayed in the topic’s row. The number in the brackets indicates the number of destination clusters the topic is being replicated to. Clicking  expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:         If topics are being replicated to the cluster you are logged in to, the topics have the following indication that geo-replication is set up for them. Clicking the From &lt;cluster-name&gt; link opens the geo-replication panel with more information about the origin cluster:   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. The command creates one replicator for each topic. To set up more that one geo-replicators at once, list each topic you want to replicate using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.          Optional: You can specify to add a prefix to the name of the new replicated topic that is created on the destination cluster by using the --prefix &lt;prefix-name&gt; option.      Optional: Select whether you want to include message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Set one of the following:                  Use the --from earliest option to include available message history in geo-replication. This means all available message data for the topic is copied.          Use the --from latest option to exclude available message history. This means that only message data from the time of setting up replication is copied.                    For example, to use all options to create the geo-replicators:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt; --from &lt;earliest or latest&gt; --prefix &lt;topic-name-prefix&gt;For example:cloudctl es geo-replicator-create --destination DestinationClusterId --topics MyTopicName1,MyTopicName2 --from latest --prefix GeoReplica- When your geo-replication is set up, you can monitor and manage it. Switching clusters When one of your origin IBM Event Streams clusters experiences problems and goes down, you are notified on the destination cluster UI that the origin cluster is offline. You can switch your applications over to use the geo-replicated topics on the destination cluster as follows.  In IBM Event Streams 2018.3.1, use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right.  Go to the Connect a client tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: Copy the Broker URL to connect an application to this topic.      Certificates: Download a certificate that is required by your Kafka clients to connect securely to this cluster.      API key: To connect securely to IBM Event Streams, your application needs an API key with permission to access the cluster and resources such as topics. Follow the instructions to generate an API key authorized to connect to the cluster, and select what level of access you want it to grant to your resources (topics). You can then select which topics you want included or to include all topics, and set consumer groups as well.       In IBM Event Streams 2018.3.0, use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab, and click the name of the topic you want your application to use.  Click the Connection information tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Connection information: Make a note of the topic name and the Broker URL to connect an application to this topic.      Certificates: Download a certificate that is required by your Kafka clients to connect securely to this cluster.      Credentials and access control: To connect securely to IBM Event Streams, your application needs an API key with permission to access the cluster and resources such as topics. Follow the link on the UI to generate an API key.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Decide whether you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic.       To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute         To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --execute   These methods also avoid the need to make code changes to your client application. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "2018.3.1",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming the copying of data for each topic, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name that includes the helm release name.      The health of the geo-replication for that origin cluster: CREATING, PAUSED, STOPPING, ASSIGNING, OFFLINE, and ERROR.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See details in the Origin locations section.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend copying of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication from the origin cluster.       More options &gt; Restart failed replicators: To restart geo-replication from the origin cluster for geo-replicators that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: RUNNING, RESUME, RESUMING, PAUSING, REMOVING, and ERROR. When the status is ERROR, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See the section Destination locations.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for which you want to manage geo-replication, and choose from one of the following options:           More options &gt; Pause running replicator: To pause a geo-replicator and suspend copying of data to the destination cluster.      Resume button: To resume a geo-replicator for the destination cluster.       More options &gt; Restart failed replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove replicator: To remove a geo-replicator from the destination cluster.      You can take the same actions for all of the geo-replicators in a destination cluster using the  More options menu in the top right when browsing  destination cluster details (for example, pausing all geo-replicators or removing the whole cluster as a destination). Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters  Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination siliconvalley_es_byl6xThe command returns the following information:    Details of destination cluster siliconvalley_es_byl6xCluster ID               Cluster name    REST API URL                 Skip SSL validation?destination_byl6x        destination     https://9.30.119.223:31764   trueGeo-replicator detailsName                               Status    Origin bootstrap servers   Origin topic   Destination topictopic1__to__origin_topic1_evzoo    RUNNING   192.0.2.24:32237           topic1         origin_topic1topic2__to__topic2_vdpr0           PAUSED    192.0.2.24:32237           topic2         topic2topic3__to__topic3_9jc71           ERROR     192.0.2.24:32237           topic3         topic3topic4__to__topic4_nk87o           PENDING   192.0.2.24:32237           topic4         topic4      To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:          cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      You can also remove a cluster as a destination using the following command:  cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;  Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.      Tip: You can use short options instead of spelling out the long version. For example, use -d instead of --destination, or -n instead of --name. Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an error from Kafka that prevents it from continuing to produce or consume, such as an authentication error or all brokers being unavailable, it will stop replicating and report a status of Error. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/georeplication/health/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "2018.3.1",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Connectors are available for copying data in both directions. Available connectors   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue.Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. About Kafka Connect When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems. Distributed mode is more appropriate for production use. When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to IBM MQ.When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For performance reasons, it is recommended to run the Kafka Connect worker close to the queue manager to minimise the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "2018.3.1",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. This document contains steps for running the connector in standalone mode for development and test purposes. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.  The Kafka Connect runtime environment that comes as part of an Apache Kafka distribution. These instructions are for Apache Kafka 2.0.0 or later.Downloading the connector You can obtain the Kafka Connect source connector for IBM MQ as follows:   Log in to your IBM Event Streams UI.  Click the Toolbox tab, and click Kafka Connect source connector for IBM MQ.  Download both the connector JAR and the sample connector properties files from the page.Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the connector yourself as described in the README. Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Setting up Apache Kafka To send messages from IBM MQ to IBM Event Streams, create a topic and obtain security information for your Event Streams installation. You then use this information later to configure the connection to your Event Streams instance. You can also send IBM MQ messages to Apache Kafka running locally on your machine, see the public GitHub repository for more details.  In IBM Event Streams 2018.3.1, use the following steps:   Log in to your IBM Event Streams UI.  Click the Topics tab.  If you have not previously created the topic to use with the connector, create it now by clicking Create topic.  Select the topic in the list of topics.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and choose a location for the downloaded file that can be accessed by the Kafka Connect worker.  Go to the API key section, and follow the instructions to generate an API key authorized to connect to the cluster and produce to the topic. In IBM Event Streams 2018.3.0, use the following steps:   Log in to your IBM Event Streams UI.  Click the Topics tab.  If you have not previously created the topic to use with the connector, create it now.  Select the topic in the list of topics.  Click the Connection information tab.  Copy the Broker URL. This is the Kafka bootstrap server.  In the Certificates section, click Download Java truststore, and choose a location for the downloaded file that can be accessed by the Kafka Connect worker.  Generate an API key authorized to connect to the cluster and produce to the topic, as described in Securing the connection.Note: For the distributed worker, the API key will also need to be able to write to the Kafka Connect framework’s internal topics. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. To provide connection details for IBM MQ, use the sample connector properties file you downloaded (mq-source.properties). Create a copy of it and save it to the location where you have the connector JAR file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.For example: mq.queue.manager=QM1mq.connection.name.list=localhost(1414)mq.channel.name=MYSVRCONNmq.queue=MYQSOURCEmq.user.name=alicemq.password=passw0rdtopic=TSOURCESee the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Configuring the connector to connect to IBM Event Streams or Apache Kafka To provide the connection details for your Kafka cluster, the Kafka distribution includes a file called connect-standalone.properties. Edit the file to include the following connection information:   A list of one or more Kafka brokers for bootstrapping connections.  Whether the cluster requires connections to use SSL/TLS.  Authentication credentials if the cluster requires clients to authenticate.To connect to IBM Event Streams, you will need the broker URL and security details you collected earlier when you configured IBM Event Streams. The following example shows the required properties for the Kafka Connect standalone properties file: bootstrap.servers=&lt;broker_url&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=&lt;certs.jks_file_location&gt;producer.ssl.truststore.password=&lt;truststore_password&gt;producer.sasl.mechanism=PLAINproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";You replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path of your downloaded truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key. Note: If you are running Apache Kafka locally you can use the default connect-standalone.properties file. Generate a consumer application To test the connector you will need an application to consume events from your topic.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Click Generate application under Starter application  Enter a name for the application  Select only Consume messages  Select Choose existing topic and choose the topic you provided in the MQ connector configuration  Click Generate  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningRunning the connector   Open a terminal window and change to the Kafka root directory. Start the connector worker as follows, replacing the &lt;path-to-*&gt; and &lt;jar-version&gt; placeholders:     CLASSPATH=&lt;path-to-connector-jar&gt;/kafka-connect-mq-source-&lt;jar-version&gt;-jar-with-dependencies.jar bin/connect-standalone.sh config/connect-standalone.properties &lt;path-to-mq-properties&gt;/mq-source.properties        The log output will include the following messages that indicate the connector worker has started and successfully connected to IBM MQ:      INFO Created connector mq-source INFO Connection to MQ established        Navigate to the UI of the sample application you generated earlier and start consuming messages from IBM Event Streams.  To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;The messages are printed by the Kafka console consumer, and are transferred from the IBM MQ queue into the Kafka topic using the queue manager you configured. NOTE: Messages sent to the IBM MQ queue can also be viewed by clicking the Topics tab in the IBM Event Streams UI and selecting your topic name. Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "2018.3.1",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. This document contains steps for running the connector in standalone mode for development and test purposes.  The Kafka Connect sink connector for IBM MQ is supported in IBM Event Streams 2018.3.1 and later. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.  The Kafka Connect runtime environment that comes as part of an Apache Kafka distribution. These instructions are for Apache Kafka 2.0.0 or later.Downloading the connector You can obtain the Kafka Connect sink connector for IBM MQ as follows:   Log in to your IBM Event Streams UI.  Click the Toolbox tab, and click Kafka Connect sink connector for IBM MQ.  Download both the connector JAR and the sample connector properties files from the page.Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the connector yourself as described in the README. Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Setting up Apache Kafka To send messages from IBM Event Streams to IBM MQ, create a topic and obtain security information for your Event Streams installation. You then use this information later to configure the connection to your Event Streams instance. You can also send IBM MQ messages to Apache Kafka running locally on your machine, see the public GitHub repository for more details.   Log in to your IBM Event Streams UI.  Click the Topics tab.  If you have not previously created the topic to use with the connector, create it now by clicking Create topic.  Select the topic in the list of topics.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and choose a location for the downloaded file that can be accessed by the Kafka Connect worker.  Go to the API key section and follow the instructions to generate an API key authorized to connect to the cluster, and to produce to and consume from the topic.Note: For the distributed worker, the API key will also need to be able to write to the Kafka Connect framework’s internal topics. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. To provide connection details for IBM MQ, use the sample connector properties file you downloaded (mq-sink.properties). Create a copy of it and save it to the location where you have the connector JAR file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.For example: mq.queue.manager=QM1mq.connection.name.list=localhost(1414)mq.channel.name=MYSVRCONNmq.queue=MYQSINKmq.user.name=alicemq.password=passw0rdtopics=TSINKSee the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Configuring the connector to connect to IBM Event Streams or Apache Kafka To provide the connection details for your Kafka cluster, the Kafka distribution includes a file called connect-standalone.properties. Edit the file to include the following connection information:   A list of one or more Kafka brokers for bootstrapping connections.  Whether the cluster requires connections to use SSL/TLS.  Authentication credentials if the cluster requires clients to authenticate.To connect to IBM Event Streams, you will need the broker URL and security details you collected earlier when you configured IBM Event Streams. The following example shows the required properties for the Kafka Connect standalone properties file: bootstrap.servers=&lt;broker_url&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=&lt;certs.jks_file_location&gt;consumer.ssl.truststore.password=&lt;truststore_password&gt;consumer.sasl.mechanism=PLAINconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";You replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path of your downloaded truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key. Note: If you are running Apache Kafka locally you can use the default connect-standalone.properties file. Generate a producer application To test the connector you will need an application to produce events to your topic.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Click Generate application under Starter application  Enter a name for the application  Select only Produce messages  Select Choose existing topic and choose the topic you provided in the MQ connector configuration  Click Generate  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningRunning the connector   Open a terminal window and change to the Kafka root directory. Start the connector worker as follows, replacing the &lt;path-to-*&gt; and &lt;jar-version&gt; placeholders:     CLASSPATH=&lt;path-to-connector-jar&gt;/kafka-connect-mq-sink-&lt;jar-version&gt;-jar-with-dependencies.jar bin/connect-standalone.sh config/connect-standalone.properties &lt;path-to-mq-properties&gt;/mq-sink.properties        The log output will include the following messages that indicate the connector worker has started and successfully connected to IBM MQ:      INFO Created connector mq-sink INFO Connection to MQ established        Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "2018.3.1",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka shell scripts and configuration files. To download the file and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the  Support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.11-2.0.0.tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.  Download and copy the connector .properties file to the z/OS System as well. Depending on the connector you want to use:          Download the source connector files      Download the sink connector files      Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts from ISO8859-1 to EBCDIC encoding:   bin/connect-standalone.sh  bin/kafka-run-class.shIf you want to run a distributed Kafka Connect worker, convert bin/connect-distributed.sh instead of bin/connect-standalone.sh. To convert the files:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.11-2.0.0.tar  Change to the resulting kafka_&lt;version&gt; directory.  Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.sh  Ensure that the worker and connector configuration files are also in EBCDIC encoding.   If you are starting with files from the Kafka distribution, convert them by following the same steps as described here for the shell scripts.   For example, if you want to use the MQ source connector in a standalone setup, convert the config/connect-standalone.properties file from your Kafka distribution, and also convert your mq-source.properties file.   If you are editing the files directly on z/OS, you are already using EBCDIC.Configuring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.0.1-jar-with-dependencies.jar. If you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/connecting/mq/zos/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "2018.3.1",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Using the UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready. To find out more about the problem:   Click the message to expand it, and then expand the section for the component that does not have a green tick next to it.  Click the Pod is not ready link to open more details about the problem. The link opens the IBM Cloud Private UI. Log in as an administrator.  To understand why the IBM Event Streams resource is not available, click the Events tab to view details about the cause of the problem.  For more detailed information about the problem, click the Overview tab, and click  More options &gt; View logs on the right in the Pod details panel.  For guidance on resolving common problems that might occur, see the troubleshooting section.Using the CLI You can check the health of your IBM Event Streams environment using the Kubernetes CLI.   Ensure you have the Kubernetes command line tool installed, and configure access to your cluster.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:kubectl -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.  To retrieve further details about the pods, including events affecting them, use the following command:kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about using the kubectl command for debugging, see the Kubernetes documentation. Note: After a component restarts, the kubectl command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the –previous option to the kubectl logs command. Tip: You can also use the management logging service, or Elastic Stack, deployed by IBM Cloud Private to find more log information. Setting up the built-in Elastic Stack is part of the installation planning tasks. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "2018.3.1",
        "excerpt":"Monitoring the health of your Kafka cluster ensures your operations run smoothly. Event Streams collects metrics from all of the Kafka brokers and exports them to a Prometheus-based monitoring platform. The metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. You can use the metrics as follows:   View a selection of metrics on a preconfigured dashboard in the Event Streams UI.      Create dashboards in the Grafana service that is provided in IBM Cloud Private. You can download example Grafana dashboards for Event Streams from GitHub.     For more information about the monitoring capabilities provided in IBM Cloud Private, including Grafana, see the IBM Cloud Private documentation.     Create alerts so that metrics that meet predefined criteria are used to send notifications to emails, Slack, PagerDuty, and so on. For an example of how to use the metrics to trigger alert notifications, see how you can set up notifications to Slack.Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitor dashboard.   Log in to Event Streams as an administrator  Click the Monitor tab. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Click a chart to drill down into more detail.  Click 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/cluster-health/",
        "teaser":null},{
        "title": "Modifying Kafka broker configurations",
        "collection": "2018.3.1",
        "excerpt":"You can use the IBM Event Streams CLI to dynamically modify brokers and cluster-wide configuration settings for your IBM Event Streams instance. You can also use the IBM Event Streams CLI together with a ConfigMap to modify static (read-only) configuration settings. Configuration options For a list of all configuration settings you can specify for Kafka brokers, see the Kafka documentation. Some of the broker configuration settings can be updated without restarting the broker, while others require a restart:   read-only: Requires a broker restart for the update to take effect.  per-broker: Can be updated dynamically for each broker without a broker restart.  cluster-wide: Can be updated dynamically as a cluster-wide default, or as a per-broker value for testing purposes.See the Dynamic Update Mode column in the Kafka documentation for the update mode of each broker configuration. Note: You cannot modify the following properties.   broker.id  listeners  zookeeper.connect  advertised.listeners  inter.broker.listener.name  listener.security.protocol.map  authorizer.class.name  principal.builder.class  sasl.enabled.mechanisms  log.dirs  inter.broker.protocol.version  log.message.format.versionModifying broker and cluster settings You can modify per-broker and cluster-wide configuration settings dynamically (without a broker restart) by using the IBM Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  To modify a per-broker configuration setting: cloudctl es broker-config --broker &lt;broker_id&gt; --config &lt;name&gt;=&lt;value&gt;  To modify a cluster-wide configuration setting: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt;You can also update your read-only configuration settings that require a broker restart by using the IBM Event Streams CLI. Note: Read-only settings require a ConfigMap to be set. If you did not create and specify a ConfigMap during the installation process, you can create a ConfigMap later with the required Kafka configuration settings or create a blank one to use later. Use the following command to make the ConfigMap available to your IBM Event Streams instance if you did not create a ConfigMap during installation: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. You can use the IBM Event Streams CLI to modify read-only configuration settings as follows: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt; --static-config-all-brokers ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/modifying-configs/",
        "teaser":null},{
        "title": "Running Helm upgrade commands",
        "collection": "2018.3.1",
        "excerpt":"You can use the helm upgrade command to upgrade your chart version or to modify configuration settings for your Event Streams installation. To run Helm upgrade commands, you must have a copy of the original Helm charts file that you used to install IBM Event Streams. To retrieve the charts file using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Catalog in the top navigation menu.  If you are using the Community Edition, search for ibm-eventstreams-dev and select it from the result. If you are using Event Streams, search for ibm-eventstreams-prod and select it from the result.  Select the latest version number from the drop-down list on the left.  To download the file, go to the SOURCE &amp; TAR FILES section on the left and click the link. The ibm-eventstreams-dev-&lt;version&gt;.tgz file is downloaded.Alternatively, if you downloaded IBM Event Streams from IBM Passport Advantage, you can also retrieve the charts file by looking for a file called ibm-eventstreams-prod-&lt;version&gt;.tgz within the downloaded archive. If you no longer have a copy, you can download the file again from IBM Passport Advantage. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/helm-upgrade-command/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "2018.3.1",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. You can start with the default installation parameters when deploying IBM Event Streams, and test the system with a workload that is representative of your requirements. For this purpose, IBM Event Streams provides a workload generator application to test message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Increase the number of Kafka brokers in the cluster To set this at the time of installation, you can use the --set kafka.brokers=&lt;NUMBER&gt; option in your helm install command if using the CLI, or enter the number in the Kafka brokers field of the Configure page if using the UI. To modify the number of Kafka brokers for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.brokers=&lt;NUMBER&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the CPU limit available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. A description of the syntax for these values can be found in the Kubernetes documentation. Increase the amount of memory available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values into the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI. The syntax for these values can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the memory available to supporting systems If you have significantly increased the memory available to Kafka brokers, you will likely need to make a similar increase in the memory available to the other components that support the Kafka brokers. Ensure you consider the following two components. The metrics reporter component captures the monitoring statistics for cluster, broker, and topic activity. The memory requirements for this component will increase with the number of topic partitions in the cluster, and the throughput on those topics. To set this at the time of installation, you can use the following options:--set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt;--set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; A description of the syntax for these values can be found in the Kubernetes documentation. The message indexer indexes the messages on topics to allow them to be searched in the IBM Event Streams UI. The memory requirements for this component will increase with the cluster message throughput. To set this at the time of installation, you can use the --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt; option in your helm install command if using the CLI, or enter the values into the Memory limits for Index Manager nodes fields of the Configure page if using the UI. The syntax for the container memory limits can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values  --set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt; --set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt;  &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Custom JVM tuning for Kafka brokers If you have specific requirements, you might need to further tune the JVMs running the Kafka brokers, such as modifying the garbage collection policies. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. To provide custom JVM parameters at the time of installation, you can use --set kafka.heapOpts=&lt;JVMOPTIONS&gt; option in your helm install command. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.heapOpts=&lt;JVMOPTIONS&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Use a faster storage class for PVCs used by Kafka brokers The speed of the storage available to Kafka brokers will impact performance. Set this at the time of installation with the --set kafka.persistence.dataPVC.storageClassName=&lt;STORAGE_CLASS&gt; option in your helm install command if using the CLI, or by entering the required storage class into the Storage class name field of the Kafka persistent storage settings section of the Configure page if using the UI. For more information about available storage classes, see the IBM Cloud Private documentation. Increase the disk space available to each Kafka broker The Kafka brokers will require sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods or sizes, and more topic partitions. Set this at the time of installation with the --set kafka.persistence.dataPVC.size=&lt;SIZE&gt; option in your helm install command if using the CLI, or by entering the required persistence size into the Size field of the Kafka persistent storage settings section of the Configure page if using the UI. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "2018.3.1",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both.  Kafka quotas are supported in IBM Event Streams 2018.3.1 and later. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For further information about quotas, see the Kafka documentation. Setting quotas You can set quotas by using the Event Streams CLI as follows:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI:cloudctl es init  Use the entity-config command option to set quotas as follows.Decide what you want to limit by using a quota type, and set it with the --config &lt;quota_type&gt;option, where &lt;quota_type&gt; can be one of the following:   producer_byte_rate - This quota limits the number of bytes that a producer application is allowed to send per second.  consumer_byte_rate - This quota limits the number of bytes that a consumer application is allowed to receive per second.  request_percentage - This quota limits all clients based on thread utilisation.Decide whether you want to apply the quota to users or client IDs. To apply to users, use the --user &lt;user&gt; option. Event Streams supports 2 types of users: actual user principal names, or application service IDs.   A quota defined for a user principal name is only applied to that specific user name. To specify a principal name, you must prefix the value for the --user parameter with u-, for example, --user \"u-testuser1\"  A quota defined for a service ID is applied to all applications that are using API keys that have been bound to the specific service ID. To specify a service ID, you must prefix the value for the --user parameter with s-, for example, --user \"s-consumer_service_id\"To apply to client IDs, use the --client &lt;client id&gt; option. Client IDs are defined in the application using the client.id property. A client ID identifies an application making a request. You can apply the quota setting to all users or client IDs by using the --user-default or --client-default parameters, respectively. Quotas set for specific users or client IDs override default values set by these parameters. By using these quota type and user or client ID parameters, you can set quotas using the following combinations: cloudctl es entity-config --user &lt;user&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --user-default --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client &lt;client id&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client-default --config &lt;quota_type&gt;=&lt;value&gt; Examples For example, the following setting specifies that user u-testuser1 can only send 2048 bytes of data per second:cloudctl es entity-config --user \"u-testuser1\" --config producer_byte_rate=2048 For example, the following setting specifies that all application client IDs can only receive 2048 bytes of data per second:cloudctl es entity-config --client-default --config consumer_byte_rate=2048 The cloudctl es entity-config command is dynamic, so any quota setting is applied immediately without the need to restart clients. Note: If you run any of the commands with the --default parameter, the specified quota is reset to the system default value for that user or client ID (which is unlimited).For example:    cloudctl es entity-config --user \"s-consumer_service_id\" --default --config producer_byte_rate ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/administering/quotas/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "2018.3.1",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/intro/",
        "teaser":null},{
        "title": "Diagnosing installation issues",
        "collection": "2018.3.1",
        "excerpt":" To help troubleshoot and resolve installation issues, you can run a diagnostic script that checks your deployment for potential problems. Important: Do not use the script before the installation process completes. Despite a successful installation message, some processes might still need to complete, and it can take up to 10 minutes before IBM Event Streams is available to use. To run the script:   Download the installation-diagnostic-script.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the script as follows: ./installation-diagnostic-script.sh -n &lt;namespace&gt; -r &lt;release-name&gt;If you have been waiting for more than an hour, add the --restartoldpods option to recreate lost events (by default, events are deleted after an hour). This option restarts Failed or Pending pods that are an hour old or more. For example, the following installation has pods that are in pending state, and running the diagnostic script reveals that the issue is caused by not having sufficient memory and CPU resources available to the pods: Starting release diagnostics...Checking kafka-sts pods...kafka-sts pods foundChecking zookeeper-sts pods...zookeeper-sts pods foundChecking the ibm-es-iam-secret API Key...API Key foundChecking for Pending pods...Pending pods found, checking pod for failed events...------------------Name: caesar-ibm-es-kafka-sts-0Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-1Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-2Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------No failed events found for pod caesar-ibm-es-rest-deploy-6ff498d779-stf79------------------Checking for CrashLoopBackOff pods...No CrashLoopBackOff pods foundRelease diagnostics complete. Please review output to identify potential problems.If unable to identify or fix problems, please contact support.","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/diagnosing-installation-issues/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "2018.3.1",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible sypmtoms and causes. IBM Event Streams not available after installation After a successsful installation message is displayed, IBM Event Streams might not be available to use yet. It can take up to 10 minutes before IBM Event Streams is available to use. The IBM Cloud Private installation might return a successful completion message before all Event Streams services start up. If the installation continues to be unavailable, run the installation diognastics scripts. Insufficient system resources You can specify the memory and CPU requirements when IBM Event Streams is installed. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims: occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory: occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU: occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.For example, if each Kafka broker is set to require 80 GB of memory on a system that only has 16 GB available per node, you might see the following error message:  To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). If a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which may reside on the same node. To correct this issue, uninstall IBM Event Streams. Install again using lower resource requirements, or increase the amount of system resources available to the pod. Problems with secrets When using a non-default Docker registry, you might need to provide a secret which stores the user ID and password to access that registry. If there are issues with the secret that holds the user ID and password used to access the Docker registry, the events for a pod will show an error similar to the following.  To resolve this issue correct the secret and install IBM Event Streams again. Installation failure stating object already exists If a secret that does not exist is specified during installation, the process fails even if no secret is required to access the Docker registry. The default Docker image registry at ibmcom does not require a secret specifying the user ID and password. To correct this, install IBM Event Streams again without specifying a secret. If you are using a Docker image registry that does require a secret, attempting to install again might fail stating that an object already exists, for example: Internal service error : rpc error: code = Unknown desc = rolebindings.rbac.authorization.k8s.io \"elh-ibm-es-secret-copy-crb-sys\" already existsDelete the left over object cited and other objects before trying to install again. For instructions, see how to fully clean up after uninstallation. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Full cleanup after uninstallation",
        "collection": "2018.3.1",
        "excerpt":"The uninstallation process might leave behind artifacts that you have to clear manually. Security resources A service ID is created as part of installing Event Streams, which defines the identity for securing communication between internal components. To delete this service ID after uninstalling Event Streams, run the following command: cloudctl iam service-id-delete eventstreams-&lt;release&gt;-service-id -f Kubernetes resources Use the following command to find the list of IBM Event Streams objects associated with the release’s namespace: kubectl get &lt;type&gt; -n &lt;namespace&gt; | grep ibm-es Where type is each of   pods  clusterroles  clusterrolebindings  roles  rolebindings  configmaps  serviceaccounts  statefulsets  deployments  jobs  pods  pvc (see Note later)  secretsThere are also a number of Event Streams objects created in the kube-system namespace. To list these objects run the following command: kubectl get pod -a -n kube-system | grep ibm-es Note: These commands might return objects that should not be deleted. For example, do not delete secrets or system clusterroles if the kubectl output is not piped to grep. Note: If persistent volume claims (PVCs) are deleted (the objects returned when specifying “pvc” in the commands above), the data associated with the PVCs is also deleted. This includes any persistent Kafka data on disk. Consider whether this is the desired result before deleting any PVCs. To find which objects need to be manually cleared look for the following string in the output of the previously mentioned commands: &lt;release&gt;-ibm-es You can either navigate through the IBM Cloud Private cluster management console to Workloads &gt; or Configuration &gt; to find the objects and delete them, or use the following command: kubectl delete &lt;type&gt; &lt;name&gt; -n &lt;namespace&gt; For example, to delete a leftover rolebinding called eventstreams-ibm-eventstreams-secret-copy-crb-ns, run the following command: kubectl delete rolebinding eventstreams-ibm-eventstreams-secret-copy-crb-ns -n es Be cautious of deleting persistent volume claims (PVCs) as the data on the disk that is associated with that persistent volume will also be deleted. This includes Event Streams message data. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/cleanup-uninstall/",
        "teaser":null},{
        "title": "ConsumerTimeoutException when pods available",
        "collection": "2018.3.1",
        "excerpt":"Symptoms Attempts to communicate with a pod results in timeout errors such as kafka.consumer.ConsumerTimeoutException. Causes When querying the status of pods in the Kubernetes cluster, pods show as being in Ready state can still be in the process of starting up. This latency is a result of the external ports being active on the pods before the underlying services are ready to handle requests. The period of this latency depends on the configured topology and performance characteristics of the system in use. Resolving the problem Allow additional time for pod startup to complete before attempting to communicate with it. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/pod-timeout-error/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "2018.3.1",
        "excerpt":"Symptoms The following error message is displayed when setting up replication by using the CLI: FAILEDEvent Streams API request failed:Error response from server. Status code: 400. The resource request is invalid. Missing required parameter topic nameThe message does not provide accurate information about the cause of the error. Causes When providing the list of topics to geo-replicate, you added spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, instead of --topics MyTopicName1, MyTopicName2, MyTopicName3, enter --topics MyTopicName1,MyTopicName2,MyTopicName3. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "2018.3.1",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutExceptionCauses This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "2018.3.1",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log into the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the IBM Cloud Private cluster and initialized the command line tool. Resolving the problem Ensure you log in to the IBM Cloud Private cluster as follows: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;After logging in to IBM Cloud Private, initialize the IBM Event Streams CLI as follows: cloudctl es initFinally, run the operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no ImagePolicies' error",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no ImagePolicies in the \"&lt;name&gt;\" namespace, where &lt;name&gt; is the namespace into which you are deploying the chart. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs if there are no image policies defined for the target namespace. To confirm this, list the policies as follows: kubectl get imagepolicyYou should see a message stating No resources found. Resolving the problem If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioTo apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/no-image-policy/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no matching repositories in the ImagePolicies' error",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no matching repositories in the ImagePolicies. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs when there are image policies defined for the namespace into which the chart is being deployed, but none of them include the required repositories. To confirm this, list the image policies defined as follows: kubectl get imagepolicyFor each image policy, you can check which repositories it includes as follows: kubectl describe imagepolicy &lt;imagePolicyName&gt;If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioResolving the problem To apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/image-policy-missing-repository/",
        "teaser":null},{
        "title": "Chart deployment starts but no helm release is created",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When the IBM Event Streams (not the Community Edition) chart is deployed, the process appears to start successfully but the helm release and set of expected pods are not created. You can confirm the helm release has not been created by running the following command: helm listIn this case you will not see an entry for the Helm release name you provided when you started the deployment process. In addition, you will see that only a single pod is initially created and then subsequently removed after a couple of minutes. You can check which pods are running using the following command: kubectl get podsImmediately after starting the deployment process, you will see a single pod created named &lt;releaseName&gt;-ibm-es-secret-copy-job-&lt;uid&gt;. If you ‘describe’ the pod, you will receive the error message Failed to pull image, and a further message stating either Authentication is required or unauthorized: BAD_CREDENTIAL. After a couple of minutes this pod is deleted and no pods will be reported by the kubectl command. If you query the defined jobs as follows, you will see one named &lt;releaseName&gt;-ibm-es-secret-copy-job: kubectl get jobsFinally if you ‘describe’ the job as follows, you will see that it reports a failed pod status: kubectl describe job &lt;releaseName&gt;-ibm-es-secret-copy-jobFor example, the job description will include the following: Pods Statuses:            0 Running / 0 Succeeded / 1 FailedCauses This situation occurs if there a problem with the image pull secret being used to authorize access to the Docker image repository you specified when the chart was deployed. When you ‘describe’ the secret copy pod, if you see the error message Authentication is required, this indicates that the secret you specified does not exist. If you see the error message unauthorized: BAD_CREDENTIAL, this indicates that the secret was found but one of the fields present within it is not correct. To confirm which secrets are deployed, run the following command: kubectl get secretsResolving the problem To delete a secret thats not correctly defined, use the following command: kubectl delete secret &lt;secretName&gt;To create a new secret for use in chart deployment, run the following command: kubectl create secret docker-registry &lt;secretName&gt; --docker-server=&lt;serverAddress:serverPort&gt; --docker-username=&lt;dockerUser&gt; --docker-password=&lt;dockerPassword&gt; --docker-email=&lt;yourEmailAddress&gt;For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=John.Smith@ibm.comAfter you have confirmed that the required secret is correctly defined, re-run the chart deployment process. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/no-helm-release-is-created/",
        "teaser":null},{
        "title": "The Messages page is blank",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Messages page loads, but then becomes blank when viewing any topic (either real or simulated). Causes The vm.max_map_count property on one or more of your nodes is below the required value of 262144. This causes the message indexing capabilities to fail, resulting in this behaviour. Resolving the problem Ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node:     sudo sysctl -w vm.max_map_count=262144    echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/messages-page-blank/",
        "teaser":null},{
        "title": "Unable to connect to Kafka cluster",
        "collection": "2018.3.1",
        "excerpt":"Symptoms The following error is displayed when trying to connect to your Kafka cluster using SSL, for example, when running the Kafka Connect source connector for IBM MQ: org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failedCauses The Java process might replace the IP address of your cluster with the corresponding hostname value found in your /etc/hosts file. For example, to be able to access Docker images from your IBM Cloud Private cluster, you might have added an entry in your /etc/hosts file that corresponds to the IP address of your cluster, such as 192.0.2.24 mycluster.icp. In such cases, the following Java exception is displayed after the previously mentioned error message: Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching XXXXX found.Resolving the problem If you see the exception mentioned previously, comment out the hostname value in your /etc/hosts file to solve this connection issue. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/kafka-connection-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "Geo-replication fails to start with 'Could not connect to origin cluster' error",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When geo-replicating a topic to a destination cluster with 2 or more geo-replication worker nodes, the topic replication fails to start. The Event Streams UI reports the following error: Could not connect to origin cluster.In addition, the logs for the replicator worker nodes contain the following error message: org.apache.kafka.connect.errors.ConnectException: SSL handshake failedCauses The truststore on the geo-replication worker node that hosts the replicator task does not contain the certificate for the origin cluster. Resolving the problem You can either manually add the certificate to the truststore in each of the geo-replicator worker nodes, or you can scale the number of geo-replicator worker nodes down to 1 if suitable for your setup. Manually adding certificates To manually add the certificate to the truststore in each of the geo-replicator worker nodes:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersA list of destination cluster IDs are displayed. Find the name of the destination cluster you are attempting to geo-replicate topics to.  Retrieve information about the destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;The failed geo-replicator name is in the list of geo-replicators returned.  Log into the destination cluster, and use kubectl exec to run the keytool command to import the certificate into the truststore in each geo-replication worker node:    kubectl exec -it -n &lt;namespace&gt; -c replicator \\&lt;releaseName&gt;-ibm-es-replicator-deploy-&lt;replicator-pod-id&gt; \\-- bash -c \\\"keytool -importcert \\-keystore /opt/replicator/security/cacerts \\-alias &lt;geo-replicator_name&gt; \\-file /etc/consumer-credentials/cert_&lt;geo-replicator_name&gt; \\-storepass changeit \\-trustcacerts -noprompt\"        The command either succeeds with a \"Certificate was added\" message or fails with a \"Certificate not imported, alias &lt;geo-replicator_name&gt; already exists\" message. In both cases, the truststore for that pod is ready to be used.     Repeat the command for each replicator worker node to ensure the certificate is imported into the truststore on all replicator pods.  Log in to the origin cluster, and restart the failed geo-replicator using the following cloudctl command:cloudctl es geo-replicator-restart -d &lt;geo-replication-cluster-id&gt; -n  &lt;geo-replicator_name&gt;Scaling the number of nodes To scale the number of geo-replicator worker nodes to 1:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following kubectl command:kubectl scale --replicas 1 deployment &lt;releaseName&gt;-ibm-es-replicator-deploy","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/georeplication-connect-error/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "2018.3.1",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes The most likely cause of this problem is that the user attempting to authenticate is part of an IBM Cloud Private team that has not been associated with the  Event Streams instance. Resolving the problem Configure the IBM Cloud Private team that the user is part of to work with the Event Streams instance by running the iam-add-release-to-team CLI command. Run the command as follows: cloudctl es iam-add-release-to-team --namespace &lt;namespace for the Event Streams instance&gt; --release &lt;release name of the Event Streams instance&gt; --team &lt;name of the IBM Cloud Private team that the user is imported into&gt; The user can authenticate and sign in to the Event Streams UI after the command runs successfully. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "Pods fail to start, status is blocked",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When installing Event Streams on IBM Cloud Private 3.1.1 running on Red Hat Enterprise Linux, the following error message is displayed: Error: timed out waiting for the conditionThe installation might still complete, but Event Streams is not available. When you check the status of the pods by using the command kubectl get pods, they are showing as blocked: NAME                                     READY     STATUS    RESTARTS   AGEes1-kafka-ibm-es-secret-copy-job-zkgk8   0/1       Blocked   0          34ses1-kafka-ibm-es-elastic-sts-0           0/1       Blocked   0          34ses1-kafka-ibm-es-elastic-sts-1           0/1       Blocked   0          34ses1-kafka-ibm-es-indexmgr-deploy-69-45   0/1       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-0             0/4       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-1             0/4       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-2             0/4       Blocked   0          34ses1-kafka-ibm-es-proxy-deploy-56bb       0/1       Blocked   0          34ses1-kafka-ibm-es-proxy-deploy-56bbc4     0/1       Blocked   0          34ses1-kafka-ibm-es-rest-deploy-c76f46      0/3       Blocked   0          34ses1-kafka-ibm-es-role-mappings-vjb8      0/1       Blocked   0          34ses1-kafka-ibm-es-ui-deploy-575779998d    0/3       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-0         0/1       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-1         0/1       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-2         0/1       Blocked   0          34sWhen you describe the pods for more information, the following error message is displayed: Status:             PendingReason:             AppArmorMessage:            Cannot enforce AppArmor: AppArmor is not enabled on the hostFor example, taking the first pod from the previous list and running kubectl describe provides the following message: kubectl describe pod/es1-kafka-ibm-es-secret-copy-job-zkgk8 -n event-streamsName:               es1-kafka-ibm-es-secret-copy-job-zkgk8...Annotations:        container.apparmor.security.beta.kubernetes.io/ips-copier=runtime/default                    kubernetes.io/psp=ibm-restricted-psp                    seccomp.security.alpha.kubernetes.io/pod=docker/defaultStatus:             PendingReason:             AppArmorMessage:            Cannot enforce AppArmor: AppArmor is not enabled on the hostIP:Controlled By:      Job/es1-kafka-ibm-es-secret-copy-jobCauses Pods fail to start due to a setting in the ibm-restricted-psp PodSecurityPolicy that Event Streams uses to install. The AppArmor strategy plug-in in the pod admission controller is injecting the AppArmor annotations into the pod, and the admission controller blocks the pod from starting. Resolving the problem Upgrading to IBM Cloud Private version 3.1.2 resolves the issue. An alternative solution is to edit the ibm-restricted-psp PodSecurityPolicy by using the following command: kubectl edit PodSecurityPolicy ibm-restricted-psp Remove the following lines: apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/defaultapparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/pods-apparmor-blocked/",
        "teaser":null},{
        "title": "Kafka client applications are unable to connect to the cluster. Users are unable to login to the UI.",
        "collection": "2018.3.1",
        "excerpt":"Symptoms Client applications are unable to produce or consume messages. The logs for producer and consumer applications contain the following error message: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.serversThe Event Streams UI reports the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes An invalid host name or IP address was specified in the External access settings when configuring the installation. Resolving the problem You need to reinstall Event Streams and supply the correct external host name or IP address. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/client-connect-error/",
        "teaser":null},{
        "title": "Failed to read 'log header' errors in Kafka logs",
        "collection": "2018.3.1",
        "excerpt":"Symptoms When Event Streams is configured to use GlusterFS as a storage volume, the Kafka logs show errors containing messages similar to the following: [2020-05-12 06:40:19,249] ERROR [ReplicaManager broker=2] Error processing fetch with max size 1048576 from consumer on partition &lt;TOPIC-NAME&gt;-0: (fetchOffset=10380908, logStartOffset=-1, maxBytes=1048576, currentLeaderEpoch=Optional.empty) (kafka.server.ReplicaManager)org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@a5e333e6`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 95236164.These errors mean that Kafka has been unable to read files from the Gluster volume. This can cause replicas to fall out of sync. Cause See Kafka issue 7282GlusterFS has performance settings that will allow requests for data to be served from replicas when they are not in sync with the leader. This causes problems for Kafka when it attempts to read a replica log segment before it has been fully written by Gluster. Resolving the problem Apply the following settings to each Gluster volume that is used by an Event Streams Kafka broker: gluster volume set &lt;volumeName&gt; performance.quick-read offgluster volume set &lt;volumeName&gt; performance.io-cache offgluster volume set &lt;volumeName&gt; performance.write-behind offgluster volume set &lt;volumeName&gt; performance.stat-prefetch offgluster volume set &lt;volumeName&gt; performance.read-ahead offgluster volume set &lt;volumeName&gt; performance.readdir-ahead offgluster volume set &lt;volumeName&gt; performance.open-behind offgluster volume set &lt;volumeName&gt; performance.client-io-threads offThese settings can be applied while the Gluster volume is online. The Kafka broker will not need to be modified, the broker will be able to read from the volume after the change is applied. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2018.3.1/troubleshooting/failed-to-read-log/",
        "teaser":null},{
        "title": "Home",
        "collection": "2019.1.1",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "2019.1.1",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka® project.  Event Streams release 2019.1.1 uses the Kafka 2.1.1 release and supports the use of all Kafka interfaces. IBM Event Streams builds upon the IBM Cloud Private platform to deploy Apache Kafka in a resilient and manageable way. It includes a UI design aimed at application developers getting started with Apache Kafka, as well as users operating a production cluster. IBM Event Streams is available in two editions:   IBM Event Streams Community Edition is a free version intended for trials and demonstration purposes. It can be installed and used without charge.  IBM Event Streams is a paid-for version intended for enterprise use, and includes additional features such as geo-replication.IBM Event Streams features include:   Apache Kafka deployment that maximizes the spread of Kafka brokers across the nodes of the IBM Cloud Private cluster. This creates a highly-available configuration making the deployment resilient to many classes of failure with automatic restart of brokers included.  Health check information and options to resolve issues with your clusters and brokers.  Geo-replication of your topics between clusters to enable disaster recovery and scalability.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition).  UI for browsing messages to help view and filter hundreds of thousands of messages, including options to drill in and see message details from a set time.  Encrypted communication between internal components and encrypted storage by using features available in IBM Cloud Private.  Security with authentication and authorization using IBM Cloud Private.","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "2019.1.1",
        "excerpt":"Find out what is new in IBM Event Streams version 2019.1.1. Support for Red Hat OpenShift Container Platform Event Streams 2019.1.1 is supported on Red Hat OpenShift Container Platform 3.9 and 3.10. Support for IBM Cloud Private version 3.1.2 In addition to IBM Cloud Private 3.1.1, Event Streams 2019.1.1 is also supported on IBM Cloud Private 3.1.2. New REST interface for sending event data to Event Streams Event Streams provides a producer API to help connect your existing systems to your Event Streams Kafka cluster, and produce messages to Event Streams over a secure HTTP endpoint. Connect external monitoring tools to Event Streams You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster. Find out more about monitoring with external tools. New Producer dashboard A new dashboard has been added to help monitor the health of your topics. The Producer dashboard provides information about producer activity for a selected topic. Kafka version upgraded to 2.1.1 Event Streams 2019.1.1 uses the Kafka 2.1.1 release. If you are upgrading from a previous version of Event Streams, follow the post-upgrade tasks to upgrade your Kafka version. Default resource requirements have changed. See the updated tables for the Event Streams resource requirements. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "2019.1.1",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers. The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "2019.1.1",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the desired rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");  props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "2019.1.1",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "2019.1.1",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "2019.1.1",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "2019.1.1",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "2019.1.1",
        "excerpt":"To try out Event Streams, you can install a basic deployment of the Community Edition. IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. Note: These instructions do not include setting up persistent storage, so your data and configuration settings are not retained in the event of a restart. For more features and full IBM support, install IBM Event Streams.   If you do not have IBM Cloud Private installed already, you can download and install IBM Cloud Private-CE.Note: IBM Event Streams Community Edition is not supported on Red Hat OpenShift Container Platform.  Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.The default user name is admin, and the default password is admin.  Create a namespace where you will install your Event Streams instance:          From the navigation menu, click Manage &gt; Namespaces.      Click Create Namespace.      Enter a name for your namespace.      Ensure you have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace.      Click Create.        Click Catalog in the top navigation menu.  Search for ibm-eventstreams-dev and select it from the result. This is the Helm chart for the IBM Event Streams Community Edition. The README is displayed.  Click Configure.  Enter a release name, select the target namespace you created previously, and accept the terms of the license agreement.You can leave all other settings at their default values.  Click Install.  Verify your installation and log in to start using Event Streams.These steps install a basic instance of IBM Event Streams Community Edition that you can try out. You can also configure your installation to change the default settings as required, for example, to set up persistent storage. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/trying-out/",
        "teaser":null},{
        "title": "Pre-requisites",
        "collection": "2019.1.1",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. Container environment IBM Event Streams 2019.1.1 is supported on the following platforms and systems:             Container platform      Systems                  Red Hat OpenShift Container Platform 3.9 and 3.10 with IBM cloud foundational services 3.1.2*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.1.1, 3.1.2, and 3.2.0.1907 (or later fix pack)      - Linux® 64-bit (x86_64) systems - Linux on IBM® Z systems              IBM Cloud Private 3.1.2      - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.1.1 has Helm chart version 1.2.0 and includes Kafka version 2.1.1. For an overview of supported component and platform versions, see the support matrix. Ensure you have the following set up for your environment:   If you are installing Event Streams on the OpenShift Container Platform, ensure you have OpenShift installed, and also ensure you install and integrate IBM Cloud Private version 3.1.2. Note: IBM Event Streams Community Edition is not supported on Red Hat OpenShift Container Platform.  Install and configure IBM Cloud Private.  Important: In high throughput environments, ensure you configure an external load balancer for your IBM Cloud Private cluster to take full advantage of Event Streams scaling, and avoid potential bottlenecks.  Note: IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage.  If you are installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS), ensure your proxy address uses lowercase characters.  Install the Kubernetes command line tool, and configure access to your cluster.  If you are installing Event Streams on the OpenShift Container Platform, ensure you also install the OpenShift Container Platform CLI.  Install the IBM Cloud Private Command Line Interface (CLI).  Install the Helm CLI required for your version of IBM Cloud Private, and add the IBM Cloud Private internal Helm repository called local-charts to the Helm CLI as an external repository.  For message indexing capabilities (enabled by default), ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node: sudo sysctl -w vm.max_map_count=262144echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required.Hardware requirements The Helm chart for IBM Event Streams specifies default values for the CPU and memory usage of the Apache Kafka brokers and Apache ZooKeeper servers. See the following table for memory requirements of each Helm chart component. Ensure you have sufficient physical memory to service these requirements. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other IBM Event Streams components which might be required to reside on the same node. Ensure you have one IBM Cloud Private worker node per Kafka broker, and a minimum of 3 worker nodes available for use by IBM Event Streams. Helm resource requirements The following table lists the resource requirements of the IBM Event Streams Helm chart. For details about the requirements for each pod and their containers, see the tables in the following sections.             Pod      Number of replicas      Total CPU per pod      Total memory per pod (Gi)                  Kafka      3*      1*      3.5*              ZooKeeper      3      0.1*      1*              Geo-replicator      0*      1 per replica      1 per replica              Administration UI      1      1      1              Administration server      1      4.5      2.5              REST producer server      1      4      2              REST proxy      1      unlimited      unlimited              Collector      1      1      1              Network proxy      2      unlimited      unlimited              Access controller      1      0.1      0.25              Index manager      1      unlimited      unlimited              Elasticsearch      2      unlimited      4      Important: You can configure the settings marked with an asterisk (*). Note: Before installing IBM Event Streams (not Community Edition), consider the number of Kafka replicas and geo-replicator nodes you plan to use. Each Kafka replica and geo-replicator node is a separate chargeable unit. The CPU and memory limits for some components are not limited by the chart, so they inherit the resource limits for the namespace that the chart is being installed into. If there are no resource limits set for the namespace, the containers run with unbounded CPU and memory limits. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Kafka pod             Container      CPU per container      Memory per container (Gi)                  Kafka      1*      2*              Metrics reporter      unlimited      1.5*              Metrics proxy      unlimited      unlimited              Healthcheck      unlimited      unlimited      ZooKeeper pod             Container      CPU per container      Memory per container (Gi)                  ZooKeeper      0.1*      1*      Geo-replicator pod             Container      CPU per container      Memory per container (Gi)                  Replicator      1      1              Metrics reporter      unlimited      unlimited      Administration UI pod             Container      CPU per container      Memory per container (Gi)                  UI      1      1              Redis      unlimited      unlimited              Proxy      unlimited      unlimited      Administration server pod             Container      CPU per container      Memory per container (Gi)                  Rest      4      2              Codegen      0.5      0.5      REST producer server pod             Container      CPU per container      Memory per container (Gi)                  Rest-producer      4      2      REST proxy pod             Container      CPU per container      Memory per container (Gi)                  Rest-proxy      unlimited      unlimited      Collector pod             Container      CPU per container      Memory per container (Gi)                  Collector      unlimited      unlimited      Network proxy pod             Container      CPU per container      Memory per container (Gi)                  Proxy      unlimited      unlimited      Access controller pod             Container      CPU per container      Memory per container (Gi)                  Access controller      0.1      0.25              Redis      unlimited      unlimited      Index manager pod             Container      CPU per container      Memory per container (Gi)                  Index manager      unlimited      unlimited      Elasticsearch pod             Container      CPU per container      Memory per container (Gi)                  Elastic      unlimited      4      PodSecurityPolicy requirements To install the Event Streams chart, you must have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace. You can define the PodSecurityPolicy when creating the namespace for your installation. Event Streams applies network policies to control the traffic within the namespace where it is deployed, limiting the traffic to that required by Event Streams. For more information about the network policies and the traffic they permit, see network policies. For more information about PodSecurityPolicy definitions, see the IBM Cloud Private documentation. Note: The PodSecurityPolicy requirements do not apply to the Red Hat OpenShift Container Platform. Red Hat OpenShift SecurityContextConstraints Requirements If you  are installing on the OpenShift Container Platform, the Event Streams chart requires a custom SecurityContextConstraints to be bound to the target namespace prior to installation. The custom SecurityContextConstraints controls the permissions and capabilities required to deploy this chart. You can enable this custom SecurityContextConstraints resource using the supplied pre-installation setup script. Network requirements IBM Event Streams is supported for use with IPv4 networks only. File systems for storage If you want to set up persistent storage, you must have physical volumes available, backed by one of the following file systems:   NFS version 4  GlusterFS version 3.10.1IBM Event Streams user interface The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterClients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8  Oracle Java 8You can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning for installation",
        "collection": "2019.1.1",
        "excerpt":"Consider the following when planning your installation. IBM Event Streams Community Edition The IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. You can install the Community Edition from the catalog included with IBM Cloud Private. IBM Event Streams IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can install IBM Event Streams by downloading the image from IBM Passport Advantage, and making it available in the IBM Cloud Private catalog. Note: If you do not have IBM Cloud Private already, IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage as well, and install as a prerequisite. IBM Cloud Private Foundation can only be used to deploy IBM Event Streams. No other service can be deployed without upgrading IBM Cloud Private. Persistent storage Persistence is not enabled by default, so no persistent volumes are required. Enable persistence if you want messages in topics and configuration settings to be retained in the event of a restart. You should enable persistence for production use and whenever you want your data to survive a restart. If you plan to have persistent volumes, consider the disk space required for storage. Also, as both Kafka and ZooKeeper rely on fast write access to disks, ensure you use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation. If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. The number of Kafka brokers and ZooKeeper servers depends on your setup, for default requirements, see the resource requirements table. You either need to create a persistent volume for each Kafka broker and ZooKeeper server, or specify a storage class that supports dynamic provisioning. Kafka and ZooKeeper can use different storage classes to control how physical volumes are allocated. See the IBM Cloud Private documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the IBM Cloud Private Cluster administrator role. Important: When creating persistent volumes to use with IBM Event Streams, ensure you set Access mode to ReadWriteOnce. More information about persistent volumes and the system administration steps required before installing IBM Event Streams can be found in the Kubernetes documentation. If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. The administrator will add these to a central pool before the Helm chart can be installed. The installation will then claim the required number of persistent volumes from this pool. For manual creation, dynamic provisioning must be disabled when configuring your installation. It is up to the administrator to provide appropriate storage to contain these physical volumes. If these persistent volumes are to be created automatically at the time of installation, the system administrator must enable support for this prior to installing IBM Event Streams. For automatic creation, enable dynamic provisioning when configuring your installation, and provide the storage class names to define the persistent volumes that get allocated to the deployment. Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. ConfigMap for Kafka static configuration You can choose to create a ConfigMap to specify Kafka configuration settings for your IBM Event Streams installation. This is optional. You can use a ConfigMap to override default Kafka configuration settings when installing IBM Event Streams. You can also use a ConfigMap to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. To create a ConfigMap:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Note: To create a ConfigMap, you must have the Operator, Administrator, or Cluster administrator role in IBM Cloud Private.  To create a ConfigMap from an existing Kafka server.properties file, use the following command (where namespace is where you install Event Streams):  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  To create a blank ConfigMap for future configuration updates, use the following command:  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt;Geo-replication You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. Find out more about geo-replication. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Connecting clients By default, Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. If you want clients to connect through a different route, specify the target endpoint host name or IP address when configuring your installation. Sizing considerations Consider the capacity requirements of your deployment before installing IBM Event Streams. See the information about scaling for guidance. You can modify the capacity settings for existing installations as well. Logging IBM Cloud Private uses the Elastic Stack for managing logs (Elasticsearch, Logstash, and Kibana products). IBM Event Streams logs are written to stdout and are picked up by the default Elastic Stack setup. Consider setting up the IBM Cloud Private logging for your environment to help resolve problems with your deployment and aid general troubleshooting. See the IBM Cloud Private documentation about logging for information about the built-in Elastic Stack. As part of setting up the IBM Cloud Private logging for IBM Event Streams, ensure you consider the following:   Capacity planning guidance: set up your system to have sufficient resources towards the capture, storage, and management of logs.  Log retention: The logs captured using the Elastic Stack persist during restarts. However, logs older than a day are deleted at midnight by default to prevent log data from filling up available storage space. Consider changing the log data retention in line with your capacity planning. Longer retention of logs provides access to older data that might help troubleshoot problems.You can use log data to investigate any problems affecting your system health. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Private monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing You require a license to use IBM Event Streams. Licensing is based on a Virtual Processing Cores (VPC) metric. An IBM Event Streams deployment consists of a number of different types of containers, as described in the components of the helm chart. To use IBM Event Streams you must have a license for all of the virtual cores that are available to all Kafka and Geo-replicator containers deployed. All other container types are pre-requisite components that are supported as part of IBM Event Streams, and do not require additional licenses. The number of virtual cores available to each Kafka and geo-replicator container can be specified during installation or modified later. To check the number of cores, use the IBM Cloud Private metering report as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Platform &gt; Metering.  Select your namespace, and select IBM Event Streams (Chargeable).  Click Containers.  Go to the Containers section on the right, and ensure you select the Usage tab.  Select Capped Processors from the first drop-down list, and select 1 Month from the second drop-down list.A page similar to the following is displayed:  Click Download Report, and save the CSV file to a location of your choice.  Open the downloaded report file.  Look for the month in Period, for example, 2018/9, then in the rows underneath look for IBM Event Streams (Chargeable), and check the CCores/max Cores column. The value is the maximum aggregate number of cores provided to all Kafka and geo-replicator containers. You are charged based on this number.For example, the following excerpt from a downloaded report shows that for the period 2018/9 the chargeable IBM Event Streams containers had a total of 4 cores available (see the highlighted fields):","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/planning/",
        "teaser":null},{
        "title": "Capacity planning",
        "collection": "2019.1.1",
        "excerpt":"When preparing for your IBM Event Streams installation, consider the capacity requirements for your system. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster configuration log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster configuration log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration cleanup.policy determines the way the log segments for the topic are managed. For more information about the broker configurations and topic-level configurations, see the Kafka documentation. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click the Topics tab, then click Create topic, and click Advanced). Log cleanup by deletion If the topic-level configuration cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The broker configuration log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The broker configuration log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second.  The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding keys which have later messages in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/capacity-planning/",
        "teaser":null},{
        "title": "Installing on IBM Cloud Private",
        "collection": "2019.1.1",
        "excerpt":"IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can also install a basic deployment of Event Streams Community Edition to try it out. Before you begin   Ensure you have set up your environment according to the prerequisites, including your IBM Cloud Private environment.  The Event Streams installation process creates and runs jobs in the target namsepace (the namespace where you are installing Event Streams) and in the kube-system namespace. If you are using host groups with namespace isolation configured in your IBM Cloud Private cluster, ensure you have sufficient worker nodes available to the kube-system namespace to perform the installation (at least one worker node, or more, depending on your setup). Otherwise,  the namespace isolation causes the installation process to hang with jobs in pending state.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:          The master host and port for your IBM Cloud Private cluster. These values are set during the installation of IBM Cloud Private. The default port is 8443. Make a note of these values, and enter them in the steps that have https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;      The SSH password if you are connecting remotely to the master host of your IBM Cloud Private cluster.        Ensure your proxy address uses lowercase characters. This is a setting that often needs to be checked when installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS). If the address is in uppercase, edit the ibmcloud-cluster-info ConfigMap in the kube-public namespace, and change the uppercase characters to lowercase for the proxy_address parameter: kubectl edit configmap -n ibmcloud-cluster-info -n kube-public  Ensure you have the IBM Cloud Private monitoring service installed. Usually monitoring is installed by default. However, some deployment methods might not install it. For example, monitoring might not be part of the default deployment when installing IBM Cloud Private on Azure by using Terraform. Without this service, parts of the Event Streams UI do not work. You can install the monitoring service from the Catalog or CLI for existing  deployments.Preparing the platform Prepare your platform for installing Event Streams as follows. Create a namespace You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. To create a namespace, you must have the Cluster administrator role.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  From the navigation menu, click Manage &gt; Namespaces.  Click Create Namespace.  Enter a name for your namespace.  Ensure you have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace.  Click Create.See the IBM Cloud Private documentation for more information about creating namespaces. Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site and make it available in your catalog.   Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform.  Ensure you configure your Docker CLI to access your cluster.  Log in to your cluster from the IBM Cloud Private CLI and log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443docker login &lt;cluster_CA_domain&gt;:8500        Note: The default value for the cluster_CA_domain parameter is mycluster.icp. If necessary add an entry to your system’s host file to allow it to be resolved. For more information, see the IBM Cloud Private documentation.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-archive --archive &lt;PPA-image-name.tar.gz&gt;     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository for the installation as follows. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private. The default port is 8443. Create an image pull secret Create an image pull secret for the namespace where you intend to install Event Streams (this is the namespace created earlier). The secret enables access to the internal Docker repository provided by IBM Cloud Private. To create a secret, use the following command: kubectl create secret docker-registry regcred --docker-server=&lt;cluster_CA_domain&gt;:8500 --docker-username=&lt;user-name&gt; --docker-password=&lt;password&gt; --docker-email=&lt;your-email&gt; -n &lt;namespace_for_event_streams&gt; For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=john.smith@ibm.com -n event-streams For more information about creating image pull secrets, see the IBM Cloud Private documentation. Create an image policy Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation.To create an image policy:   Create a .yaml file with the following content, then replace &lt;cluster_CA_domain&gt; with the correct value for your IBM Cloud Private environment, and replace the &lt;namespace_for_event_streams&gt; value with the name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;cluster_CA_domain&gt;:8500/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-prod and select it from the result. The IBM Event Streams README is displayed.  If you are installing Event Streams on IBM Cloud Private 3.1.1 running on Red Hat Enterprise Linux, remove AppArmor settings in the PodSecurityPolicy to avoid installation issues.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.Important: As part of the configuration process, enter the name of the secret you created previously in the Image pull secret field.  Click Install.  Verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/installing/",
        "teaser":null},{
        "title": "Installing on OpenShift",
        "collection": "2019.1.1",
        "excerpt":"IBM Event Streams makes using Apache Kafka in the enterprise easy and intuitive, and is now fully supported on the  Red Hat OpenShift Container Platform. Overview You can install Event Streams on the Red Hat OpenShift Container Platform. The solution includes key IBM cloud foundational services such as installation, security, monitoring, and lifecycle management. These services help manage your Event Streams installation, and are provided by IBM Cloud Private.  The benefits of the solution mean you have a container platform from which you can perform administrative tasks in Red Hat OpenShift while taking some foundational services Event Streams relies on from IBM Cloud Private. Any service task related to Kubernetes can be performed in both Red Hat OpenShift Container Platform and IBM Cloud Private. For example, you can perform administrative tasks through either platform, such as managing storage, reviewing status of components, and reviewing logs and events from each component. Certain aspects of managing your Event Streams installation require the use of the IBM cloud foundational services provided by IBM Cloud Private. These services are as follows:   Installing the chart  Applying updates and fix packs  Modifying installation settings  Managing authentication and access (IAM)  Reviewing metering  Reviewing monitoring and metricsImportant: This documentation assumes the use of IBM Cloud Private for the IBM cloud foundational services required for managing your Event Streams installation. Before you begin   Ensure you have set up your environment according to the prerequisites, including setting up your OpenShift Container Platform and your IBM Cloud Private integration.  The Event Streams installation process creates and runs jobs in the target namsepace (the namespace where you are installing Event Streams) and in the kube-system namespace. If you are using host groups with namespace isolation configured in your IBM Cloud Private cluster, ensure you have sufficient worker nodes available to the kube-system namespace to perform the installation (at least one worker node, or more, depending on your setup). Otherwise,  the namespace isolation causes the installation process to hang with jobs in pending state.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:          The master host and port for your IBM Cloud Private cluster. These values are set during the installation of IBM Cloud Private. The default port is 5443. Make a note of these values, and enter them in the steps that have https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;      The master port for your OpenShift Container Platform web console. The default port is 7443. The master host address is the same as the address for your IBM Cloud Private cluster. Make a note of the port value, and enter that port together with the IBM Cloud Private master host in the steps that have https://&lt;Cluster Master Host&gt;:&lt;OpenShift Master API Port&gt;      The SSH password if you are connecting remotely to the master host of your IBM Cloud Private cluster.      Note: The installation process involves steps in both the web consoles and command lines of IBM Cloud Private and OpenShift Container Platform. Create a project (namespace) You perform this step by using the OpenShift Container Platform web console. You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. When you create a project in the OpenShift Container Platform, a namespace with the same name is also created. This is the namespace to use when installing your Event Streams instance.   Go to the OpenShift Container Platform web console in your browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;OpenShift Master API Port&gt;. The default port is 7443. The master host address is the same as the address for your IBM Cloud Private cluster.  Log in using the user name and password provided to you by your administrator.  Create an OpenShift project for your Event Streams installation.For example, log into the OpenShift Container Platform web console in your browser, click the Create project button, and type a unique name, display name, and description for the new project. This creates a project and a namespace.Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site, and save the archive to the host where the IBM Cloud Private master cluster is installed. Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform (for example, the Event Streams package for the Red Hat OpenShift Container Platform includes rhel in the package name). Preparing the platform Prepare your platform for installing Event Streams as follows. Important: You must perform the following steps by using a terminal opened on the host where the IBM Cloud Private master cluster is installed. If you are on a different host, you must first connect to the host machine by using SSH before logging in.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private. The default port is 5443.  Run the following command to avoid certificate errors when running kubectl and oc commands later:kubectl config set-cluster mycluster --insecure-skip-tls-verify=trueRun the setup script You perform this step by using the IBM Cloud Private CLI. You must run the following setup script to prepare the platform.   Go to the Event Streams archive you downloaded from IBM Passport Advantage, and locate the file called ibm-eventstreams-rhel-prod-&lt;version&gt;.tgz.  Extract the PPA tar.gz archive, go to the /charts directory, and extract the chart .tgz archive.  In your terminal window, change to the following directory: /pak_extensions/pre-install  Run the setup script as follows: ./scc.sh &lt;namespace&gt; Where &lt;namespace&gt; is the namespace (project) you created for your Event Streams installation earlier.Look up the registry address You perform this step by using the OpenShift CLI. Look up the internal OpenShift Docker registry address by using the following command: oc get svc docker-registry -n default The following is an example output: NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGEdocker-registry   ClusterIP   198.51.100.24   &lt;none&gt;        5000/TCP   2dThe &lt;OpenShift_Docker_registry_address&gt; is made up of the values from the CLUSTER-IP and PORT fields as follows: &lt;CLUSTER-IP&gt;:&lt;PORT&gt; In this example, the &lt;OpenShift_Docker_registry_address&gt; is 198.51.100.24:5000. Make a note of the address, including the port number. You will need it later in the installation process. Load the archive into the catalog Make the downloaded archive available in your catalog by using the IBM Cloud Private CLI.       Log in to the Docker private image registry:docker login -u any_value -p $(oc whoami -t) &lt;OpenShift_Docker_registry_address&gt;     Where the &lt;OpenShift_Docker_registry_address&gt; is the internal OpenShift Docker registry address you looked up earlier, including the port number, for example: 198.51.100.24:5000.     Note: The docker login command uses a session token (oc whoami -t) in the password field to perform authentication. This means the -u user name field is required, but not used by Docker.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-ppa-archive --archive &lt;PPA-image-name.tar.gz&gt; --registry &lt;OpenShift_Docker_registry_address/namespace&gt;     For example:cloudctl catalog load-ppa-archive --archive eventstreams.rhel.2019.1.1.x86.pak.tar.gz --registry 198.51.100.24:5000/event-streams     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository for the installation as follows. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private. The default port is 5443.  Run the following command to avoid certificate errors when running kubectl and oc commands later:kubectl config set-cluster mycluster --insecure-skip-tls-verify=trueCreate an image pull secret Create an image pull secret for the namespace where you intend to install Event Streams (this is the name of the project created earlier). The secret enables access to the internal Docker repository provided by the OpenShift Container Platform. To create a secret, use the following command: kubectl create secret docker-registry regcred --docker-server=&lt;OpenShift_Docker_registry_address&gt; --docker-username=&lt;any_value&gt; --docker-password=$(oc whoami -t) --docker-email=&lt;any_value&gt; -n &lt;namespace&gt; where:   --docker-server is the internal OpenShift Docker registry address you looked up earlier.  --docker-username can be any value. Docker uses a session token (oc whoami -t) in the password field to perform authentication. This means the --docker-username user name field is required, but not used by Docker.  --docker-email can be any value. It is required, but not used by Docker.  -n: is the project namespace (this is the name of the project created earlier).For example: kubectl create secret docker-registry regcred --docker-server=198.51.100.24:5000 --docker-username=user --docker-password=$(oc whoami -t) --docker-email=john.smith@ibm.com -n event-streams For more information about creating image pull secrets, see the IBM Cloud Private documentation. Create an image policy Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation.To create an image policy:   Create a .yaml file with the following content, then replace &lt;OpenShift_Docker_registry_address&gt; with the address you looked up earlier, and replace the &lt;namespace_for_event_streams&gt; value with the project name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;OpenShift_Docker_registry_address&gt;/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart You perform this step in a browser by using the IBM Cloud Private cluster management console. Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation. The default port is 5443.Ensure you log in as a user that has the Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-rhel-prod and select it from the result. The IBM Event Streams README is displayed.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.Important: You might see the following warnings on this page. These warnings are harmless and can be safely ignored as the OpenShift Container Platform does not use PodSecurityPolicy settings.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.Important: As part of the configuration process, enter the name of the secret you created previously in the Image pull secret field.Note: Ensure the Docker image registry field value includes the OpenShift Docker registry address and the namespace, for example: 198.51.100.24:5000/event-streams  Click Install.  Verify your installation and consider other post-installation tasks, such as fixing certificate errors affecting the IBM Cloud Private CLI.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/installing-openshift/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "2019.1.1",
        "excerpt":"Enabling persistent storage Set persistent storage for Kafka and ZooKeeper in your IBM Event Streams installation. To enable persistent storage for Kafka:   Go to the Kafka persistent storage settings section.  Select the Enable persistent storage for Apache Kafka check box.  Optional: Select the Use dynamic provisioning for Apache Kafka check box and provide a storage class name if the Persistent Volumes will be created dynamically.To enable persistent storage for ZooKeeper:   Go to the ZooKeeper settings section.  Select the Enable persistent storage for ZooKeeper servers check box.  Optional: Select the Use dynamic provisioning for ZooKeeper servers check box and provide a storage class name if the Persistent Volumes will be created dynamically.Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Specifying a ConfigMap for Kafka configuration If you have a ConfigMap for Kafka configuration settings, you can provide it to your IBM Event Streams installation to use. Enter the name in the Cluster configuration ConfigMap field of the Kafka broker settings section. Important: The ConfigMap must be in the same namespace as where you intend to install the IBM Event Streams release. Setting geo-replication nodes When installing IBM Event Streams as an instance intended for geo-replication, configure the number of geo-replication worker nodes in the Geo-replication settings section by setting the number of nodes required in the Geo-replicator workers field. Note: If you want to set up a cluster as a destination for geo-replication, ensure you set a minimum of 2 nodes for high availability reasons. Consider the number of geo-replication nodes to run on a destination cluster. You can also set up destination clusters and configure the number of geo-replication worker nodes for an existing installation later. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Configuring external access By default, external Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. You simply leave the External hostname/IP address field of the External access settings section blank. If you want clients to connect through a different route such as a load balancer, use the field to specify the host name or IP address of the endpoint. Also ensure you configure security for your cluster by setting certificate details in the Secure connection settings section. By default, a self-signed certificate is created during installation and the Private key, TLS certificate, and CA certificate fields can be left blank. If you want to use an existing certificate, select provided under Certificate type, and provide these additional keys and certificate values as base 64-encoded strings. Alternatively, you can generate your own certificates. After installation, set up external access by checking the port number to use for external connections and ensuring the necessary certificates are configured within your client environment. Configuring external monitoring tools You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. To set this up, you need to:   Have a third-party monitoring tool set up to be used within your IBM Cloud Private cluster.  Enable access to the broker JMX port by selecting the Enable secure JMX connections check box in the Kafka broker settings section.  Provide any configuration settings required by your monitoring tool to be applied to Event Streams. For example, Datadog requires you to deploy an agent on your IBM Cloud Private system that requires configuration settings to work with Event Streams.  Configure your applications to connect to a secure JMX port.Configuration reference Configure your Event Streams installation by setting the following parameters as needed. Global install settings The following table describes the parameters for setting global installation options.             Field      Description      Default                  Docker image registry      Docker images are fetched from this registry. The format is &lt;cluster_name&gt;:&lt;port&gt;/&lt;namespace&gt;.      ibmcom              Image pull secret      If using a registry that requires authentication, the name of the secret containing credentials.      None              Image pull policy      Controls when Docker images are fetched from the registry.      IfNotPresent              File system group ID      Specify the ID of the group that owns the file system intended to be used for persistent volumes. Volumes that support ownership management must be owned and writable by this group ID.      None              Architecture scheduling preferences      Select the platform you want to install Event Streams on.      amd64      Insights - help us improve our product The following table describes the options for product improvement analytics.             Field      Description      Default                  Share my product usage data      Select to enable product usage analytics to be transmitted to IBM for business reporting and product usage understanding.      Not selected (false)      Note: The data gathered helps IBM understand how IBM Event Streams is used, and can help build knowledge about typical deployment scenarios and common user preferences. The aim is to improve the overall user experience, and the data could influence decisions about future enhancements. For example, information about the configuration options used the most often could help IBM provide better default values for making the installation process easier. The data is only used by IBM and is not shared outside of IBM.If you enable analytics, but want to opt out later, or want more information, contact us. Kafka broker settings The following table describes the options for configuring Kafka brokers.             Field      Description      Default                  CPU limit for Kafka brokers      The maximum CPU resource that is allowed for each Kafka broker when the broker is heavily loaded expressed in CPU units.      1000m              Memory limit for Kafka brokers      The maximum amount of memory that will be allocated for each Kafka broker when the broker is heavily loaded. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi              CPU request for Kafka brokers      The expected CPU resource that will be required for each Kafka broker expressed in CPU units.      1000m              Memory request for Kafka brokers      The base amount of memory allocated for each Kafka broker. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi              Kafka brokers      Number of brokers in the Kafka cluster.      3              Cluster configuration ConfigMap      Provide the name of a ConfigMap containing Kafka configuration to apply changes to Kafka’s server.properties. See how to create a ConfigMap for your installation.      None              Enable secure JMX connections      Select to make each Kafka broker’s JMX port accessible to secure connections from applications running inside the IBM Cloud Private cluster. When access is enabled, you can configure your applications to connect to a secure JMX port and read Kafka metrics. Also, see External monitoring settings for application-specific configuration requirements.      Not selected (false)      Kafka persistent storage settings The following table describes the options for configuring persistent storage.             Field      Description      Default                  Enable persistent storage for Apache Kafka      Set whether to store Apache Kafka broker data on Persistent Volumes.      Not selected (false)              Use dynamic provisioning for Apache Kafka      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache Kafka. Selecting will dynamically create Persistent Volume Claims for the Kafka brokers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for the Apache Kafka brokers.      datadir              Storage class name      Storage Class to use for Kafka brokers if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Kafka nodes.      4Gi      ZooKeeper settings The following table describes the options for configuring ZooKeeper.             Field      Description      Default                  CPU limit for ZooKeeper servers      The maximum CPU resource that is allowed for each ZooKeeper server when the server is heavily loaded, expressed in CPU units.      100m              CPU request for ZooKeeper servers      The expected CPU resource that will be required for each ZooKeeeper server, expressed in CPU units.      100m              Enable persistent storage for ZooKeeper servers      Set whether to store Apache ZooKeeper data on Persistent Volumes.      Not selected (false)              Use dynamic provisioning for ZooKeeper servers      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache ZooKeeper. Selecting will dynamically create Persistent Volume Claims for the ZooKeeper servers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for Apache ZooKeeper.      datadir              Storage class name      Storage Class to use for Apache ZooKeeper if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Apache ZooKeeper.      2Gi      External access settings The following table describes the options for configuring external access to Kafka.             Field      Description      Default                  External hostname/IP address      The external hostname or IP address to be used by external clients. Leave blank to default to the IP address of the cluster master node.      None      Secure connection settings The following table describes the options for configuring secure connections.             Field      Description      Default                  Certificate type      Select whether you want to have a self-signed certificate generated during installation, or if you will provide your own certificate details.      selfsigned              Private key      If you set Certificate type to provided, this is the base64-encoded TLS key or private key.      None              TLS certificate      If you set Certificate type to provided, this is the base64-encoded TLS certificate or public key certificate.      None              CA certificate      If you set Certificate type to provided, this is the base64-encoded TLS cacert or Certificate Authority Root Certificate.      None      Message indexing settings The following table describes the options for configuring message indexing.             Field      Description      Default                  Enable message indexing      Set whether to enable message indexing to enhance browsing the messages on topics.      Selected (true)              Memory limits for Index Manager nodes      The maximum amount of memory allocated for index manager nodes. The value should be a plain integer using one of these suffixes: Gi, G, Mi, M.      2Gi      Geo-replication settings The following table describes the options for configuring geo-replicating topics between clusters.             Field      Description      Default                  Geo-replicator workers      Number of workers to support geo-replication.      0      Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). External monitoring The following table describes the options for configuring external monitoring tools.             Field      Description      Default                  Datadog - Autodiscovery annotation check templates for Kafka brokers      YAML object that contains the Datadog Autodiscovery annotations for configuring the Kafka JMX checks. The Datadog prefix and container identifier is applied automatically to the annotation, so only use the template name as the object’s keys (for example, check_names). For more information about setting up monitoring with Datadog, see the Datadog tutorial.      None      Generating your own certificates You can create your own certificates for configuring external access. When prompted, answer all questions with the appropriate information.   Create the certificate to use for the Certificate Authority (CA):openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 365 -out ca.pem  Generate a RSA 2048-bit private key:  openssl genrsa -out es.key 2048  Other key lengths and algorithms are also supported. The following cipher suites are supported, using TLS 1.2 and later only:          TLS_RSA_WITH_AES_128_GCM_SHA256      TLS_RSA_WITH_AES_256_GCM_SHA384      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\        Note: The string “TLS” is interchangeable with “SSL” and vice versa. For example, where TLS_RSA_WITH_AES_128_CBC_SHA is specified, SSL_RSA_WITH_AES_128_CBC_SHA also applies. For more information about each cipher suite, go to the  Internet Assigned Numbers Authority (IANA) site, and search for the selected cipher suite ID.     Create a certificate signing request for the key generated in the previous step:openssl req -new -key es.key -out es.csr  Sign the request with the CA certificate created in step 1:openssl x509 -req -in es.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out es.pem  Encode your generated file to a base64 string. This can be done using command line tools such as base64, for example, to encode the file created in step 1:cat ca.pem | base64 &gt; ca.b64Completing these steps creates the following files which, after being encoded to a base64 string, can be used to configure your installation:   ca.pem : CA public certificate  es.pem : Release public certificate  es.key : Release private key","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/configuring/",
        "teaser":null},{
        "title": "Migrating from Community Edition",
        "collection": "2019.1.1",
        "excerpt":"You can migrate from the IBM Event Streams Community Edition to IBM Event Streams. Migrating involves removing your previous Community Edition installation and installing IBM Event Streams in the same namespace and using the same release name. Using this procedure,your settings and data are also migrated to the new installation if you had persistent volumes enabled previously.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Delete the Community Edition installation, making a note of the namespace and release name:helm delete --purge &lt;release_name&gt;This command does not delete the PersistentVolumeClaim (PVC) instances. Your PVCs are reused in your new IBM Event Streams installation with the same release name.  Install IBM Event Streams in the same namespace and using the same release name as used for your previous  Community Edition installation. Ensure you select the ibm-eventstreams-prod chart, and apart from the namespace and release name, also ensure you retain the same configuration settings you used for your previous installation, such as persistent volume settings.IBM Event Streams is installed with the configuration settings and data migrated from the Community Edition to your new IBM Event Streams installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/migrating/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "2019.1.1",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying your installation To verify that your IBM Event Streams installation deployed successfully, check the status of your release as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate your installation in the NAME column, and ensure the STATUS column for that row states Deployed.  Optional: Click the name of your installation to check further details of your IBM Event Streams installation. For example, you can check the ConfigMaps used, or check the logs for your pods.  Log in to your IBM Event Streams UI to get started.Installing the command-line interface The IBM Event Streams command-line interface (CLI) is a plugin for the IBM Cloud Private CLI. You can use the IBM Event Streams CLI to manage your IBM Event Streams instance from the command line, such as creating, deleting, and updating topics. To install the IBM Event Streams CLI:   Ensure you have the IBM Cloud Private CLI installed.  Log in to the IBM Event Streams as an administrator.  Click the Toolbox tab.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the IBM Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;full_path&gt;/es-pluginTo start the IBM Event Streams CLI and check all available command options in the CLI, use the cloudctl es command. To get help on each command, use the --help option. To use the IBM Event Streams CLI against a deployed IBM Cloud Private cluster, run the following commands, replacing &lt;master_ip_address&gt; with your master node IP address, &lt;master_port_number&gt; with the master node port number, and &lt;my_cluster&gt; with your cluster name: cloudctl login -a https://&lt;master_ip_address&gt;:&lt;master_port_number&gt; -c &lt;my_cluster&gt;cloudctl es initFirewall and load balancer settings In your firewall settings, ensure you enable communication for the node ports that IBM Event Streams services use. If you are using an external load balancer for your master or proxy nodes in a high availability environment, ensure that the external ports are forwarded to the appropriate master and proxy nodes. To find the node ports to expose by using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column, and click the name.  Scroll down to the Service table. The table lists information about your Event Streams services.  In the Service table, look for NodePort in the TYPE column.In each row that has NodePort as type, look in the PORT(S) column to find the port numbers you need to ensure are open to communication.The port numbers are paired as &lt;internal_number:external_number&gt;, where you need the second (external) numbers to be open (for example, 30314 in 32000:30314).The following image provides an example of the table:To find the node ports to expose by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to list information about your Event Streams services:kubectl get services -n &lt;namespace&gt;The following is an example of the output (this is the same result as shown in the UI example previously):    NAME                                              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                           AGEmy-eventstreams-ibm-es-access-controller-svc      ClusterIP   None         &lt;none&gt;        8443/TCP                                                          111dmy-eventstreams-ibm-es-elastic-svc                ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP                                                 111dmy-eventstreams-ibm-es-indexmgr-svc               ClusterIP   None         &lt;none&gt;        9080/TCP,8080/TCP                                                 111dmy-eventstreams-ibm-es-kafka-broker-svc-0         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-broker-svc-1         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-broker-svc-2         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-headless-svc         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,8081/TCP                               111dmy-eventstreams-ibm-es-proxy-svc                  NodePort    10.0.0.118   &lt;none&gt;        30000:32417/TCP,30001:31557/TCP,30051:32712/TCP,30101:32340/TCP   111dmy-eventstreams-ibm-es-replicator-svc             ClusterIP   None         &lt;none&gt;        8083/TCP                                                          111dmy-eventstreams-ibm-es-rest-proxy-svc             NodePort    10.0.0.86    &lt;none&gt;        32000:30314/TCP                                                   111dmy-eventstreams-ibm-es-rest-svc                   ClusterIP   10.0.0.224   &lt;none&gt;        9080/TCP                                                          111dmy-eventstreams-ibm-es-ui-svc                     NodePort    10.0.0.192   &lt;none&gt;        32000:30634/TCP                                                   111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-0   ClusterIP   10.0.0.236   &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-1   ClusterIP   10.0.0.125   &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-2   ClusterIP   10.0.0.87    &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-headless-svc     ClusterIP   None         &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111d      For your firewall settings, ensure the external ports are open. For example, in the previous UI example, it is the second number for the highlighted NodePort rows: 30314, 30634, 32417, 31557, 32712, and 32340. For your load balancer settings, you need to expose the following ports:   For the CLI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-rest-proxy-svc row. In the previous example, the port is the second number in the PORT(S) column of the my-eventstreams-ibm-es-rest-proxy-svc row: 30314.  For the UI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-ui-svc row. In the previous example, the port is the second number in the PORT(S) column of the my-eventstreams-ibm-es-ui-svc row: 30634.  For Kafka, ensure you forward the external port to the proxy node. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-proxy-svc row. In the previous example, the ports are the second numbers in the PORT(S) column of the my-eventstreams-ibm-es-proxy-svc row: 32417, 31557, 32712, and 32340.Connecting clients You can set up external client access during installation. After installation, clients can connect to the Kafka cluster by using the externally visible IP address for the Kubernetes cluster. The port number for the connection is allocated automatically and varies between installations. To look up this port number after the installation is complete:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  In the NAME column, locate and click the release name used during installation.  Scroll down through the sections and locate the Service section.  In the NAME column, locate and click the &lt;releasename&gt;-ibm-es-proxy-svc NodePort entry.  In the Type column, locate the list of Node port links.  Locate the top entry in the list named bootstrap &lt;bootstrap port&gt;/TCP.  If no external hostname was specified when IBM Event Streams was installed, this is the IP address and port number that external clients should connect to.  If an external hostname was specified when IBM Event Streams was installed, clients should connect to that external hostname using this bootstrap port number.Before connecting a client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or export the self-signed public certificate from the browser. To export the self-signed public certificate from the browser:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.Red Hat OpenShift only: fixing certificate errors If you are installing Event Streams on the OpenShift Container Platform, there is a known issue that causes kubectl and oc commands to result in a certificate error after logging in with the IBM Cloud Private cloudctl login command. As a temporary fix, you can run the following command after running cloudctl login: kubectl config set-cluster mycluster --insecure-skip-tls-verify=true To permanently resolve this issue, edit the existing cluster-ca-cert system secret to add an additional certificate as described in the  IBM Cloud Private documentation. Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Private teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/post-installation/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "2019.1.1",
        "excerpt":"You can uninstall IBM Event Streams by using the UI or the CLI. Using the UI To delete the Event Streams installation by using the UI:   Log in to the Event Streams UI as an administrator.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Delete in the corresponding row.  Optional: If you enabled persistence during installation, you also need to manually remove  PersistentVolumes and PersistentVolumeClaims.Using the CLI Delete the Event Streams installation by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command:helm delete --purge &lt;release_name&gt;  Optional: If you enabled persistence during installation, you also need to manually remove PersistentVolumeClaims and PersistentVolumes. Use the Kubernetes command line tool as follows:          To delete PersistentVolumeClaims:kubectl delete pvc &lt;PVC_name&gt; -n &lt;namespace&gt;      To delete PersistentVolumes:kubectl delete pv &lt;PV_name&gt;      Cleaning up after uninstallation The uninstallation process might leave behind artifacts that you have to clear manually. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading to 2019.1.1",
        "collection": "2019.1.1",
        "excerpt":"Upgrade your installation to the latest version of IBM Event Streams as follows. You can upgrade to Event Streams version 2019.1.1 from version 2018.3.1. If you have version 2018.3.0, you must first upgrade your Event Streams version to 2018.3.1, and then upgrade IBM Cloud Private to version 3.1.1 or 3.1.2, before following these steps to upgrade to Event Streams version 2019.1.1. Important: Event Streams only supports upgrading to a newer chart version. Do not select an earlier chart version when upgrading. If you want to revert to an earlier version of Event Streams, see the instructions for rolling back. Prerequisites   Ensure you have IBM Cloud Private version 3.1.1 or later installed.  If you are upgrading IBM Event Streams (not Community Edition), download the package for the version you want to upgrade to, and make it available to your IBM Cloud Private instance.Using the UI   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Upgrade in the corresponding row.  Select the chart version to upgrade to from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click Upgrade.The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting. After the upgrade completes, you must perform the post-upgrade tasks. Using the CLI   Ensure you have the latest helm chart version available on your local file system.          You can retrieve the charts from the UI.      Alternatively, if you downloaded the archive from IBM Passport Advantage, the chart file is included in the archive. Extract the PPA archive, and locate the chart file in the /charts directory, for example: ibm-eventstreams-prod-1.2.0.tgz.        Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.Important: You must have the Cluster Administrator role to upgrade the chart.  Save the configuration settings of your existing installation to a file as follows:helm get values &lt;release-name&gt; &gt; &lt;saved-configuration-settings&gt;.yamlFor example:helm get values eventstreams1 &gt; config-settings.yaml  Run the helm upgrade command as follows, referencing the file where you saved your configuration settings and the helm chart you want to upgrade to:helm upgrade -f &lt;saved-configuration-settings&gt;.yaml --set global.image.repository=\"&lt;cluster_CA_domain&gt;:8500\" &lt;release-name&gt; &lt;latest-chart-version&gt;For example, to upgrade the Community Edition:helm upgrade -f config-settings.yaml --set global.image.repository=\"ibmcom\" eventstreams1 /Users/admin/upgrade/ibm-eventstreams-dev-1.2.0.tgzFor example, to upgrade by using a chart downloaded in the PPA archive:helm upgrade -f config-settings.yaml --set global.image.repository=\"mycluster.icp:8500\" eventstreams1 /Users/admin/upgrade/ibm-eventstreams-prod-1.2.0.tgzThe upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting. After the upgrade completes, you must perform the post-upgrade tasks. Post-upgrade tasks Additional steps required after upgrading are described in the following sections. Retrieve new port number for UI Important: The upgrade process changes the port number for the UI. You must refresh the IBM Cloud Private UI and determine the URL for the Event Streams UI again to obtain the new port number. You can then log in to the Event Streams UI. Update inter-broker protocol version Event Streams 2019.1.1 includes an update to Apache Kafka version 2.1.1. Apache Kafka 2.1 includes changes to the internal schema used to store consumer offsets. It also includes changes to the inter-broker protocol. These changes are not enabled by default, so you need manually enable the changes by running the following command. Warning: It is not possible to downgrade to previous versions after you enable the changes. Do not perform this final upgrade step until you have verified that the upgrade to Event Streams 2019.1.1 has been successful.   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command:cloudctl es cluster-config --static-config-all-brokers --config log.message.format.version=2.1 --config inter.broker.protocol.version=2.1Note: This command causes each of the brokers to restart. However, the Kafka cluster remains available during the restarts. Access management If you have IBM Cloud Private teams set up for access management, you must associate the teams again with your IBM Event Streams instance after successfully completing the upgrade. To use your upgraded Event Streams instance with existing IBM Cloud Private teams, re-apply the security resources to any teams you have defined as follows:   Check the teams you use:          Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Enter an IBM Cloud Private administrator user name and password.      From the navigation menu, click Manage &gt; Identity &amp; Access &gt; Teams. Look for the teams you use with your Event Streams instance.        Ensure you have installed the latest version of the Event Streams CLI.  Run the following command for each team that references your instance of Event Streams:  cloudctl es iam-add-release-to-team --namespace &lt;namespace&gt; --helm-release &lt;helm-release&gt; --team &lt;team-name&gt;Update UI bookmarks If you have any bookmarks to the UI, you need to update them because the port number for the Event Streams UI changes as part of the upgrade to version 2019.1.1. Browser certificates If you trusted certificates in your browser for using the Event Streams UI, you might not be able to access the UI after upgrading. To resolve this issue, you must delete previous certificates and trust new ones. Check the browser help for instructions, the process for deleting and accepting certificates varies depending on the type of browser you have. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/upgrading/",
        "teaser":null},{
        "title": "Rolling back",
        "collection": "2019.1.1",
        "excerpt":"You can revert to an earlier version of Event Streams under certain conditions. Prerequisites Rolling back your Event Streams installation to an earlier version is only supported in the following cases:   You can only roll back from a newer Helm chart version to an older chart version.  You can only roll back to Event Streams 2018.3.1 (Helm chart version 1.1.0). Rolling back to earlier chart versions is not supported.  Rollback is only supported if the inter-broker protocol version has not been changed. Warning: It is not possible to revert to a previous version if you have changed the inter-broker protocol version.Remove oauth job When rolling back from Event Streams version 2019.1.1 to 2018.3.1 (Helm chart version 1.2.0 to 1.1.0), first remove the oauth job from the  kube-system namespace (kube-system is the namespace for objects created by the Kubernetes system). Use the following command to remove the oauth job: kubectl -n kube-system delete job &lt;release-name&gt;-ibm-es-ui-oauth2-client-reg Where &lt;release-name&gt; is the name that identifies your Event Streams installation. Rolling back Using the UI   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Rollback in the corresponding row.  Select the chart version to roll back to (1.1.0).  Click Rollback.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.Important: You must have the Cluster Administrator role to roll back a chart version.  Run the helm history command to view previous versions you can roll back to:helm history &lt;release-name&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation.For example:    $ helm history event-streamsREVISION        UPDATED                         STATUS          CHART                           DESCRIPTION1               Mon Oct 15 14:27:12 2018        SUPERSEDED      ibm-eventstreams-prod-1.0.0     Install complete2               Mon Dec 10 16:49:29 2018        SUPERSEDED      ibm-eventstreams-prod-1.1.0     Upgrade complete3               Fri Mar 29 12:16:34 2019        DEPLOYED        ibm-eventstreams-prod-1.2.0     Upgrade complete        Run the helm rollback command as follows:helm rollback &lt;release-name&gt; &lt;revision&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation, and &lt;revision&gt; is a number from the REVISION column that corresponds to the version you want to revert to, as displayed in the result of the helm history command.For example:helm rollback event-streams 2Post-rollback tasks Important: The rollback process changes the port number for the UI. You must refresh the IBM Cloud Private UI and determine the URL for the Event Streams UI again to obtain the new port number. You can then log in to the Event Streams UI. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/installing/rolling-back/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "2019.1.1",
        "excerpt":"Log in to your IBM Event Streams installation. To determine the IBM Event Streams UI URL:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column.  Expand the Launch link in the row and click admin-ui-https.The IBM Event Streams log in page is displayed.Note: You can also determine the IBM Event Streams UI URL by using the CLI. Click the release name and scroll to the Notes section at the bottom of the page and follow the instructions. You can then use the URL to log in.  Use your IBM Cloud Private administrator user name and password to access the UI. Use the same username and password as you use to log in to IBM Cloud Private.From the Getting started page, you can start exploring Event Streams through a simulated topic, or through learning about the concepts of the underlying technology. You can also generate a starter application that lets you learn more about writing applications. For more useful applications, tools, and connectors, go to the Toolbox tab. Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Private session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/getting-started/logging-in/",
        "teaser":null},{
        "title": "Generating a starter application",
        "collection": "2019.1.1",
        "excerpt":"To learn more about how you can create applications that can take advantage of IBM Event Streams capabilities, generate a starter application. The starter application can produce and consume messages, and you can specify the topic where you want to send messages to. About the application The starter application provides a demonstration of a Java application running on WebSphere Liberty that sends events, receives events, or both, by using IBM Event Streams. Project contents The Java classes in the application.kafka package are designed to be independent of the specific use case for Kafka. The application.kafka sample code is used by the application.demo package and can also be used to understand the elements required to create your own Kafka application. The src/main/java/application/demo folder contains the framework for running the sample in a user interface, providing an easy way to view message propagation. Security The starter application is generated with the correct security configurations to connect to IBM Event Streams. These security configurations include a .jks file for the certificate and API keys for producing and consuming messages. For more information, see the  instructions for connecting clients. Note: The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application. Generating and running To generate the application:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Log in to your IBM Event Streams UI.  Click the Generate a starter application tile, or click the Toolbox tab, and go to the Starter application section and click Generate application.  Provide a name for the application.  Decide whether you want the application to produce or consume messages, or both.  Specify a target topic for the messages from the application.  Click Generate. The application is created.  Download the compressed file and extract the file to a preferred location.  Navigate to the extracted file, and run the following command to build and deploy the application:   mvn install liberty:run-server  Access the successfully deployed sample application using the following URL: http://localhost:9080/  Click Close and go to topics to see your topic and check the messages generated by the application.Note: Some of these options depend on your access permissions. If you are not permitted to create topics, you will not be able to create a topic as part of building the starter application. If you are not permitted to write to topics, you will not be able to create a starter application that produces messages, only consumes them. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads ",
        "collection": "2019.1.1",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the following details in the configuration file.             Attribute      Description                         bootstrap.servers      The URL used for bootstrapping knowledge about the rest of the cluster. You can find this address in the Event Streams UI as described later.                     ssl.truststore.location      The location of the JKS keystore used to securley communicate with your IBM Event Streams instance. You can downloaded the JKS keystore file from the Event Streams UI as described later.                     sasl.jaas.config      Set to org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;password&gt;\";, where &lt;password&gt; is replaced by an API key. This is needed to authorize production to your topic. To generate API keys, go to the Event Streams UI as described later.             Obtaining configuration details Obtain the required configuration details as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  Go to the Connect a client tab.  Locate the details:          For the bootstrap.servers, copy the address from the Bootstrap server section.      To downloaded the JKS keystore file, go to the Certificates section, and download the server certificate from the Java truststore section. Set the ssl.truststore.location to the full path and name of the downloaded file.      To generate API keys, go to the API key section and follow the instructions.      You can secure access to your topics as described in managing access. Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t myTopic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called myTopic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Using user-defined loads You can generate a custom message load using your own settings. For example, to test the load to the topic called myTopic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t myTopic -T 1000 -n 60000 -r 1024 The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      int      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      int      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      bool      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      int      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      int      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "2019.1.1",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:     &lt;dependency&gt;     &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;     &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;     &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:     implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.1.0' implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25' implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.25'        Ensure you set up security.Securing the connection You must secure the connection from your client applications to IBM Event Streams. To secure the connection, you must obtain the following:   A copy of the server-side public certificate added to your client-side trusted certificates.  An API key generated from the IBM Cloud Private UI.Before connecting an external client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or use the following instructions to retrieve a copy. Copy the server-side public certificate and generate an API key as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.  To generate API keys, go to the API key section and follow the instructions.Configuring your client Add the certificate details and the API key to your Kafka client application to set up a secure connection from your application to your Event Streams instance. For example, for Java: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;broker_url&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");Replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key copied from its file. Note: You can copy the connection code snippet from the Event Streams UI with the broker URL already filled in for you. After logging in, click Connect to this cluster on the right, and click the Sample code tab. Copy the snippet from the Sample connection code section into your Kafka client application. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/getting-started/client/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "2019.1.1",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      No, see managing access                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No, requires ZooKeeper access      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No, requires ZooKeeper access      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No, requires ZooKeeper access      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No, requires ZooKeeper access      cloudctl es entity-config              kafka-configs.sh --entity-type users      No, requires ZooKeeper access      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      No, requires ZooKeeper access      cloudctl es topics              kafka-topics.sh --describe      No, requires ZooKeeper access      cloudctl es topic              kafka-topics.sh --create      No, requires ZooKeeper access      cloudctl es topic-create              kafka-topics.sh --delete      No, requires ZooKeeper access      cloudctl es topic-delete              kafka-topics.sh --alter --config      No, requires ZooKeeper access      cloudctl es topic-update              kafka-topics.sh --alter --partitions      No, requires ZooKeeper access      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      No, requires ZooKeeper access      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "2019.1.1",
        "excerpt":"You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:   Cluster (cluster): you can control which users and applications can connect to the cluster.  Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.What roles can I assign? Roles define the levels of access a user or application has to resources. The following table describes the roles you can assign in IBM Cloud Private.             Role      Permitted actions      Example actions                  Viewer      Viewers have permissions to perform read-only actions within IBM Event Streams such as viewing resources.      Allow an application to connect to a cluster by assigning read access to the cluster resource.              Editor      Editors have permissions beyond the Viewer role, including writing to IBM Event Streams resources such as topics.      Allow an application to produce to topics by assigning editor access to topic resources.              Operator      Operators have permissions beyond the Editor role, including creating and editing IBM Event Streams resources.      Allow access to create resources by assigning operator access to the IBM Event Streams instance.              Auditor      No actions are currently assigned to this role.                     Administrator      Administrators have permissions beyond the Operator role to complete privileged actions.      Allow full access to all resources by assigning administrator access to the IBM Event Streams instance.      Mapping service actions to roles Access control in Apache Kafka is defined in terms of operations and resources. In IBM Event Streams, the operations are grouped into a smaller set of service actions, and the service actions are then assigned to roles. The mapping between Kafka operations and service actions is described in the following table. If you understand the Kafka authorization model, this tells you how IBM Event Streams maps operations into service actions.             Resource type      Kafka operation      Service action                  Cluster      Describe      -                     Describe Configs      -                     Idempotent Write      -                     Create      cluster.manage                     Alter      RESERVED                     Alter Configs      cluster.manage                     Cluster Action      RESERVED              Topic      Describe      -                     Describe Configs      topic.read                     Read      topic.read                     Write      topic.write                     Create      topic.manage                     Delete      topic.manage                     Alter      topic.manage                     Alter Configs      topic.manage              Group      Describe      -                     Read      group.read                     Delete      group.manage              Transactional ID      Describe      -                     Write      txnid.write      In addition, IBM Event Streams adds another service action called cluster.read. This service action is used to control connection access to the cluster. Note: Where the service action for an operation is shown in the previous table as a dash -, the operation is permitted to all roles. The mapping between service actions and IBM Event Streams roles is described in the following table.             Resource type      Administrator      Operator      Editor      Viewer                  Cluster      cluster.read      cluster.read      cluster.read      cluster.read                     cluster.manage      cluster.manage                            Topic      topic.read      topic.read      topic.read      topic.read                     topic.write      topic.write      topic.write                            topic.manage      topic.manage                            Group      group.read      group.read      group.read      group.read                     group.manage      group.manage                            Transactional ID      txnid.write      txnid.write      txnid.write             Assigning access to users If you have not set up IBM Cloud Private teams, the default  admin user has unlimited access to all resources. The default admin user is defined at the time of installation in the IBM Cloud Private config.yaml file by using the default_admin_user parameter. If you are using IBM Cloud Private teams, you must associate the team with the IBM Event Streams instance to apply the team members’ roles to the resources within the instance, including any users that have the Cluster Administrator role. You can do this by using the cloudctl es iam-add-release-to-team command. This command creates policies that grant access to resources based on the roles in the team. It is possible to refine user access to specific resources further and limit actions they can take against resources by using the IBM Cloud Private APIs. If you require such granular settings for security, contact us. Note: It can take up to 10 minutes after assigning access before users can perform tasks associated with their permissions. Common scenarios for users The following table summarizes common IBM Event Streams scenarios and the roles you need to assign.             Permission      Role required                  Allow full access to all resources      Administrator              Create and delete topics      Operator or higher              Generate the starter application to produce messages      Editor or higher              View the messages on a topic      Viewer or higher      Assigning access to applications Each application that connects to IBM Event Streams provides credentials associated with an IBM Cloud Private service ID. You assign access to a service ID by creating service policies. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Select the ServiceID you are interested in or create one.Warning: Do not use the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. This service ID is reserved to be used within the Events Streams cluster.Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information:   The role assigned to the policy. For example, Viewer, Editor, or Operator.  The type of service the policy applies to. For example, IBM Event Streams.  The instance of the service to be secured.  The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance.  The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance.You can create a single policy that does not specify either the resource type or the resource identifier. This kind of policy applies its role to all resources in the IBM Event Streams instance. If you want more precise access control, you can create a separate policy for each specific resource that the service ID will use. Note: It can take up to 10 minutes after assigning access before applications can perform tasks associated with their permissions. Common scenarios for applications If you choose to use a single policy to grant access to all resources in the IBM Event Streams instance, the following table summarizes the roles required for common scenarios.             Permission      Policies required                  Connect to the cluster      1. Role: Viewer or higher              Consume from a topic      1. Role: Viewer or higher              Produce to a topic      1. Role: Editor or higher              Use all features of the Kafka Streams API      1. Role: Operator or higher      Alternatively, you can assign specific service policies for the individual resources. The following table summarizes common IBM Event Streams scenarios and the service policies you need to assign.             Permission      Policies required                  Connect to the cluster      1. Resource type: cluster Role: Viewer or higher              Produce to a topic      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher              Produce to a topic using a transactional ID      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher  3. Resource type: txnid  Resource identifier: transactional_id Role: Editor or higher              Consume from a topic (no consumer group)      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher              Consume from a topic in a consumer group      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher  3. Resource type: group  Resource identifier: name_of_consumer_group Role: Viewer or higher      Revoking access for an application You can revoke access to IBM Event Streams by deleting the IBM Cloud Private service ID or API key that the application is using. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Find the Service ID being used by the application in the Service IDs list.  Remove either the service ID or the API key that the application is using. Removing the service ID also removes all API keys that are owned by the service ID.Warning: Do not remove the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. Removing this service ID corrupts your deployment, which can only be resolved by reinstalling Event Streams.  Remove the service ID by clicking  Menu overflow &gt; Remove in the row of the service ID. Click Remove Service ID on the confirmation dialog.  Remove the API key by clicking the service ID. On the service ID page, click API keys. Locate the API key being used by the application in the API keys list. CLick  Menu overflow &gt; Remove in the row of the API key. Click Remove API key on the confirmation dialog.Note: Revoking a service ID or API key in use by any Kafka client might not disable access for the application immediately. The API key is stored in a token cache in Kafka which has a 23 hour expiration period. When the token cache expires, it is refreshed from IBM Cloud Private and any revoked service IDs or API keys are reflected in the new token cache, causing application access be be disabled. To immediately disable application access, you can force a refresh of the Kafka token cache by restarting each Kafka broker. To do this without causing downtime, you can patch the stateful set by using the following command: kubectl -n &lt;namespace&gt; patch sts &lt;release_name&gt;-ibm-es-kafka-sts -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"restarted\":\"123\"}}}}}' This does not make changes to the broker configuration, but it still causes the Kafka brokers to restart one at a time, meaning no downtime is experienced. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "2019.1.1",
        "excerpt":"Network connections into the IBM Event Streams deployment are secured using TLS. By default, data within the Event Streams deployment is not encrypted. To secure this data, you must ensure that any storage and communication channels are encrypted as follows:   Encrypt data at rest by using disk encryption or encrypting volumes using dm-crypt.  Encrypt internal network traffic within the cluster with IPSec:          On AMD64 platforms (x86-64), you must use IBM Cloud Private version 3.1.1 or later to encrypt traffic with IPsec.      On s390x platforms, you must use IBM Cloud Private version 3.1.2 or later to encrypt traffic with IPsec.        Encrypt messages in applications.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/security/encrypting-data/",
        "teaser":null},{
        "title": "Configuring secure JMX connections",
        "collection": "2019.1.1",
        "excerpt":"You can set up the Kafka broker JMX ports to be accessible to secure connections from within the IBM Cloud Private cluster. This grants applications deployed inside the cluster read-only access to Kafka metrics. By default, Kafka broker JMX ports are not accessible from outside the Kubernetes pod. To enable access, ensure you select the Enable secure JMX connections check box in the Kafka broker settings section when installing Event Streams. You can also enable secure JMX connections for existing installations by modifying your settings. When access is enabled, you can configure your applications to connect securely to the JMX port as follows. Enabling external connections When Event Streams is installed with the Enable secure JMX connections option, the Kafka broker is configured to start the JMX port with SSL and authentication enabled. The JMX port 9999 is opened on the Kafka pod and is accessible from within the cluster using the hostname &lt;releasename&gt;-ibm-es-kafka-broker-svc-&lt;brokerNum&gt;.&lt;namespace&gt;.svc To retrieve the local name of the service you can use the following command (the results do not have the &lt;namespace&gt;.svc suffix): kubectl -n &lt;namespace&gt; get services To connect to the JMX port, clients must use the following Java options:   javax.net.ssl.trustStore=&lt;path to trustStore&gt;  javax.net.ssl.trustStorePassword=&lt;password for trustStore&gt;In addition, clients must provide a username and password when initiating the JMX connection. Providing configuration values When secure JMX connections is enabled, a Kubernetes secret named &lt;releasename&gt;ibm-es-jmx-secret is created inside the Event Streams namespace. The secret contains the following content:             Name      Description                  truststore.jks      A Java truststore containing the certificates needed for SSL communication with the Kafka broker JMX port.              trust_store_password      The password associated with the truststore.              jmx_username      The user that is authenticated to connect to the JMX port.              jmx_password      The password for the authenticated user.      The Kubernetes secret’s contents must then be mounted as volumes and environment variables inside the application pod to provide the required runtime configuration to create a JMX connection. For example: apiVersion: v1kind: Podspec:  containers:    - name: container1      env:        - name: jmx_username          secretRef:            secretName: es-secret      ...      volumeMounts:        - name: es-volume          mountPath: /path/to/volume/on/pod/file/system  ...  volumes:    - name: es-volume      fromSecret:        secretName: es-secret        items:          - name: truststore.jks            path: jks.jksIf the connecting application is not installed inside the Event Streams namespace, it must be copied to the application namespace using the following command: kubectl -n &lt;releaseNamespace&gt; get secret &lt;releasename&gt;ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;applicationNamespace&gt; apply -f -","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/security/secure-jmx-connections/",
        "teaser":null},{
        "title": "Network policies",
        "collection": "2019.1.1",
        "excerpt":"The following tables provide information about the permitted network connections for each Event Streams pod. Kafka pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods, REST Producer pods, and Geo-replicator pods to port 8084      Kafka access              TCP      REST API pods to port 7070      Querying Kafka status              TCP      Proxy pods to port 8093      Proxied Kafka traffic              TCP      Other Kafka pods to port 9092      Kafka cluster traffic              TCP      To port 8081 on the IBM Cloud Private master host      Prometheus collecting metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      ZooKeeper pods on port 2181      Cluster metadata access              TCP      Other Kafka pods on port 9092      Kafka cluster traffic              TCP      Index Manager pods on port 8080      Kafka metrics              TCP      Access Controller pods on port 8443      Security API access              TCP      Collector pods on port 7888      Submitting metrics              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      ZooKeeper pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods and REST API pods to port 2181      ZooKeeper traffic              TCP      Other ZooKeeper pods to ports 2888 and 3888      ZooKeeper cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Other ZooKeeper pods on port 2888 and 3888      ZooKeeper cluster traffic      Geo-replicator pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods to port 8083      Geo-replicator API traffic              TCP      Other geo-replicator pods to port 8083      Geo-replicator cluster traffic              TCP      To port 8080 on the IBM Cloud Private master host      Allow Prometheus to collect metrics        Outgoing connections permitted: AnyAdministration UI pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST proxy pods on port 9080      REST API access              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access              TCP      Access Controller pods on port 8443      Access Controller API access              TCP      Port 4300 on the IBM Cloud Private master host      ICP identity API access      Administration server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 9080      Proxied REST API calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on ports 8084 and 7070      Kafka admin access              TCP      Index Manager pods on port 9080      Metric API access              TCP      Geo-replicator pods on port 8083      Geo-replicator API access              TCP      ZooKeeper pods on port 2181      ZooKeeper admin access              TCP      Anywhere      Coordination with REST API in other ES instances              UDP      Anywhere on port 53 on the IBM Cloud Private master host      Coordination with REST API in other ES instances      REST producer server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 8080      Proxied REST Producer calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8084      Sending Kafka messages      REST proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST API pods on port 9080      Proxying REST API calls              TCP      REST Producer pods on port 8080      Proxying REST Producer calls      Collector pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 7888      Receiving metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8080      Prometheus connections      Network proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on port 8093      Kafka client traffic              TCP      REST proxy pods on port 9080      Kafka admin      Access Controller pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods, REST API pods, and UI pods to port 8443      Allow components to make auth checks        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      Index manager pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 8080      Receiving metrics              TCP      Elastic and REST API pods to port 9080      Metrics access        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Elastic pods on port 9200      Elasticsearch admin access              TCP      REST proxy pods on port 9080      REST API access      Elasticsearch pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Index Manager pods to port 9200      Elasticsearch admin access              TCP      Other ElasticSearch pods to port 9300      ElasticSearch cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Index Manager pods on port 9080      Elastic admin              TCP      Other ElasticSearch pods on port 9300      ElasticSearch cluster traffic      Install jobs pod       Incoming connections permitted: None         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access      Telemetry pod       Incoming connections permitted: None         Outgoing connections permitted: Any   ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/security/network-policies/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "2019.1.1",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR Information Portal  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement. Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settings using the IBM Event Streams CLI. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemUser roles can be used to control access to data stored in IBM Event Streams accessed over these APIs. In addition, the Kubernetes APIs can be used to access cluster configuration and resources, including but not limited to logs that may contain message data. Access and autorization controls can be used to control which users are able to access this cluster level information. Data Processing Encryption of connection to IBM Event Streams: Connections to IBM Event Streams are secured using TLS. When deploying IBM Event Streams, the default setting for the charts .Values.global.tls.type is “selfsigned”. In this case, a self signed certificate is generated for use creating secure connections. Alternatively, .Values.global.tls.type can be set to “provided” and the TLS certificate (.Values.global.tls.cert), TLS private key (.Values.global.tls.key) and CA certificate (.Values.global.tls.cacert) can be specified to use an existing configuration. If a self signed certificate is used, a certificate and key are generated for each installation of IBM Event Streams and stored securely within a Kubernetes secret. Clients can access the public key via any web browser in the usual manner.If the certificate is provided, you are responsible for provisioning this certificate, for ensuring it is trusted by the clients you will use and for protecting the key. Encryption of connections within IBM Event Streams: Data in motion between pods the IBM Cloud Private deployment should be encrypted using IPSec. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "2019.1.1",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster to maintain the correct message order on the geo-replicated topic. This behavior continues even when geo-replication is paused. You can set up geo-replication by using the IBM Event Streams UI or CLI. When replication is set up and working, you can switch to another cluster when needed. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "2019.1.1",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Prepare your destination cluster by setting the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Decide whether you want to include message history in the geo-replication, or only copy messages from the time of setting up geo-replication. By default, the message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  Decide whether the replicated topics on the destination cluster should have the same name as their corresponding topics on the origin cluster, or if a prefix should be added to the topic name. The prefix is the release name of the origin cluster. By default, the replicated topics on the destination cluster have the same name.Preparing destination clusters Before you can set up geo-replication and start replicating topics, you must configure the number of geo-replication workers on the destination cluster. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. You can use the same approach to determine the number as used when setting the number of brokers for your installation. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of installing IBM Event Streams, or you can modify an existing installation, even if you already have geo-replication set up and running on that installation. Configuring a new installation If you are installing a new IBM Event Streams instance for use as a destination cluster, you can specify the number of workers when configuring the installation. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI You have the option to specify the number of workers during the installation process on the Configure page. Go to the Geo-replication section and specify the number of workers in the Geo-replicator workers field. Using the CLI You have the option to specify the number of workers during the installation process by adding the --set replicator.replicas=&lt;number-of-workers&gt; to your helm install command. Configuring an existing installation If you decide to use an existing IBM Event Streams instance as a destination cluster, or want to change the number of workers on an existing instance used as a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing IBM Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Go to the Geo-replication settings section and modify the Geo-replicator workers field to the required number of workers.Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems.  Click Upgrade.Using the CLI To modify the number of workers by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Use the following helm command to modify the number of workers:helm upgrade --reuse-values --set replicator.replicas=&lt;number-of-workers&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.2.0.tgz --tls","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "2019.1.1",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Go to the Origin locations section, and click Generate connection information for this cluster under Want to replicate topics to this cluster?  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Click Add destination cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Alternatively, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the destination cluster: cloudctl es init  Run the following command to create an API key for your destination cluster:cloudctl es geo-cluster-apikey The command provides the API URL and the API key required for creating a destination cluster, for example:--api-address https://192.0.2.24:32046 --api-key H4C2S6Moq7KuDcYRJaM4Ye_6-XShEnB6JHnATaDaBFQZ  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the origin cluster: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add --api-address &lt;api-url-from-step-3&gt; --api-key &lt;api-key-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.  Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip: You can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.  Optional: Select whether to add a prefix to the name of the new replicated topic that is created on the destination cluster. Click Add prefix to destination topic names to add the release name of the origin cluster as a prefix to the replicated topics.  Optional: Select whether you want to include the message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Click Include message history if you want to include history.  Click Create to create geo-replicators for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.Note: After clicking Create, it might take up to 5 to 10 minutes before geo-replication becomes active.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row as follows:       If topics are being replicated from the cluster you are logged into, the  icon is displayed in the topic’s row. The number in the brackets indicates the number of destination clusters the topic is being replicated to. Clicking  expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:         If topics are being replicated to the cluster you are logged in to, the topics have the following indication that geo-replication is set up for them. Clicking the From &lt;cluster-name&gt; link opens the geo-replication panel with more information about the origin cluster:   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. The command creates one replicator for each topic. To set up more that one geo-replicators at once, list each topic you want to replicate using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.          Optional: You can specify to add a prefix to the name of the new replicated topic that is created on the destination cluster by using the --prefix &lt;prefix-name&gt; option.      Optional: Select whether you want to include message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Set one of the following:                  Use the --from earliest option to include available message history in geo-replication. This means all available message data for the topic is copied.          Use the --from latest option to exclude available message history. This means that only message data from the time of setting up replication is copied.                    For example, to use all options to create the geo-replicators:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt; --from &lt;earliest or latest&gt; --prefix &lt;topic-name-prefix&gt;For example:cloudctl es geo-replicator-create --destination DestinationClusterId --topics MyTopicName1,MyTopicName2 --from latest --prefix GeoReplica- When your geo-replication is set up, you can monitor and manage it. Switching clusters When one of your origin IBM Event Streams clusters experiences problems and goes down, you are notified on the destination cluster UI that the origin cluster is offline. You can switch your applications over to use the geo-replicated topics on the destination cluster as follows.   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right.  Go to the Connect a client tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: Copy the Broker URL to connect an application to this topic.      Certificates: Download a certificate that is required by your Kafka clients to connect securely to this cluster.      API key: To connect securely to IBM Event Streams, your application needs an API key with permission to access the cluster and resources such as topics. Follow the instructions to generate an API key authorized to connect to the cluster, and select what level of access you want it to grant to your resources (topics). You can then select which topics you want included or to include all topics, and set consumer groups as well.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Decide whether you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic.       To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute         To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --execute   These methods also avoid the need to make code changes to your client application. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "2019.1.1",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming the copying of data for each topic, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name that includes the helm release name.      The health of the geo-replication for that origin cluster: CREATING, PAUSED, STOPPING, ASSIGNING, OFFLINE, and ERROR.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See details in the Origin locations section.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend copying of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication from the origin cluster.       More options &gt; Restart failed replicators: To restart geo-replication from the origin cluster for geo-replicators that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: RUNNING, RESUME, RESUMING, PAUSING, REMOVING, and ERROR. When the status is ERROR, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See the section Destination locations.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for which you want to manage geo-replication, and choose from one of the following options:           More options &gt; Pause running replicator: To pause a geo-replicator and suspend copying of data to the destination cluster.      Resume button: To resume a geo-replicator for the destination cluster.       More options &gt; Restart failed replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove replicator: To remove a geo-replicator from the destination cluster.      You can take the same actions for all of the geo-replicators in a destination cluster using the  More options menu in the top right when browsing  destination cluster details (for example, pausing all geo-replicators or removing the whole cluster as a destination). Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters  Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination siliconvalley_es_byl6xThe command returns the following information:    Details of destination cluster siliconvalley_es_byl6xCluster ID               Cluster name    REST API URL                 Skip SSL validation?destination_byl6x        destination     https://9.30.119.223:31764   trueGeo-replicator detailsName                               Status    Origin bootstrap servers   Origin topic   Destination topictopic1__to__origin_topic1_evzoo    RUNNING   192.0.2.24:32237           topic1         origin_topic1topic2__to__topic2_vdpr0           PAUSED    192.0.2.24:32237           topic2         topic2topic3__to__topic3_9jc71           ERROR     192.0.2.24:32237           topic3         topic3topic4__to__topic4_nk87o           PENDING   192.0.2.24:32237           topic4         topic4      To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:          cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      You can also remove a cluster as a destination using the following command:  cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;  Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.      Tip: You can use short options instead of spelling out the long version. For example, use -d instead of --destination, or -n instead of --name. Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an error from Kafka that prevents it from continuing to produce or consume, such as an authentication error or all brokers being unavailable, it will stop replicating and report a status of Error. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/georeplication/health/",
        "teaser":null},{
        "title": "Event Streams producer API",
        "collection": "2019.1.1",
        "excerpt":"Event Streams provides a REST API to help connect your existing systems to your Event Streams Kafka cluster. Using the API, you can integrate Event Streams with any system that supports RESTful APIs. The REST producer API is a scalable REST interface for producing messages to Event Streams over a secure HTTP endpoint. Send event data to Event Streams, utilize Kafka technology to handle data feeds, and take advantage of Event Streams features to manage your data. Use the API to connect existing systems to Event Streams, such as IBM Z mainframe systems with IBM z/OS Connect, systems using IBM DataPower Gateway, and so on. Create produce requests from your systems into Event Streams, including specifying the message key, headers, and the topics you want to write messages to. Note: You must have Event Streams version 2019.1.1 or later to use the REST API. Producing messages using REST Use the producer API to write messages to topics. To be able to produce to a topic, you must have the following available:   The URL of the Event Streams API endpoint, including the port number.  The topic you want to produce to.  The API key that gives permission to connect and produce to the selected topic.  The Event Streams certificate.To retrieve the full URL for the Event Streams API endpoint:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Copy the full URL from the Event Streams API endpoint field, including the port number.To create a topic and generate an API key with produce permissions, and to download the certificate:   If you have not previously created the topic, create it now:cloudctl es topic-create --name &lt;topic_name&gt; --partitions 1 --replication-factor 3  Create a service ID and generate an API key:cloudctl es iam-service-id-create --name &lt;serviceId_name&gt; --role editor --topic &lt;topic_name&gt;For more information about roles, permissions, and service IDs, see the information about managing access.  Copy the API key returned by the previous command.  Download the certificate for Event Streams:cloudctl es certificates --format pemYou have now gathered all the details required to use the producer API. You can use the usual languages for making the API call. For example, to use cURL to produce messages to a topic with the producer API, run the curl command as follows: curl -v -X POST -H \"Authorization: Bearer &lt;api_key&gt;\" -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;api_key&gt; is the API key you generated earlier.  &lt;api_endpoint&gt; is the full URL copied from the Event Streams API endpoint field earlier (format https://&lt;host&gt;:&lt;port&gt;)  &lt;topic_name&gt; is the name of the topic you want to produce messages to.For full details of the API, see the API reference. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/connecting/rest-api/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "2019.1.1",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Connectors are available for copying data in both directions. Available connectors   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue.Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. About Kafka Connect When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems. Distributed mode is more appropriate for production use. When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to IBM MQ.When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For performance reasons, it is recommended to run the Kafka Connect worker close to the queue manager to minimise the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "2019.1.1",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. This document contains steps for running the connector in standalone mode for development and test purposes. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.  The Kafka Connect runtime environment that comes as part of an Apache Kafka distribution. These instructions are for Apache Kafka 2.0.0 or later.Downloading the connector You can obtain the Kafka Connect source connector for IBM MQ as follows:   Log in to your IBM Event Streams UI.  Click the Toolbox tab, and click Kafka Connect source connector for IBM MQ.  Download both the connector JAR and the sample connector properties files from the page.Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the connector yourself as described in the README. Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Setting up Apache Kafka To send messages from IBM MQ to IBM Event Streams, create a topic and obtain security information for your Event Streams installation. You then use this information later to configure the connection to your Event Streams instance. You can also send IBM MQ messages to Apache Kafka running locally on your machine, see the public GitHub repository for more details. To obtain security information:   Log in to your IBM Event Streams UI.  Click the Topics tab.  If you have not previously created the topic to use with the connector, create it now by clicking Create topic.  Select the topic in the list of topics.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and choose a location for the downloaded file that can be accessed by the Kafka Connect worker.  Go to the API key section, and follow the instructions to generate an API key authorized to connect to the cluster and produce to the topic.Note: For the distributed worker, the API key will also need to be able to write to the Kafka Connect framework’s internal topics. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. To provide connection details for IBM MQ, use the sample connector properties file you downloaded (mq-source.properties). Create a copy of it and save it to the location where you have the connector JAR file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.For example: mq.queue.manager=QM1mq.connection.name.list=localhost(1414)mq.channel.name=MYSVRCONNmq.queue=MYQSOURCEmq.user.name=alicemq.password=passw0rdtopic=TSOURCESee the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Configuring the connector to connect to IBM Event Streams or Apache Kafka To provide the connection details for your Kafka cluster, the Kafka distribution includes a file called connect-standalone.properties. Edit the file to include the following connection information:   A list of one or more Kafka brokers for bootstrapping connections.  Whether the cluster requires connections to use SSL/TLS.  Authentication credentials if the cluster requires clients to authenticate.To connect to IBM Event Streams, you will need the broker URL and security details you collected earlier when you configured IBM Event Streams. The following example shows the required properties for the Kafka Connect standalone properties file: bootstrap.servers=&lt;broker_url&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=&lt;certs.jks_file_location&gt;producer.ssl.truststore.password=&lt;truststore_password&gt;producer.sasl.mechanism=PLAINproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";You replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path of your downloaded truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key. Note: If you are running Apache Kafka locally you can use the default connect-standalone.properties file. Generate a consumer application To test the connector you will need an application to consume events from the your topic.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Click Generate application under Starter application  Enter a name for the application  Select only Consume messages  Select Choose existing topic and choose the topic you provided in the MQ connector configuration  Click Generate  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningRunning the connector   Open a terminal window and change to the Kafka root directory. Start the connector worker as follows, replacing the &lt;path-to-*&gt; and &lt;jar-version&gt; placeholders:     CLASSPATH=&lt;path-to-connector-jar&gt;/kafka-connect-mq-source-&lt;jar-version&gt;-jar-with-dependencies.jar bin/connect-standalone.sh config/connect-standalone.properties &lt;path-to-mq-properties&gt;/mq-source.properties        The log output will include the following messages that indicate the connector worker has started and successfully connected to IBM MQ:      INFO Created connector mq-source INFO Connection to MQ established        Navigate to the UI of the sample application you generated earlier and start consuming messages from IBM Event Streams.  To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;The messages are printed by the Kafka console consumer, and are transferred from the IBM MQ queue into the Kafka topic using the queue manager you configured. NOTE: Messages sent to the IBM MQ queue can also be viewed by clicking the Topics tab in the IBM Event Streams UI and selecting your topic name. Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "2019.1.1",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. This document contains steps for running the connector in standalone mode for development and test purposes. The Kafka Connect sink connector for IBM MQ is supported in IBM Event Streams 2018.3.1 and later. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.  The Kafka Connect runtime environment that comes as part of an Apache Kafka distribution. These instructions are for Apache Kafka 2.0.0 or later.Downloading the connector You can obtain the Kafka Connect sink connector for IBM MQ as follows:   Log in to your IBM Event Streams UI.  Click the Toolbox tab, and click Kafka Connect sink connector for IBM MQ.  Download both the connector JAR and the sample connector properties files from the page.Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the connector yourself as described in the README. Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and put messages on a queue. Setting up Apache Kafka To send messages from IBM Event Streams to IBM MQ, create a topic and obtain security information for your Event Streams installation. You then use this information later to configure the connection to your Event Streams instance. You can also send IBM MQ messages to Apache Kafka running locally on your machine, see the public GitHub repository for more details.   Log in to your IBM Event Streams UI.  Click the Topics tab.  If you have not previously created the topic to use with the connector, create it now by clicking Create topic.  Select the topic in the list of topics.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and choose a location for the downloaded file that can be accessed by the Kafka Connect worker.  Go to the API key section and follow the instructions to generate an API key authorized to connect to the cluster, and to produce to and consume from the topic.Note: For the distributed worker, the API key will also need to be able to write to the Kafka Connect framework’s internal topics. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. To provide connection details for IBM MQ, use the sample connector properties file you downloaded (mq-sink.properties). Create a copy of it and save it to the location where you have the connector JAR file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.For example: mq.queue.manager=QM1mq.connection.name.list=localhost(1414)mq.channel.name=MYSVRCONNmq.queue=MYQSINKmq.user.name=alicemq.password=passw0rdtopics=TSINKSee the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Configuring the connector to connect to IBM Event Streams or Apache Kafka To provide the connection details for your Kafka cluster, the Kafka distribution includes a file called connect-standalone.properties. Edit the file to include the following connection information:   A list of one or more Kafka brokers for bootstrapping connections.  Whether the cluster requires connections to use SSL/TLS.  Authentication credentials if the cluster requires clients to authenticate.To connect to IBM Event Streams, you will need the broker URL and security details you collected earlier when you configured IBM Event Streams. The following example shows the required properties for the Kafka Connect standalone properties file: bootstrap.servers=&lt;broker_url&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=&lt;certs.jks_file_location&gt;consumer.ssl.truststore.password=&lt;truststore_password&gt;consumer.sasl.mechanism=PLAINconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";You replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path of your downloaded truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key. Note: If you are running Apache Kafka locally you can use the default connect-standalone.properties file. Generate a producer application To test the connector you will need an application to produce events to your topic.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Click Generate application under Starter application  Enter a name for the application  Select only Produce messages  Select Choose existing topic and choose the topic you provided in the MQ connector configuration  Click Generate  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningRunning the connector   Open a terminal window and change to the Kafka root directory. Start the connector worker as follows, replacing the &lt;path-to-*&gt; and &lt;jar-version&gt; placeholders:     CLASSPATH=&lt;path-to-connector-jar&gt;/kafka-connect-mq-sink-&lt;jar-version&gt;-jar-with-dependencies.jar bin/connect-standalone.sh config/connect-standalone.properties &lt;path-to-mq-properties&gt;/mq-sink.properties        The log output will include the following messages that indicate the connector worker has started and successfully connected to IBM MQ:      INFO Created connector mq-sink INFO Connection to MQ established        Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "2019.1.1",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka shell scripts and configuration files. To download the file and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the Support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.11-2.1.1.tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.  Download and copy the connector .properties file to the z/OS System as well. Depending on the connector you want to use:          Download the source connector files      Download the sink connector files      Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts from ISO8859-1 to EBCDIC encoding:   bin/connect-standalone.sh  bin/kafka-run-class.shIf you want to run a distributed Kafka Connect worker, convert bin/connect-distributed.sh instead of bin/connect-standalone.sh. To convert the files:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.11-2.0.0.tar  Change to the resulting kafka_&lt;version&gt; directory.  Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.sh  Ensure that the worker and connector configuration files are also in EBCDIC encoding.   If you are starting with files from the Kafka distribution, convert them by following the same steps as described here for the shell scripts.   For example, if you want to use the MQ source connector in a standalone setup, convert the config/connect-standalone.properties file from your Kafka distribution, and also convert your mq-source.properties file.   If you are editing the files directly on z/OS, you are already using EBCDIC.Configuring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.0.1-jar-with-dependencies.jar. If you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/connecting/mq/zos/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "2019.1.1",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Using the UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready. To find out more about the problem:   Click the message to expand it, and then expand the section for the component that does not have a green tick next to it.  Click the Pod is not ready link to open more details about the problem. The link opens the IBM Cloud Private UI. Log in as an administrator.  To understand why the IBM Event Streams resource is not available, click the Events tab to view details about the cause of the problem.  For more detailed information about the problem, click the Overview tab, and click  More options &gt; View logs on the right in the Pod details panel.  For guidance on resolving common problems that might occur, see the troubleshooting section.Using the CLI You can check the health of your IBM Event Streams environment using the Kubernetes CLI.   Ensure you have the Kubernetes command line tool installed, and configure access to your cluster.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:kubectl -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.  To retrieve further details about the pods, including events affecting them, use the following command:kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about using the kubectl command for debugging, see the Kubernetes documentation. Note: After a component restarts, the kubectl command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the –previous option to the kubectl logs command. Tip: You can also use the management logging service, or Elastic Stack, deployed by IBM Cloud Private to find more log information. Setting up the built-in Elastic Stack is part of the installation planning tasks. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "2019.1.1",
        "excerpt":"Monitoring the health of your Kafka cluster ensures your operations run smoothly. Event Streams collects metrics from all of the Kafka brokers and exports them to a Prometheus-based monitoring platform. The metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. You can use the metrics as follows:   View a selection of metrics on a preconfigured dashboard in the Event Streams UI.      Create dashboards in the Grafana service that is provided in IBM Cloud Private. You can download example Grafana dashboards for Event Streams from GitHub.     For more information about the monitoring capabilities provided in IBM Cloud Private, including Grafana, see the IBM Cloud Private documentation.     Create alerts so that metrics that meet predefined criteria are used to send notifications to emails, Slack, PagerDuty, and so on. For an example of how to use the metrics to trigger alert notifications, see how you can set up notifications to Slack.You can also use external monitoring tools to monitor the deployed Event Streams Kafka cluster. For information about the health of your topics, check the producer activity dashboard. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitor dashboard.   Log in to Event Streams as an administrator  Click the Monitor tab. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Click a chart to drill down into more detail.  Click 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/cluster-health/",
        "teaser":null},{
        "title": "Monitoring topic health",
        "collection": "2019.1.1",
        "excerpt":"To gain an insight into the overall health of topics and highlight potential performance issues with systems producing to Event Streams, you can use the Producer dashboard provided for each topic. The dashboard displays aggregated information about producer activity for the selected topic through metrics such as message produce rates, message size, and an active producer count. The dashboard also displays information about each producer that has been producing to the topic. You can expand an individual producer record to gain insight into its performance through metrics such as messages produced, message size and rates, failed produce requests and any occurences where a producer has exceeded a broker quota. The information displayed on the dashboard can also be used to provide insight into potential causes when applications experience issues such as delays or ommissions when consuming messages from the topic. For example, highlighting that a particular producer has stopped producing messages, or has a lower message production rate than expected. Important: The producers dashboard is intended to help highlight producers that may be experiencing issues producing to the topic. You may need to investigate the producer applications themselves to identify an underlying problem. To access the dashboard:   Log in to Event Streams as an administrator.  Click the Topics tab.      Select the topic name from the list you want to view information about.The Producers tab is displayed with the dashboard and details about each producer. You can refine the time period for which information is displayed. You can expand each producer to view details about their activity.     Note: When a new client starts producing messages to a topic, it might take up to 5 to 10 minutes before information about the producer’s activity appears in the dashboard. In the meantime, you can go to the Messages tab to check whether messages are being produced.   Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/topic-health/",
        "teaser":null},{
        "title": "Monitoring with external tools",
        "collection": "2019.1.1",
        "excerpt":"You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. You must configure your installation to set up access for external monitoring tools. For examples about setting up monitoring with external tools such as Datadog, Prometheus, and Splunk, see the tutorials page. If you have a tool or service you want to use to monitor your clusters, you can raise a request. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/external-monitoring/",
        "teaser":null},{
        "title": "Modifying Kafka broker configurations",
        "collection": "2019.1.1",
        "excerpt":"You can use the IBM Event Streams CLI to dynamically modify brokers and cluster-wide configuration settings for your IBM Event Streams instance. You can also use the IBM Event Streams CLI together with a ConfigMap to modify static (read-only) configuration settings. Configuration options For a list of all configuration settings you can specify for Kafka brokers, see the Kafka documentation. Some of the broker configuration settings can be updated without restarting the broker, while others require a restart:   read-only: Requires a broker restart for the update to take effect.  per-broker: Can be updated dynamically for each broker without a broker restart.  cluster-wide: Can be updated dynamically as a cluster-wide default, or as a per-broker value for testing purposes.See the Dynamic Update Mode column in the Kafka documentation for the update mode of each broker configuration. Note: You cannot modify the following properties.   broker.id  listeners  zookeeper.connect  advertised.listeners  inter.broker.listener.name  listener.security.protocol.map  authorizer.class.name  principal.builder.class  sasl.enabled.mechanisms  log.dirs  inter.broker.protocol.version  log.message.format.versionModifying broker and cluster settings You can modify per-broker and cluster-wide configuration settings dynamically (without a broker restart) by using the IBM Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  To modify a per-broker configuration setting: cloudctl es broker-config --broker &lt;broker_id&gt; --config &lt;name&gt;=&lt;value&gt;  To modify a cluster-wide configuration setting: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt;You can also update your read-only configuration settings that require a broker restart by using the IBM Event Streams CLI. Note: Read-only settings require a ConfigMap to be set. If you did not create and specify a ConfigMap during the installation process, you can create a ConfigMap later with the required Kafka configuration settings or create a blank one to use later. Use the following command to make the ConfigMap available to your IBM Event Streams instance if you did not create a ConfigMap during installation: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. You can use the IBM Event Streams CLI to modify read-only configuration settings as follows: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt; --static-config-all-brokers ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/modifying-configs/",
        "teaser":null},{
        "title": "Modifying installation settings",
        "collection": "2019.1.1",
        "excerpt":"You can modify the configuration settings for your existing Event Streams installation by using the UI or the command line. The configuration changes are applied by updating the Event Streams chart. For example, you might need to modify settings to scale your installation due to changing requirements. Using the UI You can modify any of the configuration settings you specified during installation, or define values for ones previously not set at the time of installation. To modify configuration settings by using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Modify the values for the configuration settings you want to change.For example, to set the number of geo-replication workers to 4, go to the Geo-replication settings section and set the Geo-replicator workers field to 4.  Click Upgrade.Using the CLI You can modify any of the parameters you specified during installation, or define values for ones previously not set at the time of installation. For a list of all parameters, see the chart README file. To modify any of the parameter settings by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Use the following helm command to modify the value of a parameter:helm upgrade --reuse-values --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.2.0.tgz --tls","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/modifying-installation/",
        "teaser":null},{
        "title": "Running Helm upgrade commands",
        "collection": "2019.1.1",
        "excerpt":"You can use the helm upgrade command to upgrade your Event Streams version, or to modify configuration settings for your Event Streams installation. To run Helm upgrade commands, you must have a copy of the original Helm charts file that you used to install IBM Event Streams. To retrieve the charts file using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Catalog in the top navigation menu.  If you are using the Community Edition, search for ibm-eventstreams-dev and select it from the result. If you are using Event Streams, search for ibm-eventstreams-prod and select it from the result.  Select the latest version number from the drop-down list on the left.  To download the file, go to the SOURCE &amp; TAR FILES section on the left and click the link. The ibm-eventstreams-dev-&lt;version&gt;.tgz file is downloaded.Alternatively, if you downloaded IBM Event Streams from IBM Passport Advantage, you can also retrieve the charts file by looking for a file called ibm-eventstreams-prod-&lt;version&gt;.tgz within the downloaded archive. If you no longer have a copy, you can download the file again from IBM Passport Advantage. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/helm-upgrade-command/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "2019.1.1",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. You can start with the default installation parameters when deploying Event Streams, and test the system with a workload that is representative of your requirements. For this purpose, IBM Event Streams provides a workload generator application to test message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Important: To take full advantage of the scaling capabilities described in this topic, and avoid potential bottlenecks in high throughput environments, your IBM Cloud Private cluster requires an external load balancer configured (as mentioned in the prerequisites). Increase the number of Kafka brokers in the cluster To set this at the time of installation, you can use the --set kafka.brokers=&lt;NUMBER&gt; option in your helm install command if using the CLI, or enter the number in the Kafka brokers field of the Configure page if using the UI. To modify the number of Kafka brokers for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.brokers=&lt;NUMBER&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the CPU limit available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. A description of the syntax for these values can be found in the Kubernetes documentation. Increase the amount of memory available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values into the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI. The syntax for these values can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the memory available to supporting systems If you have significantly increased the memory available to Kafka brokers, you will likely need to make a similar increase in the memory available to the other components that support the Kafka brokers. Ensure you consider the following two components. The metrics reporter component captures the monitoring statistics for cluster, broker, and topic activity. The memory requirements for this component will increase with the number of topic partitions in the cluster, and the throughput on those topics. To set this at the time of installation, you can use the following options:--set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt;--set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; A description of the syntax for these values can be found in the Kubernetes documentation. The message indexer indexes the messages on topics to allow them to be searched in the IBM Event Streams UI. The memory requirements for this component will increase with the cluster message throughput. To set this at the time of installation, you can use the --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt; option in your helm install command if using the CLI, or enter the values into the Memory limits for Index Manager nodes fields of the Configure page if using the UI. The syntax for the container memory limits can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values  --set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt; --set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt;  &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Custom JVM tuning for Kafka brokers If you have specific requirements, you might need to further tune the JVMs running the Kafka brokers, such as modifying the garbage collection policies. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. To provide custom JVM parameters at the time of installation, you can use --set kafka.heapOpts=&lt;JVMOPTIONS&gt; option in your helm install command. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.heapOpts=&lt;JVMOPTIONS&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Use a faster storage class for PVCs used by Kafka brokers The speed of the storage available to Kafka brokers will impact performance. Set this at the time of installation with the --set kafka.persistence.dataPVC.storageClassName=&lt;STORAGE_CLASS&gt; option in your helm install command if using the CLI, or by entering the required storage class into the Storage class name field of the Kafka persistent storage settings section of the Configure page if using the UI. For more information about available storage classes, see the IBM Cloud Private documentation. Increase the disk space available to each Kafka broker The Kafka brokers will require sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods or sizes, and more topic partitions. Set this at the time of installation with the --set kafka.persistence.dataPVC.size=&lt;SIZE&gt; option in your helm install command if using the CLI, or by entering the required persistence size into the Size field of the Kafka persistent storage settings section of the Configure page if using the UI. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "2019.1.1",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both. Kafka quotas are supported in IBM Event Streams 2018.3.1 and later. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For more information about quotas, see the Kafka documentation. Setting quotas You can set quotas by using the Event Streams CLI as follows:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI:cloudctl es init  Use the entity-config command option to set quotas as follows.Decide what you want to limit by using a quota type, and set it with the --config &lt;quota_type&gt;option, where &lt;quota_type&gt; can be one of the following:   producer_byte_rate - This quota limits the number of bytes that a producer application is allowed to send per second.  consumer_byte_rate - This quota limits the number of bytes that a consumer application is allowed to receive per second.  request_percentage - This quota limits all clients based on thread utilisation.Decide whether you want to apply the quota to users or client IDs. To apply to users, use the --user &lt;user&gt; option. Event Streams supports 2 types of users: actual user principal names, or application service IDs.   A quota defined for a user principal name is only applied to that specific user name. To specify a principal name, you must prefix the value for the --user parameter with u-, for example, --user \"u-testuser1\"  A quota defined for a service ID is applied to all applications that are using API keys that have been bound to the specific service ID. To specify a service ID, you must prefix the value for the --user parameter with s-, for example, --user \"s-consumer_service_id\"To apply to client IDs, use the --client &lt;client id&gt; option. Client IDs are defined in the application using the client.id property. A client ID identifies an application making a request. You can apply the quota setting to all users or client IDs by using the --user-default or --client-default parameters, respectively. Quotas set for specific users or client IDs override default values set by these parameters. By using these quota type and user or client ID parameters, you can set quotas using the following combinations: cloudctl es entity-config --user &lt;user&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --user-default --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client &lt;client id&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client-default --config &lt;quota_type&gt;=&lt;value&gt; Examples For example, the following setting specifies that user u-testuser1 can only send 2048 bytes of data per second:cloudctl es entity-config --user \"u-testuser1\" --config producer_byte_rate=2048 For example, the following setting specifies that all application client IDs can only receive 2048 bytes of data per second:cloudctl es entity-config --client-default --config consumer_byte_rate=2048 The cloudctl es entity-config command is dynamic, so any quota setting is applied immediately without the need to restart clients. Note: If you run any of the commands with the --default parameter, the specified quota is reset to the system default value for that user or client ID (which is unlimited).For example:    cloudctl es entity-config --user \"s-consumer_service_id\" --default --config producer_byte_rate ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/administering/quotas/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "2019.1.1",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/intro/",
        "teaser":null},{
        "title": "Diagnosing installation issues",
        "collection": "2019.1.1",
        "excerpt":"To help troubleshoot and resolve installation issues, you can run a diagnostic script that checks your deployment for potential problems. Important: Do not use the script before the installation process completes. Despite a successful installation message, some processes might still need to complete, and it can take up to 10 minutes before IBM Event Streams is available to use. To run the script:   Download the installation-diagnostic-script.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the script as follows: ./installation-diagnostic-script.sh -n &lt;namespace&gt; -r &lt;release-name&gt;If you have been waiting for more than an hour, add the --restartoldpods option to recreate lost events (by default, events are deleted after an hour). This option restarts Failed or Pending pods that are an hour old or more. For example, the following installation has pods that are in pending state, and running the diagnostic script reveals that the issue is caused by not having sufficient memory and CPU resources available to the pods: Starting release diagnostics...Checking kafka-sts pods...kafka-sts pods foundChecking zookeeper-sts pods...zookeeper-sts pods foundChecking the ibm-es-iam-secret API Key...API Key foundChecking for Pending pods...Pending pods found, checking pod for failed events...------------------Name: caesar-ibm-es-kafka-sts-0Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-1Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-2Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------No failed events found for pod caesar-ibm-es-rest-deploy-6ff498d779-stf79------------------Checking for CrashLoopBackOff pods...No CrashLoopBackOff pods foundRelease diagnostics complete. Please review output to identify potential problems.If unable to identify or fix problems, please contact support.","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/diagnosing-installation-issues/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "2019.1.1",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible sypmtoms and causes. IBM Event Streams not available after installation After a successsful installation message is displayed, IBM Event Streams might not be available to use yet. It can take up to 10 minutes before IBM Event Streams is available to use. The IBM Cloud Private installation might return a successful completion message before all Event Streams services start up. If the installation continues to be unavailable, run the installation diognastics scripts. Insufficient system resources You can specify the memory and CPU requirements when IBM Event Streams is installed. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims: occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory: occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU: occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.For example, if each Kafka broker is set to require 80 GB of memory on a system that only has 16 GB available per node, you might see the following error message:  To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). If a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which may reside on the same node. To correct this issue, uninstall IBM Event Streams. Install again using lower resource requirements, or increase the amount of system resources available to the pod. Problems with secrets When using a non-default Docker registry, you might need to provide a secret which stores the user ID and password to access that registry. If there are issues with the secret that holds the user ID and password used to access the Docker registry, the events for a pod will show an error similar to the following.  To resolve this issue correct the secret and install IBM Event Streams again. Installation failure stating object already exists If a secret that does not exist is specified during installation, the process fails even if no secret is required to access the Docker registry. The default Docker image registry at ibmcom does not require a secret specifying the user ID and password. To correct this, install IBM Event Streams again without specifying a secret. If you are using a Docker image registry that does require a secret, attempting to install again might fail stating that an object already exists, for example: Internal service error : rpc error: code = Unknown desc = rolebindings.rbac.authorization.k8s.io \"elh-ibm-es-secret-copy-crb-sys\" already existsDelete the left over object cited and other objects before trying to install again. For instructions, see how to fully clean up after uninstallation. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Full cleanup after uninstallation",
        "collection": "2019.1.1",
        "excerpt":"The uninstallation process might leave behind artifacts that you have to clear manually. Security resources A service ID is created as part of installing Event Streams, which defines the identity for securing communication between internal components. To delete this service ID after uninstalling Event Streams, run the following command: cloudctl iam service-id-delete eventstreams-&lt;release&gt;-service-id -f Kubernetes resources Use the following command to find the list of IBM Event Streams objects associated with the release’s namespace: kubectl get &lt;type&gt; -n &lt;namespace&gt; | grep ibm-es Where type is each of   pods  clusterroles  clusterrolebindings  roles  rolebindings  configmaps  serviceaccounts  statefulsets  deployments  jobs  pods  pvc (see Note later)  secretsThere are also a number of Event Streams objects created in the kube-system namespace. To list these objects run the following command: kubectl get pod -a -n kube-system | grep ibm-es Note: These commands might return objects that should not be deleted. For example, do not delete secrets or system clusterroles if the kubectl output is not piped to grep. Note: If persistent volume claims (PVCs) are deleted (the objects returned when specifying “pvc” in the commands above), the data associated with the PVCs is also deleted. This includes any persistent Kafka data on disk. Consider whether this is the desired result before deleting any PVCs. To find which objects need to be manually cleared look for the following string in the output of the previously mentioned commands: &lt;release&gt;-ibm-es You can either navigate through the IBM Cloud Private cluster management console to Workloads &gt; or Configuration &gt; to find the objects and delete them, or use the following command: kubectl delete &lt;type&gt; &lt;name&gt; -n &lt;namespace&gt; For example, to delete a leftover rolebinding called eventstreams-ibm-eventstreams-secret-copy-crb-ns, run the following command: kubectl delete rolebinding eventstreams-ibm-eventstreams-secret-copy-crb-ns -n es Be cautious of deleting persistent volume claims (PVCs) as the data on the disk that is associated with that persistent volume will also be deleted. This includes Event Streams message data. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/cleanup-uninstall/",
        "teaser":null},{
        "title": "ConsumerTimeoutException when pods available",
        "collection": "2019.1.1",
        "excerpt":"Symptoms Attempts to communicate with a pod results in timeout errors such as kafka.consumer.ConsumerTimeoutException. Causes When querying the status of pods in the Kubernetes cluster, pods show as being in Ready state can still be in the process of starting up. This latency is a result of the external ports being active on the pods before the underlying services are ready to handle requests. The period of this latency depends on the configured topology and performance characteristics of the system in use. Resolving the problem Allow additional time for pod startup to complete before attempting to communicate with it. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/pod-timeout-error/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "2019.1.1",
        "excerpt":"Symptoms The following error message is displayed when setting up replication by using the CLI: FAILEDEvent Streams API request failed:Error response from server. Status code: 400. The resource request is invalid. Missing required parameter topic nameThe message does not provide accurate information about the cause of the error. Causes When providing the list of topics to geo-replicate, you added spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, instead of --topics MyTopicName1, MyTopicName2, MyTopicName3, enter --topics MyTopicName1,MyTopicName2,MyTopicName3. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "2019.1.1",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutExceptionCauses This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "2019.1.1",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log into the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the IBM Cloud Private cluster and initialized the command line tool. Resolving the problem Ensure you log in to the IBM Cloud Private cluster as follows: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;After logging in to IBM Cloud Private, initialize the IBM Event Streams CLI as follows: cloudctl es initFinally, run the operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no ImagePolicies' error",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no ImagePolicies in the \"&lt;name&gt;\" namespace, where &lt;name&gt; is the namespace into which you are deploying the chart. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs if there are no image policies defined for the target namespace. To confirm this, list the policies as follows: kubectl get imagepolicyYou should see a message stating No resources found. Resolving the problem If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioTo apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/no-image-policy/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no matching repositories in the ImagePolicies' error",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no matching repositories in the ImagePolicies. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs when there are image policies defined for the namespace into which the chart is being deployed, but none of them include the required repositories. To confirm this, list the image policies defined as follows: kubectl get imagepolicyFor each image policy, you can check which repositories it includes as follows: kubectl describe imagepolicy &lt;imagePolicyName&gt;If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioResolving the problem To apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/image-policy-missing-repository/",
        "teaser":null},{
        "title": "Chart deployment starts but no helm release is created",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When the IBM Event Streams (not the Community Edition) chart is deployed, the process appears to start successfully but the helm release and set of expected pods are not created. You can confirm the helm release has not been created by running the following command: helm listIn this case you will not see an entry for the Helm release name you provided when you started the deployment process. In addition, you will see that only a single pod is initially created and then subsequently removed after a couple of minutes. You can check which pods are running using the following command: kubectl get podsImmediately after starting the deployment process, you will see a single pod created named &lt;releaseName&gt;-ibm-es-secret-copy-job-&lt;uid&gt;. If you ‘describe’ the pod, you will receive the error message Failed to pull image, and a further message stating either Authentication is required or unauthorized: BAD_CREDENTIAL. After a couple of minutes this pod is deleted and no pods will be reported by the kubectl command. If you query the defined jobs as follows, you will see one named &lt;releaseName&gt;-ibm-es-secret-copy-job: kubectl get jobsFinally if you ‘describe’ the job as follows, you will see that it reports a failed pod status: kubectl describe job &lt;releaseName&gt;-ibm-es-secret-copy-jobFor example, the job description will include the following: Pods Statuses:            0 Running / 0 Succeeded / 1 FailedCauses This situation occurs if there a problem with the image pull secret being used to authorize access to the Docker image repository you specified when the chart was deployed. When you ‘describe’ the secret copy pod, if you see the error message Authentication is required, this indicates that the secret you specified does not exist. If you see the error message unauthorized: BAD_CREDENTIAL, this indicates that the secret was found but one of the fields present within it is not correct. To confirm which secrets are deployed, run the following command: kubectl get secretsResolving the problem To delete a secret thats not correctly defined, use the following command: kubectl delete secret &lt;secretName&gt;To create a new secret for use in chart deployment, run the following command: kubectl create secret docker-registry &lt;secretName&gt; --docker-server=&lt;serverAddress:serverPort&gt; --docker-username=&lt;dockerUser&gt; --docker-password=&lt;dockerPassword&gt; --docker-email=&lt;yourEmailAddress&gt;For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=John.Smith@ibm.comAfter you have confirmed that the required secret is correctly defined, re-run the chart deployment process. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/no-helm-release-is-created/",
        "teaser":null},{
        "title": "The Messages page is blank",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Messages page loads, but then becomes blank when viewing any topic (either real or simulated). Causes The vm.max_map_count property on one or more of your nodes is below the required value of 262144. This causes the message indexing capabilities to fail, resulting in this behaviour. Resolving the problem Ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node:     sudo sysctl -w vm.max_map_count=262144    echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/messages-page-blank/",
        "teaser":null},{
        "title": "Unable to connect to Kafka cluster",
        "collection": "2019.1.1",
        "excerpt":"Symptoms The following error is displayed when trying to connect to your Kafka cluster using SSL, for example, when running the Kafka Connect source connector for IBM MQ: org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failedCauses The Java process might replace the IP address of your cluster with the corresponding hostname value found in your /etc/hosts file. For example, to be able to access Docker images from your IBM Cloud Private cluster, you might have added an entry in your /etc/hosts file that corresponds to the IP address of your cluster, such as 192.0.2.24 mycluster.icp. In such cases, the following Java exception is displayed after the previously mentioned error message: Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching XXXXX found.Resolving the problem If you see the exception mentioned previously, comment out the hostname value in your /etc/hosts file to solve this connection issue. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/kafka-connection-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "Geo-replication fails to start with 'Could not connect to origin cluster' error",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When geo-replicating a topic to a destination cluster with 2 or more geo-replication worker nodes, the topic replication fails to start. The Event Streams UI reports the following error: Could not connect to origin cluster.In addition, the logs for the replicator worker nodes contain the following error message: org.apache.kafka.connect.errors.ConnectException: SSL handshake failedCauses The truststore on the geo-replication worker node that hosts the replicator task does not contain the certificate for the origin cluster. Resolving the problem You can either manually add the certificate to the truststore in each of the geo-replicator worker nodes, or you can scale the number of geo-replicator worker nodes down to 1 if suitable for your setup. Manually adding certificates To manually add the certificate to the truststore in each of the geo-replicator worker nodes:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersA list of destination cluster IDs are displayed. Find the name of the destination cluster you are attempting to geo-replicate topics to.  Retrieve information about the destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;The failed geo-replicator name is in the list of geo-replicators returned.  Log into the destination cluster, and use kubectl exec to run the keytool command to import the certificate into the truststore in each geo-replication worker node:    kubectl exec -it -n &lt;namespace&gt; -c replicator \\&lt;releaseName&gt;-ibm-es-replicator-deploy-&lt;replicator-pod-id&gt; \\-- bash -c \\\"keytool -importcert \\-keystore /opt/replicator/security/cacerts \\-alias &lt;geo-replicator_name&gt; \\-file /etc/consumer-credentials/cert_&lt;geo-replicator_name&gt; \\-storepass changeit \\-trustcacerts -noprompt\"        The command either succeeds with a \"Certificate was added\" message or fails with a \"Certificate not imported, alias &lt;geo-replicator_name&gt; already exists\" message. In both cases, the truststore for that pod is ready to be used.     Repeat the command for each replicator worker node to ensure the certificate is imported into the truststore on all replicator pods.  Log in to the origin cluster, and restart the failed geo-replicator using the following cloudctl command:cloudctl es geo-replicator-restart -d &lt;geo-replication-cluster-id&gt; -n  &lt;geo-replicator_name&gt;Scaling the number of nodes To scale the number of geo-replicator worker nodes to 1:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following kubectl command:kubectl scale --replicas 1 deployment &lt;releaseName&gt;-ibm-es-replicator-deploy","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/georeplication-connect-error/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "2019.1.1",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes The most likely cause of this problem is that the user attempting to authenticate is part of an IBM Cloud Private team that has not been associated with the  Event Streams instance. Resolving the problem Configure the IBM Cloud Private team that the user is part of to work with the Event Streams instance by running the iam-add-release-to-team CLI command. Run the command as follows: cloudctl es iam-add-release-to-team --namespace &lt;namespace for the Event Streams instance&gt; --release &lt;release name of the Event Streams instance&gt; --team &lt;name of the IBM Cloud Private team that the user is imported into&gt; The user can authenticate and sign in to the Event Streams UI after the command runs successfully. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "Pods fail to start, status is blocked",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When installing Event Streams on IBM Cloud Private 3.1.1 running on Red Hat Enterprise Linux, the following error message is displayed: Error: timed out waiting for the conditionThe installation might still complete, but Event Streams is not available. When you check the status of the pods by using the command kubectl get pods, they are showing as blocked: NAME                                     READY     STATUS    RESTARTS   AGEes1-kafka-ibm-es-secret-copy-job-zkgk8   0/1       Blocked   0          34ses1-kafka-ibm-es-elastic-sts-0           0/1       Blocked   0          34ses1-kafka-ibm-es-elastic-sts-1           0/1       Blocked   0          34ses1-kafka-ibm-es-indexmgr-deploy-69-45   0/1       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-0             0/4       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-1             0/4       Blocked   0          34ses1-kafka-ibm-es-kafka-sts-2             0/4       Blocked   0          34ses1-kafka-ibm-es-proxy-deploy-56bb       0/1       Blocked   0          34ses1-kafka-ibm-es-proxy-deploy-56bbc4     0/1       Blocked   0          34ses1-kafka-ibm-es-rest-deploy-c76f46      0/3       Blocked   0          34ses1-kafka-ibm-es-role-mappings-vjb8      0/1       Blocked   0          34ses1-kafka-ibm-es-ui-deploy-575779998d    0/3       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-0         0/1       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-1         0/1       Blocked   0          34ses1-kafka-ibm-es-zookeeper-sts-2         0/1       Blocked   0          34sWhen you describe the pods for more information, the following error message is displayed: Status:             PendingReason:             AppArmorMessage:            Cannot enforce AppArmor: AppArmor is not enabled on the hostFor example, taking the first pod from the previous list and running kubectl describe provides the following message: kubectl describe pod/es1-kafka-ibm-es-secret-copy-job-zkgk8 -n event-streamsName:               es1-kafka-ibm-es-secret-copy-job-zkgk8...Annotations:        container.apparmor.security.beta.kubernetes.io/ips-copier=runtime/default                    kubernetes.io/psp=ibm-restricted-psp                    seccomp.security.alpha.kubernetes.io/pod=docker/defaultStatus:             PendingReason:             AppArmorMessage:            Cannot enforce AppArmor: AppArmor is not enabled on the hostIP:Controlled By:      Job/es1-kafka-ibm-es-secret-copy-jobCauses Pods fail to start due to a setting in the ibm-restricted-psp PodSecurityPolicy that Event Streams uses to install. The AppArmor strategy plug-in in the pod admission controller is injecting the AppArmor annotations into the pod, and the admission controller blocks the pod from starting. Resolving the problem Upgrading to IBM Cloud Private version 3.1.2 resolves the issue. An alternative solution is to edit the ibm-restricted-psp PodSecurityPolicy by using the following command: kubectl edit PodSecurityPolicy ibm-restricted-psp Remove the following lines: apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/defaultapparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/pods-apparmor-blocked/",
        "teaser":null},{
        "title": "Kafka client applications are unable to connect to the cluster. Users are unable to login to the UI.",
        "collection": "2019.1.1",
        "excerpt":"Symptoms Client applications are unable to produce or consume messages. The logs for producer and consumer applications contain the following error message: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.serversThe Event Streams UI reports the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes An invalid host name or IP address was specified in the External access settings when configuring the installation. Resolving the problem You need to reinstall Event Streams and supply the correct external host name or IP address. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/client-connect-error/",
        "teaser":null},{
        "title": "Rollback fails",
        "collection": "2019.1.1",
        "excerpt":"Symptoms Rolling back from Event Streams version 2019.1.1 to 2018.3.1 (Helm chart version 1.2.0 to 1.1.0) results in an Invalid request (..) field is immutable error. The rollback status shows as failed, for example: $ helm history event-streamsREVISION        UPDATED                         STATUS          CHART                           DESCRIPTION1               Tue Dec 10 16:28:19 2018        SUPERSEDED      ibm-eventstreams-prod-1.1.0     Install complete2               Fri Mar 29 14:15:24 2019        SUPERSEDED      ibm-eventstreams-prod-1.2.0     Upgrade complete3               Fri Mar 29 15:23:46 2019        FAILED          ibm-eventstreams-prod-1.1.0     Rollback \"event-streams\" failed: Job.batch \"event-streams...Attempting to log in to the Event Streams UI on the new port results in the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes The oauth job was not removed before performing the rollback steps. Resolving the problem Run the following command: kubectl -n kube-system get job &lt;release-name&gt;-ibm-es-ui-oauth2-client-reg -o json | jq 'del(.spec.selector)' | jq 'del(.spec.template.metadata.labels)' | kubectl replace --force -f - Where &lt;release-name&gt; is the name that identifies your Event Streams installation. If you are still experiencing issues with your installation, you might need to uninstall Event Streams, and clean up after uninstallation before installing again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/rollback-fails/",
        "teaser":null},{
        "title": "The UI cannot load data",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Monitor and the Topics &gt; Producers tabs do not load, displaying the following message:  Causes The IBM Cloud Private monitoring service might not be installed. In general, the monitoring service is installed by default during the  IBM Cloud Private installation. However, some deployment methods do not install the service. Resolving the problem Install the IBM Cloud Private monitoring service from the Catalog or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/problem-with-piping/",
        "teaser":null},{
        "title": "Failed to read 'log header' errors in Kafka logs",
        "collection": "2019.1.1",
        "excerpt":"Symptoms When Event Streams is configured to use GlusterFS as a storage volume, the Kafka logs show errors containing messages similar to the following: [2020-05-12 06:40:19,249] ERROR [ReplicaManager broker=2] Error processing fetch with max size 1048576 from consumer on partition &lt;TOPIC-NAME&gt;-0: (fetchOffset=10380908, logStartOffset=-1, maxBytes=1048576, currentLeaderEpoch=Optional.empty) (kafka.server.ReplicaManager)org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@a5e333e6`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 95236164.These errors mean that Kafka has been unable to read files from the Gluster volume. This can cause replicas to fall out of sync. Cause See Kafka issue 7282GlusterFS has performance settings that will allow requests for data to be served from replicas when they are not in sync with the leader. This causes problems for Kafka when it attempts to read a replica log segment before it has been fully written by Gluster. Resolving the problem Apply the following settings to each Gluster volume that is used by an Event Streams Kafka broker: gluster volume set &lt;volumeName&gt; performance.quick-read offgluster volume set &lt;volumeName&gt; performance.io-cache offgluster volume set &lt;volumeName&gt; performance.write-behind offgluster volume set &lt;volumeName&gt; performance.stat-prefetch offgluster volume set &lt;volumeName&gt; performance.read-ahead offgluster volume set &lt;volumeName&gt; performance.readdir-ahead offgluster volume set &lt;volumeName&gt; performance.open-behind offgluster volume set &lt;volumeName&gt; performance.client-io-threads offThese settings can be applied while the Gluster volume is online. The Kafka broker will not need to be modified, the broker will be able to read from the volume after the change is applied. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.1.1/troubleshooting/failed-to-read-log/",
        "teaser":null},{
        "title": "Home",
        "collection": "2019.2.1",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "2019.2.1",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka® project. Event Streams version 2019.2.1 includes Kafka release 2.2.0, and supports the use of all Kafka interfaces. IBM Event Streams builds upon the IBM Cloud Private platform to deploy Apache Kafka in a resilient and manageable way. It includes a UI design aimed at application developers getting started with Apache Kafka, as well as users operating a production cluster. IBM Event Streams is available in two editions:   IBM Event Streams Community Edition is a free version intended for trials and demonstration purposes. It can be installed and used without charge.  IBM Event Streams is a paid-for version intended for enterprise use, and includes additional features such as geo-replication.IBM Event Streams features include:   Apache Kafka deployment that maximizes the spread of Kafka brokers across the nodes of the IBM Cloud Private cluster. This creates a highly-available configuration making the deployment resilient to many classes of failure with automatic restart of brokers included.  Health check information and options to resolve issues with your clusters and brokers.  Geo-replication of your topics between clusters to enable disaster recovery and scalability.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition).  UI for browsing messages to help view and filter hundreds of thousands of messages, including options to drill in and see message details from a set time.  Encrypted communication between internal components and encrypted storage by using features available in IBM Cloud Private.  Security with authentication and authorization using IBM Cloud Private.","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "2019.2.1",
        "excerpt":"Releases 2019.2.2 and 2019.2.3 IBM Event Streams 2019.2.2 and 2019.2.3 introduce support for the IBM Cloud Pak for Integration solution. For information about installing Event Streams as a capability, see the IBM Cloud Pak for Integration documentation. Important: Event Streams 2019.2.2 and 2019.2.3 are only available as part of the IBM Cloud Pak for Integration solution. Event Streams 2019.2.2 and 2019.2.3 have the same features as the 2019.2.1 release. Release 2019.2.1 Find out what is new in IBM Event Streams version 2019.2.1. Schemas and schema registry Event Streams now supports schemas to define the structure of message data, and provides an Event Streams schema registry to manage schemas. Connector catalog Event Streams includes a connector catalog listing all connectors that have been verified with Event Streams, including  community-provided connectors and connectors supported by IBM. Enhanced support for Kafka Connect Event Streams now provides enhanced support for Kafka Connect to help integrate external systems with your Event Streams instance. TLS encryption for inter-pod communication You now have the option to encrypt inter-pod communication by using TLS to enhance security. Red Hat Universal Base Image (UBI) All IBM Event Streams images are now based on Red Hat Universal Base Image (UBI) 8. Support for IBM Z has changed Red Hat Universal Base Image (UBI) 8 runs on z13 or later IBM mainframe systems. Kafka version upgraded to 2.2.0 Event Streams version 2019.2.1 includes Kafka release 2.2.0, and supports the use of all Kafka interfaces. IBM Event Streams Community Edition available on Red Hat OpenShift Container Platform You can now install IBM Event Streams Community Edition on the Red Hat OpenShift Container Platform. Support for IBM Cloud Private version 3.2.0 In addition to IBM Cloud Private 3.1.2, Event Streams 2019.2.1 is also supported on IBM Cloud Private 3.2.0. Default resource requirements have changed. See the updated tables for the Event Streams resource requirements. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "2019.2.1",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers (Kafka brokers). The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "2019.2.1",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the desired rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");  props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "2019.2.1",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "2019.2.1",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "2019.2.1",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "2019.2.1",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "2019.2.1",
        "excerpt":"To try out Event Streams, you can install a basic deployment of the Community Edition. IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. Note: These instructions do not include setting up persistent storage, so your data and configuration settings are not retained in the event of a restart. For more features and full IBM support, install IBM Event Streams. On IBM Cloud Private IBM Event Streams Community Edition is included in the IBM Cloud Private catalog.   If you do not have IBM Cloud Private installed already, you can download and install IBM Cloud Private-CE.  Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Create a namespace where you will install your Event Streams instance:          From the navigation menu, click Manage &gt; Namespaces.      Click Create Namespace.      Enter a name for your namespace.      Ensure you have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace.      Click Create.        Click Catalog in the top navigation menu.  Search for ibm-eventstreams-dev and select it from the result. This is the Helm chart for the IBM Event Streams Community Edition. The README is displayed.  Click Configure.  Enter a release name, select the target namespace you created previously, and accept the terms of the license agreement.You can leave all other settings at their default values.  Click Install.  Verify your installation and log in to start using Event Streams.These steps install a basic instance of IBM Event Streams Community Edition that you can try out. You can also configure your installation to change the default settings as required, for example, to set up persistent storage. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). On OpenShift Container Platform If you are using OpenShift Container Platform, you can set up an integration with IBM Cloud Private, and install the IBM Event Streams Community Edition included in the catalog.       Ensure you have the right version of OpenShift installed and integrated with the right version of IBM Cloud Private. For supported versions, see the support table.     For example, install OpenShift 3.11, and integrate it with IBM Cloud Private 3.2.0.     Create a project in OpenShift. This will also create a namespace with the same name in IBM Cloud Private. You then use this namespace to install your Event Streams instance. For example, to create a project by using the web console:          Go to the OpenShift Container Platform web console in your browser by using the URL https://&lt;OpenShift Cluster Address&gt;:&lt;OpenShift Cluster API Port&gt;. The default port is 7443. The master host address is the same as the address for your IBM Cloud Private cluster.      Log in using the user name and password provided to you by your administrator.      Click the Create project button, and type a unique name, display name, and description for the new project. This creates a project and a namespace.        Download and run the setup script as follows:Important: You must perform the following steps by using a terminal opened on the host where the IBM Cloud Private master cluster is installed. If you are on a different host, you must first connect to the host machine by using SSH before logging in.                  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.         The default port is 5443 in IBM Cloud Private 3.1.2, while it is 443 if you are using IBM Cloud Private 3.2.0.             Download the files from GitHub.      Change to the location where you downloaded the files, and run the setup script as follows: ./scc.sh &lt;namespace&gt; Where &lt;namespace&gt; is the namespace (project) you created for your Event Streams installation earlier.        Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-rhel-dev and select it from the result. This is the Helm chart for the IBM Event Streams Community Edition. The README is displayed.  Click Configure.  Enter a release name, select the target namespace you created previously, and accept the terms of the license agreement.You can leave all other settings at their default values.  Click Install.  Verify your installation and log in to start using Event Streams.These steps install a basic instance of IBM Event Streams Community Edition that you can try out. You can also configure your installation to change the default settings as required, for example, to set up persistent storage. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/trying-out/",
        "teaser":null},{
        "title": "Pre-requisites",
        "collection": "2019.2.1",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. Container environment IBM Event Streams 2019.2.1 is supported on the following platforms and systems:             Container platform      Systems                  Red Hat OpenShift Container Platform 3.11 with IBM cloud foundational services 3.2.0.1907* (or later fix pack) and 3.2.1 *      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              Red Hat OpenShift Container Platform 3.10 with IBM Cloud Private 3.1.2*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.1.2, 3.2.0.1907 (or later fix pack), and 3.2.1      - Linux® 64-bit (x86_64) systems - Linux on IBM® z13 or later systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.2.1 has Helm chart version 1.3.0 and includes Kafka version 2.2.0. For an overview of supported component and platform versions, see the support matrix. Ensure you have the following set up for your environment:   If you are installing Event Streams on the OpenShift Container Platform, ensure you have the right version of OpenShift installed and integrated with the right version of IBM Cloud Private. See previous table for supported versions. For example,  install OpenShift 3.11, and integrate it with IBM Cloud Private 3.2.0.      Install and configure IBM Cloud Private.Important: In high throughput environments, ensure you configure your IBM Cloud Private cluster to include an external load balancer and an internal network. These configuration options help take full advantage of Event Streams scaling and Kafka settings, and avoid potential performance bottlenecks. For more information, see the performance planning topic.     Note: IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage.     If you are installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS), ensure your proxy address uses lowercase characters.  If you are installing Event Streams on an IBM Cloud Private cluster deployed on Microsoft Azure, ensure you first register a Service Principal (an application in the Azure Active Directory). For information about creating a Service Principal, see the terraform documentation.  Install the Kubernetes command line tool, and configure access to your cluster.  If you are installing Event Streams on the OpenShift Container Platform, ensure you also install the OpenShift Container Platform CLI.  Install the IBM Cloud Private Command Line Interface (CLI).  Install the Helm CLI required for your version of IBM Cloud Private, and add the IBM Cloud Private internal Helm repository called local-charts to the Helm CLI as an external repository.      For message indexing capabilities (enabled by default), ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node: sudo sysctl -w vm.max_map_count=262144echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.conf     Important: This property might have already been updated by other workloads to be higher than the minimum required.   Hardware requirements The Helm chart for IBM Event Streams specifies default values for the CPU and memory usage of the Apache Kafka brokers and Apache ZooKeeper servers. See the following table for memory requirements of each Helm chart component. Ensure you have sufficient physical memory to service these requirements. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other IBM Event Streams components which might be required to reside on the same node. Ensure you have one IBM Cloud Private worker node per Kafka broker, and a minimum of 3 worker nodes available for use by IBM Event Streams. Ensure each worker node runs on a separate physical server. See the guidance about Kafka high availability for more information. Helm resource requirements The Event Streams helm chart has the following resource requirements based on resource request and limit settings. Requests and limits are Kubernetes concepts for controlling resource types such as CPU and memory.   Requests set the minimum requirements a container requires to be scheduled. If your system does not have the required request value, then your services will not start up.  Limits set the value beyond which a container cannot consume the resource. It is the upper limit within your system for the service.For more information about resource requests and limits, see the Kubernetes documentation. The following table lists the aggregate resource requirements of the Event Streams helm chart. The table includes totals for both request and limit values of all pods and their containers. Each container in a pod has its own request and limit values, but as pods run as a group, these values need to be added together to understand the total requirements for a pod. For details about the requirements for each  container within individual pods, see the tables in the following sections. These are the minimum requirements for an Event Streams installation, and will be used unless you change them when configuring your installation. They are based on the default resource request and limit settings of the chart. Installing with these settings is suitable for a starter deployment intended for testing purposes and trying out Event Streams. For a production setup, ensure you set higher values, and also consider important configuration options for IBM Cloud Private such as setting up a load balancer and an internal network. For more information about planning for a production setup, including requirements for a baseline production environment, see the performance planning topic.             Pod group      Configurable replicas      Total CPU request per pod group (cores)      Total CPU limit per pod group (cores)      Total memory request per pod group (Gi)      Total memory limit per pod group (Gi)                  Kafka      3*      9.6* (3.2 per broker)      11.4* (3.8 per broker)      14.4* (4.8 per broker)      14.4* (4.8 per broker)              Event Streams core             5.1 without persistence      12.4 without persistence      7.7 without persistence      10.6 without persistence                            6.1 with persistence enabled      13.4 with persistence enabled      8.2 with persistence enabled      11.1 with persistence enabled              Message indexing             1.5      2.5      4.4      8.4              Geo-replication      0*      0.9 per replica      1.6 per replica      2.5 per replica      2.5 per replica              TOTAL FOR EVENT STREAMS             17.1 without persistence      27.9 without persistence      29 without persistence      35.9 without persistence                            18.1 with persistence enabled      28.9 with persistence enabled      29.5 with persistence enabled      36.4 with persistence enabled      Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. Before installing IBM Event Streams (not Community Edition), consider the number of Kafka replicas and geo-replicator nodes you plan to use. Each Kafka replica and geo-replicator node is a separate chargeable unit. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Kafka group The following pods and their containers are part of this group. Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. Kafka pod Number of replicas: 3*             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Kafka      1*      1*      2*      2*              Metrics reporter      0.4      0.6      1.5      1.5              Metrics proxy      0.5      0.5      1      1              Healthcheck      0.2      0.2      0.1      0.1              TLS proxy      0.1      0.5      0.1      0.1      Network proxy pod Number of replicas: 3 (matches the number of Kafka replicas)             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Proxy      1      1      0.1      0.1      Event Streams core group The following pods and their containers are part of this group. Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. ZooKeeper pod Number of replicas: 3             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  ZooKeeper      0.1*      0.1*      0.75      1              TLS proxy      0.1      0.1      0.1      0.1      Administration UI pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  UI      1      1      1      1              Redis      0.1      0.1      0.1      0.1      Administration server pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest      0.5      4      1      2              Codegen      0.2      0.5      0.3      0.5              TLS proxy      0.1      0.1      0.1      0.1      REST producer server pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest-producer      0.5      4      1      2      REST proxy pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest-proxy      0.5      0.5      0.25      0.25      Collector pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Collector      0.1      0.1      0.05      0.05              TLS proxy      0.1      0.1      0.1      0.1      Access controller pod Number of replicas: 2             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Access controller      0.1      0.1      0.25      0.25              Redis      0.1      0.1      0.1      0.1      Schema Registry pod Number of replicas:   1 without persistence  2 with persistence enabled            Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Schema Registry      0.5      0.5      0.25      0.25              Avro service      0.5      0.5      0.25      0.25      Message indexing group The following pods and their containers are part of this group. Index manager pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Index manager      0.2      0.2      0.1      0.1              TLS proxy      0.1      0.1      0.1      0.1      Elasticsearch pod Number of replicas: 2             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Elastic      0.5*      1*      2*      4*              TLS proxy      0.1      0.1      0.1      0.1      Geo-replicator group This group only contains the geo-replicator pod. Number of replicas: 0* Note: This means there is no geo-replication enabled by default. The values in the table are the default minimum values for 1 replica.             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Replicator      0.5      1      1      1              Metrics reporter      0.4      0.6      1.5      1.5      PodSecurityPolicy requirements To install the Event Streams chart, you must have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace. You can define the PodSecurityPolicy when creating the namespace for your installation. Event Streams applies network policies to control the traffic within the namespace where it is deployed, limiting the traffic to that required by Event Streams. For more information about the network policies and the traffic they permit, see network policies. For more information about PodSecurityPolicy definitions, see the IBM Cloud Private documentation. Note: The PodSecurityPolicy requirements do not apply to the Red Hat OpenShift Container Platform. Red Hat OpenShift SecurityContextConstraints Requirements If you  are installing on the OpenShift Container Platform, the Event Streams chart requires a custom SecurityContextConstraints to be bound to the target namespace prior to installation. The custom SecurityContextConstraints controls the permissions and capabilities required to deploy this chart. You can enable this custom SecurityContextConstraints resource using the supplied pre-installation setup script. Network requirements IBM Event Streams is supported for use with IPv4 networks only. File systems for storage If you want to set up persistent storage, you must have physical volumes available, backed by one of the following file systems:   NFS version 4  GlusterFS version 3.10.1  IBM Spectrum Scale version 5.0.3.0IBM Event Streams user interface The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterClients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8  Oracle Java 8You can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning for installation",
        "collection": "2019.2.1",
        "excerpt":"Consider the following when planning your installation. Available editions IBM Event Streams is available in two editions:       IBM Event Streams Community Edition is a free version intended for trial and demonstration purposes. It can be installed and used without charge. You can install the Community Edition from the catalog included with IBM Cloud Private.         IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can install IBM Event Streams by downloading the image from IBM Passport Advantage, and making it available in the IBM Cloud Private catalog.   Note: If you do not have IBM Cloud Private already, IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage as well, and install as a prerequisite. IBM Cloud Private Foundation can only be used to deploy IBM Event Streams. No other service can be deployed without upgrading IBM Cloud Private. Performance considerations When preparing for your Event Streams installation, review your workload requirements and consider the configuration options available for performance tuning both your IBM Cloud Private and Event Streams installations. For more information, see the performance planning topic. Kafka high availability Kafka is designed for high availability and fault tolerance. To reduce the impact of Event Streams Kafka broker failures, spread your brokers across several IBM Cloud Private worker nodes by ensuring you have at least as many worker nodes as  brokers. For example, for 3 Kafka brokers, ensure you have at least 3 worker nodes running on separate physical servers. Kafka ensures that topic-partition replicas are spread across available brokers up to the replication factor specified. Usually, all of the replicas will be in-sync, meaning that they are all fully up-to-date, although some replicas can temporarily be out-of-sync, for example, when a broker has just been restarted. The replication factor controls how many replicas there are, and the minimum in-sync configuration controls how many of the replicas need to be in-sync for applications to produce and consume messages with no loss of function. For example, a typical configuration has a replication factor of 3 and minimum in-sync replicas set to 2. This configuration can tolerate 1 out-of-sync replica, or 1 worker node or broker outage with no loss of function, and 2 out-of-sync replicas, or 2 worker node or broker outages with loss of function but no loss of data. The combination of brokers spread across nodes together with the replication feature make a single Event Streams cluster highly available. Persistent storage Persistence is not enabled by default, so no persistent volumes are required. Enable persistence if you want your data, such as messages in topics, schemas, and configuration settings to be retained in the event of a restart. You should enable persistence for production use and whenever you want your data to survive a restart. If you plan to have persistent volumes, consider the disk space required for storage. Also, as both Kafka and ZooKeeper rely on fast write access to disks, ensure you use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation. If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. The number of Kafka brokers and ZooKeeper servers depends on your setup, for default requirements, see the resource requirements table. Schema registry requires a single physical volume. You either need to create a persistent volume for each physical volume, or specify a storage class that supports dynamic provisioning. Each component can use a different storage class to control how physical volumes are allocated. Note: When creating persistent volumes backed by an NFS file system, ensure the path provided has the access permission set to 775. See the IBM Cloud Private documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the IBM Cloud Private Cluster administrator role. Important: When creating persistent volumes for each component, ensure the correct Access mode is set for the volumes as described in the following table.             Component      Access mode                  Kafka      ReadWriteOnce              ZooKeeper      ReadWriteOnce              Schema registry      ReadWriteMany      More information about persistent volumes and the system administration steps required before installing IBM Event Streams can be found in the Kubernetes documentation. If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. The administrator will add these to a central pool before the Helm chart can be installed. The installation will then claim the required number of persistent volumes from this pool. For manual creation, dynamic provisioning must be disabled when configuring your installation. It is up to the administrator to provide appropriate storage to contain these physical volumes. If these persistent volumes are to be created automatically at the time of installation, the system administrator must enable support for this prior to installing IBM Event Streams. For automatic creation, enable dynamic provisioning when configuring your installation, and provide the storage class names to define the persistent volumes that get allocated to the deployment. Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Using IBM Spectrum Scale If you are using IBM Spectrum Scale for persistence, see the IBM Storage Enabler for Containers with IBM Spectrum Scale documentation for more information about creating storage classes. The system administrator must enable support for the automatic creation of persistent volumes prior to installing Event Streams. To do this, enable dynamic provisioning when configuring your installation, and provide the storage class names to define the persistent volumes that get allocated to the deployment. Note: When creating storage classes for an IBM Spectrum Scale file system, ensure you specify both the UID and GID in the storage class definition. Then, when installing Event Streams, ensure you set the File system group ID field to the GID specified in the storage class definition. Also, ensure that this user exists on all IBM Spectrum Scale and GUI nodes. Securing communication between pods You can enhance your security by encrypting the internal communication between Event Streams pods by using TLS. By default, TLS communication between pods is disabled. You can enable encryption between pods when configuring your Event Streams installation. You can also enable TLS encryption between pods for existing Event Streams installations. Important: If you are using the paid-for version of Event Streams (not Community Edition), all message data is encrypted using TLS, but communication between the geo-replicator and administration server pods is not encrypted (see tables in resource requirements). ConfigMap for Kafka static configuration You can choose to create a ConfigMap to specify Kafka configuration settings for your IBM Event Streams installation. This is optional. You can use a ConfigMap to override default Kafka configuration settings when installing IBM Event Streams. You can also use a ConfigMap to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. To create a ConfigMap:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Note: To create a ConfigMap, you must have the Operator, Administrator, or Cluster administrator role in IBM Cloud Private.  To create a ConfigMap from an existing Kafka server.properties file, use the following command (where namespace is where you install Event Streams):  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  To create a blank ConfigMap for future configuration updates, use the following command:  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt;Geo-replication You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. Find out more about geo-replication. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Connecting clients By default, Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. If you want clients to connect through a different route, specify the target endpoint host name or IP address when configuring your installation. Logging IBM Cloud Private uses the Elastic Stack for managing logs (Elasticsearch, Logstash, and Kibana products). IBM Event Streams logs are written to stdout and are picked up by the default Elastic Stack setup. Consider setting up the IBM Cloud Private logging for your environment to help resolve problems with your deployment and aid general troubleshooting. See the IBM Cloud Private documentation about logging for information about the built-in Elastic Stack. As part of setting up the IBM Cloud Private logging for IBM Event Streams, ensure you consider the following:   Capacity planning guidance for logging: set up your system to have sufficient resources towards the capture, storage, and management of logs.  Log retention: The logs captured using the Elastic Stack persist during restarts. However, logs older than a day are deleted at midnight by default to prevent log data from filling up available storage space. Consider changing the log data retention in line with your capacity planning. Longer retention of logs provides access to older data that might help troubleshoot problems.You can use log data to investigate any problems affecting your system health. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Private monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing You require a license to use IBM Event Streams. Licensing is based on a Virtual Processing Cores (VPC) metric. An IBM Event Streams deployment consists of a number of different types of containers, as described in the components of the helm chart. To use IBM Event Streams you must have a license for all of the virtual cores that are available to all Kafka and Geo-replicator containers deployed. All other container types are pre-requisite components that are supported as part of IBM Event Streams, and do not require additional licenses. The number of virtual cores available to each Kafka and geo-replicator container can be specified during installation or modified later. To check the number of cores, use the IBM Cloud Private metering report as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Platform &gt; Metering.  Select your namespace, and select IBM Event Streams (Chargeable).  Click Containers.  Go to the Containers section on the right, and ensure you select the Usage tab.  Select Capped Processors from the first drop-down list, and select 1 Month from the second drop-down list.A page similar to the following is displayed:  Click Download Report, and save the CSV file to a location of your choice.  Open the downloaded report file.  Look for the month in Period, for example, 2018/9, then in the rows underneath look for IBM Event Streams (Chargeable), and check the CCores/max Cores column. The value is the maximum aggregate number of cores provided to all Kafka and geo-replicator containers. You are charged based on this number.For example, the following excerpt from a downloaded report shows that for the period 2018/9 the chargeable IBM Event Streams containers had a total of 4 cores available (see the highlighted fields):","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/planning/",
        "teaser":null},{
        "title": "Performance and capacity planning",
        "collection": "2019.2.1",
        "excerpt":"When preparing for your IBM Event Streams installation, consider the performance and capacity requirements for your system. Guidance for production environments The prerequisites for Event Streams provide information about the minimum resources requirements for a test environment. For a baseline production deployment of Event Streams, increase the following values.   Set the CPU request and limit values for Kafka brokers to 4000m. You can use the kafka.resources.requests.cpu and kafka.resources.limits.cpu options if you are using the command line, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI.  Set the memory request and limit values for Kafka brokers to at least 6Gi. You can use the kafka.resources.requests.memory and kafka.resources.limits.memory options if you are using the command line, or enter the values in the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI.You can set higher values when configuring your installation, or set them later. Note: This guidance sets the requests and limits to the same values. You might need to set the limits to higher values depending on your intended workload. Remember to add the increases to the minimum resource requirement values, and ensure the increased settings can be served by your system. Important: For high throughput environments, also ensure you prepare your IBM Cloud Private installation beforehand. Depending on your workload, you can further scale Event Streams and fine tune Kafka performance to accommodate the increased requirements. Scaling Event Streams If required by your planned workload, you can further increase the number of Kafka brokers, and the amount of CPU and memory available to them. For changing other values, see the guidance about scaling Event Streams. A performance report based on example case studies is available to provide guidance for setting these values. Tuning Event Streams Kafka performance You can further fine-tune the performance settings of your Event Streams Kafka brokers to suit your requirements. Kafka provides a range of parameters to set, but consider the following ones when reviewing performance requirements. You can set these parameters when installing Event Streams, or you can modify them later.   The num.replica.fetchers parameter sets the number of threads available on each broker to replicate messages from topic leaders. Increasing this setting increasesI/O parallelism in the follower broker, and can help reduce bottlenecks and message latency. You can start by setting this value to match the number of brokers deployed in the system. Note: Increasing this value results in brokers using more CPU resources and network bandwidth.  The num.io.threads parameter sets the number of threads available to a broker for processing requests. As the load on each broker increases, handling requests can become a bottleneck. Increasing this parameter value can help mitigate this issue. The value to set depends on the overall system load and the processing power of the worker nodes, which varies for each deployment. There is a correlation between this setting and the num.network.threads setting.  The num.network.threads parameter sets the number of threads available to the broker for receiving and sending requests and responses to the network. The value to set depends on the overall network load, which varies for each deployment. There is a correlation between this setting and the num.io.threads setting.  The replica.fetch.min.bytes, replica.fetch.max.bytes, and replica.fetch.response.max.bytes parameters control the minimum and maximum sizes for message payloads whenperforming inter-broker replication. Set these values to be greater than the message.max.bytes parameter to ensure that all messages sent by a producer can be replicatedbetween brokers. The value to set depends on message throughput and average size, which varies for each deployment.To set these parameter values, you can use a ConfigMap that specifies Kafka configuration settings for your Event Streams installation. Setting before installation If you are creating the ConfigMap and setting the parameters when installing Event Streams, you can add these parameters to the properties file with the required values.   Add the parameters and their values to the Kafka server.properties file, for example:    num.io.threads=24num.network.threads=9num.replica.fetchers=3replica.fetch.max.bytes=5242880replica.fetch.min.bytes=1048576replica.fetch.response.max.bytes=20971520        Create the ConfigMap as described in the planning for installation section, for example:kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  When installing Event Streams, ensure you provide the ConfigMap to the installation.Modifying an existing installation If you are updating an existing Event Streams installation, you can use the ConfigMap you already have for the Kafka configuration settings, and include the parameters and their values in the ConfigMap. You can then apply the new settings by updating the ConfigMap as described modifying Kafka broker configurations, for example: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Alternatively, you can modify broker configuration settings dynamically by using the Event Streams CLI as described in modifying Kafka broker configurations, for example: cloudctl es cluster-config --config num.replica.fetchers=4 Important: Using the Event Streams CLI overrides the values specified in the ConfigMap. In addition, the CLI enforces constraints to avoid certain parameters to be misconfigured. For example, you cannot set num.replica.fetches to a value greater than double its current value. This means that you might have to make incremental updates to the value, for example: cloudctl es cluster-config --config num.replica.fetchers=2cloudctl es cluster-config --config num.replica.fetchers=4cloudctl es cluster-config --config num.replica.fetchers=8cloudctl es cluster-config --config num.replica.fetchers=9Performance considerations for IBM Cloud Private For high throughput environments, consider the following configuration options when setting up your IBM Cloud Private environment.   Set up an external load balancer for your IBM Cloud Private cluster to provide a dedicated external access point for the cluster that provides intelligent routing algorithms.  Set up a dedicated internal network for inter-broker traffic to avoid contention between internal processes and external traffic.Important: You must consider and set these IBM Cloud Private configuration options before installing Event Streams. Setting up a load balancer In high throughput environments, configure an external load balancer for your IBM Cloud Private cluster. Without a load balancer, a typical Event Streams installation includes a master node for allowing external traffic into the cluster. There are also worker nodes that host Kafka brokers. Each broker has an advertised listener that consists of the master node’s IP address and a unique node port within the cluster. This means the worker nodes can be identified without being exposed externally. When a producer connects to the Event Streams master node through the bootstrap port, they are sent metadata that identifies partition leaders for the topics hosted by the brokers. So, access to the cluster is based on the address &lt;master_node:bootstrap_port&gt;, and identification is based on the advertised listener addresses within the cluster, which has a node port to uniquely identify the specific broker. For example, the connection is made to the &lt;master_node:bootstrap_port&gt; address, for example: 192.0.2.24:30724 The advertised listener is then made up of the &lt;master_node:unique_port&gt; address, for example: 192.0.2.24:88945 The producer then sends messages to the advertised listener for a partition leader of a topic. These requests go through the master node and are passed to the right worker node in Event Streams based on the internal IP address for that specific advertised listener that identifies the broker. This means all traffic is routed through the master node before being distributed across the cluster. If the master node is overloaded by service requests, network traffic, or system operations, it becomes a bottleneck for incoming requests.  A load balancer replaces the master node as the entry point into the cluster, providing a dedicated service that typically runs on a separate node. In this case the bootstrap address points to the load balancer instead of the master node. The load balancer passes incoming requests to any of the available worker nodes. The worker node then forwards the request onto the correct broker within the cluster based on its advertised listener address. Setting up a load balancer provides more control over how requests are forwarded into the cluster (for example, round-robin, least congested, and so on), and frees up the master node for system operations.  For more information about configuring an external load balancer for your cluster, see the IBM Cloud Private documentation. Important: When using a load balancer for IBM Cloud Private, ensure you set the address for your endpoint in the External hostname/IP address field field when installing your Event Streams instance. Setting up an internal network Communication between brokers can generate significant network traffic in high usage scenarios. Topic configuration such as replication factor settings can also impact traffic volume. For high performance setups, enable an internal network to handle workload traffic within the cluster. To configure an internal network for inter-broker workload traffic, enable a second network interface on each node, and configure the config.yaml before installing IBM Cloud Private. For example, use the calico_ip_autodetection_method setting to configure the master node IP address on the second network as follows: calico_ip_autodetection_method: can-reach=&lt;internal_ip_address_for_master_node&gt; For more information about setting up a second network, see the IBM Cloud Private documentation. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster configuration log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster configuration log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration cleanup.policy determines the way the log segments for the topic are managed. For more information about the broker configurations and topic-level configurations, see the Kafka documentation. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click the Topics tab, then click Create topic, and click Advanced). Log cleanup by deletion If the topic-level configuration cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The broker configuration log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The broker configuration log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second.  The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding keys which have later messages in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/capacity-planning/",
        "teaser":null},{
        "title": "Installing on IBM Cloud Private",
        "collection": "2019.2.1",
        "excerpt":"IBM Event Streams is the paid-for version intended for enterprise use, and includes full IBM support and additional features such as geo-replication. You can also install a basic deployment of Event Streams Community Edition to try it out. Before you begin   Ensure you have set up your environment according to the prerequisites, including your IBM Cloud Private environment.  The Event Streams installation process creates and runs jobs in the target namsepace (the namespace where you are installing Event Streams) and in the kube-system namespace. If you are using host groups with namespace isolation configured in your IBM Cloud Private cluster, ensure you have sufficient worker nodes available to the kube-system namespace to perform the installation (at least one worker node, or more, depending on your setup). Otherwise,  the namespace isolation causes the installation process to hang with jobs in pending state.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:          The master host and port for your IBM Cloud Private cluster. These values are set during the installation of IBM Cloud Private. The default port is 8443. Make a note of these values, and enter them in the steps that have https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;      The SSH password if you are connecting remotely to the master host of your IBM Cloud Private cluster.        Ensure your proxy address uses lowercase characters. This is a setting that often needs to be checked when installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS). If the address is in uppercase, edit the ibmcloud-cluster-info ConfigMap in the kube-public namespace, and change the uppercase characters to lowercase for the proxy_address parameter: kubectl edit configmap -n ibmcloud-cluster-info -n kube-public  Ensure you have the IBM Cloud Private monitoring service installed. Usually monitoring is installed by default. However, some deployment methods might not install it. For example, monitoring might not be part of the default deployment when installing IBM Cloud Private on Azure by using Terraform. Without this service, parts of the Event Streams UI do not work. You can install the monitoring service from the Catalog or CLI for existing  deployments.Preparing the platform Prepare your platform for installing Event Streams as follows. Create a namespace You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. To create a namespace, you must have the Cluster administrator role.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  From the navigation menu, click Manage &gt; Namespaces.  Click Create Namespace.  Enter a name for your namespace.  Ensure you have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace.  Click Create.See the IBM Cloud Private documentation for more information about creating namespaces. Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site and make it available in your catalog.   Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform.  Ensure you configure your Docker CLI to access your cluster.  Log in to your cluster from the IBM Cloud Private CLI and log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443docker login &lt;cluster_CA_domain&gt;:8500        Note: The default value for the cluster_CA_domain parameter is mycluster.icp. If necessary add an entry to your system’s host file to allow it to be resolved. For more information, see the IBM Cloud Private documentation.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-archive --archive &lt;PPA-image-name.tar.gz&gt;     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository for the installation as follows. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private. The default port is 8443. Create an image pull secret Create an image pull secret for the namespace where you intend to install Event Streams (this is the namespace created earlier). The secret enables access to the internal Docker repository provided by IBM Cloud Private. To create a secret, use the following command: kubectl create secret docker-registry regcred --docker-server=&lt;cluster_CA_domain&gt;:8500 --docker-username=&lt;user-name&gt; --docker-password=&lt;password&gt; --docker-email=&lt;your-email&gt; -n &lt;namespace_for_event_streams&gt; For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=john.smith@ibm.com -n event-streams For more information about creating image pull secrets, see the IBM Cloud Private documentation. Create an image policy Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation.To create an image policy:   Create a .yaml file with the following content, then replace &lt;cluster_CA_domain&gt; with the correct value for your IBM Cloud Private environment, and replace the &lt;namespace_for_event_streams&gt; value with the name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;cluster_CA_domain&gt;:8500/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-prod and select it from the result. The IBM Event Streams README is displayed.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.Important: As part of the configuration process, enter the name of the secret you created previously in the Image pull secret field.  Click Install.  Verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/installing/",
        "teaser":null},{
        "title": "Installing on OpenShift",
        "collection": "2019.2.1",
        "excerpt":"IBM Event Streams makes using Apache Kafka in the enterprise easy and intuitive, and is now fully supported on the Red Hat OpenShift Container Platform. Overview You can install Event Streams on the Red Hat OpenShift Container Platform. The solution includes key IBM cloud foundational services such as installation, security, monitoring, and lifecycle management. These services help manage your Event Streams installation, and are provided by IBM Cloud Private.  The benefits of the solution mean you have a container platform from which you can perform administrative tasks in Red Hat OpenShift while taking some foundational services Event Streams relies on from IBM Cloud Private. Any service task related to Kubernetes can be performed in both Red Hat OpenShift Container Platform and IBM Cloud Private. For example, you can perform administrative tasks through either platform, such as managing storage, reviewing status of components, and reviewing logs and events from each component. Certain aspects of managing your Event Streams installation require the use of the IBM cloud foundational services provided by IBM Cloud Private. These services are as follows:   Installing the chart  Applying updates and fix packs  Modifying installation settings  Managing authentication and access (IAM)  Reviewing metering  Reviewing monitoring and metricsImportant: This documentation assumes the use of IBM Cloud Private for the IBM cloud foundational services required for managing your Event Streams installation. Before you begin   Ensure you have set up your environment according to the prerequisites, including setting up your OpenShift Container Platform and your IBM Cloud Private integration.  The Event Streams installation process creates and runs jobs in the target namsepace (the namespace where you are installing Event Streams) and in the kube-system namespace. If you are using host groups with namespace isolation configured in your IBM Cloud Private cluster, ensure you have sufficient worker nodes available to the kube-system namespace to perform the installation (at least one worker node, or more, depending on your setup). Otherwise,  the namespace isolation causes the installation process to hang with jobs in pending state.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:                  The master host and port for your IBM Cloud Private cluster. These values are set during the installation of IBM Cloud Private. The default port is 5443 in IBM Cloud Private 3.1.2, while it is 443 if you are using IBM Cloud Private 3.2.0.         Make a note of these values, and enter them in the steps that have https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;         Note: An administrator can retrieve the IBM Cloud Private cluster master address and port number from the ConfigMap in kube-public as follows:         kubectl get cm ibmcloud-cluster-info -n kube-public -o yaml         See the cluster_address value for the master address, and the cluster_router_https_port for the port number.                     The master port for your OpenShift Container Platform web console. The default port is 7443. If you are using IBM Cloud Private 3.1.2, the master host address is the same as the address for your IBM Cloud Private cluster. If you are using IBM Cloud Private 3.2.0, the master host address is different.         Make a note of the port value, and enter that port together with the IBM Cloud Private master host in the steps that have https://&lt;OpenShift Cluster Address&gt;:&lt;OpenShift Cluster API Port&gt;                     The SSH password if you are connecting remotely to the master host of your IBM Cloud Private cluster.             Note: The installation process involves steps in both the web consoles and command lines of IBM Cloud Private and OpenShift Container Platform. Create a project (namespace) You perform this step by using the OpenShift Container Platform web console. You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. When you create a project in the OpenShift Container Platform, a namespace with the same name is also created. This is the namespace to use when installing your Event Streams instance.   Go to the OpenShift Container Platform web console in your browser by using the URL https://&lt;OpenShift Cluster Address&gt;:&lt;OpenShift Cluster API Port&gt;. The default port is 7443. The master host address is the same as the address for your IBM Cloud Private cluster.  Log in using the user name and password provided to you by your administrator.  Create an OpenShift project for your Event Streams installation.For example, log into the OpenShift Container Platform web console in your browser, click the Create project button, and type a unique name, display name, and description for the new project. This creates a project and a namespace.Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site, and save the archive to the host where the IBM Cloud Private master cluster is installed. Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform (for example, the Event Streams package for the Red Hat OpenShift Container Platform includes rhel in the package name). Preparing the platform Prepare your platform for installing Event Streams as follows. Important: You must perform the following steps by using a terminal opened on the host where the IBM Cloud Private master cluster is installed. If you are on a different host, you must first connect to the host machine by using SSH before logging in.       Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.     The default port is 5443 in IBM Cloud Private 3.1.2, while it is 443 if you are using IBM Cloud Private 3.2.0.         Run the following command to avoid certificate errors when running kubectl and oc commands later:kubectl config set-cluster mycluster --insecure-skip-tls-verify=true   Run the setup script You perform this step by using the IBM Cloud Private CLI. You must run the following setup script to prepare the platform.   Go to the Event Streams archive you downloaded from IBM Passport Advantage, and locate the file called ibm-eventstreams-rhel-prod-&lt;version&gt;.tgz.  Extract the PPA tar.gz archive.  In your terminal window, change to the following directory: /pak_extensions/pre-install  Run the setup script as follows: ./scc.sh &lt;namespace&gt; Where &lt;namespace&gt; is the namespace (project) you created for your Event Streams installation earlier.Look up the registry address You perform this step by using the OpenShift CLI. Look up the internal OpenShift Docker registry address by using the following command: oc get svc docker-registry -n default The following is an example output: NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGEdocker-registry   ClusterIP   198.51.100.24   &lt;none&gt;        5000/TCP   2dThe &lt;OpenShift_Docker_registry_address&gt; is made up of the values from the CLUSTER-IP and PORT fields as follows: &lt;CLUSTER-IP&gt;:&lt;PORT&gt; In this example, the &lt;OpenShift_Docker_registry_address&gt; is 198.51.100.24:5000. Make a note of the address, including the port number. You will need it later in the installation process. Load the archive into the catalog Make the downloaded archive available in your catalog by using the IBM Cloud Private CLI.       Log in to the Docker private image registry:docker login -u any_value -p $(oc whoami -t) &lt;OpenShift_Docker_registry_address&gt;     Where the &lt;OpenShift_Docker_registry_address&gt; is the internal OpenShift Docker registry address you looked up earlier, including the port number, for example: 198.51.100.24:5000.     Note: The docker login command uses a session token (oc whoami -t) in the password field to perform authentication. This means the -u user name field is required, but not used by Docker.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-ppa-archive --archive &lt;PPA-image-name.tar.gz&gt; --registry &lt;OpenShift_Docker_registry_address&gt;/&lt;namespace-to-install-into&gt;     For example:cloudctl catalog load-ppa-archive --archive eventstreams.rhel.2019.2.1.x86.pak.tar.gz --registry 198.51.100.24:5000/event-streams     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository for the installation as follows. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator. Log in as described in earlier. Create an image pull secret Create an image pull secret for the namespace where you intend to install Event Streams (this is the name of the project created earlier). The secret enables access to the internal Docker repository provided by the OpenShift Container Platform. To create a secret, use the following command: kubectl create secret docker-registry regcred --docker-server=&lt;OpenShift_Docker_registry_address&gt; --docker-username=&lt;any_value&gt; --docker-password=$(oc whoami -t) --docker-email=&lt;any_value&gt; -n &lt;namespace&gt; where:   --docker-server is the internal OpenShift Docker registry address you looked up earlier.  --docker-username can be any value. Docker uses a session token (oc whoami -t) in the password field to perform authentication. This means the --docker-username user name field is required, but not used by Docker.  --docker-email can be any value. It is required, but not used by Docker.  -n: is the project namespace (this is the name of the project created earlier).For example: kubectl create secret docker-registry regcred --docker-server=198.51.100.24:5000 --docker-username=user --docker-password=$(oc whoami -t) --docker-email=john.smith@ibm.com -n event-streams For more information about creating image pull secrets, see the IBM Cloud Private documentation. Create an image policy Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation. Note: If you are using IBM Cloud Private 3.2.0, you only need to follow these steps if the image-security-enforcement service is enabled. If the service is not enabled, you can ignore these steps. To create an image policy:   Create a .yaml file with the following content, then replace &lt;OpenShift_Docker_registry_address&gt; with the address you looked up earlier, and replace the &lt;namespace_for_event_streams&gt; value with the project name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;OpenShift_Docker_registry_address&gt;/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart You perform this step in a browser by using the IBM Cloud Private cluster management console. Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.The default port is 5443 in IBM Cloud Private 3.1.2, while it is 443 if you are using IBM Cloud Private 3.2.0.Ensure you log in as a user that has the Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-rhel-prod and select it from the result. The IBM Event Streams README is displayed.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.Important: You might see the following warnings on this page. These warnings are harmless and can be safely ignored as the OpenShift Container Platform does not use PodSecurityPolicy settings.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.Important: As part of the configuration process, enter the name of the secret you created previously in the Image pull secret field.Note: Ensure the Docker image registry field value includes the OpenShift Docker registry address and the namespace, for example: 198.51.100.24:5000/event-streams  Click Install.  Verify your installation and consider other post-installation tasks, such as fixing certificate errors affecting the IBM Cloud Private CLI.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/installing-openshift/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "2019.2.1",
        "excerpt":"Enabling persistent storage If you want your data to be preserved in the event of a restart, set persistent storage for Kafka, ZooKeeper, and schemas in your IBM Event Streams installation. To enable persistent storage for Kafka:   Go to the Kafka persistent storage settings section.  Select the Enable persistent storage for Apache Kafka check box.  Optional: Select the Use dynamic provisioning for Apache Kafka check box and provide a storage class name if the Persistent Volumes will be created dynamically.To enable persistent storage for ZooKeeper:   Go to the ZooKeeper settings section.  Select the Enable persistent storage for ZooKeeper servers check box.  Optional: Select the Use dynamic provisioning for ZooKeeper servers check box and provide a storage class name if the Persistent Volumes will be created dynamically.To enable persistent storage for schemas:   Go to the Schema Registry settings section.  Select the Enable persistent storage for Schema Registry API servers check box.  Optional: Select the Use dynamic provisioning for Schema Registry API servers check box and provide a storage class name if the Persistent Volumes will be created dynamically.Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Enabling encryption between pods To enable TLS encryption for communication between Event Streams pods, set the Pod to pod encryption field of the Global install settings section to Enabled. By default, encryption between pods is disabled. Specifying a ConfigMap for Kafka configuration If you have a ConfigMap for Kafka configuration settings, you can provide it to your IBM Event Streams installation to use. Enter the name in the Cluster configuration ConfigMap field of the Kafka broker settings section. Important: The ConfigMap must be in the same namespace as where you intend to install the IBM Event Streams release. Setting geo-replication nodes When installing IBM Event Streams as an instance intended for geo-replication, configure the number of geo-replication worker nodes in the Geo-replication settings section by setting the number of nodes required in the Geo-replicator workers field. Note: If you want to set up a cluster as a destination for geo-replication, ensure you set a minimum of 2 nodes for high availability reasons. Consider the number of geo-replication nodes to run on a destination cluster. You can also set up destination clusters and configure the number of geo-replication worker nodes for an existing installation later. Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Configuring external access By default, external Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. You simply leave the External hostname/IP address field of the External access settings section blank. If you want clients to connect through a different route such as a load balancer, use the field to specify the host name or IP address of the endpoint. Also ensure you configure security for your cluster by setting certificate details in the Secure connection settings section. By default, a self-signed certificate is created during installation and the Private key, TLS certificate, and CA certificate fields can be left blank. If you want to use an existing certificate, select provided under Certificate type, and provide these additional keys and certificate values as base 64-encoded strings. Alternatively, you can generate your own certificates. After installation, set up external access by checking the port number to use for external connections and ensuring the necessary certificates are configured within your client environment. Configuring external monitoring tools You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. To set this up, you need to:   Have a third-party monitoring tool set up to be used within your IBM Cloud Private cluster.  Enable access to the broker JMX port by selecting the Enable secure JMX connections check box in the Kafka broker settings section.  Provide any configuration settings required by your monitoring tool to be applied to Event Streams. For example, Datadog requires you to deploy an agent on your IBM Cloud Private system that requires configuration settings to work with Event Streams.  Configure your applications to connect to a secure JMX port.Configuration reference Configure your Event Streams installation by setting the following parameters as needed. Global install settings The following table describes the parameters for setting global installation options.             Field      Description      Default                  Docker image registry      Docker images are fetched from this registry. The format is &lt;cluster_name&gt;:&lt;port&gt;/&lt;namespace&gt;.      ibmcom              Image pull secret      If using a registry that requires authentication, the name of the secret containing credentials.      None              Image pull policy      Controls when Docker images are fetched from the registry.      IfNotPresent              File system group ID      Specify the ID of the group that owns the file system intended to be used for persistent volumes. Volumes that support ownership management must be owned and writable by this group ID.      None              Architecture      The worker node architecture on which to deploy Event Streams.      amd64              Pod to pod encryption      Select whether you want to enable TLS encryption for communication between pods.      Disabled              Kubernetes internal DNS domain name      If you have changed the default DNS domain name from cluster.local in your Kubernetes installation, then this field must be set to the same value. You cannot change this value after installation.      cluster.local      Insights - help us improve our product The following table describes the options for product improvement analytics.             Field      Description      Default                  Share my product usage data      Select to enable product usage analytics to be transmitted to IBM for business reporting and product usage understanding.      Not selected (false)      Note: The data gathered helps IBM understand how IBM Event Streams is used, and can help build knowledge about typical deployment scenarios and common user preferences. The aim is to improve the overall user experience, and the data could influence decisions about future enhancements. For example, information about the configuration options used the most often could help IBM provide better default values for making the installation process easier. The data is only used by IBM and is not shared outside of IBM.If you enable analytics, but want to opt out later, or want more information, contact us. Kafka broker settings The following table describes the options for configuring Kafka brokers.             Field      Description      Default                  CPU request for Kafka brokers      The minimum required CPU core for each Kafka broker. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      1000m              CPU limit for Kafka brokers      The maximum amount of CPU core allocated to each Kafka broker when the broker is heavily loaded. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      1000m              Memory request for Kafka brokers      The minimum amount of memory required for each Kafka broker in bytes. Specify integers with one of these suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Memory limit for Kafka brokers      The maximum amount of memory in bytes allocated to each Kafka broker when the broker is heavily loaded. Specify integers with suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Kafka brokers      Number of brokers in the Kafka cluster.      3              Cluster configuration ConfigMap      Provide the name of a ConfigMap containing Kafka configuration to apply changes to Kafka’s server.properties. See how to create a ConfigMap for your installation.      None              Enable secure JMX connections      Select to make each Kafka broker’s JMX port accessible to secure connections from applications running inside the IBM Cloud Private cluster. When access is enabled, you can configure your applications to connect to a secure JMX port and read Kafka metrics. Also, see External monitoring settings for application-specific configuration requirements.      Not selected (false)      Kafka persistent storage settings The following table describes the options for configuring persistent storage.             Field      Description      Default                  Enable persistent storage for Apache Kafka      Set whether to store Apache Kafka data on a persistent volume. Enabling storage ensures the data is preserved if the pod is stopped.      Not selected (false)              Use dynamic provisioning for Apache Kafka      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache Kafka. Selecting will dynamically create Persistent Volume Claims for the Kafka brokers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for the Apache Kafka brokers.      datadir              Storage class name      Storage Class to use for Kafka brokers if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Kafka nodes.      4Gi      ZooKeeper settings The following table describes the options for configuring ZooKeeper.             Field      Description      Default                  CPU request for ZooKeeper servers      The minimum required CPU core for each ZooKeeeper server. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      100m              CPU limit for ZooKeeper servers      The maximum amount of CPU core allocated to each ZooKeeper server when the server is heavily loaded. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      100m              Enable persistent storage for ZooKeeper servers      Set whether to store Apache ZooKeeper data on a persistent volume. Enabling storage ensures the data is preserved if the pod is stopped.      Not selected (false)              Use dynamic provisioning for ZooKeeper servers      Set whether to use a Storage Class when provisioning Persistent Volumes for Apache ZooKeeper. Selecting will dynamically create Persistent Volume Claims for the ZooKeeper servers.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for Apache ZooKeeper.      datadir              Storage class name      Storage Class to use for Apache ZooKeeper if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for Apache ZooKeeper.      2Gi      External access settings The following table describes the options for configuring external access to Kafka.             Field      Description      Default                  External hostname/IP address      The external hostname or IP address to be used by external clients. Leave blank to default to the IP address of the cluster master node.      None      Secure connection settings The following table describes the options for configuring secure connections.             Field      Description      Default                  Certificate type      Select whether you want to have a self-signed certificate generated during installation, or if you will provide your own certificate details.      selfsigned              Private key      If you set Certificate type to provided, this is the base64-encoded TLS key or private key.      None              TLS certificate      If you set Certificate type to provided, this is the base64-encoded TLS certificate or public key certificate.      None              CA certificate      If you set Certificate type to provided, this is the base64-encoded TLS cacert or Certificate Authority Root Certificate.      None      Message indexing settings The following table describes the options for configuring message indexing.             Field      Description      Default                  Enable message indexing      Set whether to enable message indexing to enhance browsing the messages on topics.      Selected (true)              CPU request for Elastic Search nodes      The minimum required CPU core for each Elastic Search node. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      500m              CPU limit for Elastic Search nodes      The maximum amount of CPU core allocated to each Elastic Search node. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      1000m              Memory request for Elastic Search nodes      The minimum amount of memory required for each Elastic Search node in bytes. Specify integers with one of these suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Memory limits for Elastic Search nodes      The maximum amount of memory allocated to each Elastic Search node in bytes. Specify integers with suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      4Gi      Geo-replication settings The following table describes the options for configuring geo-replicating topics between clusters.             Field      Description      Default                  Geo-replicator workers      Number of workers to support geo-replication.      0      Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). Schema Registry settings             Field      Description      Default                  Enable persistent storage for Schema Registry API servers      Set whether to store Schema Registry data on a persistent volume. Enabling storage ensures the schema data is preserved if the pod is stopped.      Not selected (false)              Use dynamic provisioning for Schema Registry API servers      Set whether to use a Storage Class when provisioning Persistent Volumes for schemas. Selecting will dynamically create Persistent Volume Claims for schemas.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for schemas.      datadir              Storage class name      Storage Class to use for schemas if dynamically provisioning Persistent Volume Claims.      None              Size      Size to use for the Persistent Volume Claims created for schemas.      100Mi      External monitoring The following table describes the options for configuring external monitoring tools.             Field      Description      Default                  Datadog - Autodiscovery annotation check templates for Kafka brokers      YAML object that contains the Datadog Autodiscovery annotations for configuring the Kafka JMX checks. The Datadog prefix and container identifier is applied automatically to the annotation, so only use the template name as the object’s keys (for example, check_names). For more information about setting up monitoring with Datadog, see the Datadog tutorial.      None      Generating your own certificates You can create your own certificates for configuring external access. When prompted, answer all questions with the appropriate information.   Create the certificate to use for the Certificate Authority (CA):openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 365 -out ca.pem  Generate a RSA 2048-bit private key:  openssl genrsa -out es.key 2048  Other key lengths and algorithms are also supported. The following cipher suites are supported, using TLS 1.2 and later only:          TLS_RSA_WITH_AES_128_GCM_SHA256      TLS_RSA_WITH_AES_256_GCM_SHA384      TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384        Note: The string “TLS” is interchangeable with “SSL” and vice versa. For example, where TLS_RSA_WITH_AES_128_CBC_SHA is specified, SSL_RSA_WITH_AES_128_CBC_SHA also applies. For more information about each cipher suite, go to the  Internet Assigned Numbers Authority (IANA) site, and search for the selected cipher suite ID.     Create a certificate signing request for the key generated in the previous step:openssl req -new -key es.key -out es.csr  Sign the request with the CA certificate created in step 1:openssl x509 -req -in es.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out es.pem  Encode your generated file to a base64 string. This can be done using command line tools such as base64, for example, to encode the file created in step 1:cat ca.pem | base64 &gt; ca.b64Completing these steps creates the following files which, after being encoded to a base64 string, can be used to configure your installation:   ca.pem : CA public certificate  es.pem : Release public certificate  es.key : Release private key","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/configuring/",
        "teaser":null},{
        "title": "Migrating from Community Edition",
        "collection": "2019.2.1",
        "excerpt":"You can migrate from the IBM Event Streams Community Edition to IBM Event Streams. Migrating involves removing your previous Community Edition installation and installing IBM Event Streams in the same namespace and using the same release name. Using this procedure,your settings and data are also migrated to the new installation if you had persistent volumes enabled previously.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Delete the Community Edition installation, making a note of the namespace and release name:helm delete --purge &lt;release_name&gt;This command does not delete the PersistentVolumeClaim (PVC) instances. Your PVCs are reused in your new IBM Event Streams installation with the same release name.  Install IBM Event Streams in the same namespace and using the same release name as used for your previous  Community Edition installation. Ensure you select the ibm-eventstreams-prod chart, and apart from the namespace and release name, also ensure you retain the same configuration settings you used for your previous installation, such as persistent volume settings.IBM Event Streams is installed with the configuration settings and data migrated from the Community Edition to your new IBM Event Streams installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/migrating/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "2019.2.1",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying your installation To verify that your Event Streams installation deployed successfully, check the status of your release as follows.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate your installation in the NAME column, and ensure the STATUS column for that row states Deployed.  Optional: Click the name of your installation to check further details of your Event Streams installation. For example, you can check the ConfigMaps used, or check the logs for your pods.  Log in to your IBM Event Streams UI to get started.Installing the command-line interface (CLI) The Event Streams CLI is a plugin for the IBM Cloud Private CLI. Use the CLI to manage your Event Streams instance from the command line, such as creating, deleting, and updating topics. To install the Event Streams CLI:   Ensure you have the IBM Cloud Private CLI installed.  Log in to the Event Streams as an administrator.  Click the Toolbox tab.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;full_path&gt;/es-pluginTo start the Event Streams CLI and check all available command options in the CLI, use the cloudctl es command. To get help on each command, use the --help option. To use the Event Streams CLI against a deployed IBM Cloud Private cluster, run the following commands, replacing &lt;master_ip_address&gt; with your master node IP address, &lt;master_port_number&gt; with the master node port number, and &lt;my_cluster&gt; with your cluster name: cloudctl login -a https://&lt;master_ip_address&gt;:&lt;master_port_number&gt; -c &lt;my_cluster&gt;cloudctl es initFirewall and load balancer settings In your firewall settings, ensure you enable communication for the node ports that Event Streams services use. If you are using an external load balancer for your master or proxy nodes in a high availability environment, ensure that the external ports are forwarded to the appropriate master and proxy nodes. To find the node ports to expose by using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your Event Streams installation in the NAME column, and click the name.  Scroll down to the Service table. The table lists information about your Event Streams services.  In the Service table, look for NodePort in the TYPE column.In each row that has NodePort as type, look in the PORT(S) column to find the port numbers you need to ensure are open to communication.The port numbers are paired as &lt;internal_number:external_number&gt;, where you need the second (external) numbers to be open (for example, 30314 in 32000:30314).The following image provides an example of the table:To find the node ports to expose by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to list information about your Event Streams services:kubectl get services -n &lt;namespace&gt;The following is an example of the output (this is the same result as shown in the UI example previously):    NAME                                              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                           AGEmy-eventstreams-ibm-es-access-controller-svc      ClusterIP   None         &lt;none&gt;        8443/TCP                                                          111dmy-eventstreams-ibm-es-elastic-svc                ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP                                                 111dmy-eventstreams-ibm-es-indexmgr-svc               ClusterIP   None         &lt;none&gt;        9080/TCP,8080/TCP                                                 111dmy-eventstreams-ibm-es-kafka-broker-svc-0         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-broker-svc-1         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-broker-svc-2         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,7070/TCP                               111dmy-eventstreams-ibm-es-kafka-headless-svc         ClusterIP   None         &lt;none&gt;        9092/TCP,8093/TCP,9094/TCP,8081/TCP                               111dmy-eventstreams-ibm-es-proxy-svc                  NodePort    10.0.0.118   &lt;none&gt;        30000:32417/TCP,30001:31557/TCP,30051:32712/TCP,30101:32340/TCP   111dmy-eventstreams-ibm-es-replicator-svc             ClusterIP   None         &lt;none&gt;        8083/TCP                                                          111dmy-eventstreams-ibm-es-rest-proxy-svc             NodePort    10.0.0.86    &lt;none&gt;        32000:30314/TCP                                                   111dmy-eventstreams-ibm-es-rest-svc                   ClusterIP   10.0.0.224   &lt;none&gt;        9080/TCP                                                          111dmy-eventstreams-ibm-es-ui-svc                     NodePort    10.0.0.192   &lt;none&gt;        32000:30634/TCP                                                   111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-0   ClusterIP   10.0.0.236   &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-1   ClusterIP   10.0.0.125   &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-fixed-ip-svc-2   ClusterIP   10.0.0.87    &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111dmy-eventstreams-ibm-es-zookeeper-headless-svc     ClusterIP   None         &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        111d      For your firewall settings, ensure the external ports are open. For example, in the previous UI example, it is the second number for the highlighted NodePort rows: 30314, 30634, 32417, 31557, 32712, and 32340. For your load balancer settings, you need to expose the following ports:   For the CLI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-rest-proxy-svc row. In the previous example, the port is the second number in the PORT(S) column of the my-eventstreams-ibm-es-rest-proxy-svc row: 30314.  For the UI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-ui-svc row. In the previous example, the port is the second number in the PORT(S) column of the my-eventstreams-ibm-es-ui-svc row: 30634.  For Kafka, ensure you forward the external port to the proxy node. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-proxy-svc row. In the previous example, the ports are the second numbers in the PORT(S) column of the my-eventstreams-ibm-es-proxy-svc row: 32417, 31557, 32712, and 32340.Connecting clients You can set up external client access during installation. After installation, clients can connect to the Kafka cluster by using the externally visible IP address for the Kubernetes cluster. The port number for the connection is allocated automatically and varies between installations. To look up this port number after the installation is complete:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Workloads &gt; Helm Releases.  In the NAME column, locate and click the release name used during installation.  Scroll down through the sections and locate the Service section.  In the NAME column, locate and click the &lt;releasename&gt;-ibm-es-proxy-svc NodePort entry.  In the Type column, locate the list of Node port links.  Locate the top entry in the list named bootstrap &lt;bootstrap port&gt;/TCP.  If no external hostname was specified when Event Streams was installed, this is the IP address and port number that external clients should connect to.  If an external hostname was specified when Event Streams was installed, clients should connect to that external hostname using this bootstrap port number.Before connecting a client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or export the self-signed public certificate from the browser. To export the self-signed public certificate from the browser:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.Red Hat OpenShift only: fixing certificate errors If you are installing Event Streams on the OpenShift Container Platform, there is a known issue that causes kubectl and oc commands to result in a certificate error after logging in with the IBM Cloud Private cloudctl login command. As a temporary fix, you can run the following command after running cloudctl login: kubectl config set-cluster mycluster --insecure-skip-tls-verify=true To permanently resolve this issue, edit the existing cluster-ca-cert system secret to add an additional certificate as described in the  IBM Cloud Private documentation. Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Private teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/post-installation/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "2019.2.1",
        "excerpt":"You can uninstall IBM Event Streams by using the UI or the CLI. Using the UI To delete the Event Streams installation by using the UI:   Log in to the Event Streams UI as an administrator.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Delete in the corresponding row.  Optional: If you enabled persistence during installation, you also need to manually remove  PersistentVolumes and PersistentVolumeClaims.Using the CLI Delete the Event Streams installation by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command:helm delete --purge &lt;release_name&gt;  Optional: If you enabled persistence during installation, you also need to manually remove PersistentVolumeClaims and PersistentVolumes. Use the Kubernetes command line tool as follows:          To delete PersistentVolumeClaims:kubectl delete pvc &lt;PVC_name&gt; -n &lt;namespace&gt;      To delete PersistentVolumes:kubectl delete pv &lt;PV_name&gt;      Cleaning up after uninstallation The uninstallation process might leave behind artifacts that you have to clear manually. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading to 2019.2.1",
        "collection": "2019.2.1",
        "excerpt":"Upgrade your installation to the latest version of IBM Event Streams as follows. You can upgrade to Event Streams version 2019.2.1 from version 2019.1.1. If you have an earlier version, you must first upgrade your Event Streams version to 2019.1.1, before following these steps to upgrade to version 2019.2.1. Important: Event Streams only supports upgrading to a newer chart version. Do not select an earlier chart version when upgrading. If you want to revert to an earlier version of Event Streams, see the instructions for rolling back. Prerequisites   Ensure you have IBM Cloud Private version 3.1.2 or later installed.  If you are upgrading IBM Event Streams (not Community Edition), download the package for the version you want to upgrade to, and make it available to your IBM Cloud Private instance.Using the UI   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Upgrade in the corresponding row.  Select the chart version to upgrade to from the Version drop-down list.  Ensure you have Using previous configured values set to Reuse Values.Note: Do not change any of the settings in the Parameters section. You can modify configuration settings after upgrade, for example, enable encryption between pods.  Click Upgrade.The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting. After the upgrade completes, you must perform the post-upgrade tasks. Using the CLI   Ensure you have the latest helm chart version available on your local file system.          You can retrieve the charts from the UI.      Alternatively, if you downloaded the archive from IBM Passport Advantage, the chart file is included in the archive. Extract the PPA archive, and locate the chart file in the /charts directory, for example: ibm-eventstreams-prod-1.3.0.tgz.        Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.Important: You must have the Cluster Administrator role to upgrade the chart.      Run the helm upgrade command as follows, referencing the helm chart you want to upgrade to:helm upgrade &lt;release-name&gt; &lt;latest-chart-version&gt;     For example, to upgrade the Community Edition:helm upgrade eventstreams1 /Users/admin/upgrade/ibm-eventstreams-dev-1.3.0.tgzFor example, to upgrade by using a chart downloaded in the PPA archive:helm upgrade eventstreams1 /Users/admin/upgrade/ibm-eventstreams-prod-1.3.0.tgz     Note: Do not set any parameter value during the upgrade, for example, helm upgrade --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls. You can modify configuration settings after upgrade, for example, enable encryption between pods.   The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting. After the upgrade completes, you must perform the post-upgrade tasks. Post-upgrade tasks Additional steps required after upgrading are described in the following sections. Retrieve new port number for UI Important: The upgrade process changes the port number for the UI. You must refresh the IBM Cloud Private UI and determine the URL for the Event Streams UI again to obtain the new port number. You can then log in to the Event Streams UI. Set up access management If you have IBM Cloud Private teams set up for access management, you must associate the teams again with your IBM Event Streams instance after successfully completing the upgrade. To use your upgraded Event Streams instance with existing IBM Cloud Private teams, re-apply the security resources to any teams you have defined as follows:   Check the teams you use:          Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Enter an IBM Cloud Private administrator user name and password.      From the navigation menu, click Manage &gt; Identity &amp; Access &gt; Teams. Look for the teams you use with your Event Streams instance.        Ensure you have installed the latest version of the Event Streams CLI.  Run the following command for each team that references your instance of Event Streams:  cloudctl es iam-add-release-to-team --namespace &lt;namespace&gt; --helm-release &lt;helm-release&gt; --team &lt;team-name&gt;Update UI bookmarks If you have any bookmarks to the UI, you need to update them because the port number for the Event Streams UI changes as part of the upgrade to version 2019.2.1. Update browser certificates If you trusted certificates in your browser for using the Event Streams UI, you might not be able to access the UI after upgrading. To resolve this issue, you must delete previous certificates and trust new ones. Check the browser help for instructions, the process for deleting and accepting certificates varies depending on the type of browser you have. Enable encryption between pods For enhanced security, consider encrypting the internal communication between Event Streams pods by using TLS. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/upgrading/",
        "teaser":null},{
        "title": "Rolling back",
        "collection": "2019.2.1",
        "excerpt":"You can revert to an earlier version of Event Streams under certain conditions. Prerequisites Rolling back your Event Streams 2019.2.1 installation to an earlier version is only supported in the following cases:   You can only roll back from a newer Helm chart version to an older chart version.  You can only roll back to Event Streams 2019.1.1 (Helm chart version 1.2.0). Rolling back to earlier chart versions is not supported.Rolling back Using the UI   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Rollback in the corresponding row.  Select the chart version to roll back to (1.2.0).  Click Rollback.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.Important: You must have the Cluster Administrator role to roll back a chart version.  Run the helm history command to view previous versions you can roll back to:helm history &lt;release-name&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation.For example:    $ helm history event-streamsREVISION        UPDATED                         STATUS          CHART                           DESCRIPTION1               Mon Oct 15 14:27:12 2018        SUPERSEDED      ibm-eventstreams-prod-1.0.0     Install complete2               Mon Dec 10 16:49:29 2018        SUPERSEDED      ibm-eventstreams-prod-1.1.0     Upgrade complete3               Fri Mar 29 12:16:34 2019        SUPERSEDED      ibm-eventstreams-prod-1.2.0     Upgrade complete4               Fri Jun 28 16:16:34 2019        DEPLOYED        ibm-eventstreams-prod-1.3.0     Upgrade complete        Run the helm rollback command as follows:helm rollback &lt;release-name&gt; &lt;revision&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation, and &lt;revision&gt; is a number from the REVISION column that corresponds to the version you want to revert to, as displayed in the result of the helm history command.For example:helm rollback event-streams 3","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/installing/rolling-back/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "2019.2.1",
        "excerpt":"Log in to your IBM Event Streams installation. To determine the IBM Event Streams UI URL:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column.  Expand the Launch link in the row and click admin-ui-https.The IBM Event Streams log in page is displayed.Note: You can also determine the IBM Event Streams UI URL by using the CLI. Click the release name and scroll to the Notes section at the bottom of the page and follow the instructions. You can then use the URL to log in.  Use your IBM Cloud Private administrator user name and password to access the UI. Use the same username and password as you use to log in to IBM Cloud Private.From the Getting started page, you can start exploring Event Streams through a simulated topic, or through learning about the concepts of the underlying technology. You can also generate a starter application that lets you learn more about writing applications. For more useful applications, tools, and connectors, go to the Toolbox tab. Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Private session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/getting-started/logging-in/",
        "teaser":null},{
        "title": "Generating a starter application",
        "collection": "2019.2.1",
        "excerpt":"To learn more about how you can create applications that can take advantage of IBM Event Streams capabilities, generate a starter application. The starter application can produce and consume messages, and you can specify the topic where you want to send messages to. About the application The starter application provides a demonstration of a Java application running on WebSphere Liberty that sends events, receives events, or both, by using IBM Event Streams. Project contents The Java classes in the application.kafka package are designed to be independent of the specific use case for Kafka. The application.kafka sample code is used by the application.demo package and can also be used to understand the elements required to create your own Kafka application. The src/main/java/application/demo folder contains the framework for running the sample in a user interface, providing an easy way to view message propagation. Security The starter application is generated with the correct security configurations to connect to IBM Event Streams. These security configurations include a .jks file for the certificate and API keys for producing and consuming messages. For more information, see the  instructions for connecting clients. Note: The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application. Generating and running To generate the application:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Log in to your IBM Event Streams UI.  Click the Generate a starter application tile, or click the Toolbox tab, and go to the Starter application section and click Generate application.  Provide a name for the application.  Decide whether you want the application to produce or consume messages, or both.  Specify a target topic for the messages from the application.  Click Generate. The application is created.  Download the compressed file and extract the file to a preferred location.  Navigate to the extracted file, and run the following command to build and deploy the application:   mvn install liberty:run-server  Access the successfully deployed sample application using the following URL: http://localhost:9080/  Click Close and go to topics to see your topic and check the messages generated by the application.Note: Some of these options depend on your access permissions. If you are not permitted to create topics, you will not be able to create a topic as part of building the starter application. If you are not permitted to write to topics, you will not be able to create a starter application that produces messages, only consumes them. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads ",
        "collection": "2019.2.1",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the following details in the configuration file.             Attribute      Description                         bootstrap.servers      The URL used for bootstrapping knowledge about the rest of the cluster. You can find this address in the Event Streams UI as described later.                     ssl.truststore.location      The location of the JKS keystore used to securley communicate with your IBM Event Streams instance. You can downloaded the JKS keystore file from the Event Streams UI as described later.                     sasl.jaas.config      Set to org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;password&gt;\";, where &lt;password&gt; is replaced by an API key. This is needed to authorize production to your topic. To generate API keys, go to the Event Streams UI as described later.             Obtaining configuration details Obtain the required configuration details as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  Go to the Connect a client tab.  Locate the details:          For the bootstrap.servers, copy the address from the Bootstrap server section.      To downloaded the JKS keystore file, go to the Certificates section, and download the server certificate from the Java truststore section. Set the ssl.truststore.location to the full path and name of the downloaded file.      To generate API keys, go to the API key section and follow the instructions.      You can secure access to your topics as described in managing access. Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t myTopic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called myTopic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Using user-defined loads You can generate a custom message load using your own settings. For example, to test the load to the topic called myTopic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t myTopic -T 1000 -n 60000 -r 1024 The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      int      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      int      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      bool      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      int      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      int      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "2019.2.1",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:     &lt;dependency&gt;     &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;     &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;     &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:     implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.2.1' implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25' implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.25'        Ensure you set up security.Securing the connection You must secure the connection from your client applications to IBM Event Streams. To secure the connection, you must obtain the following:   A copy of the server-side public certificate added to your client-side trusted certificates.  An API key generated from the IBM Cloud Private UI.Before connecting an external client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or use the following instructions to retrieve a copy. Copy the server-side public certificate and generate an API key as follows:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.  To generate API keys, go to the API key section and follow the instructions.Configuring your client Add the certificate details and the API key to your Kafka client application to set up a secure connection from your application to your Event Streams instance. For example, for Java: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;broker_url&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");Replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key copied from its file. Note: You can copy the connection code snippet from the Event Streams UI with the broker URL already filled in for you. After logging in, click Connect to this cluster on the right, and click the Sample code tab. Copy the snippet from the Sample connection code section into your Kafka client application. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/getting-started/client/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "2019.2.1",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      No, see managing access                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No, requires ZooKeeper access      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No, requires ZooKeeper access      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No, requires ZooKeeper access      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No, requires ZooKeeper access      cloudctl es entity-config              kafka-configs.sh --entity-type users      No, requires ZooKeeper access      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      Yes      cloudctl es topics              kafka-topics.sh --describe      Yes      cloudctl es topic              kafka-topics.sh --create      Yes      cloudctl es topic-create              kafka-topics.sh --delete      Yes      cloudctl es topic-delete              kafka-topics.sh --alter --config      Yes      cloudctl es topic-update              kafka-topics.sh --alter --partitions      Yes      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      Yes      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Schemas overview",
        "collection": "2019.2.1",
        "excerpt":"Apache Kafka can handle any data, but it does not validate the information in the messages. However, efficient handling of data often requires that it includes specific information in a certain format. Using schemas, you can define the structure of the data in a message, ensuring that both producers and consumers use the correct structure. Schemas help producers create data that conforms to a predefined structure, defining the fields that need to be present together with the type of each field. This definition then helps consumers parse that data and interpret it correctly. Event Streams supports schemas and includes a schema registry for using and managing schemas. It is common for all of the messages on a topic to use the same schema. The key and value of a message can each be described by a schema.  Schema registry Schemas are stored in the Event Streams schema registry. In addition to storing a versioned history of schemas, it provides an interface for retrieving them. Each Event Streams cluster has its own schema registry. Your producers and consumers validate the data against the specified schema stored in the schema registry. This is in addition to going through Kafka brokers. The schemas do not need to be transferred in the messages this way, meaning the messages are smaller than without using a schema registry.  If you are migrating to use Event Streams as your Kafka solution, and have been using a schema registry from a different provider, you can migrate to using the Event Streams schema registry. Apache Avro data format Schemas are defined using Apache Avro, an open-source data serialization technology commonly used with Apache Kafka. It provides an efficient data encoding format, either by using the compact binary format or a more verbose, but human-readable JSON format. The Event Streams schema registry uses Apache Avro data formats. When messages are sent in the Avro format, they contain the data and the unique identifier for the schema used. The identifier specifies which schema in the registry is to be used for the message. Avro has support for a wide range of data types, including primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Learn more about how you can create schemas in Event Streams.  Serialization and deserialization A producing application uses a serializer to produce messages conforming to a specific schema. As mentioned earlier, the message contains the data in Avro format, together with the the schema identifier. A consuming application then uses a deserializer to consume messages that have been serialized using the same schema. When a consumer reads a message sent in Avro format, the deserializer finds the identifier of the schema in the message, and retrieves the schema from the schema registry to deserialize the data. This process provides an efficient way of ensuring that data in messages conform to the required structure. Serializers and deserializers that automatically retrieve the schemas from the schema registry as required are provided or generated by IBM Event Streams. If you need to use schemas in an environment for which serializers or deserializers are not provided, you can use the command line or UI directly to retrieve the schemas.  Versions and compatibility Whenever you add a schema, and any subsequent versions of the same schema, Event Streams validates the format automatically and warns of any issues. You can evolve your schemas over time to accommodate changing requirements. You simply create a new version of an existing schema, and the schema registry ensures that the new version is compatible with the existing version, meaning that producers and consumers using the existing version are not broken by the new version. When you create a new version of the schema, you simply add it to the registry and version it. You can then set your producers and consumers that use the schema to start using the new version. Until they do, both producers and consumers are warned that a new version of the schema is available.  Lifecycle When a new version is used, you can deprecate the previous version. Deprecating means that producing and consuming applications still using the deprecated version are warned that a new version is available to upgrade to. When you upgrade your producers to use the new version, you can disable the older version so it can no longer be used, or you can remove it entirely from the schema registry. You can use the Event Streams UI or CLI to manage the lifecycle of schemas, including registering, versioning, deprecating, and so on.  How to get started with schemas   Create schemas  Add schemas to schema registry  Set your Java or non-Java applications to use schemas  Manage schema lifecycle","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/overview/",
        "teaser":null},{
        "title": "Creating and adding schemas",
        "collection": "2019.2.1",
        "excerpt":"You can create schemas in Avro format. You can then use the Event Streams UI or CLI to add the schemas to the schema registry. Creating schemas Event Streams supports Apache Avro schemas. Avro schemas are written in JSON to define the format of the messages. For more information about Avro schemas, see the Avro documentation.  The Event Streams schema registry imports, stores, and uses Avro schemas to serialize and deserialize Kafka messages. The schema registry supports Avro schemas using the record complex type. The record type can include multiple fields of any data type, primitive or complex. Define your Avro schema files and save them by using the .avsc or .json file extension. For example, the following Avro schema defines a Book record in the org.example namespace, and contains the Title, Author, and Format fields with different data types: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },    ]}Adding schemas to the registry To use schemas in Kafka applications, import your schema definitions into the schema registry. Your applications can then retrieve the schemas from the registry as required. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab and then click Add schema.  Click Add file and select your Avro schema file. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.   Optional: Edit the Schema name and Version fields.          The name of the record defined in the Avro schema file is added to the Schema name field. You can edit this field to add a different name for the schema. Changing the Schema name field does not update the Avro schema definition itself.      The value 1.0.0 is automatically added to the Version field as the initial version of the schema. You can edit this field to set a different version number for the schema.        Click Add schema. The schema is added to the list of schemas in the Event Streams schema registry.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to add a schema to the schema registry:cloudctl es schema-add --name &lt;schema-name&gt; --version &lt;schema-version&gt; --file &lt;path-to-schema-file&gt;Adding new schema versions The Event Streams schema registry can store multiple versions of the same schema. As your applications and environments evolve, your schemas need to change to accommodate the requirements. You can import, manage, and use different versions of a schema. As your schemas change, consider the options for managing their lifecycle. Note: A new version of a schema must be compatible with previous versions. This means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. For example, the following Avro schema defines a new version of the Book record, adding a PageCount field. By including a default value for this field, messages that were serialized with the previous version of this schema (which would not have a PageCount value) can still be deserialized using this version. {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },        {\"name\": \"PageCount\",  \"type\": \"int\", \"default\": 0}    ]}Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab.  Locate your schema in the list of registered schemas and click its name. The list of versions for the schema is displayed.  Click Add new version to add a new version of the schema.  Click Add file and select the file that contains the new version of your schema. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Set a value in the Version field to be the version number for this iteration of the schema. For the current list of all versions, click View all versions.  Click Add schema. The schema version is added to the list of all versions for the schema.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to list all schemas in the schema registry, and select the schema name you want to add a new version to:cloudctl es schemas  Run the following command to add a new version of the schema to the registry:cloudctl es schema-add --name &lt;schema-name-from-previous-step&gt; --version &lt;new-schema-version&gt; --file &lt;path-to-new-schema-file&gt;","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/creating/",
        "teaser":null},{
        "title": "Managing schema lifecycle",
        "collection": "2019.2.1",
        "excerpt":"You can have multiple versions of a schema stored in the Event Streams schema registry. Kafka producers and consumers retrieve the right schema version they use from the registry based on a unique identifier and version. When a new schema version is added, you can set both the producer and consumer applications to use that version. You then have the following options to handle earlier versions. The lifecycle is as follows:   Add schema  Add new schema version  Deprecate earlier versions or deprecate entire schema  Disable version or entire schema  Remove version or entire schemaDeprecating If you want your applications to use a new version of a schema, you can set the earlier version to Deprecated. When a version is deprecated, the applications using that version receive a message to warn them to stop using it. Applications can continue to use the schema, but warnings will be written to application logs about the schema version being deprecated. You can customize the message to be provided in the logs. Deprecated versions are still available in the registry and can be used again. Note: You can deprecate a entire schema, not just the versions of that schema. If the entire schema is set to deprecated, then all of its versions are reported as deprecated (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab.  Select the schema you want to deprecate from the list.  Set the entire schema or a selected version of the schema to be deprecated:          If you want to deprecate the entire schema and all its versions, click the Manage schema tab, and set Mark schema as deprecated to on.      To deprecate a specific version, select it from the list, and click the Manage schema tab for that version. Then set Mark schema as deprecated to on.      Deprecated schemas and versions are marked with a Deprecated flag on the UI. You can re-activate a schema or its version by setting Mark schema as deprecated to off. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to deprecate a schema version:cloudctl es schema-modify --deprecate --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To deprecate an entire schema, do not specify the --version &lt;schema-version&gt; option.     To re-activate a schema version:cloudctl es schema-modify --activate --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To re-activate an entire schema, do not specify the --version &lt;schema-version&gt; option.   Disabling If you want your applications to stop using a specific schema, you can set the schema version to Disabled. If you disable a version, applications will be prevented from producing and consuming messages using it. You can re-enable it again to allow applications to use the schema again. When a schema is disabled, applications that want to use the schema receive an ERROR. Java producers using the Event Streams schema registry serdes library will throw a SchemaDisabledException when attempting to producemessages using a disabled schema version. For example, the message and stack trace for a disabled schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaDisabledException: Schema \"Test_Schema\" version \"1.0.0\" is disabled.\tat com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:174)\tat com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:41)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:884)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:846)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:733)\tat Producer.main(Producer.java:92)Note: You can disable a entire schema, not just the versions of that schema. If the entire schema is disabled, then all of its versions are disabled as well, which means no version of the schema can be used by applications (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab.  Select the schema you want to disable from the list.  Set the entire schema or a selected version of the schema to be disabled:          If you want to disable the entire schema and all its versions, click the Manage schema tab, and click Disable schema, then click Disable.      To disable a specific version, select it from the list, and click the Manage schema tab for that version. Then click Disable version, then click Disable.You can re-enable a schema by clicking Enable schema, and re-enable a schema version by clicking  Re-enable version.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to disable a schema version:cloudctl es schema-modify --disable --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To disable an entire schema, do not specify the --version &lt;schema-version&gt; option.     To re-enable a schema version:cloudctl es schema-modify --enable --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To re-enable an entire schema, do not specify the --version &lt;schema-version&gt; option.   Removing If a schema version has not been used for a period of time, you can remove it from the schema registry. Removing a schema version means it will be permanently deleted from the schema registry of your Event Streams instance, and applications will be prevented from producing and consuming messages using it. If a schema is no longer available in the registry, Java applications that want to use the schema receive a SchemaNotFoundException message. For example, the message and stack trace when producing a message with a missing schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaNotFoundException: Schema \"Test_Schema\" not found    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.handleErrorResponse(SchemaRegistryRestAPIClient.java:145)    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.get(SchemaRegistryRestAPIClient.java:120)    at com.ibm.eventstreams.serdes.SchemaRegistry.downloadSchema(SchemaRegistry.java:253)    at com.ibm.eventstreams.serdes.SchemaRegistry.getSchema(SchemaRegistry.java:239)Important: You cannot reverse the removal of a schema. This action is permanent. Note: You can remove a entire schema, including all of its versions. If the entire schema is removed, then all of its versions are permanently deleted from the schema registry of your Event Streams instance. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab.  Select the schema you want to remove from the list.  Remove the entire schema or a selected version of the schema:          If you want to remove the entire schema and all its versions, click the Manage schema tab, and click Remove schema, then click Remove.      To remove a specific version, select it from the list, and click the Manage schema tab for that version. Then click Remove version, then click Remove.        Important: This action is permanent and cannot be reversed.   Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to remove a schema version:cloudctl es schema-remove --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To remove an entire schema, do not specify the --version &lt;schema-version&gt; option.     Important: This action is permanent and cannot be reversed.   ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/manage-lifecycle/",
        "teaser":null},{
        "title": "Setting Java applications to use schemas",
        "collection": "2019.2.1",
        "excerpt":"If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas. Note: If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas. Preparing the setup To use schemas stored in the Event Streams schema registry, your client applications need to be able to serialize and deserialize messages based on schemas.   Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.  Consuming application then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.The Event Streams UI provides help with setting up your Java applications to use schemas. To set up your Java applications to use the Event Streams schemas and schema registry, prepare the connection for your application as follows:   Ensure you have added schemas to the registry.  Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab.  Select a schema from the list and click the row for the schema.  Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.  Set the preferences for your connection in the Configure the schema connection section. Use the defaults or change them by clicking Change configuration.                  For producers, set the method for Message encoding.                   Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.          JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.                            For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.                   Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.          Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and so allow a Kafka consumer to continue to process further messages.                            For both producers and consumers, set in the Use generated or generic code section whether your schema is to use custom Java classes that are generated based on the schema, or generic code by using the Apache Avro API.                   Use schema-specific code (default): Your application will use custom Java classes that are generated based on this schema, using get and set methods to create and access objects. When you want to use a different schema, you will need to update your code to use a new set of specific schema Java classes.          Use generic Apache Avro schema code: Your application will create and access objects using the generic Apache Avro API. Producers and consumers that use the generic serializer and deserializer can be coded to produce or consume messages using any schema uploaded to this schema registry.                      Go to the Provide an API key section and enter an API key with the correct permissions for your purposes (for example, to be able to produce, consume, or both). Alternatively, create one now by clicking Generate API key, and follow the instructions.The API key grants your application access to the cluster and its resources.  Click Generate connection details.  Click Download to download the Java truststore file which contains the server certificate.  Click Java dependencies to download the Event Streams schema registry JAR files, and click Schema JAR to download the schema JAR file to use for your application in its code.   Alternatively, if you are using Maven, click the Use Maven tab. Follow the instructions to copy the configuration snippets for the Event Streams Maven repository to your project Maven POM file, and run the Maven install command to download and install project dependencies.  Depending on your application, click the Producer code or Consumer code tab, and copy the sample Java code snippets displayed. The sample code snippets include the settings you configured to set up your applications to use the schema.     Add the snippets into your application code as described in the following sections.Setting up producers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a producing application.  Ensure you add the location of the JAR files to the build path of your producer Kafka application.  Use the code snippets from the UI and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example: import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import com.ibm.eventstreams.serdes.SchemaInfo;import com.ibm.eventstreams.serdes.SchemaRegistry;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example: Properties props = new Properties();props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.171:30342\");props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");props.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");props.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\";\";props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);props.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);Note: Replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier and replace &lt;api_key&gt; with an API key which has the permissions needed for your application. The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Producer code section defines properties for the producer application that set it to use the schema registry and the correct schema, for example: // Set the value serializer for produced messages to use the Event Streams serializerprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"com.ibm.eventstreams.serdes.EventStreamsSerializer\");// Set the encoding type used by the message serializerprops.put(SchemaRegistryConfig.PROPERTY_ENCODING_TYPE, SchemaRegistryConfig.ENCODING_BINARY);// Get a new connection to the Schema RegistrySchemaRegistry schemaRegistry = new SchemaRegistry(props);// Get the schema from the registrySchemaInfo schema = schemaRegistry.getSchema(\"ABC_Assets_Schema\", \"1.0.0\");// Get a new specific KafkaProducerKafkaProducer&lt;String, ABC_Assets_Schema&gt; producer = new KafkaProducer&lt;&gt;(props);// Get a new specific record based on the schemaABC_Assets_Schema specificRecord = new ABC_Assets_Schema();// Add fields and values to the specific record, for example:// specificRecord.setTitle(\"this is the value for a title field\");// Prepare the record, adding the Schema Registry headersProducerRecord&lt;String, ABC_Assets_Schema&gt; producerRecord =    new ProducerRecord&lt;String, ABC_Assets_Schema&gt;(&lt;my_topic&gt;, specificRecord);producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_ID,    schema.getIdAsBytes());producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_VERSION,    schema.getVersionAsBytes());// Send the record to Kafkaproducer.send(producerRecord);// Close the producerproducer.close();The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsSerializer, telling Kafka to use the Event Streams serializer for message values when producing messages. You can also use the Event Streams serializer for message keys. Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message encoding behavior SchemaRegistryConfig.PROPERTY_ENCODING_TYPE. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to set the field values in your message.   Specific Java classes that are generated from the schema definition will have set&lt;field-name&gt; methods that can be used to easily set the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named setAuthor which takes a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the put method in the GenericRecord class to set field names and values.Note: Replace &lt;my_topic&gt; with the name of the topic to produce messages to. Setting up consumers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a consuming application.  Ensure you add the location of the JAR files to the build path of your consumer Kafka application.  Use the code snippets from the UI and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example: import java.time.Duration;import java.util.Arrays;import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.KafkaConsumer;The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example: Properties props = new Properties();props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.171:30342\");props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");props.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");props.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\";\";props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);props.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);Note: Replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier and replace &lt;api_key&gt; with an API key which has the permissions needed for your application. The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Consumer code section defines properties for the consumer application that set it to use the schema registry and the correct schema, for example: // Set the value deserializer for consumed messages to use the Event Streams deserializerprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"com.ibm.eventstreams.serdes.EventStreamsDeserializer\");// Set the behavior of the deserializer when a record cannot be deserializedprops.put(SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE, SchemaRegistryConfig.BEHAVIOR_STRICT);// Set the consumer group ID in the propertiesprops.put(\"group.id\", &lt;my_consumer_group&gt;);// Get a new KafkaConsumerKafkaConsumer&lt;String, ABC_Assets_Schema&gt; consumer = new KafkaConsumer&lt;&gt;(props);// Subscribe to the topicconsumer.subscribe(Arrays.asList(&lt;my_topic&gt;));// Poll the topic to retrieve recordswhile(true) {    ConsumerRecords&lt;String, ABC_Assets_Schema&gt; records = consumer.poll(Duration.ofSeconds(5));    for (ConsumerRecord&lt;String, ABC_Assets_Schema&gt; record : records) {        ABC_Assets_Schema specificRecord = record.value();        // Get fields and values from the specific record, for example:        // String titleValue = specificRecord.getTitle().toString();    }}The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsDeserializer, telling Kafka to use the Event Streams deserializer for message values when consuming messages. You can also use the Event Streams deserializer for message keys. Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message deserialization behavior SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to read the field values from your message.   Specific Java classes that are generated from the schema definition will have get&lt;field-name&gt; methods that can be used to easily retrieve the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named getAuthor which returns a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the get method in the GenericRecord class to set field names and values.Note: Replace &lt;my_consumer_group&gt; with the name of the consumer group to use and &lt;my_topic&gt; with the name of the topic to consume messages from. Setting up Kafka Streams applications Kafka Streams applications can also use the Event Streams schema registry serdes library to serialize and deserialize messages. For example: // Set the Event Streams serdes properties, including the override option to set the schema// and version used for serializing produced messages.Map&lt;String, Object&gt; serdesProps = new HashMap&lt;String, Object&gt;();serdesProps.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");serdesProps.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE, \"ABC_Assets_Schema\");serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE, \"1.0.0\");serdesProps.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);serdesProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");serdesProps.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");// Set up the Kafka StreamsBuilderStreamsBuilder builder = new StreamsBuilder();// Configure a Kafka Serde instance to use the Event Streams schema registry// serializer and deserializer for message valuesSerde&lt;IndexedRecord&gt; valueSerde = new EventStreamsSerdes();valueSerde.configure(serdesProps, false);// Get the stream of messages from the source topic, deserializing each message value with the// Event Streams deserializer, using the schema and version specified in the message headers.builder.stream(&lt;my_source_topic&gt;, Consumed.with(Serdes.String(), valueSerde))    // Get the 'nextcount' int field from the record.    // The Event Streams deserializer constructs instances of the generated schema-specific    // ABC_Assets_Schema_Count class based on the values in the message headers.    .mapValues(new ValueMapper&lt;IndexedRecord, Integer&gt;() {        @Override        public Integer apply(IndexedRecord val) {            return ((ABC_Assets_Schema_Count) val).getNextcount();        }    })    // Get all the records    .selectKey((k, v) -&gt; 0).groupByKey()    // Sum the values    .reduce(new Reducer&lt;Integer&gt;() {        @Override        public Integer apply(Integer arg0, Integer arg1) {            return arg0 + arg1;        }    })    .toStream()    // Map the summed value to a field in the schema-specific generated ABC_Assets_Schema class    .mapValues(        new ValueMapper&lt;Integer, IndexedRecord&gt;() {            @Override            public IndexedRecord apply(Integer val) {                ABC_Assets_Schema record = new ABC_Assets_Schema();                record.setSum(val);                return record;            }     })     // Finally, put the result to the destination topic, serializing the message value     // with the Event Streams serializer, using the overridden schema and version from the     // configuration.    .to(&lt;my_destination_topic&gt;, Produced.with(Serdes.Integer(), valueSerde));// Create and start the streamfinal KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig);streams.start();In this example, the Kafka StreamsBuilder is configured to use the com.ibm.eventstreams.serdes.EventStreamsSerdes class, telling Kafka to use the Event Streams deserializer for message values when consuming messages and the Event Streams serializer for message values when producing messages. Note: The Kafka Streams org.apache.kafka.streams.kstream API does not provide access to message headers, so to produce messages with the Event Streams schema registry headers, use the SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE and SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE configuration properties. Setting these configuration properties will mean produced messages are serialized using the provided schema version and the Event Streams schema registry message headers will be set. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: To re-use this example, replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;api_key&gt; with an API key which has read permissions for your Event Streams deployment, &lt;my_source_topic&gt; with the name of the topic to consume messages from and &lt;my_destination_topic&gt; with the name of the topic to produce messages to. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/setting-java-apps/",
        "teaser":null},{
        "title": "Setting non-Java applications to use schemas",
        "collection": "2019.2.1",
        "excerpt":"If you have producer or consumer applications created in languages other than Java, use the following guidance to set them up to use schemas. You can also use the REST producer API to send messages that are encoded with a schema. For a producer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Use an Apache Avro library for your programming language to read the schema definition from the local file and encode a Kafka message with it.  Set the schema registry headers in the Kafka message, so that consumer applications can understand which schema and version was used to encode the message, and which encoding format was used.  Send the message to Kafka.For a consumer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Consume a message from Kafka.  Check the headers for the Kafka message to ensure they match the expected schema ID and schema version ID.  Use the Apache Avro library for your programming language to read the schema definition from the local file and decode the Kafka message with it.Retrieving the schema definition from the schema registry Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Schemas tab and find your schema in the list.  Copy the schema definition into a new local file.          For the latest version of the schema, expand the row. Copy and paste the schema definition into a new local file.      For a different version of the schema, click on the row and then select the version to use from the list of schema versions. Click the Schema definition tab and then copy and paste the schema definition into a new local file.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster: cloudctl es init  Run the following command to list all the schemas in the schema registry: cloudctl es schemas  Select your schema from the list and run the following command to list all the versions of the schema: cloudctl es schema &lt;schema-name&gt;  Select your version of the schema from the list and run the following command to retrieve the schema definition for the version and copy it into a new local file: cloudctl es schema &lt;schema-name&gt; --version &lt;schema-version-id&gt; &gt; &lt;schema-definition-file&gt;.avscSetting headers in the messages you send to Event Streams Kafka Set the following headers in the message to enable applications that use the Event Streams serdes Java library to consume and deserialize the messages automatically. Setting these headers also enables the Event Streams UI to display additional details about the message. The required message header keys and values are listed in the following table.             Header name      Header key      Header value                  Schema ID      com.ibm.eventstreams.schemaregistry.schema.id      The schema ID as a string.              Schema version ID      com.ibm.eventstreams.schemaregistry.schema.version      The schema version ID as a string.              Message encoding      com.ibm.eventstreams.schemaregistry.encoding      Either JSON for Avro JSON encoding, or BINARY for Avro binary encoding.      Note: The schema version ID is the integer ID that is displayed when listing schema versions using the command cloudctl es schema &lt;schema-name&gt;. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/setting-nonjava-apps/",
        "teaser":null},{
        "title": "Migrating existing applications to the Event Streams schema registry",
        "collection": "2019.2.1",
        "excerpt":"If you are using the Confluent Platform schema registry, Event Streams provides a migration path for moving your Kafka consumers and producers over to use the Event Streams schema registry. Migrating schemas to Event Streams schema registry To migrate schemas, you can use schema auto-registration in your Kafka producer, or you can manually migrate schemas by downloading the schema definitions from the Confluent Platform schema registry and adding them to the Event Streams schema registry.  Migrating schemas with auto-registration When using auto-registration, the schema will be automatically uploaded to the Event Streams schema registry, and named with the subject ID (which is based on the subject name strategy in use) and a random suffix.  Auto-registration is enabled by default in the Confluent Platform schema registry client library. To disable it, set the auto.register.schemas property to false. Note: To auto-register schemas in the Event Streams schema registry, you need an API key that has operator role permissions (or higher) and permission to create schemas. You can generate API keys by using the ES UI or CLI. Using the UI:   Log in to your Event Streams UI as an administrator.  Click Connect to this cluster on the right.  Enter a name into the API key generation tool and click Produce, consume, create topics and schemas.  Enter your topic name or set All topics to on.  Enter the consumer group name and click Generate to generate an API key.Using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the following command to create a service ID with an API key that has permissions to auto-register schemas and produce messages to any topic:cloudctl es iam-service-id-create &lt;service-id-name&gt; --role operator --all-topics --all-schemasMigrating schemas manually To manually migrate the schemas, download the schema definitions from the Confluent Platform schema registry, and add them to the Event Streams Schema Registry. When manually adding schemas to the Event Streams Schema Registry, the provided schema name must match the subject ID used by the Confluent Platform schema registry subject name strategy. If you are using the default TopicNameStrategy, the schema name must be &lt;TOPIC_NAME&gt;-&lt;'value'|'key'&gt; If you are using the RecordNameStrategy, the schema name must be &lt;SCHEMA_DEFINITION_NAMESPACE&gt;.&lt;SCHEMA_DEFINITION_NAME&gt; For example, if you are using the default TopicNameStrategy as your subject name strategy, and you are serializing your data into the message value and producing to the MyTopic topic, then the schema name you must provide when adding the schema in the UI must be MyTopic-value For example, if you are using the RecordNameStrategy as your subject name strategy, and the schema definition file begins with the following, then the schema name you must provide when adding the schema in the UI must be org.example.Book: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [...If you are using the CLI, run the following command when adding the schema: cloudctl es schema-add --create --name org.example.Book --version 1.0.0 --file /path/to/Book.avsc Migrating a Kafka producer application To migrate a Kafka producer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your producer application to secure the connection between the producer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");        Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:    export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\       -Djavax.net.ssl.trustStorePassword=password\"      Migrating a Kafka consumer application To migrate a Kafka consumer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your consumer application to secure the connection between the consumer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");        Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:    export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\        -Djavax.net.ssl.trustStorePassword=password\"      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/migrating/",
        "teaser":null},{
        "title": "Using schemas with the REST producer API",
        "collection": "2019.2.1",
        "excerpt":"You can use schemas when producing messages with the Event Streams REST producer API. You simply add the following parameters to the API call:   schemaname: The name of the schema you want to use when producing messages.  schemaversion: The schema version you want to use when producing messages.For example, to use cURL to produce messages to a topic with the producer API, and specify the schema to be used, run the curl command as follows: curl \"https://192.0.2.171:30342/topics/&lt;topicname&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" -d '&lt;avro_encoded_message&gt;' -H \"Content-Type: application/json\" -H \"Authorization: Bearer &lt;apikey&gt;\" --cacert es-cert.pem  By adding these parameters to the API call, a lookup is done on the specified schema and its version to check if it is valid. If valid, the correct message headers are set for the produced message. Important: When using the producer API, the lookup does not validate the data in the request to see if it matches the schema. Ensure the message conforms to the schema, and that it has been encoded in the Apache Avro binary or JSON encoding format. If the message does not conform and is not encoded with either of those formats, consumers will not be able to deserialize the data. If the message has been encoded in the Apache Avro binary format, ensure the HTTP Content-Type header is set to application/octet-stream. If the message has been encoded in the Apache Avro JSON format, ensure the HTTP Content-Type header is set to application/json. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/using-with-rest-producer/",
        "teaser":null},{
        "title": "Using the schema API",
        "collection": "2019.2.1",
        "excerpt":"Event Streams provides a Java library to enable Kafka applications to serialize and deserialize messages using schemas stored in your Event Streams schema registry. Using the schema registry serdes library API, schema versions are automatically downloaded from your schema registry, checked to see if they are in a disabled or deprecated state, and cached. The schemas are used to serialize messages produced to Kafka and deserialize messages consumed from Kafka. Schemas downloaded by the schema registry serdes library API are cached in memory with a 10 minute expiration period. This means that if a schema is deprecated or disabled, it might take 10 minutes before consuming or producing applications will see the change. To change the expiration period, set the SchemaRegistryConfig.PROPERTY_SCHEMA_CACHE_REFRESH_RATE configuration property to a new milliseconds value. For more details, including code snippets that use the schema registry serdes API, see setting Java applications to use schemas. For full details of the Event Streams schema registry serdes API, see the Schema API Javadoc. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/schemas/schema-api/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "2019.2.1",
        "excerpt":"You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:   Cluster (cluster): you can control which users and applications can connect to the cluster.  Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.What roles can I assign? Roles define the levels of access a user or application has to resources. The following table describes the roles you can assign in IBM Cloud Private.             Role      Permitted actions      Example actions                  Viewer      Viewers have permissions to perform read-only actions within IBM Event Streams such as viewing resources.      Allow an application to connect to a cluster by assigning read access to the cluster resource.              Editor      Editors have permissions beyond the Viewer role, including writing to IBM Event Streams resources such as topics.      Allow an application to produce to topics by assigning editor access to topic resources.              Operator      Operators have permissions beyond the Editor role, including creating and editing IBM Event Streams resources.      Allow access to create resources by assigning operator access to the IBM Event Streams instance.              Auditor      No actions are currently assigned to this role.                     Administrator      Administrators have permissions beyond the Operator role to complete privileged actions.      Allow full access to all resources by assigning administrator access to the IBM Event Streams instance.      Mapping service actions to roles Access control in Apache Kafka is defined in terms of operations and resources. In IBM Event Streams, the operations are grouped into a smaller set of service actions, and the service actions are then assigned to roles. The mapping between Kafka operations and service actions is described in the following table. If you understand the Kafka authorization model, this tells you how IBM Event Streams maps operations into service actions.             Resource type      Kafka operation      Service action                  Cluster      Describe      -                     Describe Configs      -                     Idempotent Write      -                     Create      cluster.manage                     Alter      RESERVED                     Alter Configs      cluster.manage                     Cluster Action      RESERVED              Topic      Describe      -                     Describe Configs      topic.read                     Read      topic.read                     Write      topic.write                     Create      topic.manage                     Delete      topic.manage                     Alter      topic.manage                     Alter Configs      topic.manage              Group      Describe      -                     Read      group.read                     Delete      group.manage              Transactional ID      Describe      -                     Write      txnid.write      In addition, IBM Event Streams adds another service action called cluster.read. This service action is used to control connection access to the cluster. Note: Where the service action for an operation is shown in the previous table as a dash -, the operation is permitted to all roles. The mapping between service actions and IBM Event Streams roles is described in the following table.             Resource type      Administrator      Operator      Editor      Viewer                  Cluster      cluster.read      cluster.read      cluster.read      cluster.read                     cluster.manage      cluster.manage                            Topic      topic.read      topic.read      topic.read      topic.read                     topic.write      topic.write      topic.write                            topic.manage      topic.manage                            Group      group.read      group.read      group.read      group.read                     group.manage      group.manage                            Transactional ID      txnid.write      txnid.write      txnid.write             Assigning access to users If you have not set up IBM Cloud Private teams, the default  admin user has unlimited access to all resources. The default admin user is defined at the time of installation in the IBM Cloud Private config.yaml file by using the default_admin_user parameter. If you are using IBM Cloud Private teams, you must associate the team with the Event Streams instance to apply the team members’ roles to the resources within the instance, including any users that have the Cluster Administrator role. You can do this by using the cloudctl es iam-add-release-to-team command. Important: If you installed Event Streams 2019.2.1 on IBM Cloud Private 3.2.0.1907 or later, you must add the namespace to your team before running the cloudctl es iam-add-release-to-team command.To add the namespace to your team, use the IBM Cloud Private administrator UI or run the following command:cloudctl iam resource-add &lt;team-id&gt; -r crn:v1:icp:private:k8:mycluster:n/&lt;namespace&gt;::: Running the cloudctl es iam-add-release-to-team command creates policies that grant access to resources based on the roles in the team. It is possible to refine user access to specific resources further and limit actions they can take against resources by using the IBM Cloud Private APIs. If you require such granular settings for security, contact us. Note: It can take up to 10 minutes after assigning access before users can perform tasks associated with their permissions. Common scenarios for users The following table summarizes common IBM Event Streams scenarios and the roles you need to assign.             Permission      Role required                  Allow full access to all resources      Administrator              Create and delete topics      Operator or higher              Generate the starter application to produce messages      Editor or higher              View the messages on a topic      Viewer or higher      Assigning access to applications Each application that connects to IBM Event Streams provides credentials associated with an IBM Cloud Private service ID. You assign access to a service ID by creating service policies. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Select the ServiceID you are interested in or create one.Warning: Do not use the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. This service ID is reserved to be used within the Events Streams cluster.Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information:   The role assigned to the policy. For example, Viewer, Editor, or Operator.  The type of service the policy applies to. For example, IBM Event Streams.  The instance of the service to be secured.  The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance.  The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance.You can create a single policy that does not specify either the resource type or the resource identifier. This kind of policy applies its role to all resources in the IBM Event Streams instance. If you want more precise access control, you can create a separate policy for each specific resource that the service ID will use. Note: It can take up to 10 minutes after assigning access before applications can perform tasks associated with their permissions. Common scenarios for applications If you choose to use a single policy to grant access to all resources in the IBM Event Streams instance, the following table summarizes the roles required for common scenarios.             Permission      Policies required                  Connect to the cluster      1. Role: Viewer or higher              Consume from a topic      1. Role: Viewer or higher              Produce to a topic      1. Role: Editor or higher              Use all features of the Kafka Streams API      1. Role: Operator or higher      Alternatively, you can assign specific service policies for the individual resources. The following table summarizes common IBM Event Streams scenarios and the service policies you need to assign.             Permission      Policies required                  Connect to the cluster      1. Resource type: cluster Role: Viewer or higher              Produce to a topic      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher              Produce to a topic using a transactional ID      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher  3. Resource type: txnid  Resource identifier: transactional_id Role: Editor or higher              Consume from a topic (no consumer group)      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher              Consume from a topic in a consumer group      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher  3. Resource type: group  Resource identifier: name_of_consumer_group Role: Viewer or higher      Revoking access for an application You can revoke access to IBM Event Streams by deleting the IBM Cloud Private service ID or API key that the application is using. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Enter an IBM Cloud Private administrator user name and password.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Find the Service ID being used by the application in the Service IDs list.  Remove either the service ID or the API key that the application is using. Removing the service ID also removes all API keys that are owned by the service ID.Warning: Do not remove the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. Removing this service ID corrupts your deployment, which can only be resolved by reinstalling Event Streams.  Remove the service ID by clicking  Menu overflow &gt; Remove in the row of the service ID. Click Remove Service ID on the confirmation dialog.  Remove the API key by clicking the service ID. On the service ID page, click API keys. Locate the API key being used by the application in the API keys list. CLick  Menu overflow &gt; Remove in the row of the API key. Click Remove API key on the confirmation dialog.Note: Revoking a service ID or API key in use by any Kafka client might not disable access for the application immediately. The API key is stored in a token cache in Kafka which has a 23 hour expiration period. When the token cache expires, it is refreshed from IBM Cloud Private and any revoked service IDs or API keys are reflected in the new token cache, causing application access be be disabled. To immediately disable application access, you can force a refresh of the Kafka token cache by restarting each Kafka broker. To do this without causing downtime, you can patch the stateful set by using the following command: kubectl -n &lt;namespace&gt; patch sts &lt;release_name&gt;-ibm-es-kafka-sts -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"restarted\":\"123\"}}}}}' This does not make changes to the broker configuration, but it still causes the Kafka brokers to restart one at a time, meaning no downtime is experienced. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "2019.2.1",
        "excerpt":"Network connections into the IBM Event Streams deployment are secured using TLS. By default, data within the Event Streams deployment is not encrypted. To secure this data, you must ensure that any storage and communication channels are encrypted as follows:   Encrypt data at rest by using disk encryption or encrypting volumes using dm-crypt.  Encrypt internal network traffic by using TLS encryption for communication between pods.  Encrypt messages in applications.Enabling encryption between pods By default, TLS encryption for communication between pods is disabled. You can enable it when installing Event Streams, or you can enable it later as described in this section. To enable TLS encryption for your existing Event Streams installation, use the UI or the command line.   To enable TLS by using the UI, follow the instructions in modifying installation settings, and set the Pod to pod encryption field of the Global install settings section to Enabled.      To enable TLS by using the command line, follow the instructions in modifying installation settings, and set the global.security.tlsInternal parameter to enabled as follows:     helm upgrade --reuse-values --set global.security.tlsInternal=enabled &lt;release_name&gt; &lt;charts.tgz&gt; --tls     For example: helm upgrade --reuse-values --set global.security.tlsInternal=enabled eventstreams ibm-eventstreams-prod-1.3.0.tgz --tls   Warning: If you enable TLS encryption between pods, the message browser will not display message data from before the upgrade. Important: Enabling TLS encryption between pods might impact the connection to Event Streams. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/security/encrypting-data/",
        "teaser":null},{
        "title": "Configuring secure JMX connections",
        "collection": "2019.2.1",
        "excerpt":"You can set up the Kafka broker JMX ports to be accessible to secure connections from within the IBM Cloud Private cluster. This grants applications deployed inside the cluster read-only access to Kafka metrics. By default, Kafka broker JMX ports are not accessible from outside the Kubernetes pod. To enable access, ensure you select the Enable secure JMX connections check box in the Kafka broker settings section when installing Event Streams. You can also enable secure JMX connections for existing installations by modifying your settings. When access is enabled, you can configure your applications to connect securely to the JMX port as follows. Enabling external connections When Event Streams is installed with the Enable secure JMX connections option, the Kafka broker is configured to start the JMX port with SSL and authentication enabled. The JMX port 9999 is opened on the Kafka pod and is accessible from within the cluster using the hostname &lt;releasename&gt;-ibm-es-kafka-broker-svc-&lt;brokerNum&gt;.&lt;namespace&gt;.svc To retrieve the local name of the service you can use the following command (the results do not have the &lt;namespace&gt;.svc suffix): kubectl -n &lt;namespace&gt; get services To connect to the JMX port, clients must use the following Java options:   javax.net.ssl.trustStore=&lt;path to trustStore&gt;  javax.net.ssl.trustStorePassword=&lt;password for trustStore&gt;In addition, clients must provide a username and password when initiating the JMX connection. Providing configuration values When secure JMX connections is enabled, a Kubernetes secret named &lt;releasename&gt;ibm-es-jmx-secret is created inside the Event Streams namespace. The secret contains the following content:             Name      Description                  truststore.jks      A Java truststore containing the certificates needed for SSL communication with the Kafka broker JMX port.              trust_store_password      The password associated with the truststore.              jmx_username      The user that is authenticated to connect to the JMX port.              jmx_password      The password for the authenticated user.      The Kubernetes secret’s contents must then be mounted as volumes and environment variables inside the application pod to provide the required runtime configuration to create a JMX connection. For example: apiVersion: v1kind: Podspec:  containers:    - name: container1      env:        - name: jmx_username          secretRef:            secretName: es-secret      ...      volumeMounts:        - name: es-volume          mountPath: /path/to/volume/on/pod/file/system  ...  volumes:    - name: es-volume      fromSecret:        secretName: es-secret        items:          - name: truststore.jks            path: jks.jksIf the connecting application is not installed inside the Event Streams namespace, it must be copied to the application namespace using the following command: kubectl -n &lt;releaseNamespace&gt; get secret &lt;releasename&gt;ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;applicationNamespace&gt; apply -f -","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/security/secure-jmx-connections/",
        "teaser":null},{
        "title": "Network policies",
        "collection": "2019.2.1",
        "excerpt":"The following tables provide information about the permitted network connections for each Event Streams pod. Kafka pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods, REST Producer pods, and Geo-replicator pods to port 8084      Kafka access              TCP      REST API pods to port 7070      Querying Kafka status              TCP      Proxy pods to port 8093      Proxied Kafka traffic              TCP      Other Kafka pods to port 9092      Kafka cluster traffic              TCP      To port 8081 on the IBM Cloud Private master host      Prometheus collecting metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      ZooKeeper pods on port 2181      Cluster metadata access              TCP      Other Kafka pods on port 9092      Kafka cluster traffic              TCP      Index Manager pods on port 8080      Kafka metrics              TCP      Access Controller pods on port 8443      Security API access              TCP      Collector pods on port 7888      Submitting metrics              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      ZooKeeper pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods and REST API pods to port 2181      ZooKeeper traffic              TCP      Other ZooKeeper pods to ports 2888 and 3888      ZooKeeper cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Other ZooKeeper pods on port 2888 and 3888      ZooKeeper cluster traffic      Geo-replicator pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods to port 8083      Geo-replicator API traffic              TCP      Other geo-replicator pods to port 8083      Geo-replicator cluster traffic              TCP      To port 8080 on the IBM Cloud Private master host      Allow Prometheus to collect metrics        Outgoing connections permitted: AnyAdministration UI pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST proxy pods on port 9080      REST API access              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access              TCP      Access Controller pods on port 8443      Access Controller API access              TCP      Port 4300 on the IBM Cloud Private master host      ICP identity API access      Administration server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 9080      Proxied REST API calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on ports 8084 and 7070      Kafka admin access              TCP      Index Manager pods on port 9080      Metric API access              TCP      Geo-replicator pods on port 8083      Geo-replicator API access              TCP      ZooKeeper pods on port 2181      ZooKeeper admin access              TCP      Anywhere      Coordination with REST API in other ES instances              UDP      Anywhere on port 53 on the IBM Cloud Private master host      Coordination with REST API in other ES instances      REST producer server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 8080      Proxied REST Producer calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8084      Sending Kafka messages      REST proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST API pods on port 9080      Proxying REST API calls              TCP      REST Producer pods on port 8080      Proxying REST Producer calls      Collector pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 7888      Receiving metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8080      Prometheus connections      Network proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on port 8093      Kafka client traffic              TCP      REST proxy pods on port 9080      Kafka admin      Access Controller pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods, REST API pods, and UI pods to port 8443      Allow components to make auth checks        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      Index manager pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 8080      Receiving metrics              TCP      Elastic and REST API pods to port 9080      Metrics access        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Elastic pods on port 9200      Elasticsearch admin access              TCP      REST proxy pods on port 9080      REST API access      Elasticsearch pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Index Manager pods to port 9200      Elasticsearch admin access              TCP      Other ElasticSearch pods to port 9300      ElasticSearch cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Index Manager pods on port 9080      Elastic admin              TCP      Other ElasticSearch pods on port 9300      ElasticSearch cluster traffic      Install jobs pod       Incoming connections permitted: None         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access      Telemetry pod       Incoming connections permitted: None         Outgoing connections permitted: Any   ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/security/network-policies/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "2019.2.1",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR Information Portal  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement. Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settings using the IBM Event Streams CLI. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemUser roles can be used to control access to data stored in IBM Event Streams accessed over these APIs. In addition, the Kubernetes APIs can be used to access cluster configuration and resources, including but not limited to logs that may contain message data. Access and autorization controls can be used to control which users are able to access this cluster level information. Data Processing Encryption of connection to IBM Event Streams Connections to IBM Event Streams are secured using TLS. When deploying IBM Event Streams, the default setting for the charts .Values.global.tls.type is “selfsigned”. In this case, a self-signed certificate is generated for use creating secure connections. Alternatively, .Values.global.tls.type can be set to “provided” and the TLS certificate (.Values.global.tls.cert), TLS private key (.Values.global.tls.key) and CA certificate (.Values.global.tls.cacert) can be specified to use an existing configuration. If a self-signed certificate is used, a certificate and key are generated for each installation of IBM Event Streams and stored securely within a Kubernetes secret. Clients can access the public key via any web browser in the usual manner.If the certificate is provided, you are responsible for provisioning this certificate, for ensuring it is trusted by the clients you will use and for protecting the key. Encryption of connections within IBM Event Streams Enhance your security by encrypting the internal communication between Event Streams pods by using TLS. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "2019.2.1",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.Note: Geo-replication is only available in the paid-for version of IBM Event Streams (not available in the Community Edition). How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster to maintain the correct message order on the geo-replicated topic. This behavior continues even when geo-replication is paused. You can set up geo-replication by using the IBM Event Streams UI or CLI. When replication is set up and working, you can switch to another cluster when needed. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "2019.2.1",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Prepare your destination cluster by setting the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Decide whether you want to include message history in the geo-replication, or only copy messages from the time of setting up geo-replication. By default, the message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  Decide whether the replicated topics on the destination cluster should have the same name as their corresponding topics on the origin cluster, or if a prefix should be added to the topic name. The prefix is the release name of the origin cluster. By default, the replicated topics on the destination cluster have the same name.Preparing destination clusters Before you can set up geo-replication and start replicating topics, you must configure the number of geo-replication workers on the destination cluster. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. You can use the same approach to determine the number as used when setting the number of brokers for your installation. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of installing IBM Event Streams, or you can modify an existing installation, even if you already have geo-replication set up and running on that installation. Configuring a new installation If you are installing a new IBM Event Streams instance for use as a destination cluster, you can specify the number of workers when configuring the installation. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI You have the option to specify the number of workers during the installation process on the Configure page. Go to the Geo-replication section and specify the number of workers in the Geo-replicator workers field. Using the CLI You have the option to specify the number of workers during the installation process by adding the --set replicator.replicas=&lt;number-of-workers&gt; to your helm install command. Configuring an existing installation If you decide to use an existing IBM Event Streams instance as a destination cluster, or want to change the number of workers on an existing instance used as a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing IBM Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Go to the Geo-replication settings section and modify the Geo-replicator workers field to the required number of workers.Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems.  Click Upgrade.Using the CLI To modify the number of workers by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Use the following helm command to modify the number of workers:helm upgrade --reuse-values --set replicator.replicas=&lt;number-of-workers&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.3.0.tgz --tls","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "2019.2.1",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Go to the Origin locations section, and click Generate connection information for this cluster under Want to replicate topics to this cluster?  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Click Add destination cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Alternatively, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Click Copy connection information to copy the connection details to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the destination cluster: cloudctl es init  Run the following command to create an API key for your destination cluster:cloudctl es geo-cluster-apikey The command provides the API URL and the API key required for creating a destination cluster, for example:--api-address https://192.0.2.24:32046 --api-key H4C2S6Moq7KuDcYRJaM4Ye_6-XShEnB6JHnATaDaBFQZ  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI on the origin cluster: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add --api-address &lt;api-url-from-step-3&gt; --api-key &lt;api-key-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.  Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip: You can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.  Optional: Select whether to add a prefix to the name of the new replicated topic that is created on the destination cluster. Click Add prefix to destination topic names to add the release name of the origin cluster as a prefix to the replicated topics.  Optional: Select whether you want to include the message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Click Include message history if you want to include history.  Click Create to create geo-replicators for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.Note: After clicking Create, it might take up to 5 to 10 minutes before geo-replication becomes active.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row as follows:       If topics are being replicated from the cluster you are logged into, the  icon is displayed in the topic’s row. The number in the brackets indicates the number of destination clusters the topic is being replicated to. Clicking  expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:         If topics are being replicated to the cluster you are logged in to, the topics have the following indication that geo-replication is set up for them. Clicking the From &lt;cluster-name&gt; link opens the geo-replication panel with more information about the origin cluster:   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. The command creates one replicator for each topic. To set up more that one geo-replicators at once, list each topic you want to replicate using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.          Optional: You can specify to add a prefix to the name of the new replicated topic that is created on the destination cluster by using the --prefix &lt;prefix-name&gt; option.      Optional: Select whether you want to include message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Set one of the following:                  Use the --from earliest option to include available message history in geo-replication. This means all available message data for the topic is copied.          Use the --from latest option to exclude available message history. This means that only message data from the time of setting up replication is copied.                    For example, to use all options to create the geo-replicators:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt; --from &lt;earliest or latest&gt; --prefix &lt;topic-name-prefix&gt;For example:cloudctl es geo-replicator-create --destination DestinationClusterId --topics MyTopicName1,MyTopicName2 --from latest --prefix GeoReplica- When your geo-replication is set up, you can monitor and manage it. Switching clusters When one of your origin IBM Event Streams clusters experiences problems and goes down, you are notified on the destination cluster UI that the origin cluster is offline. You can switch your applications over to use the geo-replicated topics on the destination cluster as follows.   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right.  Go to the Connect a client tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: Copy the Broker URL to connect an application to this topic.      Certificates: Download a certificate that is required by your Kafka clients to connect securely to this cluster.      API key: To connect securely to IBM Event Streams, your application needs an API key with permission to access the cluster and resources such as topics. Follow the instructions to generate an API key authorized to connect to the cluster, and select what level of access you want it to grant to your resources (topics). You can then select which topics you want included or to include all topics, and set consumer groups as well.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Decide whether you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic.       To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute         To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --execute   These methods also avoid the need to make code changes to your client application. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "2019.2.1",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming the copying of data for each topic, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name that includes the helm release name.      The health of the geo-replication for that origin cluster: CREATING, PAUSED, STOPPING, ASSIGNING, OFFLINE, and ERROR.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See details in the Origin locations section.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend copying of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication from the origin cluster.       More options &gt; Restart failed replicators: To restart geo-replication from the origin cluster for geo-replicators that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: RUNNING, RESUME, RESUMING, PAUSING, REMOVING, and ERROR. When the status is ERROR, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  See the section Destination locations.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for which you want to manage geo-replication, and choose from one of the following options:           More options &gt; Pause running replicator: To pause a geo-replicator and suspend copying of data to the destination cluster.      Resume button: To resume a geo-replicator for the destination cluster.       More options &gt; Restart failed replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove replicator: To remove a geo-replicator from the destination cluster.      You can take the same actions for all of the geo-replicators in a destination cluster using the  More options menu in the top right when browsing  destination cluster details (for example, pausing all geo-replicators or removing the whole cluster as a destination). Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters  Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination siliconvalley_es_byl6xThe command returns the following information:    Details of destination cluster siliconvalley_es_byl6xCluster ID               Cluster name    REST API URL                 Skip SSL validation?destination_byl6x        destination     https://9.30.119.223:31764   trueGeo-replicator detailsName                               Status    Origin bootstrap servers   Origin topic   Destination topictopic1__to__origin_topic1_evzoo    RUNNING   192.0.2.24:32237           topic1         origin_topic1topic2__to__topic2_vdpr0           PAUSED    192.0.2.24:32237           topic2         topic2topic3__to__topic3_9jc71           ERROR     192.0.2.24:32237           topic3         topic3topic4__to__topic4_nk87o           PENDING   192.0.2.24:32237           topic4         topic4      To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:          cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      You can also remove a cluster as a destination using the following command:  cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;  Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.      Tip: You can use short options instead of spelling out the long version. For example, use -d instead of --destination, or -n instead of --name. Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an error from Kafka that prevents it from continuing to produce or consume, such as an authentication error or all brokers being unavailable, it will stop replicating and report a status of Error. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click the Topics tab and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/georeplication/health/",
        "teaser":null},{
        "title": "Event Streams producer API",
        "collection": "2019.2.1",
        "excerpt":"Event Streams provides a REST API to help connect your existing systems to your Event Streams Kafka cluster. Using the API, you can integrate Event Streams with any system that supports RESTful APIs. The REST producer API is a scalable REST interface for producing messages to Event Streams over a secure HTTP endpoint. Send event data to Event Streams, utilize Kafka technology to handle data feeds, and take advantage of Event Streams features to manage your data. Use the API to connect existing systems to Event Streams, such as IBM Z mainframe systems with IBM z/OS Connect, systems using IBM DataPower Gateway, and so on. Create produce requests from your systems into Event Streams, including specifying the message key, headers, and the topics you want to write messages to. Note: You must have Event Streams version 2019.1.1 or later to use the REST API. Producing messages using REST Use the producer API to write messages to topics. To be able to produce to a topic, you must have the following available:   The URL of the Event Streams API endpoint, including the port number.  The topic you want to produce to.  The API key that gives permission to connect and produce to the selected topic.  The Event Streams certificate.To retrieve the full URL for the Event Streams API endpoint:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Copy the full URL from the Event Streams API endpoint field, including the port number.To create a topic and generate an API key with produce permissions, and to download the certificate:   If you have not previously created the topic, create it now:cloudctl es topic-create --name &lt;topic_name&gt; --partitions 1 --replication-factor 3  Create a service ID and generate an API key:cloudctl es iam-service-id-create --name &lt;serviceId_name&gt; --role editor --topic &lt;topic_name&gt;For more information about roles, permissions, and service IDs, see the information about managing access.  Copy the API key returned by the previous command.  Download the certificate for Event Streams:cloudctl es certificates --format pemYou have now gathered all the details required to use the producer API. You can use the usual languages for making the API call. For example, to use cURL to produce messages to a topic with the producer API, run the curl command as follows: curl -v -X POST -H \"Authorization: Bearer &lt;api_key&gt;\" -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;api_key&gt; is the API key you generated earlier.  &lt;api_endpoint&gt; is the full URL copied from the Event Streams API endpoint field earlier (format https://&lt;host&gt;:&lt;port&gt;)  &lt;topic_name&gt; is the name of the topic you want to produce messages to.For full details of the API, see the API reference. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/rest-api/",
        "teaser":null},{
        "title": "Kafka Connect and connectors",
        "collection": "2019.2.1",
        "excerpt":"You can integrate external systems with IBM Event Streams by using the Kafka Connect framework and connectors. What is Kafka Connect? When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Use Kafka Connect to reliably move large amounts of data between your Kafka cluster and external systems. For example, it can ingest data from sources such as databases and make the data available for stream processing.  Source and sink connectors Kafka Connect uses connectors for moving data into and out of Kafka. Source connectors import data from external systems into Kafka topics, and sink connectors export data from Kafka topics into external systems. A wide range of connectors exists, some of which are commercially supported. In addition, you can write your own connectors. A number of source and sink connectors are available to use with Event Streams. See the connector catalog section for more information.  Workers Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems, and all work is performed in a single process. Distributed mode is more appropriate for production use, as it benefits from additional features such as automatic balancing of work, dynamic scaling up or down, and fault tolerance.  When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to the external system (for example, IBM MQ).When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Connector catalog The connector catalog contains a list of connectors that have been verified with Event Streams. Connectors are either supported by the community or IBM. Community support means the connectors are supported through the community by the people that created them. IBM supported connectors are fully supported as part of the official Event Streams support entitlement if you are using the paid-for version of Event Streams (not Community Edition). See the connector catalog for a list of connectors that work with Event Streams.  Setting up connectors Event Streams provides help with setting up your Kafka Connect environment, adding connectors to that environment, and starting the connectors. See the instructions about setting up and running connectors. Running connectors on IBM Cloud Private If you have IBM MQ or another service running on IBM Cloud Private, you can use Kafka Connect and one or more connectors to flow data between your instance of IBM Event Streams and the service on IBM Cloud Private. In this scenario it makes sense to run Kafka Connect in IBM Cloud Private as well. See the instructions about running Kafka Connect and connectors on IBM Cloud Private Connectors for IBM MQ Connectors are available for copying data between IBM MQ and Event Streams. There is a MQ source connector for copying data from IBM MQ into Event Streams or Apache Kafka, and a MQ sink connector for copying data from Event Streams or Apache Kafka into IBM MQ. For more information about MQ connectors, see the topic about connecting to IBM MQ. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/connectors/",
        "teaser":null},{
        "title": "Setting up and running connectors",
        "collection": "2019.2.1",
        "excerpt":"IBM Event Streams helps you set up a Kafka Connect environment, prepare the connection to other systems by adding connectors to the environment, and start Kafka Connect with the connectors to help integrate external systems. Log in to the Event Streams UI, and click the Toolbox tab. Scroll to the Connectors section and follow the guidance for each main task. You can also find additional help on this page. Setting up a Kafka Connect environment Set up the environment for hosting Kafka Connect. You can then use Kafka Connect to stream data between Event Streams and other systems. Kafka Connect can be run in standalone or distributed mode. For more details see the explanation of Kafka Connect workers. Kafka Connect includes shell and bash scripts for starting workers that take configuration files as arguments. For best results running Kafka Connect alongside Event Streams start Kafka Connect in distributed mode in Docker containers. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. To begin using Kafka Connect in distributed mode, follow the steps below, then add the connectors to your other systems and start Kafka Connect in its Docker container. Note: The Kafka Connect Docker container is designed for a Linux environment. Create topics When running in distributed mode Kafka Connect uses three topics to store configuration, current offsets and status. In standalone mode Kafka Connect uses a local file. Create the following topics:   connect-configs: This topic will store the connector and task configurations.  connect-offsets: This topic is used to store offsets for Kafka Connect.  connect-status: This topic will store status updates of connectors and tasks.Note: The topic names match the default settings. If you change these settings in your Kafka Connect properties file create topics that match the names you provided. Using the UI   Click the Topics tab.  Click Create topic.  Set Advanced to on.  Create the three topics with the following parameters, leaving other parameters as default. Name, partitions and replicas can be edited in Core configuration and cleanup policy can be edited in Log:            Name      Partitions      Replicas      Cleanup policy                  connect-configs      1      3      compact                  Name      Partitions      Replicas      Cleanup policy                  connect-offsets      25      3      compact                  Name      Partitions      Replicas      Cleanup policy                  connect-status      5      3      compact      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the following commands to create the topics:    cloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compactcloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compactcloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact      Provide or generate an API key This API key must provide permission to produce and consume messages for all topics, and also to create topics. This will enable Kafka Connect to securely connect to your IBM Event Streams cluster.   In the Event Streams UI, click the Toolbox tab. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  Go to step 4 and either paste an API key that provides permission to produce and consume messages, and also to create topics, or click Generate API key to create a new key.Download Kafka Connect On the same page, go to step 5, and click Download Kafka Connect ZIP to download the compressed file, then extract the contents to your preferred location. Adding connectors to your Kafka Connect environment Prepare Kafka Connect for connections to your other systems by adding the required connectors. If you are following on from the previous step, you can click READ NEXT at the bottom of the page. You can also access this page by clicking the Toolbox tab, scrolling to the Connectors section, and clicking Add connectors on the Add connectors to your Kafka Connect environment tile. To run a particular connector Kafka Connect must have access to a JAR file or set of JAR files for the connector. The quickest way to do this is by adding the JAR file(s) to the classpath of Kafka Connect. This is not the recommended approach because it does not provide classpath isolation for different connectors. Since version 0.11.0.0 of Kafka the recommended approach is to configure the plugin.path in the Kafka Connect properties file to point to the location of your connector JAR(s). If you are using the provided Kafka Connect ZIP, the resulting Docker image will copy all connectors in the /connectors directory into the container on the plugin.path. Copy the connector JAR file(s) you want to have available into the /connectors directory: cp &lt;path_to_your_connector&gt;.jar &lt;extracted_zip&gt;/connectors Starting Kafka Connect with your connectors If you are following on from the previous step, you can click READ NEXT at the bottom of the page. You can also access this page by clicking the Toolbox tab, scrolling to the Connectors section, and clicking Start Kafka Connect on the Start Kafka Connect with your connectors tile. You can run Kafka Connect in standalone or distributed mode by using the connect-standalone.sh or connect-distributed.sh scripts that are included in the bin directory of a Kafka install. If using the provided Kafka Connect ZIP, Kafka Connect can be built and run using Docker commands as follows.   Build Kafka Connect Docker container. Go to the location where you extracted the Kafka Connect ZIP file you downloaded earlier as part of setting up your environment, and build the Kafka Connect Docker image:    cd kafkaconnectdocker build -t kafkaconnect:0.0.1 .        Run the Docker container:docker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 kafkaconnect:0.0.1  Verify that your chosen connectors are installed in your Kafka Connect environment:curl http://localhost:8083/connector-pluginsA list of connector plugins available is displayed.Starting a connector Start a connector by using the Kafka Connect REST API. When running in distributed mode connectors are started using a POST request against your running Kafka Connect. The endpoint requires a body that includes the configuration for the connector instance you want to start. Most connectors include examples in their documentation. The Event Streams UI and CLI provide additional assistance for connecting to IBM MQ. See the connecting MQ instructions for more details. For example, to create a FileStreamSource connector you can create a file with the following contents: {   \"name\": \"my-connector\",   \"config\": {      \"connector.class\": \"FileStreamSource\",      \"file\": \"config/connect-distributed.properties\",      \"topic\":\"kafka-config-topic\"   }}  Once you have created a JSON file with the configuration for your chosen connector start the connector using the REST API:    curl -X POST http://localhost:8083/connectors \\ -H \"Content-Type: application/json\"  \\ -d @&lt;config&gt;.json        View the status of a connector by using the Kafka Connect REST API:curl http://localhost:8083/connectors/&lt;connector_name&gt;/statusRepeat for each connector you want to start.For more information about the other REST API endpoints (such as pausing, restarting, and deleting connectors) see the Kafka Connect REST API documentation. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/setting-up-connectors/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "2019.2.1",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Available connectors Connectors are available for copying data in both directions.   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue. Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. If you have IBM MQ running on IBM Cloud Private, you can use the IBM MQ Connectors to connect your Event Streams to IBM MQ on IBM Cloud Private. See the instructions about running connectors on IBM Cloud Private. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For performance reasons, it is recommended to run the Kafka Connect worker close to the queue manager to minimize the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "2019.2.1",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in a Docker container. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. For distributed mode the configuration is in JSON format and in standalone mode it is a .properties file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to your IBM Event Streams UI.  Click the Toolbox tab and scroll to the Connectors section.  Go to the Connecting to IBM MQ? tile, and click Add connectors.  Click the IBM MQ connectors link.  Ensure the MQ Source tab is selected and click on the Download MQ Source Configuration, this will display another window.  Use the relevant fields to alter the configuration of the MQ Source connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to download a .json or .properties file which can be used in distributed or standalone mode.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-source command to generate the configuration file for the MQ Source connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSOURCE and connecting to the topic TSOURCE, run the following command:    cloudctl es connector-config-mq-source --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSOURCE\" --topic=\"TSOURCE\" --file=\"mq-source\" --json        Note: Omitting the --json flag will generate a mq-source.properties file which can be used for standalone mode.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: {\t\"name\": \"mq-source\",\t\"config\": {\t\t\"connector.class\": \"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",\t\t\"tasks.max\": \"1\",\t\t\"topic\": \"TSOURCE\",\t\t\"mq.queue.manager\": \"QM1\",\t\t\"mq.connection.name.list\": \"localhost(1414)\",\t\t\"mq.channel.name\": \"MYSVRCONN\",\t\t\"mq.queue\": \"MYQSOURCE\",\t\t\"mq.user.name\": \"alice\",\t\t\"mq.password\": \"passw0rd\",\t\t\"mq.record.builder\": \"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",\t\t\"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\"\t}}A list of all the possible flags can be found by running the command cloudctl es connector-config-mq-source --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Source connector   Log in to your IBM Event Streams UI.  Click the Toolbox tab and scroll to the Connectors section.  Go to the Connecting to IBM MQ? tile, and click Add connectors.  Ensure the MQ Source tab is selected and click on the Download MQ Source JAR, this will download the MQ Source JAR file.Configuring Kafka Connect IBM Event Streams provides help with getting a Kafka Connect Environment. Follow the steps in set up Kafka Connect to get Kafka Connect running. When adding connectors add the MQ connector you downloaded earlier. Verify that the MQ source connector is available in your Kafka Connect environment:\\ $ curl http://localhost:8083/connector-plugins[{\"class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",\"type\":\"source\",\"version\":\"1.1.0\"}]Verify that the connector is running. For example, If you started a connector called mq-source:\\ $ curl http://localhost:8083/connectors[mq-source]Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ:  INFO Created connector mq-source INFO Connection to MQ establishedSend a test message   To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;  Log in to your IBM Event Streams UI.  Navigate to the the Topics tab and select the connected topic. Messages will appear in the message browser of that topic.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "2019.2.1",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in a Docker container. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and put messages on a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. For distributed mode the configuration is in JSON format and in standalone mode it is a .properties file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to your IBM Event Streams UI.  Click the Toolbox tab and scroll to the Connectors section.  Go to the Connecting to IBM MQ? tile, and click Add connectors.  Click the IBM MQ connectors link.  Ensure the MQ Sink tab is selected and click on the Download MQ Sink Configuration, this will display another window.  Use the relevant fields to alter the configuration of the MQ Sink connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to download a .json or .properties file which can be used in distributed or standalone mode.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-sink command to generate the configuration file for the MQ Sink connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSINK and connecting to the topics TSINK, run the following command:    cloudctl es connector-config-mq-sink --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSINK\" --topics=\"TSINK\" --file=\"mq-sink\" --json        Note: Omitting the --json flag will generate a mq-sink.properties file which can be used for standalone mode.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: {\t\"name\": \"mq-sink\",\t\"config\": {\t\t\"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\t\t\"tasks.max\": \"1\",\t\t\"topics\": \"TSINK\",\t\t\"mq.queue.manager\": \"QM1\",\t\t\"mq.connection.name.list\": \"localhost(1414)\",\t\t\"mq.channel.name\": \"MYSVRCONN\",\t\t\"mq.queue\": \"MYQSINK\",\t\t\"mq.user.name\": \"alice\",\t\t\"mq.password\": \"passw0rd\",\t\t\"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\t\t\"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\"\t}}A list of all the possible flags can be found by running the command cloudctl es connector-config-mq-sink --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Sink connector   Log in to your IBM Event Streams UI.  Click the Toolbox tab and scroll to the Connectors section.  Go to the Connecting to IBM MQ? tile, and click Add connectors.  Ensure the MQ Sink tab is selected and click on the Download MQ Sink JAR, this will download the MQ Sink JAR file.Configuring Kafka Connect IBM Event Streams provides help with getting a Kafka Connect Environment. Follow the steps in set up Kafka Connect to get Kafka Connect running. When adding connectors add the MQ connector you downloaded earlier. Verify that the MQ sink connector is available in your Kafka Connect environment:\\ $ curl http://localhost:8083/connector-plugins[{\"class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"type\":\"sink\",\"version\":\"1.1.0\"}]Verify that the connector is running. For example, If you started a connector called mq-sink:\\ $ curl http://localhost:8083/connectors[mq-sink]Verify the log output of Kafka Connect includes the following messages that indicate the connector has started and successfully connected to IBM MQ:  INFO Created connector mq-sink INFO Connection to MQ establishedSend a test message To test the connector you will need an application to produce events to your topic.   Log in to your IBM Event Streams UI.  Click the Toolbox tab.  Click Generate application under Starter application  Enter a name for the application  Select only Produce messages  Select Choose existing topic and choose the topic you provided in the MQ connector configuration  Click Generate  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningVerify the message is on the queue:   Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "2019.2.1",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. These instructions explain how to run Kafka Connect in both standalone and distributed mode. For more information and to help decide which mode to use see the explanation of Kafka Connect workers. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka Connect shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the Kafka Connect files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka Connect shell scripts and JAR files. To download Kafka Connect and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the Support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.11-2.2.1.tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.Download IBM MQ connectors and configuration Depending on the connector you want to use:\\   Download the source connector JAR and source configuration file  Download the sink connector JAR and configuration fileIf you want to run a standalone Kafka Connect worker you need a .properties file. To run a distributed Kafka Connect worker you need a .json file. Copy the connector JAR file(s) and the required configuration file to a directory on the z/OS Unix System Services. Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-standalone.sh  config/connect-standalone.properties  mq-source.properties or mq-sink.propertiesIf you want to run a distributed Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-distributed.sh  config/connect-distributed.shExtract the Apache Kafka distribution:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.11-2.0.0.tar  Change to the resulting kafka_&lt;version&gt; directory.Convert the shell scripts:   Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.shConvert the configuration files:   Copy the connect-standalone.properties file (or connect-distributed.properties for a distributed setup) into the current directory, for example:cp config/connect-standalone.properties ./connect-standalone.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.properties.orig &gt; config/connect-standalone.propertiesIf running in standalone mode:   Copy the MQ .properties file into the current directory, for example:cp ./mq-source.properties ./mq-source.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./mq-source.properties.orig &gt; ./mq-source.propertiesNote: For distributed mode the .json file must remain in ASCII format. Update the Kafka Connect configuration The connect-standalone.properties (or connect-distributed.properties for distributed mode) file must include the correct bootstrap.servers and SASL/SSL configuration for your Apache Kafka or Event Streams install. For example if running against Event Streams download the certificate for your install to your IBM z/OS system. Generate an API key that can produce, consume and create topics and update the connect-standalone.properties (or connect-distributed.properties) file to include: bootstrap.servers=&lt;bootstrapServers&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/opt/kafka/es-cert.jksssl.truststore.password=passwordsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=/opt/kafka/es-cert.jksproducer.ssl.truststore.password=passwordproducer.sasl.mechanism=PLAINproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=/opt/kafka/es-cert.jksconsumer.ssl.truststore.password=passwordconsumer.sasl.mechanism=PLAINconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";plugin.path=/opt/connectorsConfiguring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.1.0-jar-with-dependencies.jar. As you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/mq/zos/",
        "teaser":null},{
        "title": "Running connectors on IBM Cloud Private",
        "collection": "2019.2.1",
        "excerpt":"If you have IBM MQ or another service running on IBM Cloud Private, you can use Kafka Connect and one or more connectors to flow data between your instance of IBM Event Streams and the service on IBM Cloud Private. In this scenario it makes sense to run Kafka Connect in IBM Cloud Private as well. Downloading connectors The connector catalog contains a list of connectors that have been verified with Event Streams. Go to the connector catalog and download the JAR file(s) for any connectors you want to use. The JAR files for the IBM MQ source and sink connectors can be downloaded from the IBM Event Streams UI. Log in to your IBM Event Streams UI, click the Toolbox tab and look for the tile called “Add connectors to your Kafka Connect environment”. Building a Kafka Connect Docker image The Event Streams UI provides a toolbox page to help you get started with Kafka Connect. This provides a Dockerfile that builds a custom Kafka Connect with the Connectors you include. If you do not already have the Dockerfile, follow the steps to download the Kafka Connect ZIP and build a Docker image.   In the Event Streams UI, click the Toolbox tab. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  If you have not already done so follow the instructions to create three topics for Kafka Connect to use.  You need to provide an API key for Kafka Connect that has permission to Produce, Consume and create Topics. Paste in your API key, or click the button to generate one.NOTE: You must have a cluster admin role to generate an API key.  Click Download Kafka Connect ZIP to download the zip.  Extract the contents of the Kafka Connect .zip file to a local directory.  Copy the connector JAR files you downloaded earlier into the connectors folder in the extracted .zip foldercp &lt;path_to_your_connector&gt;.jar &lt;extracted_zip&gt;/connectors  Build the container: docker build -t kafkaconnect:0.0.1 .Uploading the Kafka Connect container To make the Kafka Connect container available on IBM Cloud Private it needs to be pushed to your IBM Cloud Private container registry.   Set up your Kubernetes command-line tool kubectl to access your IBM Cloud Private instance, for example, by running cloudctl login.  Create a namespace to deploy the Kafka Connect workers to: kubectl create namespace &lt;namespace&gt;  Log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443docker login &lt;cluster_CA_domain&gt;:8500        For more information, see the IBM Cloud Private documentation.     Retag and push the Docker image as follows:    docker tag kafkaconnect:0.0.1 &lt;cluster_CA_domain&gt;:8500/&lt;namespace&gt;/kafkaconnect:0.0.1docker push &lt;cluster_CA_domain&gt;:8500/&lt;namespace&gt;/kafkaconnect:0.0.1        Check this has worked by logging into your IBM Cloud Private UI and clicking on Container Images in the menu.Note: The namespace you provide is the one you will run the Kafka Connect workers in. Creating a Secret resource for the Kafka Connect configuration To enable updates to the Kafka Connect configuration the running container will need access to a Kubernetes resource containing the contents of connect-distributed.properties. The file is included in the extracted ZIP for Kafka Connect from the Event Streams UI. This file includes API keys so create a Secret: kubectl -n &lt;namespace&gt; create secret generic connect-distributed-config --from-file=&lt;extracted_zip&gt;/config/connect-distributed.propertiesCreating a ConfigMap resource for the Kafka Connect log4j configuration To enable updates to the Kafka Connect logging configuration create a ConfigMap with the contents of connect-log4j.properties. The file is included in the extracted ZIP for Kafka Connect from the Event Streams UI: kubectl -n &lt;namespace&gt; create configmap connect-log4j-config --from-file=&lt;extracted_zip&gt;/config/connect-log4j.propertiesCreating the Kafka Connect deployment To create the Kafka Connect deployment first create a yaml file called kafka-connect.yaml with the following contents: (Replace &lt;namespace&gt; with your IBM Cloud Private namespace) # Deployment  apiVersion: apps/v1  kind: Deployment  metadata:    name: kafkaconnect-deploy    labels:      app: kafkaconnect  spec:    replicas: 1    selector:      matchLabels:        app: kafkaconnect    template:      metadata:        namespace: &lt;namespace&gt;        labels:          app: kafkaconnect      spec:        securityContext:          runAsNonRoot: true          runAsUser: 5000        containers:          - name: kafkaconnect-container            image: kafkaconnect:0.0.1            readinessProbe:              httpGet:                path: /                port: 8083            livenessProbe:              httpGet:                path: /                port: 8083            ports:            - containerPort: 8083            volumeMounts:            - name: connect-config              mountPath: /opt/kafka/config/connect-distributed.properties              subPath: connect-distributed.properties            - name: connect-log4j              mountPath: /opt/kafka/config/connect-log4j.properties              subPath: connect-log4j.properties        volumes:        - name: connect-config          secret:            secretName: connect-distributed-config        - name: connect-log4j          configMap:            name: connect-log4j-config  ---  # Service  apiVersion: v1  kind: Service  metadata:    name: kafkaconnect-service    labels:      app: kafkaconnect-service  spec:    type: NodePort    ports:      - name: kafkaconnect        protocol: TCP        port: 8083    selector:        app: kafkaconnectThis defines the deployment that will run Kafka Connect and the service used to access it. Create the deployment and service using: kubectl -n &lt;namespace&gt; apply -f kafka-connect.yaml Use kubectl -n &lt;namespace&gt; get service kafkaconnect-service to view your running services. The port mapping shows 8083 being mapped to an external port. Use the external port to verify the IBM MQ Connectors you included have been installed: curl http://&lt;serviceIP&gt;:&lt;servicePort&gt;/connector-plugins Running a connector To start a Connector instance, you need to create a JSON file with the connector configuration. Most connectors will have an example in their documentation. For the IBM MQ connectors this file can be generated in the Event Streams UI or CLI. See connecting to IBM MQ for more details. Once you have a JSON file use the /connectors endpoint to start the connector: curl -X POST http://&lt;serviceIP&gt;:&lt;servicePort&gt;/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonFor more information about the other REST API endpoints (such as pausing, restarting, and deleting connectors) see the Kafka Connect REST API documentation. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/connecting/icp/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "2019.2.1",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Using the UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready. To find out more about the problem:   Click the message to expand it, and then expand the section for the component that does not have a green tick next to it.  Click the Pod is not ready link to open more details about the problem. The link opens the IBM Cloud Private UI. Log in as an administrator.  To understand why the IBM Event Streams resource is not available, click the Events tab to view details about the cause of the problem.  For more detailed information about the problem, click the Overview tab, and click  More options &gt; View logs on the right in the Pod details panel.  For guidance on resolving common problems that might occur, see the troubleshooting section.Using the CLI You can check the health of your IBM Event Streams environment using the Kubernetes CLI.   Ensure you have the Kubernetes command line tool installed, and configure access to your cluster.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:kubectl -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.  To retrieve further details about the pods, including events affecting them, use the following command:kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about using the kubectl command for debugging, see the Kubernetes documentation. Note: After a component restarts, the kubectl command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the –previous option to the kubectl logs command. Tip: You can also use the management logging service, or Elastic Stack, deployed by IBM Cloud Private to find more log information. Setting up the built-in Elastic Stack is part of the installation planning tasks. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "2019.2.1",
        "excerpt":"Monitoring the health of your Kafka cluster ensures your operations run smoothly. Event Streams collects metrics from all of the Kafka brokers and exports them to a Prometheus-based monitoring platform. The metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. You can use the metrics as follows:   View a selection of metrics on a configured dashboard in the Event Streams UI.      Create dashboards in the Grafana service that is provided in IBM Cloud Private. You can download example Grafana dashboards for Event Streams from GitHub.     For more information about the monitoring capabilities provided in IBM Cloud Private, including Grafana, see the IBM Cloud Private documentation.     To install the configured Grafana dashboards, follow these steps:           Download the dashboards you would like to install from Github.      Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Navigate to the IBM Cloud Private console homepage.      Click the hamburger icon on the top left.      Expand Platform.      Click the Monitoring to navigate you to the Grafana homepage.      On the Grafana homepage, click on the Home icon on the top left to bring down a view of all the pre-installed dashboards.      Click on the Import Dashboards and either paste the JSON of the dashboard you want to install or import the dashboard’s JSON file that was downloaded in step 1.      Navigate to the Grafana homepage again and click on the Home icon again then find the Dashboard you have installed to view it.        Ensure you select your namespace, release name, and other filters at the top of the dashboard to view the required information.     Create alerts so that metrics that meet predefined criteria are used to send notifications to emails, Slack, PagerDuty, and so on. For an example of how to use the metrics to trigger alert notifications, see how you can set up notifications to Slack.      Create dashboards in the Kibana service that is provided in IBM Cloud Private. You can download example Kibana dashboards for Event Streams from GitHub to monitor for specific errors in the logs and set up alerts for when a number of errors over a period of time in your Event Streams instance.     For more information about the logging capabilities provided in IBM Cloud Private, including Kibana, see the IBM Cloud Private documentation.     To download the preconfigured Kibana Dashboards, follow these steps:           Download Event Streams Kibana Dashboard.json from GitHub      Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Navigate to the IBM Cloud Private console homepage.      Click the hamburger icon on the top left.      Expand the Platform.      Click the Logging to the Kibana homepage.      Click the Management on the left.      Click on the Saved Objects.      Click the Import icon and navigate the Event Streams Kibana Dashboard.json that you have downloaded.      Click on the Dashboard tab on the left hand side menu and you should see the downloaded dashboards.      You can also use external monitoring tools to monitor the deployed Event Streams Kafka cluster. For information about the health of your topics, check the producer activity dashboard. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitor dashboard.   Log in to Event Streams as an administrator  Click the Monitor tab. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Click a chart to drill down into more detail.  Click 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/cluster-health/",
        "teaser":null},{
        "title": "Monitoring topic health",
        "collection": "2019.2.1",
        "excerpt":"To gain an insight into the overall health of topics and highlight potential performance issues with systems producing to Event Streams, you can use the Producer dashboard provided for each topic. The dashboard displays aggregated information about producer activity for the selected topic through metrics such as message produce rates, message size, and an active producer count. The dashboard also displays information about each producer that has been producing to the topic. You can expand an individual producer record to gain insight into its performance through metrics such as messages produced, message size and rates, failed produce requests and any occurences where a producer has exceeded a broker quota. The information displayed on the dashboard can also be used to provide insight into potential causes when applications experience issues such as delays or ommissions when consuming messages from the topic. For example, highlighting that a particular producer has stopped producing messages, or has a lower message production rate than expected. Important: The producers dashboard is intended to help highlight producers that may be experiencing issues producing to the topic. You may need to investigate the producer applications themselves to identify an underlying problem. To access the dashboard:   Log in to Event Streams as an administrator.  Click the Topics tab.      Select the topic name from the list you want to view information about.The Producers tab is displayed with the dashboard and details about each producer. You can refine the time period for which information is displayed. You can expand each producer to view details about their activity.     Note: When a new client starts producing messages to a topic, it might take up to 5 to 10 minutes before information about the producer’s activity appears in the dashboard. In the meantime, you can go to the Messages tab to check whether messages are being produced.   Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/topic-health/",
        "teaser":null},{
        "title": "Monitoring with external tools",
        "collection": "2019.2.1",
        "excerpt":"You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. You must configure your installation to set up access for external monitoring tools. For examples about setting up monitoring with external tools such as Datadog, Prometheus, and Splunk, see the tutorials page. If you have a tool or service you want to use to monitor your clusters, you can raise a request. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/external-monitoring/",
        "teaser":null},{
        "title": "Modifying Kafka broker configurations",
        "collection": "2019.2.1",
        "excerpt":"You can use the IBM Event Streams CLI to dynamically modify brokers and cluster-wide configuration settings for your IBM Event Streams instance. You can also use the IBM Event Streams CLI together with a ConfigMap to modify static (read-only) configuration settings. Configuration options For a list of all configuration settings you can specify for Kafka brokers, see the Kafka documentation. Some of the broker configuration settings can be updated without restarting the broker, while others require a restart:   read-only: Requires a broker restart for the update to take effect.  per-broker: Can be updated dynamically for each broker without a broker restart.  cluster-wide: Can be updated dynamically as a cluster-wide default, or as a per-broker value for testing purposes.See the Dynamic Update Mode column in the Kafka documentation for the update mode of each broker configuration. Note: You cannot modify the following properties.   broker.id  listeners  zookeeper.connect  advertised.listeners  inter.broker.listener.name  listener.security.protocol.map  authorizer.class.name  principal.builder.class  sasl.enabled.mechanisms  log.dirs  inter.broker.protocol.version  log.message.format.versionModifying broker and cluster settings You can modify per-broker and cluster-wide configuration settings dynamically (without a broker restart) by using the IBM Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  To modify a per-broker configuration setting: cloudctl es broker-config --broker &lt;broker_id&gt; --config &lt;name&gt;=&lt;value&gt;  To modify a cluster-wide configuration setting: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt;You can also update your read-only configuration settings that require a broker restart by using the IBM Event Streams CLI. Note: Read-only settings require a ConfigMap to be set. If you did not create and specify a ConfigMap during the installation process, you can create a ConfigMap later with the required Kafka configuration settings or create a blank one to use later. Use the following command to make the ConfigMap available to your IBM Event Streams instance if you did not create a ConfigMap during installation: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. You can use the IBM Event Streams CLI to modify read-only configuration settings as follows: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt; --static-config-all-brokers ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/modifying-configs/",
        "teaser":null},{
        "title": "Modifying installation settings",
        "collection": "2019.2.1",
        "excerpt":"You can modify the configuration settings for your existing Event Streams installation by using the UI or the command line. The configuration changes are applied by updating the Event Streams chart. For example, you might need to modify settings to scale your installation due to changing requirements. Using the UI You can modify any of the configuration settings you specified during installation, or define values for ones previously not set at the time of installation. To modify configuration settings by using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Modify the values for the configuration settings you want to change.For example, to set the number of geo-replication workers to 4, go to the Geo-replication settings section and set the Geo-replicator workers field to 4.  Click Upgrade.Using the CLI You can modify any of the parameters you specified during installation, or define values for ones previously not set at the time of installation. For a list of all parameters, see the chart README file. To modify any of the parameter settings by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Use the following helm command to modify the value of a parameter:helm upgrade --reuse-values --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.3.0.tgz --tls","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/modifying-installation/",
        "teaser":null},{
        "title": "Running Helm upgrade commands",
        "collection": "2019.2.1",
        "excerpt":"You can use the helm upgrade command to upgrade your Event Streams version, or to modify configuration settings for your Event Streams installation. To run Helm upgrade commands, you must have a copy of the original Helm charts file that you used to install IBM Event Streams. To retrieve the charts file using the UI:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.  Click Catalog in the top navigation menu.  If you are using the Community Edition, search for ibm-eventstreams-dev and select it from the result. If you are using Event Streams, search for ibm-eventstreams-prod and select it from the result.  Select the latest version number from the drop-down list on the left.  To download the file, go to the SOURCE &amp; TAR FILES section on the left and click the link. The ibm-eventstreams-dev-&lt;version&gt;.tgz file is downloaded.Alternatively, if you downloaded IBM Event Streams from IBM Passport Advantage, you can also retrieve the charts file by looking for a file called ibm-eventstreams-prod-&lt;version&gt;.tgz within the downloaded archive. If you no longer have a copy, you can download the file again from IBM Passport Advantage. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/helm-upgrade-command/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "2019.2.1",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. You can start with the default installation parameters when deploying Event Streams, and test the system with a workload that is representative of your requirements. For this purpose, IBM Event Streams provides a workload generator application to test message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Important: To take full advantage of the scaling capabilities described in this topic, and avoid potential bottlenecks in high throughput environments, consider options for your IBM Cloud Private environment, such as setting up a load balancer and an internal network. For more information, see the topic about performance. Increase the number of Kafka brokers in the cluster To set this at the time of installation, you can use the --set kafka.brokers=&lt;NUMBER&gt; option in your helm install command if using the CLI, or enter the number in the Kafka brokers field of the Configure page if using the UI. To modify the number of Kafka brokers for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.brokers=&lt;NUMBER&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the CPU limit available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. A description of the syntax for these values can be found in the Kubernetes documentation. Increase the amount of memory available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values into the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI. The syntax for these values can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the memory available to supporting systems If you have significantly increased the memory available to Kafka brokers, you will likely need to make a similar increase in the memory available to the other components that support the Kafka brokers. Ensure you consider the following two components. The metrics reporter component captures the monitoring statistics for cluster, broker, and topic activity. The memory requirements for this component will increase with the number of topic partitions in the cluster, and the throughput on those topics. To set this at the time of installation, you can use the following options:--set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt;--set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; A description of the syntax for these values can be found in the Kubernetes documentation. The message indexer indexes the messages on topics to allow them to be searched in the IBM Event Streams UI. The memory requirements for this component will increase with the cluster message throughput. To set this at the time of installation, you can use the --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt; option in your helm install command if using the CLI, or enter the values into the Memory limits for Index Manager nodes fields of the Configure page if using the UI. The syntax for the container memory limits can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values  --set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt; --set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt;  &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Custom JVM tuning for Kafka brokers If you have specific requirements, you might need to further tune the JVMs running the Kafka brokers, such as modifying the garbage collection policies. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. To provide custom JVM parameters at the time of installation, you can use --set kafka.heapOpts=&lt;JVMOPTIONS&gt; option in your helm install command. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.heapOpts=&lt;JVMOPTIONS&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Use a faster storage class for PVCs used by Kafka brokers The speed of the storage available to Kafka brokers will impact performance. Set this at the time of installation with the --set kafka.persistence.dataPVC.storageClassName=&lt;STORAGE_CLASS&gt; option in your helm install command if using the CLI, or by entering the required storage class into the Storage class name field of the Kafka persistent storage settings section of the Configure page if using the UI. For more information about available storage classes, see the IBM Cloud Private documentation. Increase the disk space available to each Kafka broker The Kafka brokers will require sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods or sizes, and more topic partitions. Set this at the time of installation with the --set kafka.persistence.dataPVC.size=&lt;SIZE&gt; option in your helm install command if using the CLI, or by entering the required persistence size into the Size field of the Kafka persistent storage settings section of the Configure page if using the UI. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "2019.2.1",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both. Kafka quotas are supported in IBM Event Streams 2018.3.1 and later. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For more information about quotas, see the Kafka documentation. Setting quotas You can set quotas by using the Event Streams CLI as follows:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to initialize the Event Streams CLI:cloudctl es init  Use the entity-config command option to set quotas as follows.Decide what you want to limit by using a quota type, and set it with the --config &lt;quota_type&gt;option, where &lt;quota_type&gt; can be one of the following:   producer_byte_rate - This quota limits the number of bytes that a producer application is allowed to send per second.  consumer_byte_rate - This quota limits the number of bytes that a consumer application is allowed to receive per second.  request_percentage - This quota limits all clients based on thread utilisation.Decide whether you want to apply the quota to users or client IDs. To apply to users, use the --user &lt;user&gt; option. Event Streams supports 2 types of users: actual user principal names, or application service IDs.   A quota defined for a user principal name is only applied to that specific user name. To specify a principal name, you must prefix the value for the --user parameter with u-, for example, --user \"u-testuser1\"  A quota defined for a service ID is applied to all applications that are using API keys that have been bound to the specific service ID. To specify a service ID, you must prefix the value for the --user parameter with s-, for example, --user \"s-consumer_service_id\"To apply to client IDs, use the --client &lt;client id&gt; option. Client IDs are defined in the application using the client.id property. A client ID identifies an application making a request. You can apply the quota setting to all users or client IDs by using the --user-default or --client-default parameters, respectively. Quotas set for specific users or client IDs override default values set by these parameters. By using these quota type and user or client ID parameters, you can set quotas using the following combinations: cloudctl es entity-config --user &lt;user&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --user-default --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client &lt;client id&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client-default --config &lt;quota_type&gt;=&lt;value&gt; Examples For example, the following setting specifies that user u-testuser1 can only send 2048 bytes of data per second:cloudctl es entity-config --user \"u-testuser1\" --config producer_byte_rate=2048 For example, the following setting specifies that all application client IDs can only receive 2048 bytes of data per second:cloudctl es entity-config --client-default --config consumer_byte_rate=2048 The cloudctl es entity-config command is dynamic, so any quota setting is applied immediately without the need to restart clients. Note: If you run any of the commands with the --default parameter, the specified quota is reset to the system default value for that user or client ID (which is unlimited).For example:    cloudctl es entity-config --user \"s-consumer_service_id\" --default --config producer_byte_rate ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/administering/quotas/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "2019.2.1",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/intro/",
        "teaser":null},{
        "title": "Diagnosing installation issues",
        "collection": "2019.2.1",
        "excerpt":"To help troubleshoot and resolve installation issues, you can run a diagnostic script that checks your deployment for potential problems. Important: Do not use the script before the installation process completes. Despite a successful installation message, some processes might still need to complete, and it can take up to 10 minutes before IBM Event Streams is available to use. To run the script:   Download the installation-diagnostic-script.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the script as follows: ./installation-diagnostic-script.sh -n &lt;namespace&gt; -r &lt;release-name&gt;If you have been waiting for more than an hour, add the --restartoldpods option to recreate lost events (by default, events are deleted after an hour). This option restarts Failed or Pending pods that are an hour old or more. For example, the following installation has pods that are in pending state, and running the diagnostic script reveals that the issue is caused by not having sufficient memory and CPU resources available to the pods: Starting release diagnostics...Checking kafka-sts pods...kafka-sts pods foundChecking zookeeper-sts pods...zookeeper-sts pods foundChecking the ibm-es-iam-secret API Key...API Key foundChecking for Pending pods...Pending pods found, checking pod for failed events...------------------Name: caesar-ibm-es-kafka-sts-0Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-1Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-2Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------No failed events found for pod caesar-ibm-es-rest-deploy-6ff498d779-stf79------------------Checking for CrashLoopBackOff pods...No CrashLoopBackOff pods foundRelease diagnostics complete. Please review output to identify potential problems.If unable to identify or fix problems, please contact support.","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/diagnosing-installation-issues/",
        "teaser":null},{
        "title": "Gathering logs",
        "collection": "2019.2.1",
        "excerpt":"To help IBM support troubleshoot any issues with your Event Streams installation, run the log gathering script as follows. The script collects the log files from available pods and creates a compressed file. It uses the component label names instead of the pod names as the pod names could be truncated.   Download the get-logs.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.      Run the script as follows: ./get-logs.sh -n &lt;namespace&gt; -r &lt;release-name&gt;     If you do not specify a namespace, the script retrieves logs from the default namespace as requested in the cloudctl login. If you do not specify a release name, the script gathers logs for all releases.   ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/gathering-logs/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "2019.2.1",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible sypmtoms and causes. IBM Event Streams not available after installation After a successsful installation message is displayed, IBM Event Streams might not be available to use yet. It can take up to 10 minutes before IBM Event Streams is available to use. The IBM Cloud Private installation might return a successful completion message before all Event Streams services start up. If the installation continues to be unavailable, run the installation diognastics scripts. Insufficient system resources You can specify the memory and CPU requirements when IBM Event Streams is installed. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims: occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory: occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU: occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.For example, if each Kafka broker is set to require 80 GB of memory on a system that only has 16 GB available per node, you might see the following error message:  To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). If a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which may reside on the same node. To correct this issue, uninstall IBM Event Streams. Install again using lower resource requirements, or increase the amount of system resources available to the pod. Problems with secrets When using a non-default Docker registry, you might need to provide a secret which stores the user ID and password to access that registry. If there are issues with the secret that holds the user ID and password used to access the Docker registry, the events for a pod will show an error similar to the following.  To resolve this issue correct the secret and install IBM Event Streams again. Installation failure stating object already exists If a secret that does not exist is specified during installation, the process fails even if no secret is required to access the Docker registry. The default Docker image registry at ibmcom does not require a secret specifying the user ID and password. To correct this, install IBM Event Streams again without specifying a secret. If you are using a Docker image registry that does require a secret, attempting to install again might fail stating that an object already exists, for example: Internal service error : rpc error: code = Unknown desc = rolebindings.rbac.authorization.k8s.io \"elh-ibm-es-secret-copy-crb-sys\" already existsDelete the left over object cited and other objects before trying to install again. For instructions, see how to fully clean up after uninstallation. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Full cleanup after uninstallation",
        "collection": "2019.2.1",
        "excerpt":"The uninstallation process might leave behind artifacts that you have to clear manually. Security resources A service ID is created as part of installing Event Streams, which defines the identity for securing communication between internal components. To delete this service ID after uninstalling Event Streams, run the following command: cloudctl iam service-id-delete eventstreams-&lt;release&gt;-service-id -f Kubernetes resources Use the following command to find the list of IBM Event Streams objects associated with the release’s namespace: kubectl get &lt;type&gt; -n &lt;namespace&gt; | grep ibm-es Where type is each of   pods  clusterroles  clusterrolebindings  roles  rolebindings  configmaps  serviceaccounts  statefulsets  deployments  jobs  pods  pvc (see Note later)  secretsThere are also a number of Event Streams objects created in the kube-system namespace. To list these objects run the following command: kubectl get pod -a -n kube-system | grep ibm-es Note: These commands might return objects that should not be deleted. For example, do not delete secrets or system clusterroles if the kubectl output is not piped to grep. Note: If persistent volume claims (PVCs) are deleted (the objects returned when specifying “pvc” in the commands above), the data associated with the PVCs is also deleted. This includes any persistent Kafka data on disk. Consider whether this is the desired result before deleting any PVCs. To find which objects need to be manually cleared look for the following string in the output of the previously mentioned commands: &lt;release&gt;-ibm-es You can either navigate through the IBM Cloud Private cluster management console to Workloads &gt; or Configuration &gt; to find the objects and delete them, or use the following command: kubectl delete &lt;type&gt; &lt;name&gt; -n &lt;namespace&gt; For example, to delete a leftover rolebinding called eventstreams-ibm-eventstreams-secret-copy-crb-ns, run the following command: kubectl delete rolebinding eventstreams-ibm-eventstreams-secret-copy-crb-ns -n es Be cautious of deleting persistent volume claims (PVCs) as the data on the disk that is associated with that persistent volume will also be deleted. This includes Event Streams message data. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/cleanup-uninstall/",
        "teaser":null},{
        "title": "ConsumerTimeoutException when pods available",
        "collection": "2019.2.1",
        "excerpt":"Symptoms Attempts to communicate with a pod results in timeout errors such as kafka.consumer.ConsumerTimeoutException. Causes When querying the status of pods in the Kubernetes cluster, pods show as being in Ready state can still be in the process of starting up. This latency is a result of the external ports being active on the pods before the underlying services are ready to handle requests. The period of this latency depends on the configured topology and performance characteristics of the system in use. Resolving the problem Allow additional time for pod startup to complete before attempting to communicate with it. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/pod-timeout-error/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "2019.2.1",
        "excerpt":"Symptoms The following error message is displayed when setting up replication by using the CLI: FAILEDEvent Streams API request failed:Error response from server. Status code: 400. The resource request is invalid. Missing required parameter topic nameThe message does not provide accurate information about the cause of the error. Causes When providing the list of topics to geo-replicate, you added spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, instead of --topics MyTopicName1, MyTopicName2, MyTopicName3, enter --topics MyTopicName1,MyTopicName2,MyTopicName3. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "2019.2.1",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutExceptionCauses This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "2019.2.1",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log into the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the IBM Cloud Private cluster and initialized the command line tool. Resolving the problem Ensure you log in to the IBM Cloud Private cluster as follows: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;After logging in to IBM Cloud Private, initialize the IBM Event Streams CLI as follows: cloudctl es initFinally, run the operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no ImagePolicies' error",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no ImagePolicies in the \"&lt;name&gt;\" namespace, where &lt;name&gt; is the namespace into which you are deploying the chart. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs if there are no image policies defined for the target namespace. To confirm this, list the policies as follows: kubectl get imagepolicyYou should see a message stating No resources found. Resolving the problem If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioTo apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/no-image-policy/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no matching repositories in the ImagePolicies' error",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no matching repositories in the ImagePolicies. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs when there are image policies defined for the namespace into which the chart is being deployed, but none of them include the required repositories. To confirm this, list the image policies defined as follows: kubectl get imagepolicyFor each image policy, you can check which repositories it includes as follows: kubectl describe imagepolicy &lt;imagePolicyName&gt;If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioResolving the problem To apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/image-policy-missing-repository/",
        "teaser":null},{
        "title": "Chart deployment starts but no helm release is created",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When the IBM Event Streams (not the Community Edition) chart is deployed, the process appears to start successfully but the helm release and set of expected pods are not created. You can confirm the helm release has not been created by running the following command: helm listIn this case you will not see an entry for the Helm release name you provided when you started the deployment process. In addition, you will see that only a single pod is initially created and then subsequently removed after a couple of minutes. You can check which pods are running using the following command: kubectl get podsImmediately after starting the deployment process, you will see a single pod created named &lt;releaseName&gt;-ibm-es-secret-copy-job-&lt;uid&gt;. If you ‘describe’ the pod, you will receive the error message Failed to pull image, and a further message stating either Authentication is required or unauthorized: BAD_CREDENTIAL. After a couple of minutes this pod is deleted and no pods will be reported by the kubectl command. If you query the defined jobs as follows, you will see one named &lt;releaseName&gt;-ibm-es-secret-copy-job: kubectl get jobsFinally if you ‘describe’ the job as follows, you will see that it reports a failed pod status: kubectl describe job &lt;releaseName&gt;-ibm-es-secret-copy-jobFor example, the job description will include the following: Pods Statuses:            0 Running / 0 Succeeded / 1 FailedCauses This situation occurs if there a problem with the image pull secret being used to authorize access to the Docker image repository you specified when the chart was deployed. When you ‘describe’ the secret copy pod, if you see the error message Authentication is required, this indicates that the secret you specified does not exist. If you see the error message unauthorized: BAD_CREDENTIAL, this indicates that the secret was found but one of the fields present within it is not correct. To confirm which secrets are deployed, run the following command: kubectl get secretsResolving the problem To delete a secret thats not correctly defined, use the following command: kubectl delete secret &lt;secretName&gt;To create a new secret for use in chart deployment, run the following command: kubectl create secret docker-registry &lt;secretName&gt; --docker-server=&lt;serverAddress:serverPort&gt; --docker-username=&lt;dockerUser&gt; --docker-password=&lt;dockerPassword&gt; --docker-email=&lt;yourEmailAddress&gt;For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=John.Smith@ibm.comAfter you have confirmed that the required secret is correctly defined, re-run the chart deployment process. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/no-helm-release-is-created/",
        "teaser":null},{
        "title": "The Messages page is blank",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Messages page loads, but then becomes blank when viewing any topic (either real or simulated). Causes The vm.max_map_count property on one or more of your nodes is below the required value of 262144. This causes the message indexing capabilities to fail, resulting in this behaviour. Resolving the problem Ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node:     sudo sysctl -w vm.max_map_count=262144    echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/messages-page-blank/",
        "teaser":null},{
        "title": "Unable to connect to Kafka cluster",
        "collection": "2019.2.1",
        "excerpt":"Symptoms The following error is displayed when trying to connect to your Kafka cluster using SSL, for example, when running the Kafka Connect source connector for IBM MQ: org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failedCauses The Java process might replace the IP address of your cluster with the corresponding hostname value found in your /etc/hosts file. For example, to be able to access Docker images from your IBM Cloud Private cluster, you might have added an entry in your /etc/hosts file that corresponds to the IP address of your cluster, such as 192.0.2.24 mycluster.icp. In such cases, the following Java exception is displayed after the previously mentioned error message: Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching XXXXX found.Resolving the problem If you see the exception mentioned previously, comment out the hostname value in your /etc/hosts file to solve this connection issue. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/kafka-connection-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "Geo-replication fails to start with 'Could not connect to origin cluster' error",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When geo-replicating a topic to a destination cluster with 2 or more geo-replication worker nodes, the topic replication fails to start. The Event Streams UI reports the following error: Could not connect to origin cluster.In addition, the logs for the replicator worker nodes contain the following error message: org.apache.kafka.connect.errors.ConnectException: SSL handshake failedCauses The truststore on the geo-replication worker node that hosts the replicator task does not contain the certificate for the origin cluster. Resolving the problem You can either manually add the certificate to the truststore in each of the geo-replicator worker nodes, or you can scale the number of geo-replicator worker nodes down to 1 if suitable for your setup. Manually adding certificates To manually add the certificate to the truststore in each of the geo-replicator worker nodes:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersA list of destination cluster IDs are displayed. Find the name of the destination cluster you are attempting to geo-replicate topics to.  Retrieve information about the destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;The failed geo-replicator name is in the list of geo-replicators returned.  Log into the destination cluster, and use kubectl exec to run the keytool command to import the certificate into the truststore in each geo-replication worker node:    kubectl exec -it -n &lt;namespace&gt; -c replicator \\&lt;releaseName&gt;-ibm-es-replicator-deploy-&lt;replicator-pod-id&gt; \\-- bash -c \\\"keytool -importcert \\-keystore /opt/replicator/security/cacerts \\-alias &lt;geo-replicator_name&gt; \\-file /etc/consumer-credentials/cert_&lt;geo-replicator_name&gt; \\-storepass changeit \\-trustcacerts -noprompt\"        The command either succeeds with a \"Certificate was added\" message or fails with a \"Certificate not imported, alias &lt;geo-replicator_name&gt; already exists\" message. In both cases, the truststore for that pod is ready to be used.     Repeat the command for each replicator worker node to ensure the certificate is imported into the truststore on all replicator pods.  Log in to the origin cluster, and restart the failed geo-replicator using the following cloudctl command:cloudctl es geo-replicator-restart -d &lt;geo-replication-cluster-id&gt; -n  &lt;geo-replicator_name&gt;Scaling the number of nodes To scale the number of geo-replicator worker nodes to 1:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Run the following kubectl command:kubectl scale --replicas 1 deployment &lt;releaseName&gt;-ibm-es-replicator-deploy","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/georeplication-connect-error/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "2019.2.1",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes The most likely cause of this problem is that the user attempting to authenticate is part of an IBM Cloud Private team that has not been associated with the  Event Streams instance. Resolving the problem Configure the IBM Cloud Private team that the user is part of to work with the Event Streams instance by running the iam-add-release-to-team CLI command. Run the command as follows: cloudctl es iam-add-release-to-team --namespace &lt;namespace for the Event Streams instance&gt; --release &lt;release name of the Event Streams instance&gt; --team &lt;name of the IBM Cloud Private team that the user is imported into&gt; The user can authenticate and sign in to the Event Streams UI after the command runs successfully. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "Kafka client applications are unable to connect to the cluster. Users are unable to login to the UI.",
        "collection": "2019.2.1",
        "excerpt":"Symptoms Client applications are unable to produce or consume messages. The logs for producer and consumer applications contain the following error message: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.serversThe Event Streams UI reports the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes An invalid host name or IP address was specified in the External access settings when configuring the installation. Resolving the problem You need to reinstall Event Streams and supply the correct external host name or IP address. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/client-connect-error/",
        "teaser":null},{
        "title": "Rollback fails",
        "collection": "2019.2.1",
        "excerpt":"Symptoms Rolling back from Event Streams version 2019.1.1 to 2018.3.1 (Helm chart version 1.2.0 to 1.1.0) results in an Invalid request (..) field is immutable error. The rollback status shows as failed, for example: $ helm history event-streamsREVISION        UPDATED                         STATUS          CHART                           DESCRIPTION1               Tue Dec 10 16:28:19 2018        SUPERSEDED      ibm-eventstreams-prod-1.1.0     Install complete2               Fri Mar 29 14:15:24 2019        SUPERSEDED      ibm-eventstreams-prod-1.2.0     Upgrade complete3               Fri Mar 29 15:23:46 2019        FAILED          ibm-eventstreams-prod-1.1.0     Rollback \"event-streams\" failed: Job.batch \"event-streams...Attempting to log in to the Event Streams UI on the new port results in the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes The oauth job was not removed before performing the rollback steps. Resolving the problem Run the following command: kubectl -n kube-system get job &lt;release-name&gt;-ibm-es-ui-oauth2-client-reg -o json | jq 'del(.spec.selector)' | jq 'del(.spec.template.metadata.labels)' | kubectl replace --force -f - Where &lt;release-name&gt; is the name that identifies your Event Streams installation. If you are still experiencing issues with your installation, you might need to uninstall Event Streams, and clean up after uninstallation before installing again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/rollback-fails/",
        "teaser":null},{
        "title": "The UI cannot load data",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Monitor and the Topics &gt; Producers tabs do not load, displaying the following message:  Causes The IBM Cloud Private monitoring service might not be installed. In general, the monitoring service is installed by default during the  IBM Cloud Private installation. However, some deployment methods do not install the service. Resolving the problem Install the IBM Cloud Private monitoring service from the Catalog or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/problem-with-piping/",
        "teaser":null},{
        "title": "Cannot add schemas when using IBM Cloud Private 3.1.2",
        "collection": "2019.2.1",
        "excerpt":"Symptoms If you are using IBM Cloud Private 3.1.2, and upgrade to Event Streams 2019.2.1, or install 2019.2.1 on an IBM Cloud Private 3.1.2 instance that already has or had a previous Event Streams installation, then the option to add schemas and schema versions are not available after a successful upgrade or installation. The Add schema and Add schema version buttons are not available in the UI, and you cannot add schemas or schema versions by using the Event Streams CLI. For example, when running the cloudctl es schema-add command when logged in as a user with the correct permissions (Administrator or Operator roles), the following error is displayed: cloudctl es schema-add /Users/jsmith/qp/schemas/ABC_schema_1.0.0.avscFAILEDEvent Streams API request failed:Error response from server. Status code: 403. ForbiddenUnable to add version 1.0.0 of schema ABC_schema to the registry.Causes IBM Cloud Private 3.1.2 authentication does not automatically pick up the new schema registry IAM roles if roles have been set up as part of a previous Event Streams installation on the same IBM Cloud Private instance. This happens even when using a different namespace. Resolving the problem To update the user permissions, roll the auth-pdp pods to pick up the new roles as follows:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  List the names of the auth-pdp pods:kubectl get pods -n kube-system | grep auth-pdp  Delete the auth-pdp pods by running the following command for each auth-pdp pod:kubectl delete pods -n kube-system &lt;auth-pdp-pod-name&gt;  Wait for the new auth-pdp pods to be installed automatically.  Refresh the Event Streams UI. The Add schema and Add schema version buttons are now available in the UI. The command line options also work (for example, cloudctl es schema-add).","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/cannot-add-schemas-icp312/",
        "teaser":null},{
        "title": "Cannot add schemas after installing earlier Event Streams version",
        "collection": "2019.2.1",
        "excerpt":"Symptoms If you install an earlier version of Event Streams on an IBM Cloud Private instance that already has an installation of Event Streams 2019.2.1, then the option to add schemas and schema versions becomes unavailable on the Event Streams 2019.2.1 installation. The Add schema and Add schema version buttons are not available in the UI, and you cannot add schemas or schema versions by using the Event Streams CLI. For example, when running the cloudctl es schema-add command when logged in as a user with the correct permissions (Administrator or Operator roles), the following error is displayed: cloudctl es schema-add /Users/jsmith/qp/schemas/ABC_schema_1.0.0.avscFAILEDEvent Streams API request failed:Error response from server. Status code: 403. ForbiddenUnable to add version 1.0.0 of schema ABC_schema to the registry.Causes The IAM roles for schema registry authentication that are set up as part of the Event Streams 2019.2.1 installation are overwritten by the installation of an earlier version of Event Streams on the same IBM Cloud Private instance. This happens even when using a different namespace. Resolving the problem Restore the IAM roles for the schema registry authentication:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  List the ConfigMaps in the Event Streams 2019.2.1 installation:kubectl get configmaps -n &lt;namespace&gt;  Find the role mappings ConfigMap from the list of ConfigMaps in the 2019.2.1 installation by looking for the ConfigMap with suffix role-mappings-cm.  Download and save the configuration for the job stored in the ConfigMap:kubectl get configmap &lt;RELEASE_NAME&gt;-ibm-es-role-mappings-cm -o jsonpath='{.data.job}' &gt; role-mappings-job.yml  Run the following command to restore the security role mappings:kubectl create -f ./role-mappings-job.yml","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/cannot-add-schemas-earlier-install/",
        "teaser":null},{
        "title": "Redis latency due to Transparent Huge Pages",
        "collection": "2019.2.1",
        "excerpt":"Symptoms Redis containers in the access controller and UI pods output the following message in the logs: WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.Causes The Redis documentation states that having Transparent Huge Pages enabled in your Linux kernel can cause latency and high memory usage. Resolving the problem If you are experiencing a latency problem with the access controller or UI, then disable Transparent Huge Pages by using the method recommended in the documentation for your host Linux distribution. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/redis-latency-transparent-huge-pages/",
        "teaser":null},{
        "title": "Failed to read 'log header' errors in Kafka logs",
        "collection": "2019.2.1",
        "excerpt":"Symptoms When Event Streams is configured to use GlusterFS as a storage volume, the Kafka logs show errors containing messages similar to the following: [2020-05-12 06:40:19,249] ERROR [ReplicaManager broker=2] Error processing fetch with max size 1048576 from consumer on partition &lt;TOPIC-NAME&gt;-0: (fetchOffset=10380908, logStartOffset=-1, maxBytes=1048576, currentLeaderEpoch=Optional.empty) (kafka.server.ReplicaManager)org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@a5e333e6`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 95236164.These errors mean that Kafka has been unable to read files from the Gluster volume. This can cause replicas to fall out of sync. Cause See Kafka issue 7282GlusterFS has performance settings that will allow requests for data to be served from replicas when they are not in sync with the leader. This causes problems for Kafka when it attempts to read a replica log segment before it has been fully written by Gluster. Resolving the problem Apply the following settings to each Gluster volume that is used by an Event Streams Kafka broker: gluster volume set &lt;volumeName&gt; performance.quick-read offgluster volume set &lt;volumeName&gt; performance.io-cache offgluster volume set &lt;volumeName&gt; performance.write-behind offgluster volume set &lt;volumeName&gt; performance.stat-prefetch offgluster volume set &lt;volumeName&gt; performance.read-ahead offgluster volume set &lt;volumeName&gt; performance.readdir-ahead offgluster volume set &lt;volumeName&gt; performance.open-behind offgluster volume set &lt;volumeName&gt; performance.client-io-threads offThese settings can be applied while the Gluster volume is online. The Kafka broker will not need to be modified, the broker will be able to read from the volume after the change is applied. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.2.1/troubleshooting/failed-to-read-log/",
        "teaser":null},{
        "title": "Home",
        "collection": "2019.4",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/",
        "teaser":null},{
        "title": "Introduction",
        "collection": "2019.4",
        "excerpt":"IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka® project. Event Streams version 2019.4.1 includes Kafka release 2.3.0, and supports the use of all Kafka interfaces. Event Streams versions 2019.4.2, 2019.4.3 and 2019.4.4 include Kafka release 2.3.1, and support the use of all Kafka interfaces. IBM Event Streams builds upon the IBM Cloud Private platform to deploy Apache Kafka in a resilient and manageable way. It includes a UI design aimed at application developers getting started with Apache Kafka, as well as users operating a production cluster. IBM Event Streams features include:   Apache Kafka deployment that maximizes the spread of Kafka brokers across the nodes of the IBM Cloud Private cluster. This creates a highly-available configuration making the deployment resilient to many classes of failure with automatic restart of brokers included.  Health check information and options to resolve issues with your clusters and brokers.  Geo-replication of your topics between clusters to enable disaster recovery and scalability.  UI for browsing messages to help view and filter hundreds of thousands of messages, including options to drill in and see message details from a set time.  Encrypted communication between internal components and encrypted storage by using features available in IBM Cloud Private.  Security with authentication and authorization using IBM Cloud Private.","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/overview/",
        "teaser":null},{
        "title": "What's new",
        "collection": "2019.4",
        "excerpt":"Release 2019.4.4 Event Streams 2019.4.4 contains security and bug fixes. For more information about the bugs fixed in 2019.4.4, see the issues page. Release 2019.4.3 Event Streams 2019.4.3 contains security and bug fixes. For more information about the bugs fixed in 2019.4.3, see the issues page. Release 2019.4.2 Find out what is new in IBM Event Streams version 2019.4.2. Support for Red Hat OpenShift Container Platform 4.3 Event Streams 2019.4.2 introduces support for Red Hat OpenShift Container Platform 4.3. OpenShift Container Platform 4.2 is also supported in this version, continuing the support introduced as part of Event Streams 2019.4.1 in IBM Cloud Pak for Integration. Kafka version upgraded to 2.3.1 Event Streams version 2019.4.2 includes Kafka release 2.3.1, and supports the use of all Kafka interfaces. Documentation: Highlighting differences between versions Any difference in features or behavior introduced by Event Streams 2019.4.2 compared to 2019.4.1 is highlighted in this documentation using the following graphic:  Community Edition is deprecated The IBM Event Streams Community Edition has been removed from version 2019.4.2. IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages. Release 2019.4.1 Find out what is new in IBM Event Streams version 2019.4.1. Support for Red Hat OpenShift Container Platform 4.2 IBM Event Streams 2019.4.1 installed in IBM Cloud Pak for Integration 2019.4.1 introduces support for Red Hat OpenShift Container Platform 4.2. Support for multiple availability zones Event Streams now supports running a single cluster that spans multiple zones. Support for SSL client authentication when using the REST producer In addition to using HTTP authorization, you can now use the Event Streams REST producer API with SSL client authentication. This means you can provide the required API key with each REST call by embedding it into an SSL client certificate. This is useful when you are using third-party software where you cannot control the HTTP headers sent, or systems such as CICS events over HTTP. Changing certificates for existing deployments Event Streams 2019.4.1 now supports updating your external and internal certificates for an existing deployment. Kafka version upgraded to 2.3.0 Event Streams version 2019.4.1 includes Kafka release 2.3.0, and supports the use of all Kafka interfaces. Support for Red Hat OpenShift Container Platform routes Event Streams now supports using OpenShift Container Platform routes. If you are using the OpenShift Container Platform and have upgraded to Event Streams version 2019.4.1, you can switch to using OpenShift routes. Support for IBM Cloud Private version 3.2.1 Event Streams 2019.4.1 is supported on IBM Cloud Private 3.2.1. Default resource requirements have changed See the updated tables for the Event Streams resource requirements. Deprecated features The simulated topic has been removed from the UI in Event Streams version 2019.4.1 and later. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/whats-new/",
        "teaser":null},{
        "title": "Key concepts",
        "collection": "2019.4",
        "excerpt":"Apache Kafka® forms the reliable messaging core of IBM Event Streams. It is a publish-subscribe messaging system designed to be fault-tolerant, providing a high-throughput and low-latency platform for handling real-time data feeds.  The following are some key Kafka concepts. Cluster Kafka runs as a cluster of one or more servers (Kafka brokers). The load is balanced across the cluster by distributing it amongst the servers. Topic A stream of messages is stored in categories called topics. Partition Each topic comprises one or more partitions. Each partition is an ordered list of messages. The messages on a partition are each given a monotonically increasing number called the offset. If a topic has more than one partition, it allows data to be fed through in parallel to increase throughput by distributing the partitions across the cluster. The number of partitions also influences the balancing of workload among consumers. Message The unit of data in Kafka. Each message is represented as a record, which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Kafka uses the terms record and message interchangeably. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduced record headers for this purpose. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it’s best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Producer A process that publishes streams of messages to Kafka topics. A producer can publish to one or more topics and can optionally choose the partition that stores the data. Consumer A process that consumes messages from Kafka topics and processes the feed of messages. A consumer can consume from one or more topics or partitions. Consumer group A named group of one or more consumers that together consume the messages from a set of topics. Each consumer in the group reads messages from specific partitions that it is assigned to. Each partition is assigned to one consumer in the group only.   If there are more partitions than consumers in a group, some consumers have multiple partitions.  If there are more consumers than partitions, some consumers have no partitions.To learn more, see the following information:   Producing messages  Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/key-concepts/",
        "teaser":null},{
        "title": "Producing messages",
        "collection": "2019.4",
        "excerpt":"A producer is an application that publishes streams of messages to Kafka topics. This information focuses on the Java programming interface that is part of the Apache Kafka® project. The concepts apply to other languages too, but the names are sometimes a little different. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.producer.ProducerRecord is used to represent a message from the point of view of the producer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. When a producer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The producer requests the partition and leadership information about the topic that it wants to publish to. Then the producer establishes another connection to the partition leader and can begin to publish messages. These actions happen automatically internally when your producer connects to the Kafka cluster. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed and becomes available for consumers. Each message is represented as a record which comprises two parts: key and value. The key is commonly used for data about the message and the value is the body of the message. Because many tools in the Kafka ecosystem (such as connectors to other systems) use only the value and ignore the key, it's best to put all of the message data in the value and just use the key for partitioning or log compaction. You should not rely on everything that reads from Kafka to make use of the key. Many other messaging systems also have a way of carrying other information along with the messages. Kafka 0.11 introduces record headers for this purpose. You might find it useful to read this information in conjunction with consuming messages in IBM Event Streams. Configuration settings There are many configuration settings for the producer. You can control aspects of the producer including batching, retries, and message acknowledgment. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.serializer      The class used to serialize keys.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              value.serializer      The class used to serialize values.      Java class that implements Serializer interface, such as org.apache.kafka.common.serialization.StringSerializer      No default – you must specify a value              acks      The number of servers required to acknowledge each message published. This controls the durability guarantees that the producer requires.      0, 1, all (or -1)      1              retries      The number of times that the client resends a message when the send encounters an error.      0,…      0              max.block.ms      The number of milliseconds that a send or metadata request can block waiting.      0,…      60000 (1 minute)              max.in.flight.requests.per.connection      The maximum number of unacknowledged requests that the client sends on a connection before blocking further requests.      1,…      5              request.timeout.ms      The maximum amount of time the producer waits for a response to a request. If the response is not received before the timeout elapses, the request is retried or fails if the number of retries has been exhausted.      0,…      30000 (30 seconds)      Many more configuration settings are available, but ensure that you read the Apache Kafka documentation thoroughly before experimenting with them. Partitioning When the producer publishes a message on a topic, the producer can choose which partition to use. If ordering is important, you must remember that a partition is an ordered sequence of records, but a topic comprises one or more partitions. If you want a set of messages to be delivered in order, ensure that they all go on the same partition. Themost straightforward way to achieve this is to give all of those messages the same key. The producer can explicitly specify a partition number when it publishes a message. This gives direct control, but it makes the producer code more complex because it takes on the responsibility for managing the partition selection. For more information, see the method call Producer.partitionsFor. For example, the call is described for Kafka 1.10 If the producer does not specify a partition number, the selection of partition is made by a partitioner. The default partitioner that is built into the Kafka producer works as follows:   If the record does not have a key, select the partition in a round-robin fashion.  If the record does have a key, select the partition by calculating a hash value for the key. This has the effect of selecting the same partition for all messages with the same key.You can also write your own custom partitioner. A custom partitioner can choose any scheme to assign records to partitions. For example, use just a subset of the information in the key or an application-specific identifier. Message ordering Kafka generally writes messages in the order that they are sent by the producer. However, there are situations where retries can cause messages to be duplicated or reordered. If you want a sequence of messages to be sent in order, it's very important to ensure that they are all written to the same partition. The producer is also able to retry sending messages automatically. It's often a good idea to enable this retry feature because the alternative is that your application code has to perform any retries itself. The combination of batching in Kafka and automatic retries can have the effect of duplicating messages and reordering them. For example, if you publish a sequence of three messages &lt;M1, M2, M3&gt; on a topic. The records might all fit within the same batch, so they're actually all sent to the partition leader together. The leader then writes them to the partition and replicates them as separate records. In the case of a failure, it's possible that M1 and M2 are added to the partition, but M3 is not. The producer doesn't receive an acknowledgment, so it retries sending &lt;M1, M2, M3&gt;. The new leader simply writes M1, M2 and M3 onto the partition, which now contains &lt;M1, M2, M1, M2, M3&gt;, where the duplicated M1 actually follows the original M2. If you restrict the number of requests in flight to each broker to just one, you can prevent this reordering. You might still find a single record is duplicated such as &lt;M1, M2, M2, M3&gt;, but you'll never get out of order sequences. You can also use the idempotent producer feature to prevent the duplication of M2. It's normal practice with Kafka to write the applications to handle occasional message duplicates because the performance impact of having only a single request in flight is significant. Message acknowledgments When you publish a message, you can choose the level of acknowledgments required using the acks producer configuration. The choice represents a balance between throughput and reliability. There are three levels as follows: acks=0 (least reliable) The message is considered sent as soon as it has been written to the network. There is no acknowledgment from the partition leader. As a result, messages can be lost if the partition leadership changes. This level of acknowledgment is very fast, but comes with the possibility of message loss in some situations. acks=1 (the default) The message is acknowledged to the producer as soon as the partition leader has successfully written its record to the partition. Because the acknowledgment occurs before the record is known to have reached the in-sync replicas, the message could be lost if the leader fails but the followers do not yet have the message. If partition leadership changes, the old leader informs the producer, which can handle the error and retry sending the message to the new leader. Because messages are acknowledged before their receipt has been confirmed by all replicas, messages that have been acknowledged but not yet fully replicated can be lost if the partition leadership changes. acks=all (most reliable) The message is acknowledged to the producer when the partition leader has successfully written its record and all in-sync replicas have done the same. The message is not lost if the partition leadership changes provided that at least one in-sync replica is available. Even if you do not wait for messages to be acknowledged to the producer, messages are still only available to be consumed when committed, and that means replication to the in-sync replicas is complete. In other words, the latency of sending the messages from the point of view of the producer is lower than the end-to-end latency measured from the producer sending a message to a consumer receiving the message. If possible, avoid waiting for the acknowledgment of a message before publishing the next message. Waiting prevents the producer from being able to batch together messages and also reduces the rate that messages can be published to below the round-trip latency of the network. Batching, throttling, and compression For efficiency purposes, the producer actually collects batches of records together for sending to the servers. If you enable compression, the producer compresses each batch, which can improve performance by requiring less data to be transferred over the network. If you try to publish messages faster than they can be sent to a server, the producer automatically buffers them up into batched requests. The producer maintains a buffer of unsent records for each partition. Of course, there comes a point when even batching does not allow the desired rate to be achieved. In summary, when a message is published, its record is first written into a buffer in the producer. In the background, the producer batches up and sends the records to the server. The server then responds to the producer, possibly applying a throttling delay if the producer is publishing too fast. If the buffer in the producer fills up, the producer's send call is delayed but ultimately could fail with an exception. Code snippets These code snippets are at a very high level to illustrate the concepts involved. To connect to IBM Event Streams, you first need to build the set of configuration properties. All connections to IBM Event Streams are secured using TLS and user/password authentication, so you need these properties at a minimum. Replace KAFKA_BROKERS_SASL, USER, and PASSWORD with your own credentials: Properties props = new Properties();props.put(\"bootstrap.servers\", KAFKA_BROKERS_SASL);props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"USER\\\" password=\\\"PASSWORD\\\";\");  props.put(\"security.protocol\", \"SASL_SSL\");props.put(\"sasl.mechanism\", \"PLAIN\");props.put(\"ssl.protocol\", \"TLSv1.2\");props.put(\"ssl.enabled.protocols\", \"TLSv1.2\");props.put(\"ssl.endpoint.identification.algorithm\", \"HTTPS\");To send messages, you'll also need to specify serializers for the keys and values, for example: props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");Then use a KafkaProducer to send messages, where each message is represented by a ProducerRecord. Don't forget to close the KafkaProducer when you're finished. This code just sends the message but it doesn't wait to see whether the send succeeded. Producer producer = new KafkaProducer&lt;&gt;(props);producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));  producer.close();The send() method is asynchronous and returns a Future that you can use to check its completion: Future f = producer.send(new ProducerRecord(\"T1\", \"key\", \"value\"));// Do some other stuff// Now wait for the result of the sendRecordMetadata rm = f.get();long offset = rm.offset;Alternatively, you can supply a callback when sending the message: producer.send(new ProducerRecord(\"T1\",\"key\",\"value\", new Callback() {    public void onCompletion(RecordMetadata metadata, Exception exception) {        // This is called when the send completes, either successfully or with an exception    }});For more information, see the Javadoc for the Kafka client, which is very comprehensive. To learn more, see the following information:   Consuming messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/producing-messages/",
        "teaser":null},{
        "title": "Consuming messages",
        "collection": "2019.4",
        "excerpt":"A consumer is an application that consumes streams of messages from Kafka topics. A consumer can subscribe to one or more topics or partitions. This information focuses on the Java programming interface that is part of the Apache Kafka project. The concepts apply to other languages too, but the names are sometimes a little different. When a consumer connects to Kafka, it makes an initial bootstrap connection. This connection can be to any of the servers in the cluster. The consumer requests the partition and leadership information about the topic that it wants to consume from. Then the consumer establishes another connection to the partition leader and can begin to consume messages. These actions happen automatically internally when your consumer connects to the Kafka cluster. A consumer is normally a long-running application. A consumer requests messages from Kafka by calling Consumer.poll(...) regularly. The consumer calls poll(), receives a batch of messages, processes them promptly, and then calls poll() again. When a consumer processes a message, the message is not removed from its topic. Instead, consumers can choose from several ways of letting Kafka know which messages have been processed. This process is known as committing the offset. In the programming interfaces, a message is actually called a record. For example, the Java class org.apache.kafka.clients.consumer.ConsumerRecord is used to represent a message for the consumer API. The terms record and message can be used interchangeably, but essentially a record is used to represent a message. You might find it useful to read this information in conjunction with producing messages in IBM Event Streams. Configuring consumer properties There are many configuration settings for the consumer, which control aspects of its behavior. The following table lists the most important ones:             Name      Description      Valid values      Default                  key.deserializer      The class used to deserialize keys.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              value.deserializer      The class used to deserialize values.      Java class that implements Deserializer interface, such as org.apache.kafka.common.serialization.StringDeserializer      No default – you must specify a value              group.id      An identifier for the consumer group that the consumer belongs to.      string      No default              auto.offset.reset      The behavior when the consumer has no initial offset or the current offset is no longer available in the cluster.      latest, earliest, none      latest              enable.auto.commit      Determines whether to commit the consumer’s offset automatically in the background.      true, false      true              auto.commit.interval.ms      The number of milliseconds between periodic commits of offsets.      0,…      5000 (5 seconds)              max.poll.records      The maximum number of records returned in a call to poll()      1,…      500              session.timeout.ms      The number of milliseconds within which a consumer heartbeat must be received to maintain a consumer’s membership of a consumer group.      6000-300000      10000 (10 seconds)              max.poll.interval.ms      The maximum time interval between polls before the consumer leaves the group.      1,…      300000 (5 minutes)      Many more configuration settings are available, but ensure you read the Apache Kafka documentation thoroughly before experimenting with them. Consumer groups A consumer group is a group of consumers cooperating to consume messages from one or more topics. The consumers in a group all use the same value for the group.id configuration. If you need more than one consumer to handle your workload, you can run multiple consumers in the same consumer group. Even if you only need one consumer, it's usual to also specify a value for group.id. Each consumer group has a server in the cluster called the coordinator responsible for assigning partitions to the consumers in the group. This responsibility is spread across the servers in the cluster to even the load. The assignment of partitions to consumers can change at every group rebalance. When a consumer joins a consumer group, it discovers the coordinator for the group. The consumer then tells the coordinator that it wants to join the group and the coordinator starts a rebalance of the partitions across the group including the new member. When one of the following changes take place in a consumer group, the group rebalances by shifting the assignment of partitions to the group members to accommodate the change:   a consumer joins the group  a consumer leaves the group  a consumer is considered as no longer live by the coordinator  new partitions are added to an existing topicFor each consumer group, Kafka remembers the committed offset for each partition being consumed. If you have a consumer group that has rebalanced, be aware that any consumer that has left the group will have its commits rejected until it rejoins the group. In this case, the consumer needs to rejoin the group, where it might be assigned a different partition to the one it was previously consuming from. Consumer liveness Kafka automatically detects failed consumers so that it can reassign partitions to working consumers. It uses two mechanisms to achieve this: polling and heartbeating. If the batch of messages returned from Consumer.poll(...) is large or the processing is time-consuming, the delay before calling poll() again can be significant or unpredictable. In some cases, it's necessary to configure a longmaximum polling interval so that consumers do not get removed from their groups just because message processing is taking a while. If this were the only mechanism, it would mean that the time taken to detect a failed consumer would also be long. To make consumer liveness easier to handle, background heartbeating was added in Kafka 0.10.1. The group coordinator expects group members to send it regular heartbeats to indicate that they remain active. A background heartbeat thread runs in the consumer sending regular heartbeats to the coordinator. If the coordinator does not receive a heartbeat from a group member within the session timeout, the coordinator removes the member from the group and starts a rebalance of the group. The session timeout can be much shorter than the maximum polling interval so that the time taken to detect a failed consumer can be short even if message processing takes a long time. You can configure the maximum polling interval using the max.poll.interval.ms property and the session timeout using the session.timeout.ms property. You will typically not need to use these settings unless it takes more than 5 minutes to process a batch of messages. Managing offsets For each consumer group, Kafka maintains the committed offset for each partition being consumed. When a consumer processes a message, it doesn't remove it from the partition. Instead, it just updates its current offset using a process called committing the offset. By default, IBM Event Streams retains committed offset information for 7 days. What if there is no existing committed offset? When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset. If there is no existing committed offset, the consumer can choose whether to start with the earliest or latest available message based on the setting of the auto.offset.reset property as follows:   latest (the default): Your consumer receives and consumes only messages that arrive after subscribing. Your consumer has no knowledge of messages that were sent before it subscribed, therefore you should not expect that all messages will be consumed from a topic.  earliest: Your consumer consumes all messages from the beginning because it is aware of all messages that have been sent.If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. When committed offsets are saved in Kafka and the consumers are restarted, consumers resume from the point they last stopped at. When there is a committed offset, the auto.offset.reset property is not used. Committing offsets automatically The easiest way to commit offsets is to let the Kafka consumer do it automatically. This is simple but it does give less control than committing manually. By default, a consumer automatically commits offsets every 5 seconds. This default commit happens every 5 seconds, regardless of the progress the consumer is making towards processing the messages. In addition, when the consumer calls poll(), this also causes the latest offset returned from the previous call to poll() to be committed (because it's probably been processed). If the committed offset overtakes the processing of the messages and there is a consumer failure, it's possible that some messages might not be processed. This is because processing restarts at the committed offset, which is later than the last message to be processed before the failure. For this reason, if reliability is more important than simplicity, it's usually best to commit offsets manually. Committing offsets manually If enable.auto.commit is set to false, the consumer commits its offsets manually. It can do this either synchronously or asynchronously. A common pattern is to commit the offset of the latest processed message based on a periodic timer. This pattern means that every message is processed at least once, but the committed offset never overtakes the progress of messages that are actively being processed. The frequency of the periodic timer controls the number of messages that can be reprocessed following a consumer failure. Messages are retrieved again from the last saved committed offset when the application restarts or when the group rebalances. The committed offset is the offset of the messages from which processing is resumed. This is usually the offset of the most recently processed message plus one. Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. Although it's usual to have natural variations in the produce and consume rates, the consume rate should not be slower than the produce rate for an extended period. If you observe that a consumer is processing messages successfully but occasionally appears to jump over a group of messages, it could be a sign that the consumer is not able to keep up. For topics that are not using log compaction, the amount of log space is managed by periodically deleting old log segments. If a consumer has fallen so far behind that it is consuming messages in a log segment that is deleted, it will suddenly jump forwards to the start of the next log segment. If it is important that the consumer processes all of the messages, this behavior indicates message loss from the point of view of this consumer. You can use the kafka-consumer-groups tool to see the consumer lag. You can also use the consumer API and the consumer metrics for the same purpose. Controlling the speed of message consumption If you have problems with message handling caused by message flooding, you can set a consumer option to control the speed of message consumption. Use fetch.max.bytes and max.poll.records to control how much data a call to poll() can return. Handling consumer rebalancing When consumers are added to or removed from a group, a group rebalance takes place and consumers are not able to consume messages. This results in all the consumers in a consumer group being unavailable for a short period. You could use a ConsumerRebalanceListener to manually commit offsets (if you are not using auto-commit) when notified with the \"on partitions revoked\" callback, and to pause further processing until notified of the successful rebalance using the \"on partition assigned\" callback. Exception handling Any robust application that uses the Kafka client needs to handle exceptions for certain expected situations. In some cases, the exceptions are not thrown directly because some methods are asynchronous and deliver their results using a Future or a callback. Here's a list of exceptions that you should handle in your code: [org.apache.kafka.common.errors.WakeupException] Thrown by Consumer.poll(...) as a result of Consumer.wakeup() being called. This is the standard way to interrupt the consumer's polling loop. The polling loop should exit and Consumer.close() should be called to disconnect cleanly. [org.apache.kafka.common.errors.NotLeaderForPartitionException] Thrown as a result of Producer.send(...) when the leadership for a partition changes. The client automatically refreshes its metadata to find the up-to-date leader information. Retry the operation, which should succeed with the updated metadata. [org.apache.kafka.common.errors.CommitFailedException] Thrown as a result of Consumer.commitSync(...) when an unrecoverable error occurs. In some cases, it is not possible simply to repeat the operation because the partition assignment might have changed and the consumer might no longer be able to commit its offsets. Because Consumer.commitSync(...) can be partially successful when used with multiple partitions in a single call, the error recovery can be simplified by using a separate Consumer.commitSync(...) call for each partition. [org.apache.kafka.common.errors.TimeoutException] Thrown by Producer.send(...),  Consumer.listTopics() if the metadata cannot be retrieved. The exception is also seen in the send callback (or the returned Future) when the requested acknowledgment does not come back within request.timeout.ms. The client can retry the operation, but the effect of a repeated operation depends on the specific operation. For example, if sending a message is retried, the message might be duplicated. To learn more, see the following information:   Producing messages  Partition leadership  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/consuming-messages/",
        "teaser":null},{
        "title": "Partition leadership",
        "collection": "2019.4",
        "excerpt":"Each partition has one server in the cluster that acts as the partition’s leader and other servers that act as the followers. All produce and consume requests for the partition are handled by the leader. The followers replicate the partition data from the leader with the aim of keeping up with the leader. If a follower is keeping up with the leader of a partition, the follower's replica is in-sync. When a message is sent to the partition leader, that message is not immediately available to consumers. The leader appends the record for the message to the partition, assigning it the next offset number for that partition. After all the followers for the in-sync replicas have replicated the record and acknowledged that they've written the record to their replicas, the record is now committed. The message is available for consumers. If the leader for a partition fails, one of the followers with an in-sync replica automatically takes over as the partition's leader. In practice, every server is the leader for some partitions and the follower for others. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. To learn more, see the following information:   Producing messages  Consuming messages  Apache Kafka documentation","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/partition-leadership/",
        "teaser":null},{
        "title": "Accessibility",
        "collection": "2019.4",
        "excerpt":"Accessibility features assist users who have a disability, such as restricted mobility or limited vision, to use information technology content successfully. Overview IBM Event Streams includes the following major accessibility features:   Keyboard-only operation  Operations that use a screen readerIBM Event Streams uses the latest W3C Standard, WAI-ARIA 1.0, to ensure compliance with US Section 508 and Web Content Accessibility Guidelines (WCAG) 2.0. To take advantage of accessibility features, use the latest release of your screen reader and the latest web browser that is supported by IBM Event Streams. Keyboard navigation This product uses standard navigation keys. Interface information The IBM Event Streams user interfaces do not have content that flashes 2 - 55 times per second. The IBM Event Streams web user interface relies on cascading style sheets to render content properly and to provide a usable experience. The application provides an equivalent way for low-vision users to use system display settings, including high-contrast mode. You can control font size by using the device or web browser settings. The IBM Event Streams web user interface includes WAI-ARIA navigational landmarks that you can use to quickly navigate to functional areas in the application. Related accessibility information In addition to standard IBM help desk and support websites, IBM has a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services: TTY service 800-IBM-3383 (800-426-3383) (within North America) For more information about the commitment that IBM has to accessibility, see IBM Accessibility. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/accessibility/",
        "teaser":null},{
        "title": "Notices",
        "collection": "2019.4",
        "excerpt":"This information was developed for products and services offered in theUS. This material might be available from IBM in other languages.However, you may be required to own a copy of the product or productversion in that language in order to access it. IBM may not offer the products, services, or features discussed in thisdocument in other countries. Consult your local IBM representative forinformation on the products and services currently available in yourarea. Any reference to an IBM product, program, or service is notintended to state or imply that only that IBM product, program, orservice may be used. Any functionally equivalent product, program, orservice that does not infringe any IBM intellectual property right maybe used instead. However, it is the user's responsibility to evaluateand verify the operation of any non-IBM product, program, or service. IBM may have patents or pending patent applications covering subjectmatter described in this document. The furnishing of this document doesnot grant you any license to these patents. You can send licenseinquiries, in writing, to: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US For license inquiries regarding double-byte character set (DBCS)information, contact the IBM Intellectual Property Department in yourcountry or send inquiries, in writing, to: Intellectual Property LicensingLegal and Intellectual Property LawIBM Japan Ltd.19-21, Nihonbashi-Hakozakicho, Chuo-kuTokyo 103-8510, Japan INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION\"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED,INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OFNON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.Some jurisdictions do not allow disclaimer of express or impliedwarranties in certain transactions, therefore, this statement may notapply to you. This information could include technical inaccuracies or typographicalerrors. Changes are periodically made to the information herein; thesechanges will be incorporated in new editions of the publication. IBM maymake improvements and/or changes in the product(s) and/or the program(s)described in this publication at any time without notice. Any references in this information to non-IBM websites are provided forconvenience only and do not in any manner serve as an endorsement ofthose websites. The materials at those websites are not part of thematerials for this IBM product and use of those websites is at your ownrisk. IBM may use or distribute any of the information you provide in any wayit believes appropriate without incurring any obligation to you. Licensees of this program who wish to have information about it for thepurpose of enabling: (i) the exchange of information betweenindependently created programs and other programs (including this one)and (ii) the mutual use of the information which has been exchanged,should contact: IBM Director of LicensingIBM CorporationNorth Castle Drive, MD-NC119Armonk, NY 10504-1785US Such information may be available, subject to appropriate terms andconditions, including in some cases, payment of a fee. The licensed program described in this document and all licensedmaterial available for it are provided by IBM under terms of the IBMCustomer Agreement, IBM International Program License Agreement or anyequivalent agreement between us. The performance data discussed herein is presented as derived underspecific operating conditions. Actual results may vary. The client examples cited are presented for illustrative purposes only.Actual performance results may vary depending on specific configurationsand operating conditions. The performance data and client examples cited are presented forillustrative purposes only. Actual performance results may varydepending on specific configurations and operating conditions. Information concerning non-IBM products was obtained from the suppliersof those products, their published announcements or other publiclyavailable sources. IBM has not tested those products and cannot confirmthe accuracy of performance, compatibility or any other claims relatedto non-IBM products. Questions on the capabilities of non-IBM productsshould be addressed to the suppliers of those products. Statements regarding IBM's future direction or intent are subject tochange or withdrawal without notice, and represent goals and objectivesonly. All IBM prices shown are IBM's suggested retail prices, are current andare subject to change without notice. Dealer prices may vary. This information is for planning purposes only. The information hereinis subject to change before the products described become available. This information contains examples of data and reports used in dailybusiness operations. To illustrate them as completely as possible, theexamples include the names of individuals, companies, brands, andproducts. All of these names are fictitious and any similarity to actualpeople or business enterprises is entirely coincidental. COPYRIGHT LICENSE: This information contains sample application programs in sourcelanguage, which illustrate programming techniques on various operatingplatforms. You may copy, modify, and distribute these sample programs inany form without payment to IBM, for the purposes of developing, using,marketing or distributing application programs conforming to theapplication programming interface for the operating platform for whichthe sample programs are written. These examples have not been thoroughlytested under all conditions. IBM, therefore, cannot guarantee or implyreliability, serviceability, or function of these programs. The sampleprograms are provided \"AS IS\", without warranty of any kind. IBM shallnot be liable for any damages arising out of your use of the sampleprograms. Each copy or any portion of these sample programs or any derivative workmust include a copyright notice as follows: © (your company name) (year).Portions of this code are derived from IBM Corp. Sample Programs.© Copyright IBM Corp. enter the year or years Trademarks IBM, the IBM logo, and ibm.com are trademarks or registered trademarksof International Business Machines Corp., registered in manyjurisdictions worldwide. Other product and service names might betrademarks of IBM or other companies. A current list of IBM trademarksis available on the web at \"Copyright and trademark information\" atwww.ibm.com/legal/copytrade.shtml Terms and conditions for product documentation Permissions for the use of these publications are granted subject to thefollowing terms and conditions. Applicability These terms and conditions are in addition to any terms of use for theIBM website. Personal use You may reproduce these publications for your personal, noncommercialuse provided that all proprietary notices are preserved. You may notdistribute, display or make derivative work of these publications, orany portion thereof, without the express consent of IBM. Commercial use You may reproduce, distribute and display these publications solelywithin your enterprise provided that all proprietary notices arepreserved. You may not make derivative works of these publications, orreproduce, distribute or display these publications or any portionthereof outside your enterprise, without the express consent of IBM. Rights Except as expressly granted in this permission, no other permissions,licenses or rights are granted, either express or implied, to thepublications or any information, data, software or other intellectualproperty contained therein. IBM reserves the right to withdraw the permissions granted hereinwhenever, in its discretion, the use of the publications is detrimentalto its interest or, as determined by IBM, the above instructions are notbeing properly followed. You may not download, export or re-export this information except infull compliance with all applicable laws and regulations, including allUnited States export laws and regulations. IBM MAKES NO GUARANTEE ABOUT THE CONTENT OF THESE PUBLICATIONS. THEPUBLICATIONS ARE PROVIDED \"AS-IS\" AND WITHOUT WARRANTY OF ANY KIND,EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIEDWARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR APARTICULAR PURPOSE. ","categories": ["about"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/about/notices/",
        "teaser":null},{
        "title": "Trying out Event Streams",
        "collection": "2019.4",
        "excerpt":"IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages. For more features and full IBM support, install IBM Event Streams. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/trying-out/",
        "teaser":null},{
        "title": "Pre-requisites",
        "collection": "2019.4",
        "excerpt":"Ensure your environment meets the following prerequisites before installing IBM Event Streams. Container environment 2019.4.4 IBM Event Streams 2019.4.4 is supported on the following platforms and systems:   2019.4.4 in standalone:            Container platform      Systems                  Red Hat OpenShift Container Platform 3.11 with IBM cloud foundational services 3.2.1*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.2.1      - Linux® 64-bit (x86_64) systems - Linux on IBM® z13 or later systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.4.4 has Helm chart version 1.4.3 and includes Kafka version 2.3.1. For an overview of supported component and platform versions, see the support matrix. 2019.4.3 IBM Event Streams 2019.4.3 is supported on the following platforms and systems:   2019.4.3 in IBM Cloud Pak for Integration:            Container platform      Systems                  Red Hat OpenShift Container Platform 4.2 and 4.3      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)        2019.4.3 in standalone:            Container platform      Systems                  Red Hat OpenShift Container Platform 3.11 with IBM cloud foundational services 3.2.1*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.2.1      - Linux® 64-bit (x86_64) systems - Linux on IBM® z13 or later systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.4.3 has Helm chart version 1.4.2 and includes Kafka version 2.3.1. For an overview of supported component and platform versions, see the support matrix. 2019.4.2 IBM Event Streams 2019.4.2 is supported on the following platforms and systems:   2019.4.2 in IBM Cloud Pak for Integration:            Container platform      Systems                  Red Hat OpenShift Container Platform 4.2 and 4.3      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)        2019.4.2 in standalone:            Container platform      Systems                  Red Hat OpenShift Container Platform 3.11 with IBM cloud foundational services 3.2.1*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.2.1      - Linux® 64-bit (x86_64) systems - Linux on IBM® z13 or later systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.4.2 has Helm chart version 1.4.1 and includes Kafka version 2.3.1. For an overview of supported component and platform versions, see the support matrix. 2019.4.1 IBM Event Streams 2019.4.1 is supported on the following platforms and systems:   2019.4.1 in IBM Cloud Pak for Integration:            Container platform      Systems                  Red Hat OpenShift Container Platform 4.2      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)        2019.4.1 standalone:            Container platform      Systems                  Red Hat OpenShift Container Platform 3.11 with IBM cloud foundational services 3.2.1*      - Linux® 64-bit (x86_64) systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)              IBM Cloud Private 3.2.1      - Linux® 64-bit (x86_64) systems - Linux on IBM® z13 or later systems  - Microsoft Azure (IaaS)  - Amazon Web Services (IaaS)      *Provided by IBM Cloud Private Event Streams 2019.4.1 has Helm chart version 1.4.0 and includes Kafka version 2.3.0. For an overview of supported component and platform versions, see the support matrix. Environment prerequisites Ensure you have the following set up for your environment:   If you are installing Event Streams on the OpenShift Container Platform, ensure you have the right version of OpenShift installed and integrated with the right version of IBM Cloud Private. See previous table for supported versions. For example,  install OpenShift 3.11, and integrate it with IBM Cloud Private 3.2.1.      Install and configure IBM Cloud Private.Important: In high throughput environments, ensure you configure your IBM Cloud Private cluster to include an external load balancer and an internal network. These configuration options help take full advantage of Event Streams scaling and Kafka settings, and avoid potential performance bottlenecks. For more information, see the performance planning topic.     Note: IBM Event Streams includes entitlement to IBM Cloud Private Foundation which you can download from IBM Passport Advantage.     If you are installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS), ensure your proxy address uses lowercase characters.  If you are installing Event Streams on an IBM Cloud Private cluster deployed on Microsoft Azure, ensure you first register a Service Principal (an application in the Azure Active Directory). For information about creating a Service Principal, see the terraform documentation.  Install the Kubernetes command line tool, and configure access to your cluster.  If you are installing Event Streams on the OpenShift Container Platform, ensure you also install the OpenShift Container Platform CLI.  Install the IBM Cloud Private Command Line Interface (CLI).  Install the Helm CLI required for your version of IBM Cloud Private, and add the IBM Cloud Private internal Helm repository called local-charts to the Helm CLI as an external repository.      For message indexing capabilities (enabled by default), ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node: sudo sysctl -w vm.max_map_count=262144echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.conf     Important: This property might have already been updated by other workloads to be higher than the minimum required.   Hardware requirements The Helm chart for IBM Event Streams specifies default values for the CPU and memory usage of the Apache Kafka brokers and Apache ZooKeeper servers. See the following table for memory requirements of each Helm chart component. Ensure you have sufficient physical memory to service these requirements. Kubernetes manages the allocation of containers within your cluster. This allows resources to be available for other IBM Event Streams components which might be required to reside on the same node. Ensure you have one IBM Cloud Private worker node per Kafka broker, and a minimum of 3 worker nodes available for use by IBM Event Streams. Ensure each worker node runs on a separate physical server. See the guidance about Kafka high availability for more information. Helm resource requirements The Event Streams Helm chart has the following resource requirements based on resource request and limit settings. Requests and limits are Kubernetes concepts for controlling resource types such as CPU and memory.   Requests set the minimum requirements a container requires to be scheduled. If your system does not have the required request value, then your services will not start up.  Limits set the value beyond which a container cannot consume the resource. It is the upper limit within your system for the service.For more information about resource requests and limits, see the Kubernetes documentation. The following table lists the aggregate resource requirements of the Event Streams Helm chart. The table includes totals for both request and limit values of all pods and their containers. Each container in a pod has its own request and limit values, but as pods run as a group, these values need to be added together to understand the total requirements for a pod. The table includes information about requirements for the following deployment options:   The requirements are different depending on whether you require persistent storage or not.  If you plan to set up a multizone cluster, certain pod resource requirements will be on a per zone basis.For details about the requirements for each container within individual pods and any zone implications, see the individual tables in the sections following the summary table. These are the minimum requirements for an Event Streams installation, and will be used unless you change them when configuring your installation. They are based on the default resource request and limit settings of the chart. Installing with these settings is suitable for a starter deployment intended for testing purposes and trying out Event Streams. For a production setup, ensure you set higher values, and also consider important configuration options for IBM Cloud Private such as setting up a load balancer and an internal network. For more information about planning for a production setup, including requirements for a baseline production environment, see the performance planning topic.             Pod group      Configurable replicas      Total CPU request per pod group (cores)      Total CPU limit per pod group (cores)      Total memory request per pod group (Gi)      Total memory limit per pod group (Gi)                  Kafka      3*      8.6* (3.2 per zone)      10.4* (3.8 per zone)      14.3*  (4.8 per zone)      14.3* (4.8 per zone)              Event Streams core - Not persistent             5.1      12.4      6      7.4              Event Streams core - Not persistent - Multizone             3.3 + 1.4 per zone      7.1 + 4.9 per zone      4 + 1.6 per zone      4.5 + 2.6 per zone              Event Streams core  - Persistent             6.1      13.4      6.5      7.9              Event Streams core  - Persistent  - Multizone             4.3 + 1.4 per zone      8.1 + 4.9 per zone      4.5 + 1.6 per zone      5 + 2.6 per zone              Message indexing             1.5      2.5      4.4      8.4              Geo-replication      0*      0.9 per replica      1.6 per replica      2.5 per replica      2.5 per replica              TOTAL not persistent             15.2      24.3      21.8      23.2              TOTAL persistent             16.2      26.3      25.2      30.6              TOTAL 3 zones and persistent             19.6      36.7      28.1      35.6      Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. Before installing IBM Event Streams, consider the number of Kafka replicas and geo-replicator nodes you plan to use. Each Kafka replica and geo-replicator node is a separate chargeable unit. The geo-replication numbers are included in the previous table as an indication to show per replica requirements, but not included in the TOTAL rows. Kafka group The following pods and their containers are part of this group. Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. Kafka pod Number of replicas: 3*             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Kafka      1*      1*      2*      2*              Metrics reporter      0.4      0.6      1.5      1.5              Metrics proxy      0.5      0.5      1      1              Healthcheck      0.2      0.2      0.1      0.1              TLS proxy      0.1      0.5      0.1      0.1      Network proxy pod Number of replicas: 2 by default, otherwise 1 per zone             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Proxy      1      1      0.1      0.1      Event Streams core group The following pods and their containers are part of this group. Important: The settings marked with an asterisk (*) are configurable. The values in the table are the default minimum values. ZooKeeper pod Number of replicas: 3             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  ZooKeeper      0.1*      0.1*      0.75      1              TLS proxy      0.1      0.1      0.1      0.1      Administration UI pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  UI      1      1      1      1              Redis      0.1      0.1      0.1      0.1      Administration server pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest      0.5      4      1      1              Codegen      0.2      0.5      0.3      0.5              TLS proxy      0.1      0.1      0.1      0.1      REST producer server pod Number of replicas: 1 per zone             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest-producer      0.5      4      1      2      REST proxy pod Number of replicas: 1 per zone             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Rest-proxy      0.5      0.5      0.25      0.25      Collector pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Collector      0.1      0.1      0.05      0.05              TLS proxy      0.1      0.1      0.1      0.1      Access controller pod Number of replicas: 2 by default, otherwise 1 per zone             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Access controller      0.3      0.3      0.25      0.25              Redis      0.1      0.1      0.1      0.1      Schema Registry pod Number of replicas:   1 without persistence  2 with persistence enabled            Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Schema Registry      0.5      0.5      0.25      0.25              Avro service      0.5      0.5      0.25      0.25      Message indexing group The following pods and their containers are part of this group. Index manager pod Number of replicas: 1             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Index manager      0.2      0.2      0.1      0.1              TLS proxy      0.1      0.1      0.1      0.1      Elasticsearch pod Number of replicas: 2             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Elastic      0.5*      1*      2*      4*              TLS proxy      0.1      0.1      0.1      0.1      Geo-replicator group This group only contains the geo-replicator pod. Number of replicas: 0* Note: This means there is no geo-replication enabled by default. The values in the following table are the default minimum values for 1 replica.             Container      CPU request per container (cores)      CPU limit per container (cores)      Memory request per container (Gi)      Memory limit per container (Gi)                  Replicator      0.5      1      1      1              Metrics reporter      0.4      0.6      1.5      1.5      PodSecurityPolicy requirements To install the Event Streams chart, you must have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace. You can define the PodSecurityPolicy when creating the namespace for your installation. Event Streams applies network policies to control the traffic within the namespace where it is deployed, limiting the traffic to that required by Event Streams. For more information about the network policies and the traffic they permit, see network policies. For more information about PodSecurityPolicy definitions, see the IBM Cloud Private documentation. Note: The PodSecurityPolicy requirements do not apply to the Red Hat OpenShift Container Platform. Red Hat OpenShift SecurityContextConstraints Requirements If you  are installing on the OpenShift Container Platform, the Event Streams chart requires a custom SecurityContextConstraints to be bound to the target namespace prior to installation. The custom SecurityContextConstraints controls the permissions and capabilities required to deploy this chart. You can enable this custom SecurityContextConstraints resource using the supplied pre-installation setup script. Network requirements IBM Event Streams is supported for use with IPv4 networks only. File systems for storage If you want to set up persistent storage, you must have physical volumes available, backed by one of the following file systems:   NFS version 4  GlusterFS version 3.10.1  IBM Spectrum Scale version 5.0.3.0  Kubernetes local volumes  Amazon Elastic Block Store (EBS)IBM Event Streams user interface The IBM Event Streams user interface (UI) is supported on the following web browsers:   Google Chrome version 65 or later  Mozilla Firefox version 59 or later  Safari version 11.1 or laterIBM Event Streams CLI The IBM Event Streams command line interface (CLI) is supported on the following systems:   Windows 10 or later  Linux® Ubuntu 16.04 or later  macOS 10.13 (High Sierra) or laterClients The Apache Kafka Java client included with IBM Event Streams is supported for use with the following Java versions:   IBM Java 8  Oracle Java 8You can also use other Kafka version 2.0 or later clients when connecting to Event Streams. If you encounter client-side issues, IBM can assist you to resolve those issues (see our support policy). Event Streams is designed for use with clients based on the librdkafka implementation of the Apache Kafka protocol. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/prerequisites/",
        "teaser":null},{
        "title": "Planning for installation",
        "collection": "2019.4",
        "excerpt":"Consider the following when planning your installation. Performance considerations When preparing for your Event Streams installation, review your workload requirements and consider the configuration options available for performance tuning both your IBM Cloud Private and Event Streams installations. For more information, see the performance planning topic. Kafka high availability Kafka is designed for high availability and fault tolerance. To reduce the impact of Event Streams Kafka broker failures, spread your brokers across several IBM Cloud Private worker nodes by ensuring you have at least as many worker nodes as  brokers. For example, for 3 Kafka brokers, ensure you have at least 3 worker nodes running on separate physical servers. Kafka ensures that topic-partition replicas are spread across available brokers up to the replication factor specified. Usually, all of the replicas will be in-sync, meaning that they are all fully up-to-date, although some replicas can temporarily be out-of-sync, for example, when a broker has just been restarted. The replication factor controls how many replicas there are, and the minimum in-sync configuration controls how many of the replicas need to be in-sync for applications to produce and consume messages with no loss of function. For example, a typical configuration has a replication factor of 3 and minimum in-sync replicas set to 2. This configuration can tolerate 1 out-of-sync replica, or 1 worker node or broker outage with no loss of function, and 2 out-of-sync replicas, or 2 worker node or broker outages with loss of function but no loss of data. The combination of brokers spread across nodes together with the replication feature make a single Event Streams cluster highly available. Multizone support To add further resilience to your clusters, you can also split your servers across multiple data centers or zones, so that even if one zone experiences a failure, you still have a working system. Multizone support provides the option to run a single Kubernetes cluster in multiple availability zones within the same region. Multizone clusters are clusters of either physical or virtual servers that are spread over different locations to achieve greater resiliency. If one location is shut down for any reason, the rest of the cluster is unaffected. Note: For Event Streams to work effectively within a multizone cluster, the network latency between zones must not be greater than 20 ms for Kafka to replicate data to the other brokers. Your Kubernetes container platforms can be zone aware or non-zone aware:   Zone-aware platforms set up your clusters in multiple availability zones automatically by allocating your application resources evenly across the zones.  Non-zone-aware platforms require you to manually prepare your clusters for multiple zones by specifying labels for nodes, and then providing the labels for the zones during the installation of Event Streams.For information about how to determine whether your clusters are zone aware or not, and to get ready for installing into multiple zones, see preparing for multizone clusters. Persistent storage Persistence is not enabled by default, so no persistent volumes are required. Enable persistence if you want your data, such as messages in topics, schemas, and configuration settings to be retained in the event of a restart. You should enable persistence for production use and whenever you want your data to survive a restart. If you plan to have persistent volumes, consider the disk space required for storage. Also, as both Kafka and ZooKeeper rely on fast write access to disks, ensure you use separate dedicated disks for storing Kafka and ZooKeeper data. For more information, see the disks and filesystems guidance in the Kafka documentation, and the deployment guidance in the ZooKeeper documentation. If persistence is enabled, each Kafka broker and ZooKeeper server requires one physical volume each. The number of Kafka brokers and ZooKeeper servers depends on your setup, for default requirements, see the resource requirements table. Schema registry requires a single physical volume. You either need to create a persistent volume for each physical volume, or specify a storage class that supports dynamic provisioning. Each component can use a different storage class to control how physical volumes are allocated. Note: When creating persistent volumes backed by an NFS file system, ensure the path provided has the access permission set to 775. See the IBM Cloud Private documentation for information about creating persistent volumes and creating a storage class that supports dynamic provisioning. For both, you must have the IBM Cloud Private Cluster Administrator role. Important: When creating persistent volumes for each component, ensure the correct Access mode is set for the volumes as described in the following table.             Component      Access mode                  Kafka      ReadWriteOnce              ZooKeeper      ReadWriteOnce              Schema registry      ReadWriteMany or ReadWriteOnce      More information about persistent volumes and the system administration steps required before installing IBM Event Streams can be found in the Kubernetes documentation. If these persistent volumes are to be created manually, this must be done by the system administrator before installing IBM Event Streams. The administrator will add these to a central pool before the Helm chart can be installed. The installation will then claim the required number of persistent volumes from this pool. If these persistent volumes are to be created automatically, run a provisioner for the the storage class you want to use. See the list of file systems for storage supported by Event Streams. If you want to use persistent storage, enable it when configuring your Event Streams installation, and provide the storage class details as required. Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Using IBM Spectrum Scale If you are using IBM Spectrum Scale for persistence, see the IBM Storage Enabler for Containers with IBM Spectrum Scale documentation for more information about creating storage classes. The system administrator must enable support for the automatic creation of persistent volumes prior to installing Event Streams. To do this, enable dynamic provisioning when configuring your installation, and provide the storage class names to define the persistent volumes that get allocated to the deployment. Note: When creating storage classes for an IBM Spectrum Scale file system, ensure you specify both the UID and GID in the storage class definition. Then, when installing Event Streams, ensure you set the File system group ID field to the GID specified in the storage class definition. Also, ensure that this user exists on all IBM Spectrum Scale and GUI nodes. Securing communication between pods You can enhance your security by encrypting the internal communication between Event Streams pods by using TLS. By default, TLS communication between pods is disabled. You can enable encryption between pods when configuring your Event Streams installation. You can also enable TLS encryption between pods for existing Event Streams installations. Important: All message data is encrypted using TLS, but communication between the geo-replicator and administration server pods is not encrypted (see tables in resource requirements). ConfigMap for Kafka static configuration You can choose to create a ConfigMap to specify Kafka configuration settings for your IBM Event Streams installation. This is optional. You can use a ConfigMap to override default Kafka configuration settings when installing IBM Event Streams. You can also use a ConfigMap to modify read-only Kafka broker settings for an existing IBM Event Streams installation. Read-only parameters are defined by Kafka as settings that require a broker restart. Find out more about the Kafka configuration options and how to modify them for an existing installation. To create a ConfigMap:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Note: To create a ConfigMap, you must have the Team Operator, Team Administrator, or Cluster Administrator role in IBM Cloud Private.  To create a ConfigMap from an existing Kafka server.properties file, use the following command (where namespace is where you install Event Streams):  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  To create a blank ConfigMap for future configuration updates, use the following command:  kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt;Geo-replication You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters. Geo-replication helps maintain service availability. Find out more about geo-replication. Prepare your destination cluster by setting the number of geo-replication worker nodes during installation. Connecting clients By default, Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. If you want clients to connect through a different route, specify the target endpoint host name or IP address when configuring your installation. Logging IBM Cloud Private uses the Elastic Stack for managing logs (Elasticsearch, Logstash, and Kibana products). IBM Event Streams logs are written to stdout and are picked up by the default Elastic Stack setup. Consider setting up the IBM Cloud Private logging for your environment to help resolve problems with your deployment and aid general troubleshooting. See the IBM Cloud Private documentation about logging for information about the built-in Elastic Stack. As part of setting up the IBM Cloud Private logging for IBM Event Streams, ensure you consider the following:   Capacity planning guidance for logging: set up your system to have sufficient resources towards the capture, storage, and management of logs.  Log retention: The logs captured using the Elastic Stack persist during restarts. However, logs older than a day are deleted at midnight by default to prevent log data from filling up available storage space. Consider changing the log data retention in line with your capacity planning. Longer retention of logs provides access to older data that might help troubleshoot problems.You can use log data to investigate any problems affecting your system health. Monitoring Kafka clusters IBM Event Streams uses the IBM Cloud Private monitoring service to provide you with information about the health of your Event Streams Kafka clusters. You can view data for the last 1 hour, 1 day, 1 week, or 1 month in the metrics charts. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. For more information about keeping an eye on the health of your Kafka cluster, see the monitoring Kafka topic. Licensing You require a license to use IBM Event Streams. Licensing is based on a Virtual Processing Cores (VPC) metric. An IBM Event Streams deployment consists of a number of different types of containers, as described in the components of the Helm chart. To use IBM Event Streams you must have a license for all of the virtual cores that are available to all Kafka and Geo-replicator containers deployed. All other container types are pre-requisite components that are supported as part of IBM Event Streams, and do not require additional licenses. The number of virtual cores available to each Kafka and geo-replicator container can be specified during installation or modified later. To check the number of cores, use the IBM Cloud Private metering report as follows:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Platform &gt; Metering.  Select your namespace, and select IBM Event Streams (Chargeable).  Click Containers.  Go to the Containers section on the right, and ensure you select the Usage tab.  Select Capped Processors from the first drop-down list, and select 1 Month from the second drop-down list.A page similar to the following is displayed:  Click Download Report, and save the CSV file to a location of your choice.  Open the downloaded report file.  Look for the month in Period, for example, 2018/9, then in the rows underneath look for IBM Event Streams (Chargeable), and check the CCores/max Cores column. The value is the maximum aggregate number of cores provided to all Kafka and geo-replicator containers. You are charged based on this number.For example, the following excerpt from a downloaded report shows that for the period 2018/9 the chargeable IBM Event Streams containers had a total of 4 cores available (see the highlighted fields):","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/planning/",
        "teaser":null},{
        "title": "Performance and capacity planning",
        "collection": "2019.4",
        "excerpt":"When preparing for your IBM Event Streams installation, consider the performance and capacity requirements for your system. Guidance for production environments The prerequisites for Event Streams provide information about the minimum resources requirements for a test environment. For a baseline production deployment of Event Streams, increase the following values.   Set the CPU request and limit values for Kafka brokers to 4000m. You can use the kafka.resources.requests.cpu and kafka.resources.limits.cpu options if you are using the command line, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI.  Set the memory request and limit values for Kafka brokers to at least 6Gi. You can use the kafka.resources.requests.memory and kafka.resources.limits.memory options if you are using the command line, or enter the values in the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI.You can set higher values when configuring your installation, or set them later. Note: This guidance sets the requests and limits to the same values. You might need to set the limits to higher values depending on your intended workload. Remember to add the increases to the minimum resource requirement values, and ensure the increased settings can be served by your system. Important: For high throughput environments, also ensure you prepare your IBM Cloud Private installation beforehand. Depending on your workload, you can further scale Event Streams and fine tune Kafka performance to accommodate the increased requirements. Scaling Event Streams If required by your planned workload, you can further increase the number of Kafka brokers, and the amount of CPU and memory available to them. For changing other values, see the guidance about scaling Event Streams. A performance report based on example case studies is available to provide guidance for setting these values. Tuning Event Streams Kafka performance You can further fine-tune the performance settings of your Event Streams Kafka brokers to suit your requirements. Kafka provides a range of parameters to set, but consider the following ones when reviewing performance requirements. You can set these parameters when installing Event Streams, or you can modify them later.   The num.replica.fetchers parameter sets the number of threads available on each broker to replicate messages from topic leaders. Increasing this setting increasesI/O parallelism in the follower broker, and can help reduce bottlenecks and message latency. You can start by setting this value to match the number of brokers deployed in the system. Note: Increasing this value results in brokers using more CPU resources and network bandwidth.  The num.io.threads parameter sets the number of threads available to a broker for processing requests. As the load on each broker increases, handling requests can become a bottleneck. Increasing this parameter value can help mitigate this issue. The value to set depends on the overall system load and the processing power of the worker nodes, which varies for each deployment. There is a correlation between this setting and the num.network.threads setting.  The num.network.threads parameter sets the number of threads available to the broker for receiving and sending requests and responses to the network. The value to set depends on the overall network load, which varies for each deployment. There is a correlation between this setting and the num.io.threads setting.  The replica.fetch.min.bytes, replica.fetch.max.bytes, and replica.fetch.response.max.bytes parameters control the minimum and maximum sizes for message payloads whenperforming inter-broker replication. Set these values to be greater than the message.max.bytes parameter to ensure that all messages sent by a producer can be replicatedbetween brokers. The value to set depends on message throughput and average size, which varies for each deployment.To set these parameter values, you can use a ConfigMap that specifies Kafka configuration settings for your Event Streams installation. Setting before installation If you are creating the ConfigMap and setting the parameters when installing Event Streams, you can add these parameters to the properties file with the required values.   Add the parameters and their values to the Kafka server.properties file, for example:    num.io.threads=24num.network.threads=9num.replica.fetchers=3replica.fetch.max.bytes=5242880replica.fetch.min.bytes=1048576replica.fetch.response.max.bytes=20971520        Create the ConfigMap as described in the planning for installation section, for example:kubectl -n &lt;namespace_name&gt; create configmap &lt;configmap_name&gt; --from-env-file=&lt;full_path/server.properties&gt;  When installing Event Streams, ensure you provide the ConfigMap to the installation.Modifying an existing installation If you are updating an existing Event Streams installation, you can use the ConfigMap you already have for the Kafka configuration settings, and include the parameters and their values in the ConfigMap. You can then apply the new settings by updating the ConfigMap as described modifying Kafka broker configurations, for example: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Alternatively, you can modify broker configuration settings dynamically by using the Event Streams CLI as described in modifying Kafka broker configurations, for example: cloudctl es cluster-config --config num.replica.fetchers=4 Important: Using the Event Streams CLI overrides the values specified in the ConfigMap. In addition, the CLI enforces constraints to avoid certain parameters to be misconfigured. For example, you cannot set num.replica.fetches to a value greater than double its current value. This means that you might have to make incremental updates to the value, for example: cloudctl es cluster-config --config num.replica.fetchers=2cloudctl es cluster-config --config num.replica.fetchers=4cloudctl es cluster-config --config num.replica.fetchers=8cloudctl es cluster-config --config num.replica.fetchers=9Performance considerations for IBM Cloud Private For high throughput environments, consider the following configuration options when setting up your IBM Cloud Private environment.   Set up an external load balancer for your IBM Cloud Private cluster to provide a dedicated external access point for the cluster that provides intelligent routing algorithms.  Set up a dedicated internal network for inter-broker traffic to avoid contention between internal processes and external traffic.Important: You must consider and set these IBM Cloud Private configuration options before installing Event Streams. Setting up a load balancer In high throughput environments, configure an external load balancer for your IBM Cloud Private cluster. Without a load balancer, a typical Event Streams installation includes a master node for allowing external traffic into the cluster. There are also worker nodes that host Kafka brokers. Each broker has an advertised listener that consists of the master node’s IP address and a unique node port within the cluster. This means the worker nodes can be identified without being exposed externally. When a producer connects to the Event Streams master node through the bootstrap port, they are sent metadata that identifies partition leaders for the topics hosted by the brokers. So, access to the cluster is based on the address &lt;master_node:bootstrap_port&gt;, and identification is based on the advertised listener addresses within the cluster, which has a node port to uniquely identify the specific broker. For example, the connection is made to the &lt;master_node:bootstrap_port&gt; address, for example: 192.0.2.24:30724 The advertised listener is then made up of the &lt;master_node:unique_port&gt; address, for example: 192.0.2.24:88945 The producer then sends messages to the advertised listener for a partition leader of a topic. These requests go through the master node and are passed to the right worker node in Event Streams based on the internal IP address for that specific advertised listener that identifies the broker. This means all traffic is routed through the master node before being distributed across the cluster. If the master node is overloaded by service requests, network traffic, or system operations, it becomes a bottleneck for incoming requests.  A load balancer replaces the master node as the entry point into the cluster, providing a dedicated service that typically runs on a separate node. In this case the bootstrap address points to the load balancer instead of the master node. The load balancer passes incoming requests to any of the available worker nodes. The worker node then forwards the request onto the correct broker within the cluster based on its advertised listener address. Setting up a load balancer provides more control over how requests are forwarded into the cluster (for example, round-robin, least congested, and so on), and frees up the master node for system operations.  For more information about configuring an external load balancer for your cluster, see the IBM Cloud Private documentation. Important: When using a load balancer for IBM Cloud Private, ensure you set the address for your endpoint in the External hostname/IP address field field when installing your Event Streams instance. Setting up an internal network Communication between brokers can generate significant network traffic in high usage scenarios. Topic configuration such as replication factor settings can also impact traffic volume. For high performance setups, enable an internal network to handle workload traffic within the cluster. To configure an internal network for inter-broker workload traffic, enable a second network interface on each node, and configure the config.yaml before installing IBM Cloud Private. For example, use the calico_ip_autodetection_method setting to configure the master node IP address on the second network as follows: calico_ip_autodetection_method: can-reach=&lt;internal_ip_address_for_master_node&gt; For more information about setting up a second network, see the IBM Cloud Private documentation. Disk space for persistent volumes You need to ensure you have sufficient disk space in the persistent storage for the Kafka brokers to meet your expected throughput and retention requirements. In Kafka, unlike other messaging systems, the messages on a topic are not immediately removed after they are consumed. Instead, the configuration of each topic determines how much space the topic is permitted and how it is managed. Each partition of a topic consists of a sequence of files called log segments. The size of the log segments is determined by the cluster configuration log.segment.bytes (default is 1 GB). This can be overridden by using the topic-level configuration segment.bytes. For each log segment, there are two index files called the time index and the offset index. The size of the index is determined by the cluster configuration log.index.size.max.bytes (default is 10 MB). This can be overridden by using the topic-level configuration segment.index.bytes. Log segments can be deleted or compacted, or both, to manage their size. The topic-level configuration cleanup.policy determines the way the log segments for the topic are managed. For more information about the broker configurations and topic-level configurations, see the Kafka documentation. You can specify the cluster and topic-level configurations by using the IBM Event Streams CLI. You can also set topic-level configuration when setting up the topic in the IBM Event Streams UI (click Topics in the primary navigation, then click Create topic, and set Show all available options to On). Log cleanup by deletion If the topic-level configuration cleanup.policy is set to delete (the default value), old log segments are discarded when the retention time or size limit is reached, as set by the following properties:   Retention time is set by retention.ms, and is the maximum time in milliseconds that a log segment is retained before being discarded to free up space.  Size limit is set by retention.bytes, and is the maximum size that a partition can grow to before old log segments are discarded.By default, there is no size limit, only a time limit. The default time limit is 7 days (604,800,000 ms). You also need to have sufficient disk space for the log segment deletion mechanism to operate. The broker configuration log.retention.check.interval.ms (default is 5 minutes) controls how often the broker checks to see whether log segments should be deleted. The broker configuration log.segment.delete.delay.ms (default is 1 minute) controls how long the broker waits before deleting the log segments. This means that by default you also need to ensure you have enough disk space to store log segments for an additional 6 minutes for each partition. Worked example 1 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second. The retention time period is 7 days (604,800 seconds). Each broker hosts 1 replica of the topic’s single partition. The log capacity required for the 7 days retention period can be determined as follows: 3,000 * (604,800 + 6 * 60) = 1,815,480,000 bytes. So, each broker requires approximately 2GB of disk space allocated in its persistent volume, plus approximately 20 MB of space for index files. In addition, allow at least 1 log segment of extra space to make room for the actual cleanup process. Altogether, you need a total of just over 3 GB disk space for persistent volumes. Worked example 2 Consider a cluster that has 3 brokers, and 1 topic with 1 partition with a replication factor of 3. The expected throughput is 3,000 bytes per second.  The retention size configuration is set to 2.5 GB. Each broker hosts 1 replica of the topic’s single partition. The number of log segments for 2.5 GB is 3, but you should also allow 1 extra log segment after cleanup. So, each broker needs approximately 4 GB of disk space allocated in its persistent volume, plus approximately 40 MB of space for index files. The retention period achieved at this rate is approximately 2,684,354,560 / 3,000 = 894,784 seconds, or 10.36 days. Log cleanup by compaction If the topic-level configuration cleanup.policy is set to compact, the log for the topic is compacted periodically in the background by the log cleaner. In a compacted topic, each message has a key. The log only needs to contain the most recent message for each key, while earlier messages can be discarded. The log cleaner calculates the offset of the most recent message for each key, and then copies the log from start to finish, discarding keys which have later messages in the log. As each copied segment is created, they are swapped into the log right away to keep the amount of additional space required to a minimum. Estimating the amount of space that a compacted topic will require is complex, and depends on factors such as the number of unique keys in the messages, the frequency with which each key appears in the uncompacted log, and the size of the messages. Log cleanup by using both You can specify both delete and compact values for the cleanup.policy configuration at the same time. In this case, the log is compacted, but the cleanup process also follows the retention time or size limit settings. When both methods are enabled, capacity planning is simpler than when you only have compaction set for a topic. However, some use cases for log compaction depend on messages not being deleted by log cleanup, so consider whether using both is right for your scenario. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/capacity-planning/",
        "teaser":null},{
        "title": "Preparing for multizone clusters",
        "collection": "2019.4",
        "excerpt":"Event Streams supports multiple availability zones for your clusters. Multizone clusters add resilience to your  Event Streams installation. For guidance about handling outages in a multizone setup, see managing a multizone setup. Installing as Team Administrator If you are installing as a Team Administrator, a Cluster Administrator first must download and run the Cluster Role setup script as follows.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Download the files from GitHub.      Change to the location where you downloaded the files, and run the setup script as follows:     ./node-cluster-role.sh &lt;namespace&gt; &lt;release-name&gt;     Where &lt;namespace&gt; is the namespace you created for your Event Streams installation earlier and &lt;release_name&gt; is the name of the release the Team Admin is planning on installing as.     This script sets the required Cluster Role and Cluster Rolebinding for the Team Administrator role.     Ensure you clear the Generate cluster roles checkbox when configuring your Event Streams installation.Checking if your cluster is zone aware To determine if your cluster is zone aware, run the following command as Cluster Administrator: kubectl get nodes --show-labels   If your cluster is zone aware, the following label is displayed as a result: failure-domain.beta.kubernetes.io/zone The value of the label will be the zone the node is in, for example, us-west1. If your cluster is zone aware and you want to use multiple availability zones, specify the number of zones when configuring your installation. Your zones are automatically set up during installation.  If your cluster is not zone aware, the zone label mentioned earlier is not displayed. In such cases, prepare your clusters as described in setting up non-zone-aware clusters.Setting up non-zone-aware clusters If your Kubernetes cluster is not zone aware, you can still set up multiple availability zones as follows.       Label the nodes in your cluster with failure-domain.beta.kubernetes.io/zone, and set the value to a value of your choice. Run the following command to label your nodes, for example, to allocate a node to es-zone-1:     kubectl label node &lt;node-name&gt; failure-domain.beta.kubernetes.io/zone=es-zone-1     You will need to provide these labels in the Zone labels field when installing Event Streams. This is required to distribute the resources equally across the zones in a similar way to a zone-aware cluster by setting the node affinity rules on the resources.         Distribute the 3 ZooKeeper pods across the zones by dedicating a node in each zone to a ZooKeeper pod. You do this by adding a label to the selected node in each zone as follows:     kubectl label node &lt;node-name&gt; node-role.kubernetes.io/zk=true         Distribute the Kafka broker pods as evenly as possible across the zones by dedicating a node in each zone to a Kafka pod. You do this by adding a label to the selected node in each zone as follows:     kubectl label node &lt;node-name&gt; node-role.kubernetes.io/kafka=true     Note: You need to do this for each Kafka broker. For example, if you have 3 brokers and 3 availability zones, then label 1 node in each zone; if you have 6 brokers and 3 availability zones, then label 2 nodes in each zone.         Set up the availability zones when configuring your Event Streams installation as described in configuring.   ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/preparing-multizone/",
        "teaser":null},{
        "title": "Installing on IBM Cloud Private",
        "collection": "2019.4",
        "excerpt":"Install IBM Event Streams on IBM Cloud Private as follows. Before you begin   Ensure you have set up your environment according to the prerequisites, including your IBM Cloud Private environment.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:          The master host and port for your IBM Cloud Private cluster. These values are set during the installation of IBM Cloud Private. The default port is 8443. Make a note of these values, and enter them in the steps that have https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;      The SSH password if you are connecting remotely to the master host of your IBM Cloud Private cluster.        Ensure your proxy address uses lowercase characters. This is a setting that often needs to be checked when installing Event Streams on an IBM Cloud Private cluster deployed on Amazon Web Services (AWS). If the address is in uppercase, edit the ibmcloud-cluster-info ConfigMap in the kube-public namespace, and change the uppercase characters to lowercase for the proxy_address parameter: kubectl edit configmap -n ibmcloud-cluster-info -n kube-public  Ensure you have the IBM Cloud Private monitoring service installed. Usually monitoring is installed by default. However, some deployment methods might not install it. For example, monitoring might not be part of the default deployment when installing IBM Cloud Private on Azure by using Terraform. Without this service, parts of the Event Streams UI do not work. You can install the monitoring service from the Catalog or CLI for existing  deployments.Preparing the platform Prepare your platform for installing Event Streams as follows. Create a namespace You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. To create a namespace, you must have the Cluster Administrator role.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Cluster Administrator role.  From the navigation menu, click Manage &gt; Namespaces.  Click Create Namespace.  Enter a name for your namespace.  Ensure you have the ibm-restricted-psp PodSecurityPolicy selected for the target namespace.  Click Create.See the IBM Cloud Private documentation for more information about creating namespaces. Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site and make it available in your catalog.   Go to IBM Passport Advantage, and search for “IBM Event Streams”. Download the images related to the part numbers for your platform.  Ensure you configure your Docker CLI to access your cluster.  Log in to your cluster from the IBM Cloud Private CLI and log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443docker login &lt;cluster_CA_domain&gt;:8500        Note: The default value for the cluster_CA_domain parameter is mycluster.icp. If necessary add an entry to your system’s host file to allow it to be resolved. For more information, see the IBM Cloud Private documentation.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-archive --archive &lt;PPA-image-name.tar.gz&gt;     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository by creating an image policy. Note: You only need to follow these steps if the image-security-enforcement service is enabled. If the service is not enabled, you can ignore these steps. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt; The default port is 8443. Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation. To create an image policy:   Create a .yaml file with the following content, then replace &lt;cluster_CA_domain&gt; with the correct value for your IBM Cloud Private environment, and replace the &lt;namespace_for_event_streams&gt; value with the name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;cluster_CA_domain&gt;:8500/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Team Administrator or Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-prod and select it from the result. The IBM Event Streams README is displayed.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, select a target cluster (for example, local-cluster), and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.  Click Install.  Verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/installing/",
        "teaser":null},{
        "title": "Installing on OpenShift",
        "collection": "2019.4",
        "excerpt":"IBM Event Streams makes using Apache Kafka in the enterprise easy and intuitive, and is now fully supported on the  Red Hat OpenShift Container Platform. Overview You can install Event Streams on the Red Hat OpenShift Container Platform. The solution includes key IBM cloud foundational services such as installation, security, monitoring, and lifecycle management. These services help manage your Event Streams installation, and are provided by IBM Cloud Private.  The benefits of the solution mean you have a container platform from which you can perform administrative tasks in Red Hat OpenShift while taking some foundational services Event Streams relies on from IBM Cloud Private. Any service task related to Kubernetes can be performed in both Red Hat OpenShift Container Platform and IBM Cloud Private. For example, you can perform administrative tasks through either platform, such as managing storage, reviewing status of components, and reviewing logs and events from each component. Certain aspects of managing your Event Streams installation require the use of the IBM cloud foundational services provided by IBM Cloud Private. These services are as follows:   Installing the chart  Applying updates and fix packs  Modifying installation settings  Managing authentication and access (IAM)  Reviewing metering  Reviewing monitoring and metricsImportant: This documentation assumes the use of IBM Cloud Private for the IBM cloud foundational services required for managing your Event Streams installation. Before you begin   Ensure you have set up your environment according to the prerequisites, including setting up your OpenShift Container Platform and your IBM Cloud Private integration.  Ensure you have planned for your installation, such as planning for persistent volumes if required, and creating a ConfigMap for Kafka static configuration.  Gather the following information from your administrator:                  The connection details for your IBM Cloud Private cluster in the format &lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;. An administrator can retrieve these details from the ConfigMap in kube-public as follows:         kubectl get cm ibmcloud-cluster-info -n kube-public -o yaml         See the cluster_address value for the master address, and the cluster_router_https_port for the port number.                     The connection details for your OpenShift Container Platform cluster.             Create a project (namespace) You perform this step by using the OpenShift Container Platform command line. You must use a namespace that is dedicated to your Event Streams deployment. This is required because Event Streams uses network security policies to restrict network connections between its internal components. If you plan to have multiple Event Streams instances, create namespaces to organize your IBM Event Streams deployments into, and control user access to them. When you create a project in the OpenShift Container Platform, a namespace with the same name is also created. This is the namespace to use when installing your Event Streams instance. You can create a project by using the web console or the CLI. For example, to create a project by using the CLI: oc login -u=&lt;username&gt; -p=&lt;password&gt; --server=&lt;your-openshift-server&gt; --insecure-skip-tls-verifyoc new-project &lt;project_name&gt; --description=\"&lt;description&gt;\" --display-name=\"&lt;display_name&gt;\"Download the archive Download the IBM Event Streams installation image file from the IBM Passport Advantage site, and save the archive to the host where the IBM Cloud Private master cluster is installed. Go to IBM Passport Advantage, and search for “IBM Event Streams”. Preparing the platform For the following steps, log in to IBM Cloud Private to prepare your platform for installing Event Streams. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt; Run the setup script You perform this step by using the IBM Cloud Private CLI. You must run the following setup script to prepare the platform.   Go to the Event Streams archive you downloaded from IBM Passport Advantage, and locate the file called ibm-eventstreams-prod-&lt;version&gt;.tgz.  Extract the PPA tar.gz archive.  In your terminal window, change to the following directory: /pak_extensions/pre-install  Run the setup script as follows: ./scc.sh &lt;namespace&gt; Where &lt;namespace&gt; is the namespace (project) you created for your Event Streams installation earlier.Look up the registry address You perform this step by using the Kubernetes and OpenShift CLIs. You will require two addresses for the OpenShift Docker registry:   The &lt;external_OpenShift_Docker_registry_address&gt; is required to access the OpenShift docker registry externally to load the PPA archive in a later step.  The &lt;internal_OpenShift_Docker_registry_address&gt; is required when installing the Event Streams chart later, and also required to prepare the repository if you have the image-security-enforcementservice enabled.Retrieve the external address To retrieve the external address: Look up the external OpenShift Docker registry address by using the following command: kubectl get routes docker-registry -n default The following is an example output: NAME              HOST/PORT                                               PATH  SERVICES          PORT    TERMINATION   WILDCARDdocker-registry   docker-registry-default.apps.cluster-abc.my-domain.com        docker-registry   &lt;all&gt;   passthrough   NoneThe &lt;external_OpenShift_Docker_registry_address&gt; is the values of the HOST/PORT field. Note: You can only retrieve the address if your docker registry is exposed. Retrieve the internal address To retrieve the internal address: The &lt;internal_OpenShift_Docker_registry_address&gt; is a value in the following format: docker-registry.default.svc:&lt;port&gt; Look up the internal OpenShift Docker registry port number by using the following command: oc get svc docker-registry -n default The following is an example output: NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGEdocker-registry   ClusterIP   198.51.100.24   &lt;none&gt;        5000/TCP   2dIn this example, where the namespace is default and the port is 5000, the &lt;internal_OpenShift_Docker_registry_address&gt; is docker-registry.default.svc:5000 Load the archive into the catalog Make the downloaded archive available in your catalog by using the IBM Cloud Private CLI.       Log in to the Docker private image registry:docker login -u any_value -p $(oc whoami -t) &lt;external_OpenShift_Docker_registry_address&gt;     Where the &lt;external_OpenShift_Docker_registry_address&gt; is the external OpenShift Docker registry address you looked up earlier.     Note: The docker login command uses a session token (oc whoami -t) in the password field to perform authentication. This means the -u user name field is required, but not used by Docker.         Make the Event Streams Helm chart available in the catalog by using the compressed image you downloaded from IBM Passport Advantage.cloudctl catalog load-ppa-archive --archive &lt;PPA-image-name.tar.gz&gt; --registry &lt;external_OpenShift_Docker_registry_address&gt;/&lt;namespace-to-install-into&gt;     For example:cloudctl catalog load-ppa-archive --archive eventstreams.2019.4.1.z_x86.pak.tar.gz --registry docker-registry-default.apps.cluster-abc.my-domain.com/event-streams     When the image installation completes successfully, the catalog is updated with the IBM Event Streams local chart, and the internal Docker repository is populated with the Docker images used by IBM Event Streams.   Preparing the repository Prepare your repository by creating an image policy. Note: You only need to follow these steps if the image-security-enforcement service is enabled. If the service is not enabled, you can ignore these steps. The following steps require you to run kubectl commands. To run the commands, you must be logged in to your IBM Cloud Private cluster as an administrator. Log in as described in earlier. Create an image policy for the internal Docker repository. The policy enables images to be retrieved during installation. To create an image policy:   Create a .yaml file with the following content, then replace &lt;internal_OpenShift_Docker_registry_address&gt; with the address you looked up earlier, and replace the &lt;namespace_for_event_streams&gt; value with the project name where you intend to install IBM Event Streams (set as -n event-streams in the previous example):    apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: image-policy  namespace: &lt;namespace_for_event_streams&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: &lt;internal_OpenShift_Docker_registry_address&gt;/*    policy: null        Run the following command: kubectl apply -f &lt;filename&gt;.yamlFor more information about container image security, see the IBM Cloud Private documentation. Installing the Event Streams chart You perform this step in a browser by using the IBM Cloud Private cluster management console. Install the Event Streams chart as follows.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.Ensure you log in as a user that has the Team Administrator or Cluster Administrator role.  Click Catalog in the top navigation menu.  Search for ibm-eventstreams-prod and select it from the result. The IBM Event Streams README is displayed.  Click Configure.Note: The README includes information about how to install IBM Event Streams by using the CLI. To use the CLI, follow the instructions in the README instead of clicking Configure.Important: You might see the following warnings on this page. These warnings are harmless and can be safely ignored as the OpenShift Container Platform does not use PodSecurityPolicy settings.  Enter a release name that identifies your Event Streams installation, select the target namespace you created previously, select a target cluster (for example, local-cluster), and accept the terms of the license agreement.  Expand the All parameters section to configure the settings for your installation as described in configuring. Configuration options to consider include setting up persistent storage, external access, and preparing for geo-replication.  Note: Ensure the Docker image registry field value includes the &lt;internal_OpenShift_Docker_registry_address&gt; you looked up earlier, and the namespace where you are installing Event Streams, for example: docker-registry.default.svc:5000/event-streams  Click Install.  Verify your installation and consider other post-installation tasks.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/installing-openshift/",
        "teaser":null},{
        "title": "Configuring",
        "collection": "2019.4",
        "excerpt":"Enabling persistent storage If you want your data to be preserved in the event of a restart, set persistent storage for Kafka, ZooKeeper, and schemas in your IBM Event Streams installation. To enable persistent storage for Kafka:   Go to the Kafka persistent storage settings section.  Select the Enable persistent storage for Apache Kafka check box.      To use a specific storage class, select the Enable storage class for Apache Kafka check box, and provide the name of the storage class to use for the persistent volume claims intended for Kafka. Also, provide a prefix to use for the persistent volume claims, and set the minimum size required for your intended usage.     If present, existing persistent volumes with the specified storage class are used after installation, or if a provisioner is configured for the specified storage class, new persistent volumes are created.     Note: If you enable persistent storage, and also select the Enable storage class for Apache Kafka check box, but do not provide the name of the storage class to use, the deployment will use the default storage class set.     If you do not select the Enable storage class for Apache Kafka check box, and do not provide the name of the storage class to use, the deployment will use any persistent volume claims that have at least the set size value.   To enable persistent storage for ZooKeeper:   Go to the ZooKeeper settings section.  Select the Enable persistent storage for ZooKeeper servers check box.      To use a specific storage class, select the Enable storage class for ZooKeeper servers check box, and provide the name of the storage class to use for the persistent volume claims intended for ZooKeeper. Also, provide a prefix to use for the persistent volume claims, and set the minimum size required for your intended usage.     If present, existing persistent volumes with the specified storage class are used after installation, or if a provisioner is configured for the specified storage class, new persistent volumes are created.     Note: If you enable persistent storage, and also select the Enable storage class for ZooKeeper servers check box, but do not provide the name of the storage class to use, the deployment will use the default storage class set.     If you do not select the Enable storage class for ZooKeeper servers check box, and do not provide the name of the storage class to use, the deployment will use any persistent volume claims that have at least the set size value.   To enable persistent storage for schemas:   Go to the Schema Registry settings section.  Select the Enable persistent storage for Schema Registry API servers check box.  Select the access mode to use for the persistent volumes (ReadWriteMany or ReadWriteOncein).      To use a specific storage class, select the Enable storage class for Schema Registry API servers check box, and provide the name of the storage class to use for the persistent volume claims intended for schemas. Also, provide a prefix to use for the persistent volume claims, and set the minimum size required for your intended usage.     If present, existing persistent volumes with the specified storage class are used after installation, or if a provisioner is configured for the specified storage class, new persistent volumes are created.     Note: If you enable persistent storage, and also select the Enable storage class for Schema Registry API servers check box, but do not provide the name of the storage class to use, the deployment will use the default storage class set.     If you do not select the Enable storage class for Schema Registry API servers check box, and do not provide the name of the storage class to use, the deployment will use any persistent volume claims that have at least the set size value.   Important: If membership of a specific group is required to access the file system used for persistent volumes, ensure you specify in the File system group ID field the GID of the group that owns the file system. Enabling encryption between pods To enable TLS encryption for communication between Event Streams pods, set the Pod to pod encryption field of the Global install settings section to Enabled. By default, encryption between pods is disabled. Specifying a ConfigMap for Kafka configuration If you have a ConfigMap for Kafka configuration settings, you can provide it to your IBM Event Streams installation to use. Enter the name in the Cluster configuration ConfigMap field of the Kafka broker settings section. Important: The ConfigMap must be in the same namespace as where you intend to install the IBM Event Streams release. Installing into a multizone cluster Set the Number of zones field of the Global install settings section to match the number of clusters you want to install your zones into. Enter the number of zones that contain a Kafka broker to ensure there is at least one broker in each zone. If your cluster is zone aware, then the zones are automatically allocated during installation. If your cluster is not zone aware, specify the zone label values for each zone in the Zone labels field of the Global install settings section. The list must be the same length as the number of zones. These are the labels you added as part of preparing for a multizone installation. Add the labels as an array and using YAML syntax, for example: - es-zone-0   - es-zone-1   - es-zone-2 Important: If you are installing as a Team Administrator, ensure you clear the Generate cluster roles checkbox. Setting geo-replication nodes When installing IBM Event Streams as an instance intended for geo-replication, configure the number of geo-replication worker nodes in the Geo-replication settings section by setting the number of nodes required in the Geo-replicator workers field. Note: If you want to set up a cluster as a destination for geo-replication, ensure you set a minimum of 2 nodes for high availability reasons. Consider the number of geo-replication nodes to run on a destination cluster. You can also set up destination clusters and configure the number of geo-replication worker nodes for an existing installation later. Configuring external access By default, external Kafka client applications connect to the IBM Cloud Private master node directly without any configuration required. You simply leave the External hostname/IP address field of the External access settings section blank. If you want clients to connect through a different route such as a load balancer, use the field to specify the host name or IP address of the endpoint. Also ensure you configure security for your cluster by setting certificate details in the Secure connection settings section. By default, a self-signed certificate is created during installation and the Private key, TLS certificate, and CA certificate fields can be left blank. If you want to use an existing certificate, select provided under Certificate type, and provide these additional keys and certificate values as base 64-encoded strings. Alternatively, you can generate your own certificates. After installation, set up external access by checking the port number to use for external connections and ensuring the necessary certificates are configured within your client environment. Configuring external monitoring tools You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. To set this up, you need to:   Have a third-party monitoring tool set up to be used within your IBM Cloud Private cluster.  Enable access to the broker JMX port by selecting the Enable secure JMX connections check box in the Kafka broker settings section.  Provide any configuration settings required by your monitoring tool to be applied to Event Streams. For example, Datadog requires you to deploy an agent on your IBM Cloud Private system that requires configuration settings to work with Event Streams.  Configure your applications to connect to a secure JMX port.Configuration reference Configure your Event Streams installation by setting the following parameters as needed. Global install settings The following table describes the parameters for setting global installation options.             Field      Description      Default                  Docker image registry      Docker images are fetched from this registry. The format is &lt;cluster_name&gt;:&lt;port&gt;/&lt;namespace&gt;.      ibmcom              Image pull secret      If using a registry that requires authentication, the name of the secret containing credentials.      None              Image pull policy      Controls when Docker images are fetched from the registry.      IfNotPresent              File system group ID      Specify the ID of the group that owns the file system intended to be used for persistent volumes. Volumes that support ownership management must be owned and writable by this group ID.      None              Architecture      The worker node architecture on which to deploy Event Streams.      amd64              Pod to pod encryption      Select whether you want to enable TLS encryption for communication between pods.      Disabled              Kubernetes internal DNS domain name      If you have changed the default DNS domain name from cluster.local in your Kubernetes installation, then this field must be set to the same value. You cannot change this value after installation.      cluster.local              Number of zones      The number of zones to deploy Event Streams across.      1              Zone labels      Array containing the labels for each zone. Add the labels as an array and using YAML syntax, for example:  - es-zone-0 - es-zone-1 - es-zone-2      None              Generate cluster roles      Select to generate a cluster role and cluster rolebinding with your Event Streams installation. Must be cleared if you are installing as a Team Administrator.      Selected (true)      IBM Cloud Private monitoring service The following table describes the options for monitoring service.             Field      Description      Default                  Export the Event Streams dashboards      Select to create Grafana dashboards in the IBM Cloud Private monitoring service to view information about Event Streams health, including Kafka health and performance details.      Not selected (false)      Insights - help us improve our product The following table describes the options for product improvement analytics.             Field      Description      Default                  Share my product usage data      Select to enable product usage analytics to be transmitted to IBM for business reporting and product usage understanding.      Not selected (false)      Note: The data gathered helps IBM understand how IBM Event Streams is used, and can help build knowledge about typical deployment scenarios and common user preferences. The aim is to improve the overall user experience, and the data could influence decisions about future enhancements. For example, information about the configuration options used the most often could help IBM provide better default values for making the installation process easier. The data is only used by IBM and is not shared outside of IBM.If you enable analytics, but want to opt out later, or want more information, contact us. Kafka broker settings The following table describes the options for configuring Kafka brokers.             Field      Description      Default                  CPU request for Kafka brokers      The minimum required CPU core for each Kafka broker. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      1000m              CPU limit for Kafka brokers      The maximum amount of CPU core allocated to each Kafka broker when the broker is heavily loaded. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      1000m              Memory request for Kafka brokers      The minimum amount of memory required for each Kafka broker in bytes. Specify integers with one of these suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Memory limit for Kafka brokers      The maximum amount of memory in bytes allocated to each Kafka broker when the broker is heavily loaded. Specify integers with suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Kafka brokers      Number of brokers in the Kafka cluster.      3              Cluster configuration ConfigMap      Provide the name of a ConfigMap containing Kafka configuration to apply changes to Kafka’s server.properties. See how to create a ConfigMap for your installation.      None              Enable secure JMX connections      Select to make each Kafka broker’s JMX port accessible to secure connections from applications running inside the IBM Cloud Private cluster. When access is enabled, you can configure your applications to connect to a secure JMX port and read Kafka metrics. Also, see External monitoring settings for application-specific configuration requirements.      Not selected (false)      Kafka persistent storage settings The following table describes the options for configuring persistent storage.             Field      Description      Default                  Enable persistent storage for Apache Kafka      Set whether to store Apache Kafka data on a persistent volume. Enabling storage ensures the data is preserved if the pod is stopped.      Not selected (false)              Enable storage class for Apache Kafka      Set whether to use a specific storage class when provisioning persistent volumes for Apache Kafka.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for the Apache Kafka brokers.      datadir              Storage class name      Name of the storage class to use for the persistent volume claims intended for Apache Kafka.      None              Size      Size to use for the Persistent Volume Claims created for Kafka nodes.      4Gi      ZooKeeper settings The following table describes the options for configuring ZooKeeper.             Field      Description      Default                  CPU request for ZooKeeper servers      The minimum required CPU core for each ZooKeeeper server. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      100m              CPU limit for ZooKeeper servers      The maximum amount of CPU core allocated to each ZooKeeper server when the server is heavily loaded. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      100m              Enable persistent storage for ZooKeeper servers      Set whether to store Apache ZooKeeper data on a persistent volume. Enabling storage ensures the data is preserved if the pod is stopped.      Not selected (false)              Enable storage class for ZooKeeper servers      Set whether to use a specific storage class when provisioning persistent volumes for Apache ZooKeeper.      Not selected (false)              Name      Prefix for the name of the Persistent Volume Claims used for Apache ZooKeeper.      datadir              Storage class name      Name of the storage class to use for the persistent volume claims intended for Apache ZooKeeper.      None              Size      Size to use for the Persistent Volume Claims created for Apache ZooKeeper.      2Gi      External access settings The following table describes the options for configuring external access to Kafka.             Field      Description      Default                  External hostname/IP address      The external hostname or IP address to be used by external clients. Leave blank to default to the IP address of the cluster master node.      None      Secure connection settings The following table describes the options for configuring secure connections.             Field      Description      Default                  Certificate type      Select whether you want to have a self-signed certificate generated during installation, or if you will provide your own certificate details.      selfsigned              Secret containing provided TLS certificates      If you set Certificate type to secret, enter the name of the secret that contains the certificates to use.      None              Private key      If you set Certificate type to provided, this is the base64-encoded TLS key or private key. If set to secret, this is the key name in the secret (default key name is “key”).      None              Public certificate      If you set Certificate type to provided, this is the base64-encoded public certificate. If set to secret, this is the key name in the secret (default key name is “cert”).      None              CA certificate      If you set Certificate type to provided, this is the base64-encoded Certificate Authority Root Certificate. If set to secret, this is the key name in the secret (default key name is “cacert”).      None      Important: If you provide your own certificates, ensure that the public certificate contains the required information for client applications to connect to Kafka as follows:   Depending on how your clients are connecting, ensure the certificate contains either the IP address or the Subject Alternative Name (SAN) for the host that the client applications will use.  The certificate must be valid for the value specified in the External hostname/IP address field of the External access settings section.Message indexing settings The following table describes the options for configuring message indexing.             Field      Description      Default                  Enable message indexing      Set whether to enable message indexing to enhance browsing the messages on topics.      Selected (true)              CPU request for Elastic Search nodes      The minimum required CPU core for each Elastic Search node. Specify integers, fractions (for example, 0.5), or millicore values (for example, 100m, where 100m is equivalent to .1 core).      500m              CPU limit for Elastic Search nodes      The maximum amount of CPU core allocated to each Elastic Search node. Specify integers, fractions (for example, 0.5), or millicores values (for example, 100m, where 100m is equivalent to .1 core).      1000m              Memory request for Elastic Search nodes      The minimum amount of memory required for each Elastic Search node in bytes. Specify integers with one of these suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      2Gi              Memory limits for Elastic Search nodes      The maximum amount of memory allocated to each Elastic Search node in bytes. Specify integers with suffixes: E, P, T, G, M, K, or power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki.      4Gi      Geo-replication settings The following table describes the options for configuring geo-replicating topics between clusters.             Field      Description      Default                  Geo-replicator workers      Number of workers to support geo-replication.      0      Schema Registry settings             Field      Description      Default                  Enable persistent storage for Schema Registry API servers      Set whether to store Schema Registry data on a persistent volume. Enabling storage ensures the schema data is preserved if the pod is stopped.      Not selected (false)              Enable storage class for Schema Registry API servers      Set whether to use a specific storage class when provisioning persistent volumes for schemas.      Not selected (false)              Storage Mode      Select the access mode to use for the persistent volumes (ReadWriteMany or ReadWriteOncein).      ReadWriteMany              Name      Prefix for the name of the Persistent Volume Claims used for schemas.      datadir              Storage class name      Name of the storage class to use for the persistent volume claims intended for schemas.      None              Size      Size to use for the Persistent Volume Claims created for schemas.      100Mi      External monitoring The following table describes the options for configuring external monitoring tools.             Field      Description      Default                  Datadog - Autodiscovery annotation check templates for Kafka brokers      YAML object that contains the Datadog Autodiscovery annotations for configuring the Kafka JMX checks. The Datadog prefix and container identifier is applied automatically to the annotation, so only use the template name as the object’s keys (for example, check_names). For more information about setting up monitoring with Datadog, see the Datadog tutorial.      None       REST Producer API settings The following table describes the options for configuring configuration values for the REST producer API.             Field      Description      Default                  Maximum key size      Set the maximum event key size that the REST producer API will accept in bytes.      4096              Maximum message size      Set the maximum event message size that the REST producer API will accept in bytes.      65536      Important: Do not set the Maximum message size to a higher value than the maximum message size that can be received by the Kafka broker or the individual topic (max.message.bytes). By default, the maximum message size for Kafka brokers is 1000012 bytes. If the limit is set for an individual topic, then that setting overrides the broker setting. Any message larger than the maximum limit will be rejected by Kafka. Note: Sending large requests to the REST producer increases latency, as it will take the REST producer longer to process the requests. Generating your own certificates You can create your own certificates for configuring external access. When prompted, answer all questions with the appropriate information.   Create the certificate to use for the Certificate Authority (CA):openssl req -newkey rsa:2048 -nodes -keyout ca.key -x509 -days 365 -out ca.pem  Generate a RSA 2048-bit private key:  openssl genrsa -out es.key 2048  Other key lengths and algorithms are also supported. The following cipher suites are supported, using TLS 1.2 and later only:          TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256      TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384        Note: The string “TLS” is interchangeable with “SSL” and vice versa. For example, where TLS_RSA_WITH_AES_128_GCM_SHA256 is specified, SSL_RSA_WITH_AES_128_GCM_SHA256 also applies. For more information about each cipher suite, go to the  Internet Assigned Numbers Authority (IANA) site, and search for the selected cipher suite ID.     Create a certificate signing request for the key generated in the previous step:openssl req -new -key es.key -out es.csr  Sign the request with the CA certificate created in step 1:openssl x509 -req -in es.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out es.pem  Encode your generated file to a base64 string. This can be done using command line tools such as base64, for example, to encode the file created in step 1:cat ca.pem | base64 &gt; ca.b64Completing these steps creates the following files which, after being encoded to a base64 string, can be used to configure your installation:   ca.pem : CA public certificate  es.pem : Release public certificate  es.key : Release private key","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/configuring/",
        "teaser":null},{
        "title": "Post-installation tasks",
        "collection": "2019.4",
        "excerpt":"Consider the following tasks after installing IBM Event Streams. Verifying your installation To verify that your Event Streams installation deployed successfully, check the status of your release as follows.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate your installation in the NAME column, and ensure the STATUS column for that row states Deployed.  Optional: Click the name of your installation to check further details of your Event Streams installation. For example, you can check the ConfigMaps used, or check the logs for your pods.  Log in to your IBM Event Streams UI to get started.Installing the command-line interface (CLI) The Event Streams CLI is a plugin for the IBM Cloud Private CLI. Use the CLI to manage your Event Streams instance from the command line, such as creating, deleting, and updating topics. To install the Event Streams CLI:   Ensure you have the IBM Cloud Private CLI installed.  Log in to the Event Streams as an administrator.  Click Toolbox in the primary navigation.  Go to the IBM Event Streams command-line interface section and click Find out more.  Download the Event Streams CLI plug-in for your system by using the appropriate link.  Install the plugin using the following command:cloudctl plugin install &lt;full_path&gt;/es-pluginTo start the Event Streams CLI and check all available command options in the CLI, use the cloudctl es command. To get help on each command, use the --help option. To use the Event Streams CLI against a deployed IBM Cloud Private cluster, run the following commands, replacing &lt;master_ip_address&gt; with your master node IP address, &lt;master_port_number&gt; with the master node port number, and &lt;my_cluster&gt; with your cluster name: cloudctl login -a https://&lt;master_ip_address&gt;:&lt;master_port_number&gt; -c &lt;my_cluster&gt;cloudctl es initFirewall and load balancer settings Consider the following guidance about firewall and load balancer settings for your deployment. Using OpenShift Container Platform If installed on the OpenShift Container Platform, Event Streams uses OpenShift routes. Ensure your OpenShift router is set up as required. Using IBM Cloud Private If you installed Event Streams on IBM Cloud Private, see the following guidance. In your firewall settings, ensure you enable communication for the node ports that Event Streams services use. If you are using an external load balancer for your master or proxy nodes in a high availability environment, ensure that the external ports are forwarded to the appropriate master and proxy nodes. To find the node ports to expose by using the UI:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your Event Streams installation in the NAME column, and click the name.  Scroll down to the Service table. The table lists information about your Event Streams services.      In the Service table, look for NodePort in the TYPE column.In each row that has NodePort as type, look in the PORT(S) column to find the port numbers you need to ensure are open to communication.The port numbers are paired as &lt;internal_number:external_number&gt;, where you need the second (external) numbers to be open (for example, 30314 in 32000:30314).The following image provides an example of the table:     For your firewall settings, ensure the external ports are open. For example, in the previous screen capture, it is the second number for the highlighted NodePort rows in the table: 32292, 31212, 30429, 32618, 30477, 30015, and 30594.     For your load balancer settings, you need to expose the following ports:           For the CLI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-rest-proxy-external-svc row. In the previous example, the ports are the second number in the PORT(S) column of the es1-ibm-es-rest-proxy-external-svc  row: 30477 and 30015.      For the UI, ensure you forward the external port to both the master and the proxy nodes. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-ui-svc row. In the previous example, the port is the second number in the PORT(S) column of the es1-ibm-es-ui-svc row: 30594.      For Kafka, ensure you forward the external port to the proxy node. This is the second port listed in the &lt;release_name&gt;-&lt;namespace&gt;-proxy-svc row. In the previous example, the ports are the second numbers in the PORT(S) column of the es1-ibm-es-proxy-svc row: 32292, 31212, 30429, and 32618.      To find the node ports to expose by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to list information about your Event Streams services:kubectl get services -n &lt;namespace&gt;The following is an example of the output (this is the same result as shown in the previous UI screen capture example):    NAME                                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                           AGEes1-ibm-es-access-controller-svc             ClusterIP   None           &lt;none&gt;        8443/TCP                                                          92mes1-ibm-es-collector-svc                     ClusterIP   None           &lt;none&gt;        7888/TCP                                                          92mes1-ibm-es-elastic-svc                       ClusterIP   None           &lt;none&gt;        9200/TCP,9300/TCP                                                 92mes1-ibm-es-indexmgr-svc                      ClusterIP   None           &lt;none&gt;        9080/TCP,8080/TCP                                                 92mes1-ibm-es-kafka-external-headless-svc       ClusterIP   None           &lt;none&gt;        9092/TCP,8093/TCP,8084/TCP,8085/TCP,8081/TCP,7070/TCP             92mes1-ibm-es-kafka-headless-svc                ClusterIP   None           &lt;none&gt;        9092/TCP,8093/TCP,8084/TCP,8085/TCP,8081/TCP,7070/TCP             92mes1-ibm-es-proxy-svc                         NodePort    10.0.173.129   &lt;none&gt;        30000:32292/TCP,30001:31212/TCP,30051:30429/TCP,30101:32618/TCP   92mes1-ibm-es-replicator-svc                    ClusterIP   None           &lt;none&gt;        8083/TCP                                                          92mes1-ibm-es-rest-producer-svc                 ClusterIP   10.0.2.210     &lt;none&gt;        8080/TCP                                                          92mes1-ibm-es-rest-proxy-external-svc           NodePort    10.0.105.97    &lt;none&gt;        32000:30477/TCP,32001:30015/TCP                                   92mes1-ibm-es-rest-proxy-internal-svc           ClusterIP   10.0.94.28     &lt;none&gt;        9443/TCP                                                          92mes1-ibm-es-rest-svc                          ClusterIP   10.0.76.77     &lt;none&gt;        9443/TCP                                                          92mes1-ibm-es-schemaregistry-svc                ClusterIP   10.0.13.231    &lt;none&gt;        3000/TCP                                                          92mes1-ibm-es-ui-svc                            NodePort    10.0.34.109    &lt;none&gt;        3000:30594/TCP                                                    92mes1-ibm-es-zookeeper-external-headless-svc   ClusterIP   None           &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        92mes1-ibm-es-zookeeper-headless-svc            ClusterIP   None           &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP                                        92m      Connecting clients You can set up external client access during installation. After installation, clients can connect to the Kafka cluster by using the externally visible IP address for the Kubernetes cluster. The port number for the connection is allocated automatically and varies between installations. To look up this port number after the installation is complete:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  In the NAME column, locate and click the release name used during installation.  Scroll down through the sections and locate the Service section.  In the NAME column, locate and click the &lt;releasename&gt;-ibm-es-proxy-svc NodePort entry.  In the Type column, locate the list of Node port links.  Locate the top entry in the list named bootstrap &lt;bootstrap port&gt;/TCP.  If no external hostname was specified when Event Streams was installed, this is the IP address and port number that external clients should connect to.  If an external hostname was specified when Event Streams was installed, clients should connect to that external hostname using this bootstrap port number.Before connecting a client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or export the self-signed public certificate from the browser. To export the self-signed public certificate from the browser:   Log in to the IBM Event Streams UI.  Click Connect to this cluster on the right.  On the Resources tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.Setting up access Secure your installation by managing the access your users and applications have to your Event Streams resources. For example, associate your IBM Cloud Private teams with your Event Streams instance to grant access to resources based on roles. Scaling Depending on the size of the environment that you are installing, consider scaling and sizing options. You might also need to change scale and size settings for your services over time. For example, you might need to add additional Kafka brokers over time. See how to scale your environment Considerations for GDPR readiness Consider the requirements for GDPR, including encrypting your data for protecting it from loss or unauthorized access. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/post-installation/",
        "teaser":null},{
        "title": "Migrating from Community Edition",
        "collection": "2019.4",
        "excerpt":"You can migrate from the IBM Event Streams Community Edition to IBM Event Streams. Note: IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages. Migrating involves removing your previous Community Edition installation and installing IBM Event Streams in the same namespace and using the same release name. Using this procedure,your settings and data are also migrated to the new installation if you had persistent volumes enabled previously.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Delete the Community Edition installation, making a note of the namespace and release name:helm delete --purge &lt;release_name&gt;This command does not delete the PersistentVolumeClaim (PVC) instances. Your PVCs are reused in your new IBM Event Streams installation with the same release name.  Install IBM Event Streams in the same namespace and using the same release name as used for your previous  Community Edition installation. Ensure you select the ibm-eventstreams-prod chart, and apart from the namespace and release name, also ensure you retain the same configuration settings you used for your previous installation, such as persistent volume settings.IBM Event Streams is installed with the configuration settings and data migrated from the Community Edition to your new IBM Event Streams installation. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/migrating/",
        "teaser":null},{
        "title": "Migrating from open-source Apache Kafka to Event Streams",
        "collection": "2019.4",
        "excerpt":"If you are using open-source Apache Kafka as your event-streaming platform, you can move to IBM Event Streams and benefit from its features and enterprise-level support. The main difference is that Event Streams requires connections to be secured. Prerequisites Ensure you have an Event Streams deployment available. See the instructions for installing on IBM Cloud Private, or the instructions for installing on OpenShift Container Platform. For many of the tasks, you can use the Kafka console tools. Many of the console tools work with Event Streams, as described in the using console tools topic. Re-create your topics in Event Streams In Event Streams, re-create your existing topics from your open-source Kafka cluster. To list your existing topics and configurations, run the following Kafka console tool: ./kafka-topics.sh --bootstrap-server &lt;host&gt;:&lt;port&gt; --describe A list of all your topics is displayed. Re-create each topic in Event Streams by using the UI or the CLI. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.  Follow the instructions to create the topic. Ensure you set the same partition and replica settings, as well as any other non-default settings, as you have set for the existing topic in your open source Kafka instance.  Repeat for each topic you have in your open source Kafka instance.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;      Run the following command to initialize the Event Streams CLI on the destination cluster:     cloudctl es init         Run the following command to create a topic:     cloudctl es topic-create --name &lt;topic_name&gt; --partitions &lt;number_of_partitions&gt; --replication-factor &lt;number_of_replicas&gt;     Ensure you set the same partition and replica settings, as well as any other non-default settings, as you have set for the existing topic in your open source Kafka instance.     Repeat for each topic you have in your open-source Kafka cluster.Change producer configuration Change the configuration for applications that produce messages to your open-source Kafka cluster to connect to Event Streams instead as described in securing the connections. If you are using the Kafka console tools, see the instructions for the example console producer in using the console tools to change where the messages are produced to. Change consumer configuration Change the configuration for applications that consume messages from your open-source Kafka cluster to connect to Event Streams instead as described in securing the connections. If you are using the Kafka console tools, see the instructions for the example console consumer in using the console tools to change where messages are consumed from. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/moving-from-oss-kafka/",
        "teaser":null},{
        "title": "Uninstalling",
        "collection": "2019.4",
        "excerpt":"You can uninstall IBM Event Streams by using the UI or the CLI. Using the UI To delete the Event Streams installation by using the UI:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Delete in the corresponding row.  Optional: If you enabled persistence during installation, you also need to manually remove  PersistentVolumes and PersistentVolumeClaims.Using the CLI Delete the Event Streams installation by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command:helm delete --purge &lt;release_name&gt;  Optional: If you enabled persistence during installation, you also need to manually remove PersistentVolumeClaims and PersistentVolumes. Use the Kubernetes command line tool as follows:          To delete PersistentVolumeClaims:kubectl delete pvc &lt;PVC_name&gt; -n &lt;namespace&gt;      To delete PersistentVolumes:kubectl delete pv &lt;PV_name&gt;      Cleaning up after uninstallation The uninstallation process might leave behind artifacts that you have to clear manually. ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/uninstalling/",
        "teaser":null},{
        "title": "Upgrading",
        "collection": "2019.4",
        "excerpt":"Upgrade your installation to the latest version of IBM Event Streams as follows. Upgrade paths Complete the steps in the following sections to upgrade your Event Streams version to 2019.4.x. The following upgrade paths are available:   You can upgrade to Event Streams version 2019.4.4 directly from versions 2019.4.3, 2019.4.2, 2019.4.1 or 2019.2.1.  You can upgrade to Event Streams version 2019.4.3 directly from versions 2019.4.2, 2019.4.1 or 2019.2.1.  You can upgrade to Event Streams version 2019.4.2 directly from version 2019.4.1 or 2019.2.1.  You can upgrade to Event Streams version 2019.4.1 directly from version 2019.2.1.  If you have an earlier version than 2019.2.1, you must first upgrade your Event Streams version to 2019.2.1, before upgrading to 2019.4.x.Prerequisites   Ensure you have IBM Cloud Private version 3.2.1. If you are using the OpenShift Container Platform, ensure you have version 3.11 or later. See the prerequisites for supported container environments.  The minimum resource requirements have changed. Ensure you have the required resources available.  If upgrading to version 2019.4.2, 2019.4.3 or 2019.4.4, download the fix pack from Fix Central.  If upgrading to version 2019.4.1, download the package from IBM Passport Advantage by searching for “IBM Event Streams” and 2019.4.1. Download the images related to the part numbers for your platform.  Make the packages available to your IBM Cloud Private instance as described in steps 2 to 4 of the download task.  If you have encryption between pods enabled, then ensure you disable it before starting the upgrade. After the upgrade completes successfully, you can enable the encryption again.  If upgrading from version 2019.2.1, follow the post-upgrade tasks after the upgrade completes.Important: Event Streams only supports upgrading to a newer chart version. Do not select an earlier chart version when upgrading. If you want to revert to an earlier version of Event Streams, see the instructions for rolling back. Upgrading on IBM Cloud Private You can upgrade your Event Streams version by using the IBM Cloud Private UI or CLI. Important: Event Streams only supports upgrading to a newer chart version. Do not select an earlier chart version when upgrading. If you want to revert to an earlier version of Event Streams, see the instructions for rolling back. Using the UI   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Upgrade in the corresponding row.  Select the chart version to upgrade to from the Version drop-down list.  Ensure you have Using previous configured values set to Reuse Values.Note: Do not change any of the settings in the Parameters section. You can modify configuration settings after upgrade, for example, enable encryption between pods.  Click Upgrade.The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting.  If upgrading from version 2019.2.1, follow the post-upgrade tasks after the upgrade completes.Using the CLI   Ensure you have the latest Helm chart version available on your local file system.          You can retrieve the charts from the UI.      Alternatively, if you downloaded the archive from IBM Passport Advantage or fix pack from FixCentral, the chart file is included in the archive or fix pack. Extract the PPA archive or fix pack, and locate the chart file in the /charts directory, for example: ibm-eventstreams-prod-1.4.0.tgz            Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;     Important: You must have the Team Administrator or Cluster Administrator role to upgrade the chart.         Run the helm upgrade command as follows, referencing the Helm chart you want to upgrade to:helm upgrade &lt;release-name&gt; &lt;latest-chart-version&gt;     For example, to upgrade by using a chart downloaded in the PPA archive or fix pack:helm upgrade eventstreams1 /Users/admin/upgrade/ibm-eventstreams-prod-1.4.0.tgz     Note: Do not set any parameter value during the upgrade, for example, helm upgrade --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls. You can modify configuration settings after upgrade, for example, enable encryption between pods.     The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting.     If upgrading from version 2019.2.1, follow the post-upgrade tasks after the upgrade completes.Upgrading on OpenShift Container Platform If you are using the OpenShift Container Platform, you can only upgrade by using the command line.       Ensure you have the latest Helm chart version available on your local file system.     Note: The Event Streams chart name no longer includes rhel in the name. For example, the chart name is now ibm-eventstreams-prod (not ibm-eventstreams-rhel-prod).           You can retrieve the charts from the UI.      Alternatively, if you downloaded the archive from IBM Passport Advantage, the chart file is included in the archive. Extract the PPA archive, and locate the chart file in the /charts directory, for example: ibm-eventstreams-prod-1.4.0.tgz            Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;     Important: You must have the Team Administrator or Cluster Administrator role to upgrade the chart.         Delete the Kafka StatefulSet as follows:     kubectl delete statefulset &lt;release-name&gt;-ibm-es-kafka-sts --cascade=false -n &lt;namespace&gt;     The cascade=false flag deletes the StatefulSet, but leaves the Kafka pods running.         Run the helm upgrade command as follows, referencing the Helm chart you want to upgrade to:helm upgrade &lt;release-name&gt; &lt;latest-chart-version&gt;     For example, to upgrade by using a chart downloaded in the PPA archive:helm upgrade eventstreams1 /Users/admin/upgrade/ibm-eventstreams-prod-1.4.0.tgz     Note: Do not set any parameter value during the upgrade, for example, helm upgrade --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls. You can modify configuration settings after upgrade, for example, enable encryption between pods.     The upgrade process begins and restarts your pods. During the process, the UI shows pods as unavailable and restarting.         If upgrading from version 2019.2.1, follow the post-upgrade tasks after the upgrade completes.   Post-upgrade tasks Additional steps required after upgrading are described in the following sections. Set up access management If you have IBM Cloud Private teams set up for access management, you must associate the teams again with your IBM Event Streams instance after successfully completing the upgrade. To use your upgraded Event Streams instance with existing IBM Cloud Private teams, re-apply the security resources to any teams you have defined as follows:   Check the teams you use:          Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.      From the navigation menu, click Manage &gt; Identity &amp; Access &gt; Teams. Look for the teams you use with your Event Streams instance.        Ensure you have installed the latest version of the Event Streams CLI.  Run the following command for each team that references your instance of Event Streams:  cloudctl es iam-add-release-to-team --namespace &lt;namespace&gt; --helm-release &lt;helm-release&gt; --team &lt;team-name&gt;Update browser certificates If you trusted certificates in your browser for using the Event Streams UI, you might not be able to access the UI after upgrading. To resolve this issue, you must delete previous certificates and trust new ones. Check the browser help for instructions, the process for deleting and accepting certificates varies depending on the type of browser you have. Update cluster certificates Upgrading your Event Streams version also updates your internal certificates automatically. You can manually change your internal certificates later. Internal certificates can only be self-generated certificates, they cannot be provided. You can also update your external certificates manually after an upgrade, for example, if you want to change from using self-generated certificates to provided certificates. Note: If you update your cluster certificates you might also need to update the certificates of any connecting clients. Enable encryption between pods As mentioned in the prerequisites, encryption between pods must be disabled before upgrading Event Streams. If you had pod to pod encryption enabled before upgrading, enable it again. Switch to routes If you are using the OpenShift Container Platform and have upgraded to Event Streams version 2019.4.1, you can switch to using OpenShift routes. Routes allow access to your cluster through a host name instead of the host IP address and node port combination. OpenShift Container Platform routes are the standard way of accessing applications inside an OpenShift cluster, and provide the benefit of being pre-determined and understandable. Tip: If you installed Event Streams version 2019.4.1 without upgrading from a previous version, you will already be using routes. Important: Switching to routes will alter the Event Streams certificates, and will therefore require redistributing the certificates to all your clients and updating their connection settings to use the new bootstrap route for their bootstrap server. This means the switch over will cause your clients to disconnect, resulting in downtime. To switch to routes:       Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;     Important: You must have the Team Administrator or Cluster Administrator role to upgrade the chart.         Ensure you have routes available in your container platform by running the following command:     kubectl api-resources | grep routes     If the command returns routes route.openshift.io true Route, then you have OpenShift routes available.     Ensure you have the latest Helm chart version available on your local file system.      Run the following command to configure your Event Streams installation to use routes:     helm upgrade --reuse-values --set global.security.externalCertificateLabel=upgraded --set proxy.upgradeToRoutes=true &lt;release-name&gt; &lt;latest-chart-version&gt;         After the upgrade completes successfully, and all pods are ready, run the following command to update your installation with the new routes.     NAMESPACE=&lt;namespace&gt;; kubectl patch $(kubectl get cm -n $NAMESPACE -l component=proxy -o name) -n $NAMESPACE -p \"{\\\"data\\\":{\\\"externalListeners\\\":\\\"\\\",\\\"revision\\\":\\\"$(date +%s999999999)\\\"}}\";         The Event Streams UI address changes to a route, which can be retrieved by using the following command:     echo https://$(kubectl get route -n &lt;namespace&gt; &lt;release-name&gt;-ibm-es-ui-route -o 'jsonpath={.spec.host}')         The Event Streams schema registry uses the REST API. Configuring Event Streams to use routes changes this endpoint value to a route, so ensure you update the clients that use schemas to use the route name. To retrieve the REST route name, run the following command:     echo https://$(kubectl get route -n &lt;namespace&gt; &lt;releasename&gt;-ibm-es-rest-route -o 'jsonpath={.spec.host}')     Update the connection settings for your clients that interact with Event Streams.This will require your clients to disconnect, and connect again after the changes are made, resulting in downtime.","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/upgrading/",
        "teaser":null},{
        "title": "Rolling back",
        "collection": "2019.4",
        "excerpt":"You can revert to an earlier version of Event Streams under certain conditions. Prerequisites Rolling back your Event Streams 2019.4.1 installation to an earlier version is only supported in the following cases:   You can only roll back from a newer Helm chart version to an older chart version.  You can only roll back to Event Streams 2019.2.1 (Helm chart version 1.3.0). Rolling back to earlier chart versions is not supported.  Rolling back to an earlier Event Streams version is only supported if you have not modified your settings after deployment.Rolling back Using the UI   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  Click Workloads &gt; Helm Releases from the navigation menu.  Locate the release name of your installation in the Name column, and click  More options &gt; Rollback in the corresponding row.  Select the chart version to roll back to (1.3.0).  Click Rollback.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;Important: You must have the Cluster Administrator role to roll back a chart version.  Run the helm history command to view previous versions you can roll back to:helm history &lt;release-name&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation.For example:    $ helm history event-streamsREVISION        UPDATED                         STATUS          CHART                           DESCRIPTION1               Mon Dec 17 14:27:12 2018        SUPERSEDED      ibm-eventstreams-prod-1.1.0     Install complete2               Wed Apr 10 16:49:29 2018        SUPERSEDED      ibm-eventstreams-prod-1.2.0     Upgrade complete3               Mon Jul 15 12:16:34 2019        SUPERSEDED      ibm-eventstreams-prod-1.3.0     Upgrade complete4               Wed Oct 23 16:16:34 2019        DEPLOYED        ibm-eventstreams-prod-1.4.0     Upgrade complete        Run the helm rollback command as follows:helm rollback &lt;release-name&gt; &lt;revision&gt;Where &lt;release-name&gt; is the name that identifies your Event Streams installation, and &lt;revision&gt; is a number from the REVISION column that corresponds to the version you want to revert to, as displayed in the result of the helm history command.For example:helm rollback event-streams 3Post-rollback tasks Rolling back to version 2019.2.1 deletes the restProxyExternalPort value from the release ConfigMap, which means you will not be able to access the UI or use  the schema registry feature. Use the following kubectl patch command to fix this issue.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Retrieve the restProxyExternalPort value as follows:kubectl get svc $(kubectl get svc -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}' | grep rest-proxy-external) -o jsonpath='{range .spec.ports[?(@.name==\"admin-rest-https\")]}{.nodePort}{\"\\n\"}{end}'      Run the following command:kubectl patch configmap &lt;release-name&gt;-ibm-es-release-cm -n &lt;namespace&gt; --type='json' -p='[{\"op\": \"add\", \"path\": \"/data/restProxyExternalPort\", \"value\": \"&lt;restProxyExternalPort&gt;\"}]'     Where:           &lt;release-name&gt; is the name that identifies your Event Streams installation.      &lt;namespace&gt; is the location of your installation.      &lt;restProxyExternalPort&gt; is the port you retrieved in the previous step.      ","categories": ["installing"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/installing/rolling-back/",
        "teaser":null},{
        "title": "Logging in",
        "collection": "2019.4",
        "excerpt":"Log in to your IBM Event Streams UI from a supported web browser. Determining the URL depends on your platform. Using OpenShift Container Platform If installed on the OpenShift Container Platform, Event Streams uses OpenShift routes. To retrieve the URL for your Event Streams UI, use the following command:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;      Run the following command:     kubectl get routes -n &lt;namespace&gt; -l component=ui     The following is an example output, and you use the value from the HOST/PORT column to log in to your UI in a web browser:     NAME                              HOST/PORT                                                         PATH  SERVICES                       PORT             TERMINATION        WILDCARDmy-eventstreams-ibm-es-ui-route   my-eventstreams-ibm-es-ui-route-es.apps.my-cluster.my-domain.com        my-eventstreams-ibm-es-ui-svc  admin-ui-https   passthrough/None   None      Add https:// in front of the HOST/PORT value when entering it in the web browser, in this example: https://my-eventstreams-ibm-es-ui-route-es.apps.my-cluster.my-domain.com Using IBM Cloud Private If you installed Event Streams on IBM Cloud Private, see the following guidance.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your IBM Event Streams installation in the NAME column.  Expand the Launch link in the row and click admin-ui-https.The IBM Event Streams log in page is displayed.Note: You can also determine the IBM Event Streams UI URL by using the CLI. Click the release name and scroll to the Notes section at the bottom of the page and follow the instructions. You can then use the URL to log in.  Use your IBM Cloud Private administrator user name and password to access the UI. Use the same username and password as you use to log in to IBM Cloud Private.Logging out Logging out of Event Streams does not log you out of your session entirely. To log out, you must first log out of your IBM Cloud Private session, and then log out of your Event Streams session. To log out of Event Streams:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  Click the user icon in the upper-right corner of the window, and click Log out.  Return to your Event Streams UI and click the user icon in the upper-right corner of the window, and click Log out.","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/getting-started/logging-in/",
        "teaser":null},{
        "title": "Generating a starter application",
        "collection": "2019.4",
        "excerpt":"To learn more about how you can create applications that can take advantage of IBM Event Streams capabilities, generate a starter application. The starter application can produce and consume messages, and you can specify the topic where you want to send messages to. About the application The starter application provides a demonstration of a Java application running on WebSphere Liberty that sends events, receives events, or both, by using IBM Event Streams. Project contents The Java classes in the application.kafka package are designed to be independent of the specific use case for Kafka. The application.kafka sample code is used by the application.demo package and can also be used to understand the elements required to create your own Kafka application. The src/main/java/application/demo folder contains the framework for running the sample in a user interface, providing an easy way to view message propagation. Security The starter application is generated with the correct security configurations to connect to IBM Event Streams. These security configurations include a .jks file for the certificate and API keys for producing and consuming messages. For more information, see the  instructions for connecting clients. Note: The API keys generated for the starter application can only be used to connect to the topic selected during generation. In addition, the consumer API key can only be used to connect with a consumer group ID set to the name of the generated application. Generating and running To generate the application:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click the Send and receive messages with a sample application tile, or click Toolbox in the primary navigation, go to the Generate starter application section, and click Find out more.  Click Configure and generate application.  Provide a name for the application.  Decide whether you want the application to produce or consume messages, or both.  Specify a target topic for the messages from the application.  Click Generate starter application. The application is created.  Download the compressed file and extract the file to a preferred location.  Navigate to the extracted file, and run the following command to build and deploy the application:   mvn install liberty:run-server  Access the successfully deployed sample application using the following URL: http://localhost:9080/Note: Some of these options depend on your access permissions. If you are not permitted to create topics, you will not be able to create a topic as part of building the starter application. If you are not permitted to write to topics, you will not be able to create a starter application that produces messages, only consumes them. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/getting-started/generating-starter-app/",
        "teaser":null},{
        "title": "Creating and testing message loads ",
        "collection": "2019.4",
        "excerpt":"IBM Event Streams provides a high-throughput producer application you can use as a workload generator to test message loads and help validate the performance capabilities of your cluster. You can use one of the predefined load sizes, or you can specify your own settings to test throughput. Then use the test results to ensure your cluster setup is appropriate for your requirements, or make changes as needed, for example, by changing your scaling settings. Downloading You can download the latest pre-built producer application. Alternatively, you can clone the project from GitHub. However, if you clone from GitHub, you have to build the producer. Building If you cloned the Git repository, build the producer as follows:   Ensure you have Java version 8. Also, ensure you have Apache Maven installed on your system.  Ensure you have cloned the Git project.  Open a terminal and change to the root directory.  Run the following command: mvn install.You can also specify your root directory using the -f option as follows mvn install -f &lt;path_to&gt;/pom.xml  The es-producer.jar file is created in the /target directory.Configuring The producer application requires configuration settings that you can set in the provided producer.config template configuration file. Note: The producer.config file is located in the root directory. If you downloaded the pre-built producer, you have to run the es-producer.jar with the -g option to generate the configuration file. If you build the producer application yourself, the configuration file is created and placed in the root for you when building. Before running the producer to test loads, you must specify the following details in the configuration file.             Attribute      Description                         bootstrap.servers      The URL used for bootstrapping knowledge about the rest of the cluster. You can find this address in the Event Streams UI as described later.                     ssl.truststore.location      The location of the JKS keystore used to securley communicate with your IBM Event Streams instance. You can downloaded the JKS keystore file from the Event Streams UI as described later.                     sasl.jaas.config      Set to org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;password&gt;\";, where &lt;password&gt; is replaced by an API key. This is needed to authorize production to your topic. To generate API keys, go to the Event Streams UI as described later.             Obtaining configuration details Obtain the required configuration details as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Locate the details:          For the bootstrap.servers, copy the address from the Bootstrap server section.      To generate API keys, go to the API key section, click Generate API key, and follow the instructions.      To downloaded the JKS keystore file, go to the Certificates section, and download the server certificate from the Java truststore section. Set the ssl.truststore.location to the full path and name of the downloaded file.      You can secure access to your topics as described in managing access. Running Create a load on your IBM Event Streams Kafka cluster by running the es-producer.jar command. You can specify the load size based on the provided predefined values, or you can provide specific values for throughput and total messages to determine a custom load. Using predefined loads To use a predefined load size from the producer application, use the es-producer.jar with the -s option: java -jar target/es-producer.jar -t &lt;topic-name&gt; -s &lt;small/medium/large&gt; For example, to create a large message load based on the predefined large load size, run the command as follows: java -jar target/es-producer.jar -t myTopic -s large This example creates a large message load, where the producer attempts to send a total of 6,000,000 messages at a rate of 100,000 messages per second to the topic called myTopic. The following table lists the predefined load sizes the producer application provides.             Size      Messages per second      Total messages                  small      1000      60,000              medium      10,000      600,000              large      100,000      6,000,000      Using user-defined loads You can generate a custom message load using your own settings. For example, to test the load to the topic called myTopic with custom settings that create a total load of 60,000 messages with a size of 1024 bytes each, at a maximum throughput rate of 1000 messages per second, use the es-producer.jar command as follows: java -jar target/es-producer.jar -t myTopic -T 1000 -n 60000 -r 1024 The following table lists all the parameter options for the es-producer.jar command.             Parameter      Shorthand      Longhand      Type      Description      Default                  Topic      -t      –topic      string      The name of the topic to send the produced message load to.      loadtest              Num Records      -n      –num-records      int      The total number of messages to be sent as part of the load. Note: The --size option overrides this value if used together.      60000              Payload File      -f      –payload-file      string      File to read the message payloads from. This works only for UTF-8 encoded text files. Payloads are read from this  file and a payload is randomly selected when sending messages.                     Payload Delimiter      -d      –payload-delimiter      string      Provides delimiter to be used when --payload-file is provided. This parameter is ignored if --payload-file is not provided.      \\n              Throughput      -T      –throughput      int      Throttle maximum message throughput to approximately THROUGHPUT messages per second. -1 sets it to as fast as possible. Note: The --size option overrides this value if used together.      -1              Producer Config      -c      –producer-config      string      Path to the producer configuration file.      producer.config              Print Metrics      -m      –print-metrics      bool      Set whether to print out metrics at the end of the test.      false              Num Threads      -x      –num-threads      int      The number of producer threads to run.      1              Size      -s      –size      string      Pre-defined combinations of message throughput and volume. If used, this option overrides any settings specified by the --num-records and --throughput options.                     Record Size      -r      –record-size      int      The size of each message to be sent in bytes.      100              Help      -h      –help      N/A      Lists the available parameters.                     Gen Config      -g      –gen-config      N/A      Generates the configuration file required to run the tool (producer.config).             Note: You can override the parameter values by using the environment variables listed in the following table. This is useful, for example, when using containerization, and you are unable to specify parameters on the command line.             Parameter      Environment Variable                  Throughput      ES_THROUGHPUT              Num Records      ES_NUM_RECORDS              Size      ES_SIZE              Record Size      ES_RECORD_SIZE              Topic      ES_TOPIC              Num threads      ES_NUM_THREADS              Producer Config      ES_PRODUCER_CONFIG              Payload File      ES_PAYLOAD_FILE              Payload Delimiter      ES_PAYLOAD_DELIMITER      Note: If you set the size using -s when running es-producer.jar, you can only override it if both the ES_NUM_RECORDS and ES_THROUGHPUT environment variables are set, or if ES_SIZE is set. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/getting-started/testing-loads/",
        "teaser":null},{
        "title": "Creating Kafka client applications",
        "collection": "2019.4",
        "excerpt":"The IBM Event Streams UI provides help with creating an Apache Kafka Java client application and discovering connection details for a specific topic. Creating an Apache Kafka Java client application You can create Apache Kafka Java client applications to use with IBM Event Streams. Download the JAR file from IBM Event Streams, and include it in your Java build and classpaths before compiling and running Kafka Java clients.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Apache Kafka Java client section and click Find out more.  Click the Apache Kafka Client JAR link to download the JAR file. The file contains the Java class files and related resources needed to compile and run client applications you intend to use with IBM Event Streams.  Download the JAR files for SLF4J required by the Kafka Java client for logging.  Include the downloaded JAR files in your Java build and classpaths before compiling and running your Apache Kafka Java client.  Ensure you set up security.Creating an Apache Kafka Java client application using Maven or Gradle If you are using Maven or Gradle to manage your project, you can use the following snippets to include the Kafka client JAR and dependent JARs on your classpath.   For Maven, use the following snippet in the &lt;dependencies&gt; section of your pom.xml file:     &lt;dependency&gt;     &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;     &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;     &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.slf4j&lt;/groupId&gt;     &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;     &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;        For Gradle, use the following snippet in the dependencies{} section of your build.gradle file:     implementation group: 'org.apache.kafka', name: 'kafka-clients', version: '2.2.1' implementation group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25' implementation group: 'org.slf4j', name: 'slf4j-simple', version: '1.7.25'        Ensure you set up security.Securing the connection You must secure the connection from your client applications to IBM Event Streams. To secure the connection, you must obtain the following:   A copy of the server-side public certificate added to your client-side trusted certificates.  An API key generated from the IBM Cloud Private UI.Before connecting an external client, ensure the necessary certificates are configured within your client environment. Use the TLS and CA certificates if you provided them during installation, or use the following instructions to retrieve a copy. Copy the server-side public certificate and generate an API key as follows:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  On the Resources tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate. If you are using a Java client, use the Java truststore. Otherwise, use the PEM certificate.  To generate API keys, go to the API key section, click Generate API key, and follow the instructions.Configuring your client Add the certificate details and the API key to your Kafka client application to set up a secure connection from your application to your Event Streams instance. For example, for Java: Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"&lt;broker_url&gt;\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");properties.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");Replace &lt;broker_url&gt; with your cluster’s broker URL, &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with \"password\", and &lt;api_key&gt; with the API key copied from its file. Note: You can copy the connection code snippet from the Event Streams UI with the broker URL already filled in for you. After logging in, click Connect to this cluster on the right, and click the Sample code tab. Copy the snippet from the Sample connection code section into your Kafka client application. ","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/getting-started/client/",
        "teaser":null},{
        "title": "Using Apache Kafka console tools",
        "collection": "2019.4",
        "excerpt":"Apache Kafka comes with a variety of console tools for simple administration and messaging operations. You can find these console tools in the bin directory of your Apache Kafka download. You can use many of them with IBM Event Streams, although IBM Event Streams does not permit connection to its ZooKeeper cluster. As Kafka has developed, many of the tools that previously required connection to ZooKeeper no longer have that requirement. IBM Event Streams has its own command-line interface (CLI) and this offers many of the same capabilities as the Kafka tools in a simpler form. The following table shows which Apache Kafka (release 2.0 or later) console tools work with IBM Event Streams and whether there are CLI equivalents.             Console tool      Works with IBM Event Streams      CLI equivalent                  kafka-acls.sh      No, see managing access                     kafka-broker-api-versions.sh      Yes                     kafka-configs.sh --entity-type topics      No, requires ZooKeeper access      cloudctl es topic-update              kafka-configs.sh --entity-type brokers      No, requires ZooKeeper access      cloudctl es broker-config              kafka-configs.sh --entity-type brokers --entity-default      No, requires ZooKeeper access      cloudctl es cluster-config              kafka-configs.sh --entity-type clients      No, requires ZooKeeper access      cloudctl es entity-config              kafka-configs.sh --entity-type users      No, requires ZooKeeper access      No              kafka-console-consumer.sh      Yes                     kafka-console-producer.sh      Yes                     kafka-consumer-groups.sh --list      Yes      cloudctl es groups              kafka-consumer-groups.sh --describe      Yes      cloudctl es group              kafka-consumer-groups.sh --reset-offsets      Yes      cloudctl es group-reset              kafka-consumer-groups.sh --delete      Yes      cloudctl es group-delete              kafka-consumer-perf-test.sh      Yes                     kafka-delete-records.sh      Yes      cloudctl es topic-delete-records              kafka-preferred-replica-election.sh      No                     kafka-producer-perf-test.sh      Yes                     kafka-streams-application-reset.sh      Yes                     kafka-topics.sh --list      Yes      cloudctl es topics              kafka-topics.sh --describe      Yes      cloudctl es topic              kafka-topics.sh --create      Yes      cloudctl es topic-create              kafka-topics.sh --delete      Yes      cloudctl es topic-delete              kafka-topics.sh --alter --config      Yes      cloudctl es topic-update              kafka-topics.sh --alter --partitions      Yes      cloudctl es topic-partitions-set              kafka-topics.sh --alter --replica-assignment      Yes      cloudctl es topic-partitions-set              kafka-verifiable-consumer.sh      Yes                     kafka-verifiable-producer.sh      Yes             Using the console tools with IBM Event Streams The console tools are Kafka client applications and connect in the same way as regular applications. Follow the instructions for securing a connection to obtain:   Your cluster’s broker URL  The truststore certificate  An API keyMany of these tools perform administrative tasks and will need to be authorized accordingly. Create a properties file based on the following example: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace:   &lt;certs.jks_file_location&gt; with the path to your truststore file  &lt;truststore_password&gt; with \"password\"  &lt;api_key&gt; with your API keyExample - console producer You can use the Kafka console producer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console producer in a terminal as follows: ./kafka-console-producer.sh --broker-list &lt;broker_url&gt; --topic &lt;topic_name&gt; --producer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to itExample - console consumer You can use the Kafka console consumer tool with IBM Event Streams. After you’ve created the properties file as described previously, you can run the console consumer in a terminal as follows: ./kafka-console-consumer.sh --bootstrap-server &lt;broker_url&gt; --topic &lt;topic_name&gt; --from-beginning --consumer.config &lt;properties_file&gt;Replace:   &lt;broker_url&gt; with your cluster’s broker URL  &lt;topic_name&gt; with the name of your topic  &lt;properties_file&gt; with the name of your properties file including full path to it","categories": ["getting-started"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/getting-started/using-kafka-console-tools/",
        "teaser":null},{
        "title": "Schemas overview",
        "collection": "2019.4",
        "excerpt":"Apache Kafka can handle any data, but it does not validate the information in the messages. However, efficient handling of data often requires that it includes specific information in a certain format. Using schemas, you can define the structure of the data in a message, ensuring that both producers and consumers use the correct structure. Schemas help producers create data that conforms to a predefined structure, defining the fields that need to be present together with the type of each field. This definition then helps consumers parse that data and interpret it correctly. Event Streams supports schemas and includes a schema registry for using and managing schemas. It is common for all of the messages on a topic to use the same schema. The key and value of a message can each be described by a schema.  Schema registry Schemas are stored in the Event Streams schema registry. In addition to storing a versioned history of schemas, it provides an interface for retrieving them. Each Event Streams cluster has its own schema registry. Your producers and consumers validate the data against the specified schema stored in the schema registry. This is in addition to going through Kafka brokers. The schemas do not need to be transferred in the messages this way, meaning the messages are smaller than without using a schema registry.  If you are migrating to use Event Streams as your Kafka solution, and have been using a schema registry from a different provider, you can migrate to using the Event Streams schema registry. Apache Avro data format Schemas are defined using Apache Avro, an open-source data serialization technology commonly used with Apache Kafka. It provides an efficient data encoding format, either by using the compact binary format or a more verbose, but human-readable JSON format. The Event Streams schema registry uses Apache Avro data formats. When messages are sent in the Avro format, they contain the data and the unique identifier for the schema used. The identifier specifies which schema in the registry is to be used for the message. Avro has support for a wide range of data types, including primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Learn more about how you can create schemas in Event Streams.  Serialization and deserialization A producing application uses a serializer to produce messages conforming to a specific schema. As mentioned earlier, the message contains the data in Avro format, together with the the schema identifier. A consuming application then uses a deserializer to consume messages that have been serialized using the same schema. When a consumer reads a message sent in Avro format, the deserializer finds the identifier of the schema in the message, and retrieves the schema from the schema registry to deserialize the data. This process provides an efficient way of ensuring that data in messages conform to the required structure. Serializers and deserializers that automatically retrieve the schemas from the schema registry as required are provided or generated by IBM Event Streams. If you need to use schemas in an environment for which serializers or deserializers are not provided, you can use the command line or UI directly to retrieve the schemas.  Versions and compatibility Whenever you add a schema, and any subsequent versions of the same schema, Event Streams validates the format automatically and warns of any issues. You can evolve your schemas over time to accommodate changing requirements. You simply create a new version of an existing schema, and the schema registry ensures that the new version is compatible with the existing version, meaning that producers and consumers using the existing version are not broken by the new version. When you create a new version of the schema, you simply add it to the registry and version it. You can then set your producers and consumers that use the schema to start using the new version. Until they do, both producers and consumers are warned that a new version of the schema is available.  Lifecycle When a new version is used, you can deprecate the previous version. Deprecating means that producing and consuming applications still using the deprecated version are warned that a new version is available to upgrade to. When you upgrade your producers to use the new version, you can disable the older version so it can no longer be used, or you can remove it entirely from the schema registry. You can use the Event Streams UI or CLI to manage the lifecycle of schemas, including registering, versioning, deprecating, and so on.  How to get started with schemas   Create schemas  Add schemas to schema registry  Set your Java or non-Java applications to use schemas  Manage schema lifecycle","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/overview/",
        "teaser":null},{
        "title": "Creating and adding schemas",
        "collection": "2019.4",
        "excerpt":"You can create schemas in Avro format. You can then use the Event Streams UI or CLI to add the schemas to the schema registry. Creating schemas Event Streams supports Apache Avro schemas. Avro schemas are written in JSON to define the format of the messages. For more information about Avro schemas, see the Avro documentation.  The Event Streams schema registry imports, stores, and uses Avro schemas to serialize and deserialize Kafka messages. The schema registry supports Avro schemas using the record complex type. The record type can include multiple fields of any data type, primitive or complex. Define your Avro schema files and save them by using the .avsc or .json file extension. For example, the following Avro schema defines a Book record in the org.example namespace, and contains the Title, Author, and Format fields with different data types: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },    ]}Adding schemas to the registry To use schemas in Kafka applications, import your schema definitions into the schema registry. Your applications can then retrieve the schemas from the registry as required. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation, and then click Add schema.  Click Upload definition and select your Avro schema file. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.   Optional: Edit the Schema name and Version fields.          The name of the record defined in the Avro schema file is added to the Schema name field. You can edit this field to add a different name for the schema. Changing the Schema name field does not update the Avro schema definition itself.      The value 1.0.0 is automatically added to the Version field as the initial version of the schema. You can edit this field to set a different version number for the schema.        Click Add schema. The schema is added to the list of schemas in the Event Streams schema registry.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to add a schema to the schema registry:cloudctl es schema-add --name &lt;schema-name&gt; --version &lt;schema-version&gt; --file &lt;path-to-schema-file&gt;Adding new schema versions The Event Streams schema registry can store multiple versions of the same schema. As your applications and environments evolve, your schemas need to change to accommodate the requirements. You can import, manage, and use different versions of a schema. As your schemas change, consider the options for managing their lifecycle. Note: A new version of a schema must be compatible with previous versions. This means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value. For example, the following Avro schema defines a new version of the Book record, adding a PageCount field. By including a default value for this field, messages that were serialized with the previous version of this schema (which would not have a PageCount value) can still be deserialized using this version. {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [        {\"name\": \"Title\", \"type\": \"string\"},        {\"name\": \"Author\",  \"type\": \"string\"},        {\"name\": \"Format\",         \"type\": {                    \"type\": \"enum\",                    \"name\": \"Booktype\",                    \"symbols\": [\"HARDBACK\", \"PAPERBACK\"]                 }        },        {\"name\": \"PageCount\",  \"type\": \"int\", \"default\": 0}    ]}Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation.  Locate your schema in the list of registered schemas and click its name. The list of versions for the schema is displayed.  Click Add new version to add a new version of the schema.  Click Upload definition and select the file that contains the new version of your schema. Avro schema files use the .avsc or .json file extensions.The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.  Set a value in the Version field to be the version number for this iteration of the schema. For the current list of all versions, click View all versions.  Click Add schema. The schema version is added to the list of all versions for the schema.Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the IBM Event Streams CLI on the cluster:cloudctl es init  Run the following command to list all schemas in the schema registry, and select the schema name you want to add a new version to:cloudctl es schemas  Run the following command to add a new version of the schema to the registry:cloudctl es schema-add --name &lt;schema-name-from-previous-step&gt; --version &lt;new-schema-version&gt; --file &lt;path-to-new-schema-file&gt;","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/creating/",
        "teaser":null},{
        "title": "Managing schema lifecycle",
        "collection": "2019.4",
        "excerpt":"You can have multiple versions of a schema stored in the Event Streams schema registry. Kafka producers and consumers retrieve the right schema version they use from the registry based on a unique identifier and version. When a new schema version is added, you can set both the producer and consumer applications to use that version. You then have the following options to handle earlier versions. The lifecycle is as follows:   Add schema  Add new schema version  Deprecate earlier versions or deprecate entire schema  Disable version or entire schema  Remove version or entire schemaDeprecating If you want your applications to use a new version of a schema, you can set the earlier version to Deprecated. When a version is deprecated, the applications using that version receive a message to warn them to stop using it. Applications can continue to use the schema, but warnings will be written to application logs about the schema version being deprecated. You can customize the message to be provided in the logs. Deprecated versions are still available in the registry and can be used again. Note: You can deprecate a entire schema, not just the versions of that schema. If the entire schema is set to deprecated, then all of its versions are reported as deprecated (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation.  Select the schema you want to deprecate from the list.  Set the entire schema or a selected version of the schema to be deprecated:          If you want to deprecate the entire schema and all its versions, click the Manage schema tab, and set Mark schema as deprecated to on.      To deprecate a specific version, select it from the list, and click the Manage version tab for that version. Then set Mark schema as deprecated to on.      Deprecated schemas and versions are marked with a Deprecated flag on the UI. You can re-activate a schema or its version by setting Mark schema as deprecated to off. Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to deprecate a schema version:cloudctl es schema-modify --deprecate --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To deprecate an entire schema, do not specify the --version &lt;schema-version&gt; option.     To re-activate a schema version:cloudctl es schema-modify --activate --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To re-activate an entire schema, do not specify the --version &lt;schema-version&gt; option.   Disabling If you want your applications to stop using a specific schema, you can set the schema version to Disabled. If you disable a version, applications will be prevented from producing and consuming messages using it. You can re-enable it again to allow applications to use the schema again. When a schema is disabled, applications that want to use the schema receive an ERROR. Java producers using the Event Streams schema registry serdes library will throw a SchemaDisabledException when attempting to producemessages using a disabled schema version. For example, the message and stack trace for a disabled schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaDisabledException: Schema \"Test_Schema\" version \"1.0.0\" is disabled.\tat com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:174)\tat com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:41)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:884)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:846)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:733)\tat Producer.main(Producer.java:92)Note: You can disable a entire schema, not just the versions of that schema. If the entire schema is disabled, then all of its versions are disabled as well, which means no version of the schema can be used by applications (including any new ones added). Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation.  Select the schema you want to disable from the list.  Set the entire schema or a selected version of the schema to be disabled:          If you want to disable the entire schema and all its versions, click the Manage schema tab, and click Disable schema, then click Disable.      To disable a specific version, select it from the list, and click the Manage version tab for that version. Then click Disable version, then click Disable.You can re-enable a schema by clicking Enable schema, and re-enable a schema version by clicking  Re-enable version.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to disable a schema version:cloudctl es schema-modify --disable --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To disable an entire schema, do not specify the --version &lt;schema-version&gt; option.     To re-enable a schema version:cloudctl es schema-modify --enable --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To re-enable an entire schema, do not specify the --version &lt;schema-version&gt; option.   Removing If a schema version has not been used for a period of time, you can remove it from the schema registry. Removing a schema version means it will be permanently deleted from the schema registry of your Event Streams instance, and applications will be prevented from producing and consuming messages using it. If a schema is no longer available in the registry, Java applications that want to use the schema receive a SchemaNotFoundException message. For example, the message and stack trace when producing a message with a missing schema named Test_Schema would look like this: com.ibm.eventstreams.serdes.exceptions.SchemaNotFoundException: Schema \"Test_Schema\" not found    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.handleErrorResponse(SchemaRegistryRestAPIClient.java:145)    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.get(SchemaRegistryRestAPIClient.java:120)    at com.ibm.eventstreams.serdes.SchemaRegistry.downloadSchema(SchemaRegistry.java:253)    at com.ibm.eventstreams.serdes.SchemaRegistry.getSchema(SchemaRegistry.java:239)Important: You cannot reverse the removal of a schema. This action is permanent. Note: You can remove a entire schema, including all of its versions. If the entire schema is removed, then all of its versions are permanently deleted from the schema registry of your Event Streams instance. Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation.  Select the schema you want to remove from the list.  Remove the entire schema or a selected version of the schema:          If you want to remove the entire schema and all its versions, click the Manage schema tab, and click Remove schema, then click Remove.      To remove a specific version, select it from the list, and click the Manage version tab for that version. Then click Remove version, then click Remove.        Important: This action is permanent and cannot be reversed.   Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init      Run the following command to remove a schema version:cloudctl es schema-remove --name &lt;schema-name&gt; --version &lt;schema-version&gt;     To remove an entire schema, do not specify the --version &lt;schema-version&gt; option.     Important: This action is permanent and cannot be reversed.   ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/manage-lifecycle/",
        "teaser":null},{
        "title": "Setting Java applications to use schemas",
        "collection": "2019.4",
        "excerpt":"If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas. Note: If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas. Preparing the setup To use schemas stored in the Event Streams schema registry, your client applications need to be able to serialize and deserialize messages based on schemas.   Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.  Consuming application then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.The Event Streams UI provides help with setting up your Java applications to use schemas. To set up your Java applications to use the Event Streams schemas and schema registry, prepare the connection for your application as follows:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Ensure you have added schemas to the registry.  Click Schema Registry in the primary navigation.  Select a schema from the list and click the row for the schema.  Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.  Set the preferences for your connection in the Configure the schema connection section. Use the defaults or change them by clicking Change configuration.                  For producers, set the method for Message encoding.                   Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.          JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.                            For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.                   Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.          Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and so allow a Kafka consumer to continue to process further messages.                            For both producers and consumers, set in the Use generated or generic code section whether your schema is to use custom Java classes that are generated based on the schema, or generic code by using the Apache Avro API.                   Use schema-specific code (default): Your application will use custom Java classes that are generated based on this schema, using get and set methods to create and access objects. When you want to use a different schema, you will need to update your code to use a new set of specific schema Java classes.          Use generic Apache Avro schema code: Your application will create and access objects using the generic Apache Avro API. Producers and consumers that use the generic serializer and deserializer can be coded to produce or consume messages using any schema uploaded to this schema registry.                      Click Save.  Go to the Provide an API key section and enter an API key with the correct permissions for your purposes (for example, to be able to produce, consume, or both). Alternatively, create one now by clicking Generate API key, and follow the instructions.The API key grants your application access to the cluster and its resources.  Click Generate connection details.  Download the Java truststore file which contains the server certificate.  Click Java dependencies to download the Event Streams schema registry JAR files, and click Schema JAR to download the schema JAR file to use for your application in its code.   Alternatively, if you are using Maven, click the Use Maven tab. Follow the instructions to copy the configuration snippets for the Event Streams Maven repository to your project Maven POM file, and run the Maven install command to download and install project dependencies.  Depending on your application, click the Producer or Consumer tab, and copy the sample Java code snippets displayed. The sample code snippets include the settings you configured to set up your applications to use the schema.     Add the snippets into your application code as described in the following sections.Setting up producers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a producing application.  Ensure you add the location of the JAR files to the build path of your producer Kafka application.  Use the code snippets from the UI and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example: import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import com.ibm.eventstreams.serdes.SchemaInfo;import com.ibm.eventstreams.serdes.SchemaRegistry;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example: Properties props = new Properties();props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.171:30342\");props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");props.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");props.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\";\";props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);props.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);Note: Replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier and replace &lt;api_key&gt; with an API key which has the permissions needed for your application. The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Producer code section defines properties for the producer application that set it to use the schema registry and the correct schema, for example: // Set the value serializer for produced messages to use the Event Streams serializerprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");props.put(\"value.serializer\", \"com.ibm.eventstreams.serdes.EventStreamsSerializer\");// Set the encoding type used by the message serializerprops.put(SchemaRegistryConfig.PROPERTY_ENCODING_TYPE, SchemaRegistryConfig.ENCODING_BINARY);// Get a new connection to the Schema RegistrySchemaRegistry schemaRegistry = new SchemaRegistry(props);// Get the schema from the registrySchemaInfo schema = schemaRegistry.getSchema(\"ABC_Assets_Schema\", \"1.0.0\");// Get a new specific KafkaProducerKafkaProducer&lt;String, ABC_Assets_Schema&gt; producer = new KafkaProducer&lt;&gt;(props);// Get a new specific record based on the schemaABC_Assets_Schema specificRecord = new ABC_Assets_Schema();// Add fields and values to the specific record, for example:// specificRecord.setTitle(\"this is the value for a title field\");// Prepare the record, adding the Schema Registry headersProducerRecord&lt;String, ABC_Assets_Schema&gt; producerRecord =    new ProducerRecord&lt;String, ABC_Assets_Schema&gt;(&lt;my_topic&gt;, specificRecord);producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_ID,    schema.getIdAsBytes());producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_VERSION,    schema.getVersionAsBytes());// Send the record to Kafkaproducer.send(producerRecord);// Close the producerproducer.close();The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsSerializer, telling Kafka to use the Event Streams serializer for message values when producing messages. You can also use the Event Streams serializer for message keys. Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message encoding behavior SchemaRegistryConfig.PROPERTY_ENCODING_TYPE. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to set the field values in your message.   Specific Java classes that are generated from the schema definition will have set&lt;field-name&gt; methods that can be used to easily set the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named setAuthor which takes a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the put method in the GenericRecord class to set field names and values.Note: Replace &lt;my_topic&gt; with the name of the topic to produce messages to. Setting up consumers to use schemas   Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a consuming application.  Ensure you add the location of the JAR files to the build path of your consumer Kafka application.  Use the code snippets from the UI and add them to your application code.The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example: import java.time.Duration;import java.util.Arrays;import java.util.Properties;// Import the specific schema classimport com.mycompany.schemas.ABC_Assets_Schema;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import com.ibm.eventstreams.serdes.SchemaRegistryConfig;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.KafkaConsumer;The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example: Properties props = new Properties();props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.171:30342\");props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");props.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");props.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\";\";props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);props.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);Note: Replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier and replace &lt;api_key&gt; with an API key which has the permissions needed for your application. The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. The code snippet from the Consumer code section defines properties for the consumer application that set it to use the schema registry and the correct schema, for example: // Set the value deserializer for consumed messages to use the Event Streams deserializerprops.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"com.ibm.eventstreams.serdes.EventStreamsDeserializer\");// Set the behavior of the deserializer when a record cannot be deserializedprops.put(SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE, SchemaRegistryConfig.BEHAVIOR_STRICT);// Set the consumer group ID in the propertiesprops.put(\"group.id\", &lt;my_consumer_group&gt;);// Get a new KafkaConsumerKafkaConsumer&lt;String, ABC_Assets_Schema&gt; consumer = new KafkaConsumer&lt;&gt;(props);// Subscribe to the topicconsumer.subscribe(Arrays.asList(&lt;my_topic&gt;));// Poll the topic to retrieve recordswhile(true) {    ConsumerRecords&lt;String, ABC_Assets_Schema&gt; records = consumer.poll(Duration.ofSeconds(5));    for (ConsumerRecord&lt;String, ABC_Assets_Schema&gt; record : records) {        ABC_Assets_Schema specificRecord = record.value();        // Get fields and values from the specific record, for example:        // String titleValue = specificRecord.getTitle().toString();    }}The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsDeserializer, telling Kafka to use the Event Streams deserializer for message values when consuming messages. You can also use the Event Streams deserializer for message keys. Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message deserialization behavior SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: Use the generic or generated schema-specific Java classes to read the field values from your message.   Specific Java classes that are generated from the schema definition will have get&lt;field-name&gt; methods that can be used to easily retrieve the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named getAuthor which returns a string argument value.  The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the get method in the GenericRecord class to set field names and values.Note: Replace &lt;my_consumer_group&gt; with the name of the consumer group to use and &lt;my_topic&gt; with the name of the topic to consume messages from. Setting up Kafka Streams applications Kafka Streams applications can also use the Event Streams schema registry serdes library to serialize and deserialize messages. For example: // Set the Event Streams serdes properties, including the override option to set the schema// and version used for serializing produced messages.Map&lt;String, Object&gt; serdesProps = new HashMap&lt;String, Object&gt;();serdesProps.put(SchemaRegistryConfig.PROPERTY_API_URL, \"https://192.0.2.171:30546\");serdesProps.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE, \"ABC_Assets_Schema\");serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE, \"1.0.0\");serdesProps.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");serdesProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, &lt;Java_truststore_file_location&gt;);serdesProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"password\");serdesProps.put(SaslConfigs.SASL_JAAS_CONFIG, \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\");// Set up the Kafka StreamsBuilderStreamsBuilder builder = new StreamsBuilder();// Configure a Kafka Serde instance to use the Event Streams schema registry// serializer and deserializer for message valuesSerde&lt;IndexedRecord&gt; valueSerde = new EventStreamsSerdes();valueSerde.configure(serdesProps, false);// Get the stream of messages from the source topic, deserializing each message value with the// Event Streams deserializer, using the schema and version specified in the message headers.builder.stream(&lt;my_source_topic&gt;, Consumed.with(Serdes.String(), valueSerde))    // Get the 'nextcount' int field from the record.    // The Event Streams deserializer constructs instances of the generated schema-specific    // ABC_Assets_Schema_Count class based on the values in the message headers.    .mapValues(new ValueMapper&lt;IndexedRecord, Integer&gt;() {        @Override        public Integer apply(IndexedRecord val) {            return ((ABC_Assets_Schema_Count) val).getNextcount();        }    })    // Get all the records    .selectKey((k, v) -&gt; 0).groupByKey()    // Sum the values    .reduce(new Reducer&lt;Integer&gt;() {        @Override        public Integer apply(Integer arg0, Integer arg1) {            return arg0 + arg1;        }    })    .toStream()    // Map the summed value to a field in the schema-specific generated ABC_Assets_Schema class    .mapValues(        new ValueMapper&lt;Integer, IndexedRecord&gt;() {            @Override            public IndexedRecord apply(Integer val) {                ABC_Assets_Schema record = new ABC_Assets_Schema();                record.setSum(val);                return record;            }     })     // Finally, put the result to the destination topic, serializing the message value     // with the Event Streams serializer, using the overridden schema and version from the     // configuration.    .to(&lt;my_destination_topic&gt;, Produced.with(Serdes.Integer(), valueSerde));// Create and start the streamfinal KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig);streams.start();In this example, the Kafka StreamsBuilder is configured to use the com.ibm.eventstreams.serdes.EventStreamsSerdes class, telling Kafka to use the Event Streams deserializer for message values when consuming messages and the Event Streams serializer for message values when producing messages. Note: The Kafka Streams org.apache.kafka.streams.kstream API does not provide access to message headers, so to produce messages with the Event Streams schema registry headers, use the SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE and SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE configuration properties. Setting these configuration properties will mean produced messages are serialized using the provided schema version and the Event Streams schema registry message headers will be set. For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference. Note: To re-use this example, replace the &lt;Java_truststore_file_location&gt; with the path to the Java truststore file you downloaded earlier, &lt;api_key&gt; with an API key which has read permissions for your Event Streams deployment, &lt;my_source_topic&gt; with the name of the topic to consume messages from and &lt;my_destination_topic&gt; with the name of the topic to produce messages to. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/setting-java-apps/",
        "teaser":null},{
        "title": "Setting non-Java applications to use schemas",
        "collection": "2019.4",
        "excerpt":"If you have producer or consumer applications created in languages other than Java, use the following guidance to set them up to use schemas. You can also use the REST producer API to send messages that are encoded with a schema. For a producer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Use an Apache Avro library for your programming language to read the schema definition from the local file and encode a Kafka message with it.  Set the schema registry headers in the Kafka message, so that consumer applications can understand which schema and version was used to encode the message, and which encoding format was used.  Send the message to Kafka.For a consumer application:   Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.  Consume a message from Kafka.  Check the headers for the Kafka message to ensure they match the expected schema ID and schema version ID.  Use the Apache Avro library for your programming language to read the schema definition from the local file and decode the Kafka message with it.Retrieving the schema definition from the schema registry Using the UI   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Schema Registry in the primary navigation and find your schema in the list.  Copy the schema definition into a new local file.          For the latest version of the schema, expand the row. Copy and paste the schema definition into a new local file.      For a different version of the schema, click on the row and then select the version to use from the list of schema versions. Click the Schema definition tab and then copy and paste the schema definition into a new local file.      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster: cloudctl es init  Run the following command to list all the schemas in the schema registry: cloudctl es schemas  Select your schema from the list and run the following command to list all the versions of the schema: cloudctl es schema &lt;schema-name&gt;  Select your version of the schema from the list and run the following command to retrieve the schema definition for the version and copy it into a new local file: cloudctl es schema &lt;schema-name&gt; --version &lt;schema-version-id&gt; &gt; &lt;schema-definition-file&gt;.avscSetting headers in the messages you send to Event Streams Kafka Set the following headers in the message to enable applications that use the Event Streams serdes Java library to consume and deserialize the messages automatically. Setting these headers also enables the Event Streams UI to display additional details about the message. The required message header keys and values are listed in the following table.             Header name      Header key      Header value                  Schema ID      com.ibm.eventstreams.schemaregistry.schema.id      The schema ID as a string.              Schema version ID      com.ibm.eventstreams.schemaregistry.schema.version      The schema version ID as a string.              Message encoding      com.ibm.eventstreams.schemaregistry.encoding      Either JSON for Avro JSON encoding, or BINARY for Avro binary encoding.      Note: The schema version ID is the integer ID that is displayed when listing schema versions using the command cloudctl es schema &lt;schema-name&gt;. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/setting-nonjava-apps/",
        "teaser":null},{
        "title": "Migrating existing applications to the Event Streams schema registry",
        "collection": "2019.4",
        "excerpt":"If you are using the Confluent Platform schema registry, Event Streams provides a migration path for moving your Kafka consumers and producers over to use the Event Streams schema registry. Migrating schemas to Event Streams schema registry To migrate schemas, you can use schema auto-registration in your Kafka producer, or you can manually migrate schemas by downloading the schema definitions from the Confluent Platform schema registry and adding them to the Event Streams schema registry.  Migrating schemas with auto-registration When using auto-registration, the schema will be automatically uploaded to the Event Streams schema registry, and named with the subject ID (which is based on the subject name strategy in use) and a random suffix.  Auto-registration is enabled by default in the Confluent Platform schema registry client library. To disable it, set the auto.register.schemas property to false. Note: To auto-register schemas in the Event Streams schema registry, you need an API key that has operator role permissions (or higher) and permission to create schemas. You can generate API keys by using the ES UI or CLI. Using the UI:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Click Generate API key.  Enter a name for your application, and select Produce, consume, create topics and schemas.  Click Next.  Enter your topic name or set All topics to On.  Click Next.  Enter the consumer group name or set All consumer groups to On.  Click Generate API key to generate an API key.  Click Copy API key.Using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the following command to create a service ID with an API key that has permissions to auto-register schemas and produce messages to any topic:cloudctl es iam-service-id-create &lt;service-id-name&gt; --role operator --all-topics --all-schemasMigrating schemas manually To manually migrate the schemas, download the schema definitions from the Confluent Platform schema registry, and add them to the Event Streams Schema Registry. When manually adding schemas to the Event Streams Schema Registry, the provided schema name must match the subject ID used by the Confluent Platform schema registry subject name strategy. If you are using the default TopicNameStrategy, the schema name must be &lt;TOPIC_NAME&gt;-&lt;'value'|'key'&gt; If you are using the RecordNameStrategy, the schema name must be &lt;SCHEMA_DEFINITION_NAMESPACE&gt;.&lt;SCHEMA_DEFINITION_NAME&gt; For example, if you are using the default TopicNameStrategy as your subject name strategy, and you are serializing your data into the message value and producing to the MyTopic topic, then the schema name you must provide when adding the schema in the UI must be MyTopic-value For example, if you are using the RecordNameStrategy as your subject name strategy, and the schema definition file begins with the following, then the schema name you must provide when adding the schema in the UI must be org.example.Book: {    \"type\": \"record\",    \"name\": \"Book\",    \"namespace\": \"org.example\",    \"fields\": [...If you are using the CLI, run the following command when adding the schema: cloudctl es schema-add --create --name org.example.Book --version 1.0.0 --file /path/to/Book.avsc Migrating a Kafka producer application To migrate a Kafka producer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your producer application to secure the connection between the producer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");        Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:    export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\       -Djavax.net.ssl.trustStorePassword=password\"      Migrating a Kafka consumer application To migrate a Kafka consumer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.   Configure your consumer application to secure the connection between the consumer and Event Streams.  Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.      Ensure you add the following schema properties to your Kafka producers:                             Property name          Property value                                      schema.registry.url          https://&lt;host name&gt;:&lt;API port&gt;                          basic.auth.credentials.source          SASL_INHERIT                      You can also use the following code snippet for Java applications:     props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, \"https://&lt;host name&gt;:&lt;API port&gt;\");props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, \"SASL_INHERIT\");        Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:    export KAFKA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \\        -Djavax.net.ssl.trustStorePassword=password\"      ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/migrating/",
        "teaser":null},{
        "title": "Using schemas with the REST producer API",
        "collection": "2019.4",
        "excerpt":"You can use schemas when producing messages with the Event Streams REST producer API. You simply add the following parameters to the API call:   schemaname: The name of the schema you want to use when producing messages.  schemaversion: The schema version you want to use when producing messages.For example, to use cURL to produce messages to a topic with the producer API, and specify the schema to be used, run the curl command as follows: curl \"https://192.0.2.171:30342/topics/&lt;topicname&gt;/records?schemaname=&lt;schema-name&gt;&amp;schemaversion=&lt;schema-version-name&gt;\" -d '&lt;avro_encoded_message&gt;' -H \"Content-Type: application/json\" -H \"Authorization: Bearer &lt;apikey&gt;\" --cacert es-cert.pem  By adding these parameters to the API call, a lookup is done on the specified schema and its version to check if it is valid. If valid, the correct message headers are set for the produced message. Important: When using the producer API, the lookup does not validate the data in the request to see if it matches the schema. Ensure the message conforms to the schema, and that it has been encoded in the Apache Avro binary or JSON encoding format. If the message does not conform and is not encoded with either of those formats, consumers will not be able to deserialize the data. If the message has been encoded in the Apache Avro binary format, ensure the HTTP Content-Type header is set to application/octet-stream. If the message has been encoded in the Apache Avro JSON format, ensure the HTTP Content-Type header is set to application/json. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/using-with-rest-producer/",
        "teaser":null},{
        "title": "Using the schema API",
        "collection": "2019.4",
        "excerpt":"Event Streams provides a Java library to enable Kafka applications to serialize and deserialize messages using schemas stored in your Event Streams schema registry. Using the schema registry serdes library API, schema versions are automatically downloaded from your schema registry, checked to see if they are in a disabled or deprecated state, and cached. The schemas are used to serialize messages produced to Kafka and deserialize messages consumed from Kafka. Schemas downloaded by the schema registry serdes library API are cached in memory with a 10 minute expiration period. This means that if a schema is deprecated or disabled, it might take 10 minutes before consuming or producing applications will see the change. To change the expiration period, set the SchemaRegistryConfig.PROPERTY_SCHEMA_CACHE_REFRESH_RATE configuration property to a new milliseconds value. For more details, including code snippets that use the schema registry serdes API, see setting Java applications to use schemas. For full details of the Event Streams schema registry serdes API, see the Schema API Javadoc. ","categories": ["schemas"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/schemas/schema-api/",
        "teaser":null},{
        "title": "Managing access",
        "collection": "2019.4",
        "excerpt":"You can secure your IBM Event Streams resources in a fine-grained manner by managing the access each user and application has to each resource. What resource types can I secure? Within IBM Event Streams, you can secure access to the following resource types, where the names in parentheses are the resource type names used in policy definitions:   Cluster (cluster): you can control which users and applications can connect to the cluster.  Topics (topic): you can control the ability of users and applications to create, delete, read, and write to a topic.  Consumer groups (group): you can control an application’s ability to join a consumer group.  Transactional IDs (txnid): you can control the ability to use the transaction capability in Kafka.What roles can I assign? Roles define the levels of access a user or application has to resources. The following table describes the roles you can assign in IBM Cloud Private.             Role      Permitted actions      Example actions                  Viewer      Viewers have permissions to perform read-only actions within IBM Event Streams such as viewing resources.      Allow an application to connect to a cluster by assigning read access to the cluster resource.              Editor      Editors have permissions beyond the Viewer role, including writing to IBM Event Streams resources such as topics.      Allow an application to produce to topics by assigning editor access to topic resources.              Operator      Operators have permissions beyond the Editor role, including creating and editing IBM Event Streams resources.      Allow access to create resources by assigning operator access to the IBM Event Streams instance.              Auditor      No actions are currently assigned to this role.                     Administrator      Administrators have permissions beyond the Operator role to complete privileged actions.      Allow full access to all resources by assigning administrator access to the IBM Event Streams instance.      Mapping service actions to roles Access control in Apache Kafka is defined in terms of operations and resources. In IBM Event Streams, the operations are grouped into a smaller set of service actions, and the service actions are then assigned to roles. The mapping between Kafka operations and service actions is described in the following table. If you understand the Kafka authorization model, this tells you how IBM Event Streams maps operations into service actions.             Resource type      Kafka operation      Service action                  Cluster      Describe      -                     Describe Configs      -                     Idempotent Write      -                     Create      cluster.manage                     Alter      RESERVED                     Alter Configs      cluster.manage                     Cluster Action      RESERVED              Topic      Describe      -                     Describe Configs      topic.read                     Read      topic.read                     Write      topic.write                     Create      topic.manage                     Delete      topic.manage                     Alter      topic.manage                     Alter Configs      topic.manage              Group      Describe      -                     Read      group.read                     Delete      group.manage              Transactional ID      Describe      -                     Write      txnid.write      In addition, IBM Event Streams adds another service action called cluster.read. This service action is used to control connection access to the cluster. Note: Where the service action for an operation is shown in the previous table as a dash -, the operation is permitted to all roles. The mapping between service actions and IBM Event Streams roles is described in the following table.             Resource type      Administrator      Operator      Editor      Viewer                  Cluster      cluster.read      cluster.read      cluster.read      cluster.read                     cluster.manage      cluster.manage                            Topic      topic.read      topic.read      topic.read      topic.read                     topic.write      topic.write      topic.write                            topic.manage      topic.manage                            Group      group.read      group.read      group.read      group.read                     group.manage      group.manage                            Transactional ID      txnid.write      txnid.write      txnid.write             Assigning access to users If you have not set up IBM Cloud Private teams, the default  admin user has unlimited access to all resources. The default admin user is defined at the time of installation in the IBM Cloud Private config.yaml file by using the default_admin_user parameter. If you are using IBM Cloud Private teams, you must associate the team with the IBM Event Streams instance to apply the team members’ roles to the resources within the instance, including any users that have the Cluster Administrator role. You can do this by using the cloudctl es iam-add-release-to-team command. This command creates policies that grant access to resources based on the roles in the team. It is possible to refine user access to specific resources further and limit actions they can take against resources by using the IBM Cloud Private APIs. If you require such granular settings for security, contact us. Note: It can take up to 10 minutes after assigning access before users can perform tasks associated with their permissions. Common scenarios for users The following table summarizes common IBM Event Streams scenarios and the roles you need to assign.             Permission      Role required                  Allow full access to all resources      Administrator              Create and delete topics      Operator or higher              Generate the starter application to produce messages      Editor or higher              View the messages on a topic      Viewer or higher      Assigning access to applications Each application that connects to IBM Event Streams provides credentials associated with an IBM Cloud Private service ID. You assign access to a service ID by creating service policies. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Select the ServiceID you are interested in or create one.Warning: Do not use the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. This service ID is reserved to be used within the Events Streams cluster.Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information:   The role assigned to the policy. For example, Viewer, Editor, or Operator.  The type of service the policy applies to. For example, IBM Event Streams.  The instance of the service to be secured.  The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance.  The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance.You can create a single policy that does not specify either the resource type or the resource identifier. This kind of policy applies its role to all resources in the IBM Event Streams instance. If you want more precise access control, you can create a separate policy for each specific resource that the service ID will use. Note: It can take up to 10 minutes after assigning access before applications can perform tasks associated with their permissions. Common scenarios for applications If you choose to use a single policy to grant access to all resources in the IBM Event Streams instance, the following table summarizes the roles required for common scenarios.             Permission      Policies required                  Connect to the cluster      1. Role: Viewer or higher              Consume from a topic      1. Role: Viewer or higher              Produce to a topic      1. Role: Editor or higher              Use all features of the Kafka Streams API      1. Role: Operator or higher      Alternatively, you can assign specific service policies for the individual resources. The following table summarizes common IBM Event Streams scenarios and the service policies you need to assign.             Permission      Policies required                  Connect to the cluster      1. Resource type: cluster Role: Viewer or higher              Produce to a topic      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher              Produce to a topic using a transactional ID      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Editor or higher  3. Resource type: txnid  Resource identifier: transactional_id Role: Editor or higher              Consume from a topic (no consumer group)      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher              Consume from a topic in a consumer group      1. Resource type: cluster Role: Viewer or higher  2. Resource type: topic  Resource identifier: name_of_topic Role: Viewer or higher  3. Resource type: group  Resource identifier: name_of_consumer_group Role: Viewer or higher      Revoking access for an application You can revoke access to IBM Event Streams by deleting the IBM Cloud Private service ID or API key that the application is using. You can use the IBM Cloud Private cluster management console to achieve this.   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Manage &gt; Identity &amp; Access.  From the sub-navigation menu, click Service IDs.  Find the Service ID being used by the application in the Service IDs list.  Remove either the service ID or the API key that the application is using. Removing the service ID also removes all API keys that are owned by the service ID.Warning: Do not remove the internal Event Streams service ID eventstreams-&lt;release name&gt;-service-id. Removing this service ID corrupts your deployment, which can only be resolved by reinstalling Event Streams.  Remove the service ID by clicking  Menu overflow &gt; Remove in the row of the service ID. Click Remove Service ID on the confirmation dialog.  Remove the API key by clicking the service ID. On the service ID page, click API keys. Locate the API key being used by the application in the API keys list. CLick  Menu overflow &gt; Remove in the row of the API key. Click Remove API key on the confirmation dialog.Note: Revoking a service ID or API key in use by any Kafka client might not disable access for the application immediately. The API key is stored in a token cache in Kafka which has a 23 hour expiration period. When the token cache expires, it is refreshed from IBM Cloud Private and any revoked service IDs or API keys are reflected in the new token cache, causing application access be be disabled. To immediately disable application access, you can force a refresh of the Kafka token cache by restarting each Kafka broker. To do this without causing downtime, you can patch the stateful set by using the following command: kubectl -n &lt;namespace&gt; patch sts &lt;release_name&gt;-ibm-es-kafka-sts -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"restarted\":\"123\"}}}}}' This does not make changes to the broker configuration, but it still causes the Kafka brokers to restart one at a time, meaning no downtime is experienced. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/managing-access/",
        "teaser":null},{
        "title": "Encrypting your data",
        "collection": "2019.4",
        "excerpt":"Network connections into the IBM Event Streams deployment are secured using TLS. By default, data within the Event Streams deployment is not encrypted. To secure this data, you must ensure that any storage and communication channels are encrypted as follows:   Encrypt data at rest by using disk encryption or encrypting volumes using dm-crypt.  Encrypt internal network traffic by using TLS encryption for communication between pods.  Encrypt messages in applications.Enabling encryption between pods By default, TLS encryption for communication between pods is disabled. You can enable it when installing Event Streams, or you can enable it later as described in this section. To enable TLS encryption for your existing Event Streams installation, use the UI or the command line.   To enable TLS by using the UI, follow the instructions in modifying installation settings, and set the Pod to pod encryption field of the Global install settings section to Enabled.      To enable TLS by using the command line, follow the instructions in modifying installation settings, and set the global.security.tlsInternal parameter to enabled as follows:     helm upgrade --reuse-values --set global.security.tlsInternal=enabled &lt;release_name&gt; &lt;charts.tgz&gt; --tls     For example: helm upgrade --reuse-values --set global.security.tlsInternal=enabled eventstreams ibm-eventstreams-prod-1.4.0.tgz --tls   Warning: If you enable TLS encryption between pods, the message browser will not display message data from before the upgrade. Important: Enabling TLS encryption between pods might impact the connection to Event Streams. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/encrypting-data/",
        "teaser":null},{
        "title": "Configuring secure JMX connections",
        "collection": "2019.4",
        "excerpt":"You can set up the Kafka broker JMX ports to be accessible to secure connections from within the IBM Cloud Private cluster. This grants applications deployed inside the cluster read-only access to Kafka metrics. By default, Kafka broker JMX ports are not accessible from outside the Kubernetes pod. To enable access, ensure you select the Enable secure JMX connections check box in the Kafka broker settings section when installing Event Streams. You can also enable secure JMX connections for existing installations by modifying your settings. When access is enabled, you can configure your applications to connect securely to the JMX port as follows. Enabling external connections When Event Streams is installed with the Enable secure JMX connections option, the Kafka broker is configured to start the JMX port with SSL and authentication enabled. The JMX port 9999 is opened on the Kafka pod and is accessible from within the cluster using the hostname &lt;releasename&gt;-ibm-es-kafka-broker-svc-&lt;brokerNum&gt;.&lt;namespace&gt;.svc To retrieve the local name of the service you can use the following command (the results do not have the &lt;namespace&gt;.svc suffix): kubectl -n &lt;namespace&gt; get services To connect to the JMX port, clients must use the following Java options:   javax.net.ssl.trustStore=&lt;path to trustStore&gt;  javax.net.ssl.trustStorePassword=&lt;password for trustStore&gt;In addition, clients must provide a username and password when initiating the JMX connection. Providing configuration values When secure JMX connections is enabled, a Kubernetes secret named &lt;releasename&gt;ibm-es-jmx-secret is created inside the Event Streams namespace. The secret contains the following content:             Name      Description                  truststore.jks      A Java truststore containing the certificates needed for SSL communication with the Kafka broker JMX port.              trust_store_password      The password associated with the truststore.              jmx_username      The user that is authenticated to connect to the JMX port.              jmx_password      The password for the authenticated user.      The Kubernetes secret’s contents must then be mounted as volumes and environment variables inside the application pod to provide the required runtime configuration to create a JMX connection. For example: apiVersion: v1kind: Podspec:  containers:    - name: container1      env:        - name: jmx_username          secretRef:            secretName: es-secret      ...      volumeMounts:        - name: es-volume          mountPath: /path/to/volume/on/pod/file/system  ...  volumes:    - name: es-volume      fromSecret:        secretName: es-secret        items:          - name: truststore.jks            path: jks.jksIf the connecting application is not installed inside the Event Streams namespace, it must be copied to the application namespace using the following command: kubectl -n &lt;releaseNamespace&gt; get secret &lt;releasename&gt;ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;applicationNamespace&gt; apply -f -","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/secure-jmx-connections/",
        "teaser":null},{
        "title": "Updating certificates",
        "collection": "2019.4",
        "excerpt":"Event Streams uses the following certificates for security:       External: sets the certificate to use for accessing Event Streams resources such as Kafka or the REST API (set by global.security.externalCertificateLabel, see table later).     You can set the external certificates when installing Event Streams.     You can also change the certificates later for existing Event Streams installations. This might be required for security reasons when certificates expire or need to be replaced for compliance updates, for example.         Internal: sets the certificate used for the internal TLS encryption between pods (set by global.security.internalCertificateLabel, see table later).     Upgrading your Event Streams version to 2019.4.1 also updates your internal certificates automatically. You can manually change your internal certificates later. Internal certificates can only be self-generated certificates, they cannot be provided.   Changing certificates for existing installations To update certificates for an existing Event Streams installation, use the following helm upgrade command: helm upgrade --reuse-values --set &lt;certificate label&gt;=&lt;a unique string&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls where   &lt;certificate label&gt; is one of the following:            Certificate label      Certificate to update                  global.security.externalCertificateLabel      External TLS certificatesFor example, Kafka bootstrap ports, REST producer, REST API, and so on.               global.security.internalCertificateLabel      Internal TLS certificatesUsed for the internal TLS encryption between pods.        &lt;a unique string&gt; is a different value from the last one provided at the time of installation or the previous update. The certificate labels are strings and can contain any values that are permitted in Kubernetes labels (for example, no spaces are permitted).  &lt;release_name&gt; is the name that identifies your Event Streams installation.  &lt;charts.tgz&gt; is the chart file used to install Event Streams, which needs to be available when running Helm upgrade commands.For example: helm upgrade --reuse-values --set global.security.externalCertificateLabel=upgradedlabel1 eventstreams1 ibm-eventstreams-prod-1.4.0.tgz --tls Specifying certificate type You can also specify the type of certificate you want to use with the tls.type option:   selfsigned  provided  secretThe default is selfsigned, which means the certificates are automatically generated by Event Streams (self-generated). If you have not changed the certificate type and want to continue to use selfsigned, you do not have to specify the tls.type when running the helm upgrade command. If you want to use a different type, specify the tls.type when running the command, for example, tls.type=provided. If you want to update the certificates with ones you provide, you set the certificate labels as described earlier, and also set the following:   tls.type: Type of certificate, specify provided or secret if providing your own (default is selfsigned which automatically generates the certificates when installing Event Streams).  tls.secretName: If you set tls.type to secret, enter the name of the secret that contains the certificates to use.  tls.key: If you set tls.type to provided, this is the base64-encoded TLS key or private key. If set to secret, this is the key name in the secret (default key name is “key”).  tls.cert: If you set tls.type to provided, this is the base64-encoded public certificate. If set to secret, this is the key name in the secret (default key name is “cert”).  tls.cacert: If you set tls.type to provided, this is the base64-encoded Certificate Authority Root Certificate. If set to secret, this is the key name in the secret (default key name is “cacert”).For example, when using type provided, use the following command: helm upgrade --reuse-values --set tls.type=provided --set tls.key=&lt;key&gt; --set tls.cert=&lt;public-certificate&gt; --set tls.cacert=&lt;CA-root-certificate&gt; &lt;certificate label&gt;=&lt;a unique string&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: The internal TLS certificates can only be self-generated. Updating clients after certificate change In some cases, updating the external TLS certificates can prevent existing clients from accessing the cluster. See the following scenarios to see if you need to update your clients after a certificate change. To update your client settings after a certificate change, see how to secure the connection. If you upgraded your Event Streams version If you have clients using certificates from a previous Event Streams installation, see the following for guidance about when you need to update client certificates:       If you have been using self-generated certificates and upgrade your Event Streams version from 2019.2.1 or earlier to 2019.4.1, but do not update your external certificates, then your clients will continue to work with Event Streams.     If you change your external certificates after upgrading to 2019.4.1, then you need to provide the new certificates to your clients when you first change them.     Any subsequent self-generated certificate change will not affect the clients, and they will continue to work with Event Streams.     If you have been using certificates you provided, and you change the certificates after an upgrade, then the clients will continue to work as long as the changed certificates are signed by the same Certificate Authority.  If you change the certificate type, for example, from self-generated to provided, you will need to update your clients.If you installed 2019.4.1 as a new deployment If you have clients using certificates from a Event Streams 2019.4.1 installation that was a new deployment (not an upgrade from a previous version), see the following for guidance about when you need to update client certificates:   If you installed Event Streams 2019.4.1 as a new deployment with self-signed certificates, your clients will continue to work if you change the certificates and continue to use self-signed certificates.  If you installed Event Streams 2019.4.1 as a new deployment with provided certificates, your clients will continue to work if you change the certificates as long as the changed certificates are signed by the same Certificate Authority.  If you change the certificate type, for example, from self-generated to provided, you will need to update your clients.","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/updating-certificates/",
        "teaser":null},{
        "title": "Network policies",
        "collection": "2019.4",
        "excerpt":"The following tables provide information about the permitted network connections for each Event Streams pod. Kafka pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods, REST Producer pods, and Geo-replicator pods to port 8084      Kafka access              TCP      REST API pods to port 7070      Querying Kafka status              TCP      Proxy pods to port 8093      Proxied Kafka traffic              TCP      Other Kafka pods to port 9092      Kafka cluster traffic              TCP      To port 8081 on the IBM Cloud Private master host      Prometheus collecting metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      ZooKeeper pods on port 2181      Cluster metadata access              TCP      Other Kafka pods on port 9092      Kafka cluster traffic              TCP      Index Manager pods on port 8080      Kafka metrics              TCP      Access Controller pods on port 8443      Security API access              TCP      Collector pods on port 7888      Submitting metrics              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      ZooKeeper pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods and REST API pods to port 2181      ZooKeeper traffic              TCP      Other ZooKeeper pods to ports 2888 and 3888      ZooKeeper cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Other ZooKeeper pods on port 2888 and 3888      ZooKeeper cluster traffic      Geo-replicator pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST API pods to port 8083      Geo-replicator API traffic              TCP      Other geo-replicator pods to port 8083      Geo-replicator cluster traffic              TCP      To port 8080 on the IBM Cloud Private master host      Allow Prometheus to collect metrics        Outgoing connections permitted: AnyAdministration UI pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST proxy pods on port 9080      REST API access              TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access              TCP      Access Controller pods on port 8443      Access Controller API access              TCP      Port 4300 on the IBM Cloud Private master host      ICP identity API access      Administration server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 9080      Proxied REST API calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on ports 8084 and 7070      Kafka admin access              TCP      Index Manager pods on port 9080      Metric API access              TCP      Geo-replicator pods on port 8083      Geo-replicator API access              TCP      ZooKeeper pods on port 2181      ZooKeeper admin access              TCP      Anywhere      Coordination with REST API in other ES instances              UDP      Anywhere on port 53 on the IBM Cloud Private master host      Coordination with REST API in other ES instances      REST producer server pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      REST Proxy pods to port 8080      Proxied REST Producer calls        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8084      Sending Kafka messages      REST proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      REST API pods on port 9080      Proxying REST API calls              TCP      REST Producer pods on port 8080      Proxying REST Producer calls      Collector pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 7888      Receiving metrics        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Kafka pods on port 8080      Prometheus connections      Network proxy pod       Incoming connections permitted: Any         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access              TCP      Kafka pods on port 8093      Kafka client traffic              TCP      REST proxy pods on port 9080      Kafka admin      Access Controller pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods, REST API pods, and UI pods to port 8443      Allow components to make auth checks        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Port 8443 on the IBM Cloud Private master host      ICP security / IAM access      Index manager pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Kafka pods to port 8080      Receiving metrics              TCP      Elastic and REST API pods to port 9080      Metrics access        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Elastic pods on port 9200      Elasticsearch admin access              TCP      REST proxy pods on port 9080      REST API access      Elasticsearch pod   Incoming connections permitted:            Type      Origin      Reason                  TCP      Index Manager pods to port 9200      Elasticsearch admin access              TCP      Other ElasticSearch pods to port 9300      ElasticSearch cluster traffic        Outgoing connections permitted:            Type      Destination      Reason                  TCP      Index Manager pods on port 9080      Elastic admin              TCP      Other ElasticSearch pods on port 9300      ElasticSearch cluster traffic      Install jobs pod       Incoming connections permitted: None         Outgoing connections permitted:               Type      Destination      Reason                  TCP      Port 8001 on the IBM Cloud Private master host      Kubernetes API access      Telemetry pod       Incoming connections permitted: None         Outgoing connections permitted: Any   ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/network-policies/",
        "teaser":null},{
        "title": "Considerations for GDPR",
        "collection": "2019.4",
        "excerpt":"Notice: Clients are responsible for ensuring their own compliance with various lawsand regulations, including the European Union General Data Protection Regulation.Clients are solely responsible for obtaining advice of competent legal counsel as tothe identification and interpretation of any relevant laws and regulations that mayaffect the clients’ business and any actions the clients may need to take to complywith such laws and regulations. The products, services, and other capabilitiesdescribed herein are not suitable for all client situations and may have restrictedavailability. IBM does not provide legal, accounting, or auditing advice or represent orwarrant that its services or products will ensure that clients are in compliance withany law or regulation. GDPR Overview What is GDPR? GDPR stands for General Data Protection Regulation. GDPR has been adopted by the European Union and will apply from May 25, 2018. Why is GDPR important? GDPR establishes a stronger data protection regulatory framework for processing of personal data of individuals. GDPR brings:   New and enhanced rights for individuals  Widened definition of personal data  New obligations for companies and organisations handling personal data  Potential for significant financial penalties for non-compliance  Compulsory data breach notificationThis document is intended to help you in your preparations for GDPR readiness. Read more about GDPR   EU GDPR Information Portal  IBM GDPR websiteProduct Configuration for GDPR Configuration to support data handling requirements The GDPR legislation requires that personal data is strictly controlled and that theintegrity of the data is maintained. This requires the data to be secured against lossthrough system failure and also through unauthorized access or via theft of computer equipment or storage media.The exact requirements will depend on the nature of the information that will be stored or transmitted by Event Streams.Areas for consideration to address these aspects of the GDPR legislation include:   Physical access to the assets where the product is installed  Encryption of data both at rest and in flight  Managing access to topics which hold sensitive material.Data Life Cycle IBM Event Streams is a general purpose pub-sub technology built on Apache Kafka® which canbe used for the purpose of connecting applications. Some of these applications may be IBM-owned but others may be third-party productsprovided by other technology suppliers. As a result, IBM Event Streams can be used to exchange many forms of data,some of which could potentially be subject to GDPR. What types of data flow through IBM Event Streams? There is no one definitive answer to this question because use cases vary through application deployment. Where is data stored? As messages flow through the system, message data is stored on physical storage media configured by the deployment. It may also reside in logs collectedby pods within the deployment. This information may include data governed by GDPR. Personal data used for online contact with IBM IBM Event Streams clients can submit online comments/feedback requests to contact IBM about IBM Event Streams in a variety ofways, primarily:   Public issue reporting and feature suggestions via IBM Event Streams Git Hub portal  Private issue reporting via IBM Support  Public general comment via the IBM Event Streams slack channelTypically, only the client name and email address are used to enable personal replies for the subject of the contact. The use of personal data conforms to the IBM Online Privacy Statement. Data Collection IBM Event Streams can be used to collect personal data. When assessing your use of IBM Event Streams and the demandsof GDPR, you should consider the types of personal data which in your circumstances are passing through the system. Youmay wish to consider aspects such as:   How is data being passed to an IBM Event Streams topic? Has it been encrypted or digitally signed beforehand?  What type of storage has been configured within the IBM Event Streams? Has encryption been enabled?  How does data flow between nodes in the IBM Event Streams deployment? Has internal network traffic been encrypted?Data Storage When messages are published to topics, IBM Event Streams will store the message data on stateful media within the cluster forone or more nodes within the deployment. Consideration should be given to securing this data when at rest. The following items highlight areas where IBM Event Streams may indirectly persist application provided data whichusers may also wish to consider when ensuring compliance with GDPR.   Kubernetes activity logs for containers running within the Pods that make up the IBM Event Streams deployment  Logs captured on the local file system for the Kafka container running in the Kakfa pod for each nodeBy default, messages published to topics are retained for a week after their initial receipt, but this can be configured by modifying Kafka broker settings using the IBM Event Streams CLI. Data Access The Kafka core APIs can be used to access message data within the IBM Event Streams system:   Producer API to allow data to be sent to a topic  Consumer API to allow data to be read from a topic  Streams API to allow transformation of data from an input topic to an output topic  Connect API to allow connectors to continually move data in or out of a topic from an external systemUser roles can be used to control access to data stored in IBM Event Streams accessed over these APIs. In addition, the Kubernetes APIs can be used to access cluster configuration and resources, including but not limited to logs that may contain message data. Access and autorization controls can be used to control which users are able to access this cluster level information. Data Processing Encryption of connection to IBM Event Streams Connections to IBM Event Streams are secured using TLS. When deploying IBM Event Streams, the default setting for the charts .Values.global.tls.type is “selfsigned”. In this case, a self-signed certificate is generated for use creating secure connections. Alternatively, .Values.global.tls.type can be set to “provided” and the TLS certificate (.Values.global.tls.cert), TLS private key (.Values.global.tls.key) and CA certificate (.Values.global.tls.cacert) can be specified to use an existing configuration. If a self-signed certificate is used, a certificate and key are generated for each installation of IBM Event Streams and stored securely within a Kubernetes secret. Clients can access the public key via any web browser in the usual manner.If the certificate is provided, you are responsible for provisioning this certificate, for ensuring it is trusted by the clients you will use and for protecting the key. Encryption of connections within IBM Event Streams Enhance your security by encrypting the internal communication between Event Streams pods by using TLS. Data Monitoring IBM Event Streams provides a range of monitoring features that users can exploit to gain a better understanding of how applications are performing. ","categories": ["security"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/security/gdpr-considerations/",
        "teaser":null},{
        "title": "About geo-replication",
        "collection": "2019.4",
        "excerpt":"You can deploy multiple instances of IBM Event Streams and use the included geo-replication feature to synchronize data between your clusters that are typically located in different geographical locations. The geo-replication feature creates copies of your selected topics to help with disaster recovery. Geo-replication can help with various service availability scenarios, for example:   Supporting your disaster recovery plans: you can set up geo-replication to support your disaster recovery architecture, enabling the switching to other clusters if your primary ones experience a problem.  Making mission-critical data safe: you might have mission-critical data that your applications depend on to provide services. Using the geo-replication feature, you can back up your topics to several destinations to ensure their safety and availability.  Migrating data: you can ensure your topic data can be moved to another deployment, for example, when switching from a test to a production environment.How it works The Kafka cluster where you have the topics that you want to make copies of is called the “origin cluster”. The Kafka cluster where you want to copy the selected topics to is called the “destination cluster”. So, one cluster is the origin where you want to copy the data from, while the other cluster is the destination where you want to copy the data to. Important: If you are using geo-replication for purposes of availability in the event of a data center outage or disaster, you must ensure that the origin cluster and destination cluster are installed on different systems that are isolated from each other. This ensures that any issues with the origin cluster do not affect the destination cluster. Any of your IBM Event Streams clusters can become destination for geo-replication. At the same time, the origin cluster can also be a destination for topics from other sources. Geo-replication not only copies the messages of a topic, but also copies the topic configuration, the topic’s metadata, its partitions, and even preserves the timestamps from the origin topic. After geo-replication starts, the topics are kept in sync. If you add a new partition to the origin topic, the geo-replicator adds a partition to the copy of the topic on the destination cluster to maintain the correct message order on the geo-replicated topic. This behavior continues even when geo-replication is paused. You can set up geo-replication by using the IBM Event Streams UI or CLI. When replication is set up and working, you can switch to another cluster when needed. What to replicate What topics you choose to replicate and how depend on the topic data, whether it is critical to your operations, and how you want to use it. For example, you might have transaction data for your customers in topics. Such information is critical to your operations to run reliably, so you want to ensure they have back-up copies to switch to when needed. For such critical data, you might consider setting up several copies to ensure availability. One way to do this is to set up geo-replication of 5 topics to one destination cluster, and the next 5 to another destination cluster, assuming you have 10 topics to replicate. Alternatively, you can replicate the same topics to two different destination clusters. Another example would be storing of website analytics information, such as where users clicked and how many times they did so. Such information is likely to be less important than maintaining availability for your operations, and you might choose not to replicate such topics, or only replicate them to one destination cluster. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/georeplication/about/",
        "teaser":null},{
        "title": "Planning for geo-replication",
        "collection": "2019.4",
        "excerpt":"Consider the following when planning for geo-replication:   If you want to use the CLI to set up geo-replication, ensure you have the IBM Event Streams CLI installed.  Prepare your destination cluster by setting the number of geo-replication workers.  Identify the topics you want to create copies of. This depends on the data stored in the topics, its use, and how critical it is to your operations.  Decide whether you want to include message history in the geo-replication, or only copy messages from the time of setting up geo-replication. By default, the message history is included in geo-replication. The amount of history is determined by the message retention option set when the topics were created on the origin cluster.  Decide whether the replicated topics on the destination cluster should have the same name as their corresponding topics on the origin cluster, or if a prefix should be added to the topic name. The prefix is the release name of the origin cluster. By default, the replicated topics on the destination cluster have the same name.Preparing destination clusters Before you can set up geo-replication and start replicating topics, you must configure the number of geo-replication workers on the destination cluster. The number of workers depend on the number of topics you want to replicate, and the throughput of the produced messages. You can use the same approach to determine the number as used when setting the number of brokers for your installation. For example, you can create a small number of workers at the time of installation. You can then increase the number later if you find that your geo-replication performance is not able to keep up with making copies of all the selected topics as required. Alternatively, you can start with a high number of workers, and then decrease the number if you find that the workers underperform. Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems. You can configure the number of workers at the time of installing IBM Event Streams, or you can modify an existing installation, even if you already have geo-replication set up and running on that installation. Configuring a new installation If you are installing a new IBM Event Streams instance for use as a destination cluster, you can specify the number of workers when configuring the installation. To configure the number of workers at the time of installation, use the UI or the CLI as follows. Using the UI You have the option to specify the number of workers during the installation process on the Configure page. Go to the Geo-replication section and specify the number of workers in the Geo-replicator workers field. Using the CLI You have the option to specify the number of workers during the installation process by adding the --set replicator.replicas=&lt;number-of-workers&gt; to your helm install command. Configuring an existing installation If you decide to use an existing IBM Event Streams instance as a destination cluster, or want to change the number of workers on an existing instance used as a destination cluster for scaling purposes, you can modify the number of workers by using the UI or CLI as follows. Using the UI To modify the number of workers by using the UI:   Go to where your destination cluster is installed. Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing IBM Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Go to the Geo-replication settings section and modify the Geo-replicator workers field to the required number of workers.Important: For high availability reasons, ensure you have at least 2 workers on your destination cluster in case one of the workers encounters problems.  Click Upgrade.Using the CLI To modify the number of workers by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Use the following helm command to modify the number of workers:helm upgrade --reuse-values --set replicator.replicas=&lt;number-of-workers&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.4.0.tgz --tls","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/georeplication/planning/",
        "teaser":null},{
        "title": "Setting up geo-replication",
        "collection": "2019.4",
        "excerpt":"You can set up geo-replication using the IBM Event Streams UI or CLI. You can then switch your applications to use another cluster when needed. Ensure you plan for geo-replication before setting it up. Defining destination clusters To be able to replicate topics, you must define destination clusters. The process involves logging in to your intended destination cluster and copying its connection details to the clipboard. You then log in to the origin cluster and use the connection details to point to the intended destination cluster and define it as a possible target for your geo-replication. Using the UI   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab, and click Generate connection information for this cluster.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click Add destination cluster on the Destination location tab.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Alternatively, you can also use the following steps:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click the I want this cluster to be able to receive topics from another cluster tile.  Copy the connection information to the clipboard. This information is what you need to specify the cluster as a destination for replication when you log in to your origin cluster. Note: This step generates an API key for your destination cluster that is then used by your origin cluster to authenticate it.  Log in to your origin IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right, and then go to the Geo-replication tab.  Click I want to replicate topics from this cluster to another cluster.  Paste the information you copied in step 4, wait for the validation of your payload to complete and click Connect cluster.The cluster is added as a destination to where you can replicate topics to.Note: When logged in to the destination cluster, the origin cluster will not show up in the Origin locations section until you define topics to replicate, and the geo-replicator is created.Using the CLI   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the IBM Event Streams CLI on the destination cluster: cloudctl es init  Run the following command to create an API key for your destination cluster:cloudctl es geo-cluster-apikey The command provides the API URL and the API key required for creating a destination cluster, for example:--api-address https://192.0.2.24:32046 --api-key H4C2S6Moq7KuDcYRJaM4Ye_6-XShEnB6JHnATaDaBFQZ  Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the IBM Event Streams CLI on the origin cluster: cloudctl es init  Run the following command to add the cluster as a destination to where you can replicate your topics to:cloudctl es geo-cluster-add --api-address &lt;api-url-from-step-3&gt; --api-key &lt;api-key-from-step-3&gt;Specifying what and where to replicate To select the topics you want to replicate and set the destination cluster to replicate to, use the following steps. Using the UI   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Choose a destination cluster to replicate to by clicking the name of the cluster from the Destination locations list.  Choose the topics you want to replicate by selecting the checkbox next to each, and click Geo-replicate to destination.Tip: You can also click the  icon in the topic’s row to add it to the destination cluster. The icon turns into a Remove button, and the topic is added to the list of topics that are geo-replicated to the destination cluster.  Optional: Select whether to add a prefix to the name of the new replicated topic that is created on the destination cluster. Click Add prefix to destination topic names to add the release name of the origin cluster as a prefix to the replicated topics.  Optional: Select whether you want to include the message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Click Include message history if you want to include history.  Click Create to create geo-replicators for the selected topics on the chosen destination cluster. Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.Note: After clicking Create, it might take up to 5 to 10 minutes before geo-replication becomes active.For each topic that has geo-replication set up, a visual indicator is shown in the topic’s row as follows:       If topics are being replicated from the cluster you are logged in to, the Geo-replication column displays the number of clusters the topic is being replicated to. Clicking the column for the topic expands the row to show details about the geo-replication for the topic. You can then click View to see more details about the geo-replicated topic in the side panel:         If topics are being replicated to the cluster you are logged in to, the Geo-replication column displays the number of clusters the topic is being replicated from.   Using the CLI To set up replication by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the IBM Event Streams CLI: cloudctl es init  Choose a destination cluster to replicate to by listing all available destination clusters, making the ID of the clusters available to select and copy: cloudctl es geo-clusters  Choose the topics you want to replicate by listing your topics, making their names available to select and copy: cloudctl es topics  Specify the destination cluster to replicate to, and set the topics you want to replicate. Use the required destination cluster ID and topic names retrieved in the previous steps. The command creates one replicator for each topic. To set up more that one geo-replicators at once, list each topic you want to replicate using a comma-separated list without spaces in between:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt;Geo-replication starts automatically when the geo-replicators for the selected topics are set up successfully.          Optional: You can specify to add a prefix to the name of the new replicated topic that is created on the destination cluster by using the --prefix &lt;prefix-name&gt; option.      Optional: Select whether you want to include message history in the replication, or if you only want to copy the messages from the time of setting up geo-replication. Set one of the following:                  Use the --from earliest option to include available message history in geo-replication. This means all available message data for the topic is copied.          Use the --from latest option to exclude available message history. This means that only message data from the time of setting up replication is copied.                    For example, to use all options to create the geo-replicators:cloudctl es geo-replicator-create --destination &lt;cluster-ID-from-step-3&gt; --topics &lt;comma-separated-list-of-topic-names-from-step-4&gt; --from &lt;earliest or latest&gt; --prefix &lt;topic-name-prefix&gt;For example:cloudctl es geo-replicator-create --destination DestinationClusterId --topics MyTopicName1,MyTopicName2 --from latest --prefix GeoReplica- When your geo-replication is set up, you can monitor and manage it. Switching clusters When one of your origin IBM Event Streams clusters experiences problems and goes down, you are notified on the destination cluster UI that the origin cluster is offline. You can switch your applications over to use the geo-replicated topics on the destination cluster as follows.   Log in to your destination IBM Event Streams cluster as an administrator.  Click Connect to this cluster on the right.  Go to the Connect a client tab, and use the information on the page to change your client application settings to use the geo-replicated topic on the destination cluster. You need the following information to do this:          Bootstrap server: Copy the Broker URL to connect an application to this topic.      Certificates: Download a certificate that is required by your Kafka clients to connect securely to this cluster.      API key: To connect securely to IBM Event Streams, your application needs an API key with permission to access the cluster and resources such as topics. Follow the instructions to generate an API key authorized to connect to the cluster, and select what level of access you want it to grant to your resources (topics). You can then select which topics you want included or to include all topics, and set consumer groups as well.      After the connection is configured, your client application can continue to operate using the geo-replicated topics on the destination cluster. Decide whether you want your client application to continue processing messages on the destination cluster from the point they reached on the topic on the origin cluster, or if you want your client application to start processing messages from the beginning of the topic.       To continue processing messages from the point they reached on the topic on the origin cluster, you can specify the offset for the consumer group that your client application is using:cloudctl es group-reset --group &lt;your-consumer-group-id&gt; --topic &lt;topic-name&gt; --mode datetime --value &lt;timestamp&gt;For example, the following command instructs the applications in consumer group consumer-group-1 to start consuming messages with timestamps from after midday on 28th September 2018:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode datetime --value 2018-09-28T12:00:00+00:00 --execute         To start processing messages from the beginning of the topic, you can use the --mode earliest option, for example:cloudctl es group-reset --group consumer-group-1 --topic GEOREPLICATED.TOPIC --mode earliest --execute   These methods also avoid the need to make code changes to your client application. ","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/georeplication/setting-up/",
        "teaser":null},{
        "title": "Monitoring and managing geo-replication",
        "collection": "2019.4",
        "excerpt":"When you have geo-replication set up, you can monitor and manage your geo-replication, such as checking the status of your geo-replicators, pausing and resuming the copying of data for each topic, removing replicated topics from destination clusters, and so on. From a destination cluster You can check the status of your geo-replication and manage geo-replicators (such as pause and resume) on your destination cluster. You can view the following information for geo-replication on a destination cluster:   The total number of origin clusters that have topics being replicated to the destination cluster you are logged into.  The total number of topics being geo-replicated to the destination cluster you are logged into.  Information about each origin cluster that has geo-replication set up on the destination cluster you are logged into:          The cluster name that includes the helm release name.      The health of the geo-replication for that origin cluster: Creating, Paused, Stopping, Assigning, Offline, and Error.      Number of topics replicated from each origin cluster.      Tip: As your cluster can be used as a destination for more than one origin cluster and their replicated topics, this information is useful to understand the status of all geo-replicators running on the cluster. Using the UI To view this information on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Origin locations tab for details.To manage geo-replication on the destination cluster by using the UI:   Log in to your destination IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Locate the name of the origin cluster for which you want to manage geo-replication for, and choose from one of the following options:           More options &gt; Pause running replicators: To pause geo-replication and suspend copying of data from the origin cluster.       More options &gt; Resume paused replicators: To resume geo-replication from the origin cluster.       More options &gt; Restart failed replicators: To restart geo-replication from the origin cluster for geo-replicators that experienced problems.       More options &gt; Stop replication: To stop geo-replication from the origin cluster.Important: Stopping replication also removes the origin cluster from the list.      Note: You cannot perform these actions on the destination cluster by using the CLI. From an origin cluster On the origin cluster, you can check the status of all of your destination clusters, and drill down into more detail about each destination. You can also manage geo-replicators (such as pause and resume), and remove entire destination clusters as a target for geo-replication. You can also add topics to geo-replicate. You can view the following high-level information for geo-replication on an origin cluster:   The name of each destination cluster.  The total number of topics being geo-replicated to all destination clusters from the origin cluster you are logged into.  The total number of workers running for the destination cluster you are geo-replicating topics to.You can view more detailed information about each destination cluster after they are set up and running like:   The topics that are being geo-replicated to the destination cluster.  The health status of the geo-replication on each destination cluster: Running, Resume, Resuming, Pausing, Removing, and Error. When the status is Error, the cause of the problem is also provided to aid resolution.Using the UI To view this information on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Click the Destination locations tab for details.To manage geo-replication on the origin cluster by using the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Locate the name of the destination cluster for which you want to manage geo-replication, and choose from one of the following options:           More options &gt; Pause running replicator: To pause a geo-replicator and suspend copying of data to the destination cluster.      Resume button: To resume a geo-replicator for the destination cluster.       More options &gt; Restart failed replicator: To restart a geo-replicator that experienced problems.       More options &gt; Remove replicator: To remove a geo-replicator from the destination cluster.      You can take the same actions for all of the geo-replicators in a destination cluster using the  More options menu in the top right when browsing  destination cluster details (for example, pausing all geo-replicators or removing the whole cluster as a destination). Using the CLI To view this information on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clusters  Retrieve information about a destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;For example:cloudctl es geo-cluster --destination siliconvalley_es_byl6xThe command returns the following information:    Details of destination cluster siliconvalley_es_byl6xCluster ID               Cluster name    REST API URL                 Skip SSL validation?destination_byl6x        destination     https://9.30.119.223:31764   trueGeo-replicator detailsName                               Status    Origin bootstrap servers   Origin topic   Destination topictopic1__to__origin_topic1_evzoo    RUNNING   192.0.2.24:32237           topic1         origin_topic1topic2__to__topic2_vdpr0           PAUSED    192.0.2.24:32237           topic2         topic2topic3__to__topic3_9jc71           ERROR     192.0.2.24:32237           topic3         topic3topic4__to__topic4_nk87o           PENDING   192.0.2.24:32237           topic4         topic4      To manage geo-replication on the origin cluster by using the CLI:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Run the following commands as required:          cloudctl es geo-replicator-pause --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-resume --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-restart --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      cloudctl es geo-replicator-delete --destination &lt;destination-cluster-id&gt; --name &lt;replicator-name&gt;      You can also remove a cluster as a destination using the following command:  cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt;  Note: If you are unable to remove a destination cluster due to technical issues, you can use the --force option with the geo-cluster-remove command to remove the cluster.      Tip: You can use short options instead of spelling out the long version. For example, use -d instead of --destination, or -n instead of --name. Restarting a geo-replicator with Error status Running geo-replicators constantly consume from origin clusters and produce to destination clusters. If the geo-replicator receives an error from Kafka that prevents it from continuing to produce or consume, such as an authentication error or all brokers being unavailable, it will stop replicating and report a status of Error. To restart a geo-replicator that has an Error status from the UI:   Log in to your origin IBM Event Streams cluster as an administrator.  Click Topics in the primary navigation and then click Geo-replication.  Locate the name of the destination cluster for the geo-replicator that has an Error status.  Locate the reason for the Error status under the entry for the geo-replicator.  Either fix the reported problem with the system or verify that the problem is no longer present.  Select  More options &gt; Restart failed replicator to restart the geo-replicator.","categories": ["georeplication"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/georeplication/health/",
        "teaser":null},{
        "title": "Event Streams producer API",
        "collection": "2019.4",
        "excerpt":"Event Streams provides a REST API to help connect your existing systems to your Event Streams Kafka cluster. Using the API, you can integrate Event Streams with any system that supports RESTful APIs. The REST producer API is a scalable REST interface for producing messages to Event Streams over a secure HTTP endpoint. Send event data to Event Streams, utilize Kafka technology to handle data feeds, and take advantage of Event Streams features to manage your data. Use the API to connect existing systems to Event Streams, such as IBM Z mainframe systems with IBM z/OS Connect, systems using IBM DataPower Gateway, and so on. About authorization Event Streams uses API keys to authorize writing to topics. For more information about API keys and associated service IDs, see the information about managing access. The REST producer API requires the API key to be provided with each REST call to grant access to the requested topic. This can be done in one of the following ways:   In an HTTP authorization header: You can use this method when you have control over what HTTP headers are sent with each call to the REST producer API. For example, this is the case when the API calls are made by code you control.  Embedded into an SSL client certificate (also referred to as SSL client authentication or SSL mutual authentication):You can use this method when you cannot control what HTTP headers are sent with each REST call. For example, this is the case when you are using third-party software or systems such as CICS events over HTTP.Note: You must have Event Streams version 2019.1.1 or later to use the REST API. In addition, you must have Event Streams version 2019.4.1 or later to use the REST API with SSL client authentication. Prerequisites To be able to produce to a topic, ensure you have the following available:   The URL of the Event Streams API endpoint, including the port number.  The topic you want to produce to.  The API key that gives permission to connect and produce to the selected topic.  The Event Streams certificate.  If using SSL client authentication, a client private key and certificate signed by Event Streams. See the information about creating an SSL client certificate later for details about how to create and sign an SSL client key and certificate.To retrieve the full URL for the Event Streams API endpoint, you can use the Event Streams CLI or UI. Using the CLI:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI: cloudctl es init.If you have more than one Event Streams instance installed, select the one where the topic you want to produce to is.Details of your Event Streams installation are displayed.  Copy the full URL of the desired endpoint, including the port number. If using HTTP authorization, use the Event Streams API endpoint field. For SSL client authentication, use the Event Streams SSL client auth endpoint field.Using the UI:   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Connect to this cluster on the right.  Go to the Resources tab.  Scroll down to the API endpoint section.  Click Copy API endpoint.To create a topic and generate an API key with produce permissions, and to download the certificate:   If you have not previously created the topic, create it now:cloudctl es topic-create --name &lt;topic_name&gt; --partitions 1 --replication-factor 3  Create a service ID and generate an API key:cloudctl es iam-service-id-create --name &lt;serviceId_name&gt; --role editor --topic &lt;topic_name&gt;For more information about roles, permissions, and service IDs, see the information about managing access.  Copy the API key returned by the previous command.  Download the server certificate for Event Streams:cloudctl es certificates --format pem By default, the certificate is written to a file called es-cert.pem.Key and message size limits The REST producer API has a configured limit for the key size (default is 4096 bytes) and the message size (default is 65536 bytes). If the request sent has a larger key or message size than the limits set, the request will be rejected.  Important: In Event Streams 2019.4.2, you can configure the key and message size limits at the time of installation or later as described in modifying installation settings. You can set the limit values in the REST producer API settings section if using the UI, or use the rest-producer.maxKeySize and rest-producer.maxMessageSize parameters if using the CLI. In Event Streams 2019.4.1 and earlier versions, you can update the limits as follows:   Ensure you have the Event Streams CLI installed.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  List the Event Streams deployments: kubectl get deployments  Identify the REST producer deployment in the list.It will be similar to &lt;deployment_name&gt;-ibm-es-rest-producer-deploy.  Edit the REST producer deployment:kubectl edit deployment &lt;deployment_name&gt;-ibm-es-rest-producer-deploy  In the .yaml file Locate the env section for the REST producer container under spec.template.spec.containers.  Add the following environment variables as required:          MAX_KEY_SIZE: Sets the maximum key size in bytes (default is 4096).      MAX_MESSAGE_SIZE: Sets the maximum message size in bytes (default is 65536).        Important: Do not set the MAX_MESSAGE_SIZE to a higher value than the maximum message size that can be received by the Kafka broker or the individual topic (max.message.bytes). By default, the maximum message size for Kafka brokers is 1000012 bytes. If the limit is set for an individual topic, then that setting overrides the broker setting. Any message larger than the maximum limit will be rejected by Kafka.     Note: Sending large requests to the REST producer increases latency, as it will take the REST producer longer to process the requests.     Save your changes and wait for the REST producer pod to be updated.Note: If you upgrade your Event Streams version, you will have to apply these environment variables again.Producing messages using REST with HTTP authorization Ensure you have gathered all the details required to use the producer API, as described in the prerequisites. You can use the usual languages for making the API call. For example, to use cURL to produce messages to a topic with the producer API, run the curl command as follows: curl -v -X POST -H \"Authorization: Bearer &lt;api_key&gt;\" -H \"Content-Type: text/plain\" -H \"Accept: application/json\" -d 'test message' --cacert es-cert.pem \"&lt;api_endpoint&gt;/topics/&lt;topic_name&gt;/records\" Where:   &lt;api_key&gt; is the API key you generated earlier.  &lt;api_endpoint&gt; is the full URL copied from the Event Streams API endpoint field earlier (format https://&lt;host&gt;:&lt;port&gt;).  &lt;topic_name&gt; is the name of the topic you want to produce messages to.For full details of the API, see the API reference. Producing messages using REST with SSL client authentication In cases where you cannot manipulate the HTTP headers being sent with each REST call, using an SSL client certificate instead allows the API key to be provided when the REST client opens a TLS connection to Event Streams. In this scenario, Event Streams acts as the signing authority for the client certificate, not only signing the certificate signing request, but also embedding the encrypted API key into the resulting certificate. This way, Event Streams can verify that it trusts the client system, and it can authorize access to the requested topic through the embedded API key. Before continuing with the instructions in the following sections, ensure you have gathered all the details required to use the producer API, as described in the prerequisites. Create an SSL client certificate Creating and deploying an SSL client certificate is a one-off procedure that you must do before any messages can be sent from the client system. Note: SSL security technology can be a complex topic and it is beyond the scope of this documentation to provide a full description of all its features and mechanisms. For an overview of SSL and TLS, see the IBM MQ documentation. The following is a simplified set of instructions based on the widely-used OpenSSL toolkit:       Create a private key and an associated certificate signing request (CSR): openssl req -new -newkey rsa:4096 -nodes -keyout es-client.key -out es-client.csr     This writes the private key to es-client.key and the CSR to es-client.csr in PEM format. You can provide any identifying information for the CSR subject, common name, or subject alternative names.     Note: Event Streams does not perform host name verification when it receives the certificate.     Java users can create CSRs using the keytool -certreq ... command as described in the IBM SDK, Java Technology Edition documentation. CICS users can use RACF by following the steps described in the CICS Transaction Server documentation.         Call the Event Streams CLI to sign the CSR and embed the API key. For example, you can do this by using the files created in the previous step and the API key created as mentioned in prerequisites earlier: cloudctl es sign-clientauth-csr --in es-client.csr --api-key &lt;api-key&gt; --out es-client.pem     This creates the client certificate in es-client.pem in PEM format. The sign-clientauth-csr CLI command expects and produces PEM files by default, but the DER format is also supported by using the --in-format and --out-format parameters.   Verify your client certificate You can test the validity of your client certificate and private key before configuring your client system by running the following wget command to produce a test mesage to the topic: wget -qSO- --content-on-error --certificate=&lt;client-cert&gt; --private-key=&lt;client-key&gt; --ca-cert=&lt;es-cert&gt; --header 'Content-Type: text/plain' --post-data='some test data' &lt;client-auth-endpoint&gt;/topics/&lt;topic-name&gt;/records Where:   &lt;client-cert&gt; is the SSL client certificate file in PEM format.  &lt;client-key&gt; is the private key file for the SSL client certificate in PEM format.  &lt;es-cert&gt; is the Event Streams server certificate file in PEM format.  &lt;client-auth-endpoint&gt; is the client authentication API endpoint.  &lt;topic-name&gt; is the topic to be written to.Produce messages with an SSL client certificate Consult the documentation for your system to understand how to specify the client certificate and private key for the outgoing REST calls to Event Streams. You also need to add the Event Streams server certificate to the list of trusted certificates used by your system (this is the certificate downloaded as part of the prerequisites). For example, the steps to configure a CICS URIMAP as an HTTP client is described in the  CICS Transaction Server documentation. In this case, load the client certificate and private key, together with the Event Streams server certificate into your RACF key ring. When defining the URIMAP:   Host is the client authentication API endpoint obtained as part of the prerequisites, without the leading https://  Path is /topics/&lt;topic-name&gt;/records  Certificate is the label given to the client certificate when it was loaded into the key ring.If you are using Java keystores, the client certificate can be imported by using the keytool -importcert ... command as described in the IBM SDK, Java Technology Edition documentation. Some systems require the client certificate and private key to be combined into one PKCS12 file (with extension .p12 or .pfx). For example, the following command creates a PKCS12 file from the client certificate and private key files created earlier:  openssl pkcs12 -export -inkey es-client.key -nodes -in es-client.pem -name my-client-cert -out es-client.p12 my-client-cert is an arbitrary name or alias given to the client certificate in the PKCS12 file. You will be asked to specify a password to protect the certificate in the file. You will need to provide this, as well as the alias, when configuring your system. For full details of the API, see the API reference. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/rest-api/",
        "teaser":null},{
        "title": "Kafka Connect and connectors",
        "collection": "2019.4",
        "excerpt":"You can integrate external systems with IBM Event Streams by using the Kafka Connect framework and connectors. What is Kafka Connect? When connecting Apache Kafka and other systems, the technology of choice is the Kafka Connect framework. Use Kafka Connect to reliably move large amounts of data between your Kafka cluster and external systems. For example, it can ingest data from sources such as databases and make the data available for stream processing.  Source and sink connectors Kafka Connect uses connectors for moving data into and out of Kafka. Source connectors import data from external systems into Kafka topics, and sink connectors export data from Kafka topics into external systems. A wide range of connectors exists, some of which are commercially supported. In addition, you can write your own connectors. A number of source and sink connectors are available to use with Event Streams. See the connector catalog section for more information.  Workers Kafka Connect connectors run inside a Java process called a worker. Kafka Connect can run in either standalone or distributed mode. Standalone mode is intended for testing and temporary connections between systems, and all work is performed in a single process. Distributed mode is more appropriate for production use, as it benefits from additional features such as automatic balancing of work, dynamic scaling up or down, and fault tolerance.  When you run Kafka Connect with a standalone worker, there are two configuration files:   The worker configuration file contains the properties needed to connect to Kafka. This is where you provide the details for connecting to Kafka.  The connector configuration file contains the properties needed for the connector. This is where you provide the details for connecting to the external system (for example, IBM MQ).When you run Kafka Connect with the distributed worker, you still use a worker configuration file but the connector configuration is supplied using a REST API. Refer to the Kafka Connect documentation for more details about the distributed worker. For getting started and problem diagnosis, the simplest setup is to run only one connector in each standalone worker. Kafka Connect workers print a lot of information and it’s easier to understand if the messages from multiple connectors are not interleaved. Connector catalog The connector catalog contains a list of connectors that have been verified with Event Streams. Connectors are either supported by the community or IBM. Community support means the connectors are supported through the community by the people that created them. IBM supported connectors are fully supported as part of the official Event Streams support entitlement. See the connector catalog for a list of connectors that work with Event Streams.  Setting up connectors Event Streams provides help with setting up your Kafka Connect environment, adding connectors to that environment, and starting the connectors. See the instructions about setting up and running connectors. Running connectors on IBM Cloud Private If you have IBM MQ or another service running on IBM Cloud Private, you can use Kafka Connect and one or more connectors to flow data between your instance of IBM Event Streams and the service on IBM Cloud Private. In this scenario it makes sense to run Kafka Connect in IBM Cloud Private as well. See the instructions about running Kafka Connect and connectors on IBM Cloud Private Connectors for IBM MQ Connectors are available for copying data between IBM MQ and Event Streams. There is a MQ source connector for copying data from IBM MQ into Event Streams or Apache Kafka, and a MQ sink connector for copying data from Event Streams or Apache Kafka into IBM MQ. For more information about MQ connectors, see the topic about connecting to IBM MQ. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/connectors/",
        "teaser":null},{
        "title": "Setting up and running connectors",
        "collection": "2019.4",
        "excerpt":"IBM Event Streams helps you set up a Kafka Connect environment, prepare the connection to other systems by adding connectors to the environment, and start Kafka Connect with the connectors to help integrate external systems. Log in to the Event Streams UI, and click Toolbox in the primary navigation. Scroll to the Connectors section and follow the guidance for each main task. You can also find additional help on this page. Setting up a Kafka Connect environment Set up the environment for hosting Kafka Connect. You can then use Kafka Connect to stream data between Event Streams and other systems. Kafka Connect can be run in standalone or distributed mode. For more details see the explanation of Kafka Connect workers. Kafka Connect includes shell and bash scripts for starting workers that take configuration files as arguments. For best results running Kafka Connect alongside Event Streams start Kafka Connect in distributed mode in Docker containers. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. To begin using Kafka Connect in distributed mode, follow the steps below, then add the connectors to your other systems and start Kafka Connect in its Docker container. Note: The Kafka Connect Docker container is designed for a Linux environment. Create topics When running in distributed mode Kafka Connect uses three topics to store configuration, current offsets and status. In standalone mode Kafka Connect uses a local file. Create the following topics:   connect-configs: This topic will store the connector and task configurations.  connect-offsets: This topic is used to store offsets for Kafka Connect.  connect-status: This topic will store status updates of connectors and tasks.Note: The topic names match the default settings. If you change these settings in your Kafka Connect properties file create topics that match the names you provided. Using the UI   Click Topics in the primary navigation.  Click Create topic.  Set Show all available options to On.  Create the three topics with the following parameters, leaving other parameters as default. Name, partitions, and replicas can be edited in Core configuration, and cleanup policy can be edited in Log:            Name      Partitions      Replicas      Cleanup policy                  connect-configs      1      3      compact                  Name      Partitions      Replicas      Cleanup policy                  connect-offsets      25      3      compact                  Name      Partitions      Replicas      Cleanup policy                  connect-status      5      3      compact      Using the CLI   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the following commands to create the topics:    cloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compactcloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compactcloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact      Provide or generate an API key This API key must provide permission to produce and consume messages for all topics, and also to create topics. This will enable Kafka Connect to securely connect to your IBM Event Streams cluster.   In the Event Streams UI, click Toolbox in the primary navigation. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  Go to step 4 and either paste an API key that provides permission to produce and consume messages, and also to create topics, or click Generate API key to create a new key.Download Kafka Connect On the same page, go to step 5, and click Download Kafka Connect ZIP to download the compressed file, then extract the contents to your preferred location. Adding connectors to your Kafka Connect environment Prepare Kafka Connect for connections to your other systems by adding the required connectors. If you are following on from the previous step, you can click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Add connectors on the Add connectors to your Kafka Connect environment tile. To run a particular connector Kafka Connect must have access to a JAR file or set of JAR files for the connector. The quickest way to do this is by adding the JAR file(s) to the classpath of Kafka Connect. This is not the recommended approach because it does not provide classpath isolation for different connectors. Since version 0.11.0.0 of Kafka the recommended approach is to configure the plugin.path in the Kafka Connect properties file to point to the location of your connector JAR(s). If you are using the provided Kafka Connect ZIP, the resulting Docker image will copy all connectors in the /connectors directory into the container on the plugin.path. Copy the connector JAR file(s) you want to have available into the /connectors directory: cp &lt;path_to_your_connector&gt;.jar &lt;extracted_zip&gt;/connectors Starting Kafka Connect with your connectors If you are following on from the previous step, you can click Next at the bottom of the page. You can also access this page by clicking Toolbox in the primary navigation, scrolling to the Connectors section, and clicking Start Kafka Connect on the Start Kafka Connect with your connectors tile. You can run Kafka Connect in standalone or distributed mode by using the connect-standalone.sh or connect-distributed.sh scripts that are included in the bin directory of a Kafka install. If using the provided Kafka Connect ZIP, Kafka Connect can be built and run using Docker commands as follows.   Build Kafka Connect Docker container. Go to the location where you extracted the Kafka Connect ZIP file you downloaded earlier as part of setting up your environment, and build the Kafka Connect Docker image:    cd kafkaconnectdocker build -t kafkaconnect:0.0.1 .        Run the Docker container:docker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 kafkaconnect:0.0.1  Verify that your chosen connectors are installed in your Kafka Connect environment:curl http://localhost:8083/connector-pluginsA list of connector plugins available is displayed.Starting a connector Start a connector by using the Kafka Connect REST API. When running in distributed mode connectors are started using a POST request against your running Kafka Connect. The endpoint requires a body that includes the configuration for the connector instance you want to start. Most connectors include examples in their documentation. The Event Streams UI and CLI provide additional assistance for connecting to IBM MQ. See the connecting MQ instructions for more details. For example, to create a FileStreamSource connector you can create a file with the following contents: {   \"name\": \"my-connector\",   \"config\": {      \"connector.class\": \"FileStreamSource\",      \"file\": \"config/connect-distributed.properties\",      \"topic\":\"kafka-config-topic\"   }}  Once you have created a JSON file with the configuration for your chosen connector start the connector using the REST API:    curl -X POST http://localhost:8083/connectors \\ -H \"Content-Type: application/json\"  \\ -d @&lt;config&gt;.json        View the status of a connector by using the Kafka Connect REST API:curl http://localhost:8083/connectors/&lt;connector_name&gt;/statusRepeat for each connector you want to start.For more information about the other REST API endpoints (such as pausing, restarting, and deleting connectors) see the Kafka Connect REST API documentation. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/setting-up-connectors/",
        "teaser":null},{
        "title": "Connecting to IBM MQ",
        "collection": "2019.4",
        "excerpt":"You can set up connections between IBM MQ and Apache Kafka or IBM Event Streams systems. Available connectors Connectors are available for copying data in both directions.   Kafka Connect source connector for IBM MQ: You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic.  Kafka Connect sink connector for IBM MQ: You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a MQ queue. Important: If you want to use IBM MQ connectors on IBM z/OS, you must prepare your setup first. If you have IBM MQ running on IBM Cloud Private, you can use the IBM MQ Connectors to connect your Event Streams to IBM MQ on IBM Cloud Private. See the instructions about running connectors on IBM Cloud Private. When to use Many organizations use both IBM MQ and Apache Kafka for their messaging needs. Although they’re generally used to solve different kinds of messaging problems, users often want to connect them together for various reasons. For example, IBM MQ can be integrated with systems of record while Apache Kafka is commonly used for streaming events from web applications. The ability to connect the two systems together enables scenarios in which these two environments intersect. Note: You can use an existing IBM MQ or Kafka installation, either locally or on the cloud. For performance reasons, it is recommended to run the Kafka Connect worker close to the queue manager to minimize the effect of network latency. For example, if you have a queue manager in your datacenter and Kafka in the cloud, it’s best to run the Kafka Connect worker in your datacenter. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/mq/",
        "teaser":null},{
        "title": "Running the MQ source connector",
        "collection": "2019.4",
        "excerpt":"You can use the MQ source connector to copy data from IBM MQ into IBM Event Streams or Apache Kafka. The connector copies messages from a source MQ queue to a target Kafka topic. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in a Docker container. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSOURCE, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSOURCE)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSOURCE) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and get messages from a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. For distributed mode the configuration is in JSON format and in standalone mode it is a .properties file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the source IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.  The name of the target Kafka topic.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click on the Download MQ Source Configuration, this will display another window.  Use the relevant fields to alter the configuration of the MQ Source connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to download a .json or .properties file which can be used in distributed or standalone mode.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-source command to generate the configuration file for the MQ Source connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSOURCE and connecting to the topic TSOURCE, run the following command:    cloudctl es connector-config-mq-source --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSOURCE\" --topic=\"TSOURCE\" --file=\"mq-source\" --json        Note: Omitting the --json flag will generate a mq-source.properties file which can be used for standalone mode.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: {\t\"name\": \"mq-source\",\t\"config\": {\t\t\"connector.class\": \"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",\t\t\"tasks.max\": \"1\",\t\t\"topic\": \"TSOURCE\",\t\t\"mq.queue.manager\": \"QM1\",\t\t\"mq.connection.name.list\": \"localhost(1414)\",\t\t\"mq.channel.name\": \"MYSVRCONN\",\t\t\"mq.queue\": \"MYQSOURCE\",\t\t\"mq.user.name\": \"alice\",\t\t\"mq.password\": \"passw0rd\",\t\t\"mq.record.builder\": \"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",\t\t\"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\"\t}}A list of all the possible flags can be found by running the command cloudctl es connector-config-mq-source --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Source connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Source tab is selected and click on the Download MQ Source JAR, this will download the MQ Source JAR file.Configuring Kafka Connect IBM Event Streams provides help with getting a Kafka Connect Environment. Follow the steps in set up Kafka Connect to get Kafka Connect running. When adding connectors add the MQ connector you downloaded earlier. Verify that the MQ source connector is available in your Kafka Connect environment:\\ $ curl http://localhost:8083/connector-plugins[{\"class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",\"type\":\"source\",\"version\":\"1.1.0\"}]Verify that the connector is running. For example, If you started a connector called mq-source:\\ $ curl http://localhost:8083/connectors[mq-source]Verify the log output of Kafka Connect includes the following messages that indicate the connector task has started and successfully connected to IBM MQ:  INFO Created connector mq-source INFO Connection to MQ establishedSend a test message   To add messages to the IBM MQ queue, run the amqsput sample and type in some messages:/opt/mqm/samp/bin/amqsput &lt;queue_name&gt; &lt;queue_manager_name&gt;  Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation and select the connected topic. Messages will appear in the message browser of that topic.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/mq/source/",
        "teaser":null},{
        "title": "Running the MQ sink connector",
        "collection": "2019.4",
        "excerpt":"You can use the MQ sink connector to copy data from IBM Event Streams or Apache Kafka into IBM MQ. The connector copies messages from a Kafka topic into a target MQ queue. Kafka Connect can be run in standalone or distributed mode. This document contains steps for running the connector in distributed mode in a Docker container. In this mode, work balancing is automatic, scaling is dynamic, and tasks and data are fault-tolerant. For more details on the difference between standalone and distributed mode see the explanation of Kafka Connect workers. Prerequisites The connector runs inside the Kafka Connect runtime, which is part of the Apache Kafka distribution. IBM Event Streams does not run connectors as part of its deployment, so you need an Apache Kafka distribution to get the Kafka Connect runtime environment. Ensure you have the following available:   IBM MQ v8 or later installed. Note: These instructions are for IBM MQ v9 running on Linux. If you’re using a different version or platform, you might have to adjust some steps slightly.Setting up the queue manager These sample instructions set up an IBM MQ queue manager that uses its local operating system to authenticate the user ID and password. The user ID and password you provide must already be created on the operating system where IBM MQ is running.   Log in as a user authorized to administer IBM MQ, and ensure the MQ commands are on the path.  Create a queue manager with a TCP/IP listener on port 1414:crtmqm -p 1414 &lt;queue_manager_name&gt;for example to create a queue manager called QM1 use crtmqm -p 1414 QM1  Start the queue manager:strmqm &lt;queue_manager_name&gt;  Start the runmqsc tool to configure the queue manager:runmqsc &lt;queue_manager_name&gt;  In runmqsc, create a server-connection channel:DEFINE CHANNEL(&lt;channel_name&gt;) CHLTYPE(SVRCONN)  Set the channel authentication rules to accept connections requiring userid and password:          SET CHLAUTH(&lt;channel_name&gt;) TYPE(BLOCKUSER) USERLIST('nobody')      SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)      SET CHLAUTH(&lt;channel_name&gt;) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)        Set the identity of the client connections based on the supplied context (the user ID):ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)  Refresh the connection authentication information:REFRESH SECURITY TYPE(CONNAUTH)  Create a queue for the Kafka Connect connector to use:DEFINE QLOCAL(&lt;queue_name&gt;)  Authorize the IBM MQ user ID to connect to and inquire the queue manager:   SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('&lt;user_id&gt;') AUTHADD(CONNECT,INQ)  Authorize the IBM MQ user ID to use the queue:   SET AUTHREC PROFILE(&lt;queue_name&gt;) OBJTYPE(QUEUE) PRINCIPAL('&lt;user_id&gt;') AUTHADD(ALLMQI)  Stop the runmqsc tool by typing END.For example, for a queue manager called QM1, with user ID alice, creating a server-connection channel called MYSVRCONN and a queue called MYQSINK, you run the following commands in runmqsc: DEFINE CHANNEL(MYSVRCONN) CHLTYPE(SVRCONN)SET CHLAUTH(MYSVRCONN) TYPE(BLOCKUSER) USERLIST('nobody')SET CHLAUTH('*') TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(NOACCESS)SET CHLAUTH(MYSVRCONN) TYPE(ADDRESSMAP) ADDRESS('*') USERSRC(CHANNEL) CHCKCLNT(REQUIRED)ALTER AUTHINFO(SYSTEM.DEFAULT.AUTHINFO.IDPWOS) AUTHTYPE(IDPWOS) ADOPTCTX(YES)REFRESH SECURITY TYPE(CONNAUTH)DEFINE QLOCAL(MYQSINK)SET AUTHREC OBJTYPE(QMGR) PRINCIPAL('alice') AUTHADD(CONNECT,INQ)SET AUTHREC PROFILE(MYQSINK) OBJTYPE(QUEUE) PRINCIPAL('alice') AUTHADD(ALLMQI)ENDThe queue manager is now ready to accept connection from the connector and put messages on a queue. Configuring the connector to connect to MQ The connector requires details to connect to IBM MQ and to your IBM Event Streams or Apache Kafka cluster. You can generate the sample connector configuration file for Event Streams from either the UI or the CLI. For distributed mode the configuration is in JSON format and in standalone mode it is a .properties file. The connector connects to IBM MQ using a client connection. You must provide the following connection information for your queue manager:   Comma-separated list of Kafka topics to pull events from.  The name of the IBM MQ queue manager.  The connection name (one or more host and port pairs).  The channel name.  The name of the sink IBM MQ queue.  The user name and password if the queue manager is configured to require them for client connections.Using the UI Use the UI to download a .json file which can be used in distributed mode.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click on the Download MQ Sink Configuration, this will display another window.  Use the relevant fields to alter the configuration of the MQ Sink connector.  Click Download to generate and download the configuration file with the supplied fields.  Open the downloaded configuration file and change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.Using the CLI Use the CLI to download a .json or .properties file which can be used in distributed or standalone mode.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI on the cluster:cloudctl es init  Run the connector-config-mq-sink command to generate the configuration file for the MQ Sink connector.For example, to generate a configuration file for an instance of MQ with the following information: a queue manager called QM1, with a connection point of localhost(1414), a channel name of MYSVRCONN, a queue of MYQSINK and connecting to the topics TSINK, run the following command:    cloudctl es connector-config-mq-sink --mq-queue-manager=\"QM1\" --mq-connection-name-list=\"localhost(1414)\" --mq-channel=\"MYSVRCONN\" --mq-queue=\"MYQSINK\" --topics=\"TSINK\" --file=\"mq-sink\" --json        Note: Omitting the --json flag will generate a mq-sink.properties file which can be used for standalone mode.     Change the values of mq.user.name and mq.password to the username and password that you used to configure your instance of MQ.The final configuration file will resemble the following: {\t\"name\": \"mq-sink\",\t\"config\": {\t\t\"connector.class\": \"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\t\t\"tasks.max\": \"1\",\t\t\"topics\": \"TSINK\",\t\t\"mq.queue.manager\": \"QM1\",\t\t\"mq.connection.name.list\": \"localhost(1414)\",\t\t\"mq.channel.name\": \"MYSVRCONN\",\t\t\"mq.queue\": \"MYQSINK\",\t\t\"mq.user.name\": \"alice\",\t\t\"mq.password\": \"passw0rd\",\t\t\"mq.message.builder\": \"com.ibm.eventstreams.connect.mqsink.builders.DefaultMessageBuilder\",\t\t\"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\t\t\"value.converter\": \"org.apache.kafka.connect.storage.StringConverter\"\t}}A list of all the possible flags can be found by running the command cloudctl es connector-config-mq-sink --help. Alternatively, See the sample properties file for a full list of properties you can configure, and also see the GitHub README for all available configuration options. Downloading the MQ Sink connector   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation and scroll to the Connectors section.  Go to the Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ?  Ensure the MQ Sink tab is selected and click on the Download MQ Sink JAR, this will download the MQ Sink JAR file.Configuring Kafka Connect IBM Event Streams provides help with getting a Kafka Connect Environment. Follow the steps in set up Kafka Connect to get Kafka Connect running. When adding connectors add the MQ connector you downloaded earlier. Verify that the MQ sink connector is available in your Kafka Connect environment:\\ $ curl http://localhost:8083/connector-plugins[{\"class\":\"com.ibm.eventstreams.connect.mqsink.MQSinkConnector\",\"type\":\"sink\",\"version\":\"1.1.0\"}]Verify that the connector is running. For example, If you started a connector called mq-sink:\\ $ curl http://localhost:8083/connectors[mq-sink]Verify the log output of Kafka Connect includes the following messages that indicate the connector has started and successfully connected to IBM MQ:  INFO Created connector mq-sink INFO Connection to MQ establishedSend a test message To test the connector you will need an application to produce events to your topic.   Log in to the Event Streams UI from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Toolbox in the primary navigation.  Go to the Generate starter application tile under Applications, and click Find out more.  Click Configure and generate application.  Enter a name for the application.  Select only Produce messages.  Go to the Existing topic tab and select the topic you provided in the MQ connector configuration.  Click Generate starter application.  Once the application has been generated, click Download and follow the instructions in the UI to get the application runningVerify the message is on the queue:   Navigate to the UI of the sample application you generated earlier and start producing messages to IBM Event Streams.  Use the amqsget sample to get messages from the MQ Queue:/opt/mqm/samp/bin/amqsget &lt;queue_name&gt; &lt;queue_manager_name&gt;After a short delay, the messages are printed.Advanced configuration For more details about the connector and to see all configuration options, see the GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/mq/sink/",
        "teaser":null},{
        "title": "Running connectors on IBM z/OS",
        "collection": "2019.4",
        "excerpt":"You can use the IBM MQ connectors to connect into IBM MQ for z/OS, and you can run the connectors on z/OS as well, connecting into the queue manager using bindings mode. These instructions explain how to run Kafka Connect in both standalone and distributed mode. For more information and to help decide which mode to use see the explanation of Kafka Connect workers. Before you can run IBM MQ connectors on IBM z/OS, you must prepare your Kafka files and your system as follows. Setting up Kafka to run on IBM z/OS You can run Kafka Connect workers on IBM z/OS Unix System Services. To do so, you must ensure that the Kafka Connect shell scripts and the Kafka Connect configuration files are converted to EBCDIC encoding. Download the Kafka Connect files Download Apache Kafka to a non-z/OS system to retrieve the .tar file that includes the Kafka Connect shell scripts and JAR files. To download Kafka Connect and make it available to your z/OS system:   Log in to a system that is not running IBM z/OS, for example, a Linux system.  Download Apache Kafka 2.0.0 or later to the system. IBM Event Streams provides support for Kafka Connect if you are using a Kafka version listed in the Kafka version shipped column of the Support matrix.  Extract the downloaded .tgz file, for example:gunzip -k kafka_2.11-2.3.0tgz  Copy the resulting .tar file to a directory on the z/OS Unix System Services.Download IBM MQ connectors and configuration Depending on the connector you want to use:\\   Download the source connector JAR and source configuration file  Download the sink connector JAR and configuration fileIf you want to run a standalone Kafka Connect worker you need a .properties file. To run a distributed Kafka Connect worker you need a .json file. Copy the connector JAR file(s) and the required configuration file to a directory on the z/OS Unix System Services. Convert the files If you want to run a standalone Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-standalone.sh  config/connect-standalone.properties  mq-source.properties or mq-sink.propertiesIf you want to run a distributed Kafka Connect worker, convert the following shell scripts and configuration files from ISO8859-1 to EBCDIC encoding:   bin/kafka-run-class.sh  bin/connect-distributed.sh  config/connect-distributed.shExtract the Apache Kafka distribution:   Log in to the IBM z/OS system and access the Unix System Services.  Change to an empty directory that you want to use for the Apache Kafka distribution, and copy the .tar file to the new directory.  Extract the .tar file, for example:tar -xvf kafka_2.11-2.0.0.tar  Change to the resulting kafka_&lt;version&gt; directory.Convert the shell scripts:   Copy the connect-standalone.sh shell script (or connect-distributed.sh for a distributed setup) into the current directory, for example:cp bin/connect-standalone.sh ./connect-standalone.sh.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.sh.orig &gt; bin/connect-standalone.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/connect-standalone.sh  Copy the kafka-run-class.sh shell script into the current directory, for example:cp bin/kafka-run-class.sh ./kafka-run-class.sh.orig  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./kafka-run-class.sh.orig &gt; bin/kafka-run-class.sh  Ensure the file permissions are set so that the script is executable, for example:chmod +x bin/kafka-run-class.shConvert the configuration files:   Copy the connect-standalone.properties file (or connect-distributed.properties for a distributed setup) into the current directory, for example:cp config/connect-standalone.properties ./connect-standalone.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./connect-standalone.properties.orig &gt; config/connect-standalone.propertiesIf running in standalone mode:   Copy the MQ .properties file into the current directory, for example:cp ./mq-source.properties ./mq-source.properties.orig  Determine the codeset on the IBM z/OS system by running:locale -k codeset  Convert the script to EBCDIC encoding and replace the original, for example for codeset IBM-1047:iconv -f ISO8859-1 -t IBM-1047 ./mq-source.properties.orig &gt; ./mq-source.propertiesNote: For distributed mode the .json file must remain in ASCII format. Update the Kafka Connect configuration The connect-standalone.properties (or connect-distributed.properties for distributed mode) file must include the correct bootstrap.servers and SASL/SSL configuration for your Apache Kafka or Event Streams install. For example if running against Event Streams download the certificate for your install to your IBM z/OS system. Generate an API key that can produce, consume and create topics and update the connect-standalone.properties (or connect-distributed.properties) file to include: bootstrap.servers=&lt;bootstrapServers&gt;security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/opt/kafka/es-cert.jksssl.truststore.password=passwordsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";producer.security.protocol=SASL_SSLproducer.ssl.protocol=TLSv1.2producer.ssl.truststore.location=/opt/kafka/es-cert.jksproducer.ssl.truststore.password=passwordproducer.sasl.mechanism=PLAINproducer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";consumer.security.protocol=SASL_SSLconsumer.ssl.protocol=TLSv1.2consumer.ssl.truststore.location=/opt/kafka/es-cert.jksconsumer.ssl.truststore.password=passwordconsumer.sasl.mechanism=PLAINconsumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;apiKey&gt;\";plugin.path=/opt/connectorsConfiguring the environment The IBM MQ connectors use the JMS API to connect to MQ. You must set the environment variables required for JMS applications before running the connectors on IBM z/OS. Ensure you set CLASSPATH to include com.ibm.mq.allclient.jar, and also set the JAR file for the connector you are using - this is the connector JAR file you downloaded from the Event Streams UI or built after cloning the GitHub project, for example, kafka-connect-mq-source-1.1.0-jar-with-dependencies.jar. As you are using the bindings connection mode for the connector to connect to the queue manager, also set the following environment variables:   The STEPLIB used at run time must contain the IBM MQ SCSQAUTH and SCSQANLE libraries. Specify this library in the startup JCL, or specify it by using the .profile file.From UNIX and Linux System Services, you can add these using a line in your .profile file as shown in the following code snippet, replacing thlqual with the high-level data set qualifier that you chose when installing IBM MQ:    export STEPLIB=thlqual.SCSQAUTH:thlqual.SCSQANLE:$STEPLIB        The connector needs to load a native library. Set LIBPATH to include the following directory of your MQ installation:    &lt;path_to_MQ_installation&gt;/mqm/&lt;MQ_version&gt;/java/lib      The bindings connection mode is a configuration option for the connector as described in the source connector GitHub README and in the sink connector GitHub README. Starting Kafka Connect on z/OS Kafka Connect is started using a bash script. If you do not already have bash installed on your z/OS system install it now. To install bash version 4.2.53 or later:   Download the bash archive file from Bash Version 4.2.53  Extract the archive file to get the .tar file: gzip -d bash.tar.gz  FTP the .tar file to your z/OS USS directory such as /bin  Extract the .tar file to install bash:tar -cvfo bash.tarIf bash on your z/OS system is not in /bin you need to update the kafka-run-class.sh file. For example, if bash is located in /usr/local/bin update the first line of kafka-run-class.sh to have #!/usr/local/bin/bash Starting Kafka Connect in standalone mode To start Kafka Connect in standalone mode navigate to your Kafka directory and run the connect-standalone.sh script, passing in your connect-standalone.properties and mq-source.properties or mq-sink.properties. For example: cd kafka./bin/connect-standalone.sh connect-standalone.properties mq-source.propertiesFor more details on creating the properties files see the connecting MQ documentation. Make sure connection type is set to bindings mode. Starting Kafka Connect in distributed mode To start Kafka Connect in distributed mode navigate to your Kafka directory and run the connect-distributed.sh script, passing in your connect-distributed.properties. Unlike in standalone mode, MQ properties are not passed in on startup. For example: cd kafka./bin/connect-distributed.sh connect-distributed.propertiesTo start an individual connector use the Kafka Connect REST API. For example, given a configuration file mq-source.json with the following contents: {    \"name\":\"mq-source\",        \"config\" : {            \"connector.class\":\"com.ibm.eventstreams.connect.mqsource.MQSourceConnector\",            \"tasks.max\":\"1\",            \"mq.queue.manager\":\"QM1\",            \"mq.connection.mode\":\"bindings\",            \"mq.queue\":\"MYQSOURCE\",            \"mq.record.builder\":\"com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\",            \"topic\":\"test\",            \"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",            \"value.converter\":\"org.apache.kafka.connect.converters.ByteArrayConverter\"        }    }start the connector using: curl -X POST http://localhost:8083/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonAdvanced configuration For more details about the connectors and to see all configuration options, see the source connector GitHub README or sink connector GitHub README. ","categories": ["connecting/mq"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/mq/zos/",
        "teaser":null},{
        "title": "Running connectors on IBM Cloud Private",
        "collection": "2019.4",
        "excerpt":"If you have IBM MQ or another service running on IBM Cloud Private, you can use Kafka Connect and one or more connectors to flow data between your instance of IBM Event Streams and the service on IBM Cloud Private. In this scenario it makes sense to run Kafka Connect in IBM Cloud Private as well. Downloading connectors The connector catalog contains a list of connectors that have been verified with Event Streams. Go to the connector catalog and download the JAR file(s) for any connectors you want to use. The JAR files for the IBM MQ source and sink connectors can be downloaded from the IBM Event Streams UI. Log in to your IBM Event Streams UI, click Toolbox in the primary navigation, and go to Add connectors to your Kafka Connect environment tile and click Connecting to IBM MQ? Building a Kafka Connect Docker image The Event Streams UI provides a toolbox page to help you get started with Kafka Connect. This provides a Dockerfile that builds a custom Kafka Connect with the Connectors you include. If you do not already have the Dockerfile, follow the steps to download the Kafka Connect ZIP and build a Docker image.   In the Event Streams UI, click Toolbox in the primary navigation. Scroll to the Connectors section.  Go to the Set up a Kafka Connect environment tile, and click Set up.  If you have not already done so follow the instructions to create three topics for Kafka Connect to use.  You need to provide an API key for Kafka Connect that has permission to Produce, Consume and create Topics. Paste in your API key, or click the button to generate one.NOTE: You must have a cluster admin role to generate an API key.  Click Download Kafka Connect ZIP to download the zip.  Extract the contents of the Kafka Connect .zip file to a local directory.  Copy the connector JAR files you downloaded earlier into the connectors folder in the extracted .zip foldercp &lt;path_to_your_connector&gt;.jar &lt;extracted_zip&gt;/connectors  Build the container: docker build -t kafkaconnect:0.0.1 .Uploading the Kafka Connect container To make the Kafka Connect container available on IBM Cloud Private it needs to be pushed to your IBM Cloud Private container registry.   Set up your Kubernetes command-line tool kubectl to access your IBM Cloud Private instance, for example, by running cloudctl login.  Create a namespace to deploy the Kafka Connect workers to: kubectl create namespace &lt;namespace&gt;  Log in to the Docker private image registry:    cloudctl login -a https://&lt;cluster_CA_domain&gt;:8443docker login &lt;cluster_CA_domain&gt;:8500        For more information, see the IBM Cloud Private documentation.     Retag and push the Docker image as follows:    docker tag kafkaconnect:0.0.1 &lt;cluster_CA_domain&gt;:8500/&lt;namespace&gt;/kafkaconnect:0.0.1docker push &lt;cluster_CA_domain&gt;:8500/&lt;namespace&gt;/kafkaconnect:0.0.1        Check this has worked by logging into your IBM Cloud Private UI and clicking on Container Images in the menu.Note: The namespace you provide is the one you will run the Kafka Connect workers in. Creating a Secret resource for the Kafka Connect configuration To enable updates to the Kafka Connect configuration the running container will need access to a Kubernetes resource containing the contents of connect-distributed.properties. The file is included in the extracted ZIP for Kafka Connect from the Event Streams UI. This file includes API keys so create a Secret: kubectl -n &lt;namespace&gt; create secret generic connect-distributed-config --from-file=&lt;extracted_zip&gt;/config/connect-distributed.propertiesCreating a ConfigMap resource for the Kafka Connect log4j configuration To enable updates to the Kafka Connect logging configuration create a ConfigMap with the contents of connect-log4j.properties. The file is included in the extracted ZIP for Kafka Connect from the Event Streams UI: kubectl -n &lt;namespace&gt; create configmap connect-log4j-config --from-file=&lt;extracted_zip&gt;/config/connect-log4j.propertiesCreating the Kafka Connect deployment To create the Kafka Connect deployment first create a yaml file called kafka-connect.yaml with the following contents: (Replace &lt;namespace&gt; with your IBM Cloud Private namespace) # Deployment  apiVersion: apps/v1  kind: Deployment  metadata:    name: kafkaconnect-deploy    labels:      app: kafkaconnect  spec:    replicas: 1    selector:      matchLabels:        app: kafkaconnect    template:      metadata:        namespace: &lt;namespace&gt;        labels:          app: kafkaconnect      spec:        securityContext:          runAsNonRoot: true          runAsUser: 5000        containers:          - name: kafkaconnect-container            image: kafkaconnect:0.0.1            readinessProbe:              httpGet:                path: /                port: 8083            livenessProbe:              httpGet:                path: /                port: 8083            ports:            - containerPort: 8083            volumeMounts:            - name: connect-config              mountPath: /opt/kafka/config/connect-distributed.properties              subPath: connect-distributed.properties            - name: connect-log4j              mountPath: /opt/kafka/config/connect-log4j.properties              subPath: connect-log4j.properties        volumes:        - name: connect-config          secret:            secretName: connect-distributed-config        - name: connect-log4j          configMap:            name: connect-log4j-config  ---  # Service  apiVersion: v1  kind: Service  metadata:    name: kafkaconnect-service    labels:      app: kafkaconnect  spec:    type: NodePort    ports:      - name: kafkaconnect        protocol: TCP        port: 8083    selector:        app: kafkaconnect  ---  # OpenShift Route  apiVersion: route.openshift.io/v1  kind: Route  metadata:    name: kafkaconnect-route    labels:      app: kafkaconnect  spec:    to:      kind: Service      name: kafkaconnect-service    port:      targetPort: kafkaconnect    wildcardPolicy: NoneNote: You only require the # OpenShift Route section if you are using the OpenShift Container Platform. This defines the deployment that will run Kafka Connect and the service used to access it. Create the deployment and service using: kubectl -n &lt;namespace&gt; apply -f kafka-connect.yaml Use kubectl -n &lt;namespace&gt; get service kafkaconnect-service to view your running services. The port mapping shows 8083 being mapped to an external port. Use the external port to verify the IBM MQ Connectors you included have been installed: curl http://&lt;serviceIP&gt;:&lt;servicePort&gt;/connector-plugins Running a connector To start a Connector instance, you need to create a JSON file with the connector configuration. Most connectors will have an example in their documentation. For the IBM MQ connectors this file can be generated in the Event Streams UI or CLI. See connecting to IBM MQ for more details. Once you have a JSON file use the /connectors endpoint to start the connector: curl -X POST http://&lt;serviceIP&gt;:&lt;servicePort&gt;/connectors -H \"Content-Type: application/json\" -d @mq-source.jsonFor more information about the other REST API endpoints (such as pausing, restarting, and deleting connectors) see the Kafka Connect REST API documentation. ","categories": ["connecting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/connecting/icp/",
        "teaser":null},{
        "title": "Monitoring deployment health",
        "collection": "2019.4",
        "excerpt":"Understand the health of your IBM Event Streams deployment at a glance, and learn how to find information about problems. Using the UI The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health. If there are no issues, the message states System is healthy. If any of the IBM Event Streams resources experience problems, the message states component isn’t ready. To find out more about the problem:   Click the message to expand it, and then expand the section for the component that does not have a green tick next to it.  Click the Pod is not ready link to open more details about the problem. The link opens the IBM Cloud Private UI. Log in as an administrator.  To understand why the IBM Event Streams resource is not available, click the Events tab to view details about the cause of the problem.  For more detailed information about the problem, click the Overview tab, and click  More options &gt; View logs on the right in the Pod details panel.  For guidance on resolving common problems that might occur, see the troubleshooting section.Using the CLI You can check the health of your IBM Event Streams environment using the Kubernetes CLI.   Ensure you have the Kubernetes command line tool installed, and configure access to your cluster.  To check the status and readiness of the pods, run the following command, where &lt;namespace&gt; is the space used for your IBM Event Streams installation:kubectl -n &lt;namespace&gt; get podsThe command lists the pods together with simple status information for each pod.  To retrieve further details about the pods, including events affecting them, use the following command:kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;  To retrieve detailed log data for a pod to help analyze problems, use the following command:kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; -c &lt;container_name&gt;For more information about using the kubectl command for debugging, see the Kubernetes documentation. Note: After a component restarts, the kubectl command retrieves the logs for the new instance of the container. To retrieve the logs for a previous instance of the container, add the –previous option to the kubectl logs command. Tip: You can also use the management logging service, or Elastic Stack, deployed by IBM Cloud Private to find more log information. Setting up the built-in Elastic Stack is part of the installation planning tasks. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/deployment-health/",
        "teaser":null},{
        "title": "Monitoring Kafka cluster health",
        "collection": "2019.4",
        "excerpt":"Monitoring the health of your Kafka cluster ensures your operations run smoothly. Event Streams collects metrics from all of the Kafka brokers and exports them to a Prometheus-based monitoring platform. The metrics are useful indicators of the health of the cluster, and can provide warnings of potential problems. You can use the metrics as follows:   View a selection of metrics on a preconfigured dashboard in the Event Streams UI.      Create dashboards in the Grafana service that is provided in IBM Cloud Private, and use the dashboards to monitor your Event Streams instance, including Kafka health and performance details. You can create the dashboards in the IBM Cloud Private monitoring service by selecting to Export the Event Streams dashboards when configuring your Event Streams installation.     For more information about the monitoring capabilities provided in IBM Cloud Private, including Grafana, see the IBM Cloud Private documentation.     To install the configured Grafana dashboards, follow these steps:           Download the dashboards you want to install from GitHub.      Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Navigate to the IBM Cloud Private console homepage.      Click the hamburger icon in the top left.      Expand Platform.      Click  Monitoring to navigate to the Grafana homepage.      On the Grafana homepage, click the Home icon in the top left to view all pre-installed dashboards.      Click Import Dashboards, and either paste the JSON of the dashboard you want to install or import the dashboard’s JSON file that you downloaded in step 1.      Navigate to the Grafana homepage again and click the Home icon, then find the dashboard you have installed to view it.        Ensure you select your namespace, release name, and other filters at the top of the dashboard to view the required information.     Create alerts so that metrics that meet predefined criteria are used to send notifications to emails, Slack, PagerDuty, and so on. For an example of how to use the metrics to trigger alert notifications, see how you can set up notifications to Slack.      Create dashboards in the Kibana service that is provided in IBM Cloud Private. You can download example Kibana dashboards for Event Streams from GitHub, and use the dashboards to monitor for specific errors in the logs and set up alerts for when a number of errors occur over a period of time in your Event Streams instance.     For more information about the logging capabilities provided in IBM Cloud Private, including Kibana, see the IBM Cloud Private documentation.     To download the preconfigured Kibana Dashboards, follow these steps:           Download Event Streams Kibana Dashboard.json from GitHub      Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;. The master host and port for your cluster are set during the installation of IBM Cloud Private. For more information, see the IBM Cloud Private documentation.      Navigate to the IBM Cloud Private console homepage.      Click the hamburger icon in the top left.      Expand Platform.      Click Logging to navigate to the Kibana homepage.      Click Management on the left.      Click Saved Objects.      Click the Import icon and navigate to the Event Streams Kibana Dashboard.json file that you downloaded.      Click the Dashboard tab on the left to view the downloaded dashboards.      You can also use external monitoring tools to monitor the deployed Event Streams Kafka cluster. For information about the health of your topics, check the producer activity dashboard. Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. Viewing the preconfigured dashboard To get an overview of the cluster health, you can view a selection of metrics on the Event Streams Monitor dashboard.   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Monitoring in the primary navigation. A dashboard is displayed with overview charts for messages, partitions, and replicas.  Select 1 hour, 1 day, 1 week, or 1 month to view data for different time periods.","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/cluster-health/",
        "teaser":null},{
        "title": "Monitoring topic health",
        "collection": "2019.4",
        "excerpt":"To gain an insight into the overall health of topics and highlight potential performance issues with systems producing to Event Streams, you can use the Producer dashboard provided for each topic. The dashboard displays aggregated information about producer activity for the selected topic through metrics such as message produce rates, message size, and an active producer count. The dashboard also displays information about each producer that has been producing to the topic. You can expand an individual producer record to gain insight into its performance through metrics such as messages produced, message size and rates, failed produce requests and any occurences where a producer has exceeded a broker quota. The information displayed on the dashboard can also be used to provide insight into potential causes when applications experience issues such as delays or ommissions when consuming messages from the topic. For example, highlighting that a particular producer has stopped producing messages, or has a lower message production rate than expected. Important: The producers dashboard is intended to help highlight producers that may be experiencing issues producing to the topic. You may need to investigate the producer applications themselves to identify an underlying problem. To access the dashboard:   Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).  Click Topics in the primary navigation.      Select the topic name from the list you want to view information about.The Producers tab is displayed with the dashboard and details about each producer. You can refine the time period for which information is displayed. You can expand each producer to view details about their activity.     Note: When a new client starts producing messages to a topic, it might take up to 5 to 10 minutes before information about the producer’s activity appears in the dashboard. In the meantime, you can go to the Messages tab to check whether messages are being produced.   Important: By default, the metrics data used to provide monitoring information is only stored for a day. Modify the time period for metric retention to be able to view monitoring data for longer time periods, such as 1 week or 1 month. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/topic-health/",
        "teaser":null},{
        "title": "Monitoring with external tools",
        "collection": "2019.4",
        "excerpt":"You can use third-party monitoring tools to monitor the deployed Event Streams Kafka cluster by connecting to the JMX port on the Kafka brokers and reading Kafka metrics. You must configure your installation to set up access for external monitoring tools. For examples about setting up monitoring with external tools such as Datadog, Prometheus, and Splunk, see the tutorials page. If you have a tool or service you want to use to monitor your clusters, you can contact support. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/external-monitoring/",
        "teaser":null},{
        "title": "Modifying Kafka broker configurations",
        "collection": "2019.4",
        "excerpt":"You can use the IBM Event Streams CLI to dynamically modify brokers and cluster-wide configuration settings for your IBM Event Streams instance. You can also use the IBM Event Streams CLI together with a ConfigMap to modify static (read-only) configuration settings. Configuration options For a list of all configuration settings you can specify for Kafka brokers, see the Kafka documentation. Some of the broker configuration settings can be updated without restarting the broker, while others require a restart:   read-only: Requires a broker restart for the update to take effect.  per-broker: Can be updated dynamically for each broker without a broker restart.  cluster-wide: Can be updated dynamically as a cluster-wide default, or as a per-broker value for testing purposes.See the Dynamic Update Mode column in the Kafka documentation for the update mode of each broker configuration. Note: You cannot modify the following properties.   broker.id  listeners  zookeeper.connect  advertised.listeners  inter.broker.listener.name  listener.security.protocol.map  authorizer.class.name  principal.builder.class  sasl.enabled.mechanisms  log.dirs  inter.broker.protocol.version  log.message.format.versionModifying broker and cluster settings You can modify per-broker and cluster-wide configuration settings dynamically (without a broker restart) by using the IBM Event Streams CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  To modify a per-broker configuration setting: cloudctl es broker-config --broker &lt;broker_id&gt; --config &lt;name&gt;=&lt;value&gt;  To modify a cluster-wide configuration setting: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt;You can also update your read-only configuration settings that require a broker restart by using the IBM Event Streams CLI. Note: Read-only settings require a ConfigMap to be set. If you did not create and specify a ConfigMap during the installation process, you can create a ConfigMap later with the required Kafka configuration settings or create a blank one to use later. Use the following command to make the ConfigMap available to your IBM Event Streams instance if you did not create a ConfigMap during installation: helm upgrade --reuse-values --set kafka.configMapName=&lt;configmap_name&gt; &lt;release_name&gt; &lt;charts.tgz&gt; Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. You can use the IBM Event Streams CLI to modify read-only configuration settings as follows: cloudctl es cluster-config --config &lt;name&gt;=&lt;value&gt; --static-config-all-brokers ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/modifying-configs/",
        "teaser":null},{
        "title": "Modifying installation settings",
        "collection": "2019.4",
        "excerpt":"You can modify the configuration settings for your existing Event Streams installation by using the UI or the command line. The configuration changes are applied by updating the Event Streams chart. For example, you might need to modify settings to scale your installation due to changing requirements. Using the UI You can modify any of the configuration settings you specified during installation, or define values for ones previously not set at the time of installation. To modify configuration settings by using the UI:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  From the navigation menu, click Workloads &gt; Helm Releases.  Locate the release name of your existing Event Streams cluster in the NAME column, and click  More options &gt; Upgrade in the corresponding row.  Select the installed chart version from the Version drop-down list.  Ensure you set Using previous configured values to Reuse Values.  Click All parameters in order to access all the release-related parameters.  Modify the values for the configuration settings you want to change.For example, to set the number of geo-replication workers to 4, go to the Geo-replication settings section and set the Geo-replicator workers field to 4.  Click Upgrade.Using the CLI You can modify any of the parameters you specified during installation, or define values for ones previously not set at the time of installation. For a list of all parameters, see the chart README file. To modify any of the parameter settings by using the CLI:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Use the following helm command to modify the value of a parameter:helm upgrade --reuse-values --set &lt;parameter&gt;=&lt;value&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tlsNote: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available.For example, to set the number of geo-replication workers to 4, use the following command:helm upgrade --reuse-values --set replicator.replicas=4 destination ibm-eventstreams-prod-1.4.0.tgz --tls","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/modifying-installation/",
        "teaser":null},{
        "title": "Running Helm upgrade commands",
        "collection": "2019.4",
        "excerpt":"You can use the helm upgrade command to upgrade your Event Streams version, or to modify configuration settings for your Event Streams installation. To run Helm upgrade commands, you must have a copy of the original Helm charts file that you used to install IBM Event Streams. To retrieve the charts file using the UI:   Log in to your IBM Cloud Private cluster management console as an administrator. For more information, see the IBM Cloud Private documentation.  Click Catalog in the top navigation menu.  If you are using the Community Edition, search for ibm-eventstreams-dev and select it from the result. If you are using Event Streams, search for ibm-eventstreams-prod and select it from the result.Note: IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages.  Select the latest version number from the drop-down list on the left.  To download the file, go to the SOURCE &amp; TAR FILES section on the left and click the link. The ibm-eventstreams-dev-&lt;version&gt;.tgz file is downloaded.Alternatively, if you downloaded IBM Event Streams from IBM Passport Advantage, you can also retrieve the charts file by looking for a file called ibm-eventstreams-prod-&lt;version&gt;.tgz within the downloaded archive. If you no longer have a copy, you can download the file again from IBM Passport Advantage. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/helm-upgrade-command/",
        "teaser":null},{
        "title": "Scaling",
        "collection": "2019.4",
        "excerpt":"You can modify the capacity of your IBM Event Streams system in a number of ways. See the following sections for details about the different methods, and their impact on your installation. You can start with the default installation parameters when deploying Event Streams, and test the system with a workload that is representative of your requirements. For this purpose, IBM Event Streams provides a workload generator application to test message loads. If this testing shows that your system does not have the capacity needed for the workload, whether this results in excessive lag or delays, or more extreme errors such as OutOfMemory errors, then you can incrementally make the increases detailed in the following sections, re-testing after each change to identify a configuration that meets your specific requirements. Important: To take full advantage of the scaling capabilities described in this topic, and avoid potential bottlenecks in high throughput environments, consider options for your IBM Cloud Private environment, such as setting up a load balancer and an internal network. For more information, see the topic about performance. Increase the number of Kafka brokers in the cluster To set this at the time of installation, you can use the --set kafka.brokers=&lt;NUMBER&gt; option in your helm install command if using the CLI, or enter the number in the Kafka brokers field of the Configure page if using the UI. To modify the number of Kafka brokers for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.brokers=&lt;NUMBER&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Important: When increasing the number of Kafka brokers, consider increasing the number of connections to the brokers as well. Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the number of connections to the Kafka brokers For performance reasons, if you have increased the number of Kafka brokers, increase the number of proxies that allow client applications to connect to the brokers. You can set the number of proxies to the same number as the number of brokers. You cannot set a value higher than the number of brokers installed. To set this at the time of installation, you can use the --set global.zones.proxyReplicas=&lt;NUMBER&gt; option in your helm install command using the CLI. There is no mecahnism by which this value can be set using the UI. To modify the number of proxies for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set global.zones.proxyReplicas=&lt;NUMBER&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the CPU limit available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values in the CPU request for Kafka brokers and CPU limit for Kafka brokers fields of the Configure page if using the UI. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.cpu=&lt;LIMIT&gt; --set kafka.resources.requests.cpu=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. A description of the syntax for these values can be found in the Kubernetes documentation. Increase the amount of memory available to each Kafka broker To set this at the time of installation, you can use the --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; options in your helm install command if using the CLI, or enter the values into the Memory request for Kafka brokers and Memory limit for Kafka brokers fields of the Configure page if using the UI. The syntax for these values can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.resources.limits.memory=&lt;LIMIT&gt; --set kafka.resources.requests.memory=&lt;LIMIT&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Increase the memory available to supporting systems If you have significantly increased the memory available to Kafka brokers, you will likely need to make a similar increase in the memory available to the other components that support the Kafka brokers. Ensure you consider the following two components. The metrics reporter component captures the monitoring statistics for cluster, broker, and topic activity. The memory requirements for this component will increase with the number of topic partitions in the cluster, and the throughput on those topics. To set this at the time of installation, you can use the following options:--set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt;--set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; A description of the syntax for these values can be found in the Kubernetes documentation. The message indexer indexes the messages on topics to allow them to be searched in the IBM Event Streams UI. The memory requirements for this component will increase with the cluster message throughput. To set this at the time of installation, you can use the --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt; option in your helm install command if using the CLI, or enter the values into the Memory limits for Index Manager nodes fields of the Configure page if using the UI. The syntax for the container memory limits can be found in the Kubernetes documentation. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values  --set kafka.metricsReporterResources.limits.memory=&lt;LIMIT&gt; --set kafka.metricsReporterResources.requests.memory=&lt;LIMIT&gt; --set messageIndexing.resources.limits.memory=&lt;LIMIT&gt;  &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Custom JVM tuning for Kafka brokers If you have specific requirements, you might need to further tune the JVMs running the Kafka brokers, such as modifying the garbage collection policies. Note: Take care when modifying these settings as changes can have an impact on the functioning of the product. To provide custom JVM parameters at the time of installation, you can use --set kafka.heapOpts=&lt;JVMOPTIONS&gt; option in your helm install command. To modify this for an existing IBM Event Streams installation, use the following command: helm upgrade --reuse-values --set kafka.heapOpts=&lt;JVMOPTIONS&gt; &lt;release_name&gt; &lt;charts.tgz&gt; --tls Note: Where &lt;charts.tgz&gt; is the name of the chart file you used to install IBM Event Streams, including the full path to it if in a different directory. To run helm upgrade commands, ensure you have the original chart file available. Use a faster storage class for PVCs used by Kafka brokers The speed of the storage available to Kafka brokers will impact performance. Set this at the time of installation with the --set kafka.persistence.dataPVC.storageClassName=&lt;STORAGE_CLASS&gt; option in your helm install command if using the CLI, or by entering the required storage class into the Storage class name field of the Kafka persistent storage settings section of the Configure page if using the UI. For more information about available storage classes, see the IBM Cloud Private documentation. Increase the disk space available to each Kafka broker The Kafka brokers will require sufficient storage to meet the retention requirements for all of the topics in the cluster. Disk space requirements grow with longer retention periods or sizes, and more topic partitions. Set this at the time of installation with the --set kafka.persistence.dataPVC.size=&lt;SIZE&gt; option in your helm install command if using the CLI, or by entering the required persistence size into the Size field of the Kafka persistent storage settings section of the Configure page if using the UI. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/scaling/",
        "teaser":null},{
        "title": "Setting client quotas",
        "collection": "2019.4",
        "excerpt":"Kafka quotas enforce limits on produce and fetch requests to control the broker resources used by clients. Using quotas, administrators can throttle client access to the brokers by imposing network bandwidth or data limits, or both. Kafka quotas are supported in IBM Event Streams 2018.3.1 and later. About Kafka quotas In a collection of clients, quotas protect from any single client producing or consuming significantly larger amounts of data than the other clients in the collection. This prevents issues with broker resources not being available to other clients, DoS attacks on the cluster, or badly behaved clients impacting other users of the cluster. After a client that has a quota defined reaches the maximum amount of data it can send or receive, their throughput is stopped until the end of the current quota window. The client automatically resumes receiving or sending data when the quota window of 1 second ends. By default, clients have unlimited quotas. For more information about quotas, see the Kafka documentation. Setting quotas You can set quotas by using the Event Streams CLI as follows:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to initialize the Event Streams CLI:cloudctl es init  Use the entity-config command option to set quotas as follows.Decide what you want to limit by using a quota type, and set it with the --config &lt;quota_type&gt;option, where &lt;quota_type&gt; can be one of the following:   producer_byte_rate - This quota limits the number of bytes that a producer application is allowed to send per second.  consumer_byte_rate - This quota limits the number of bytes that a consumer application is allowed to receive per second.  request_percentage - This quota limits all clients based on thread utilisation.Decide whether you want to apply the quota to users or client IDs. To apply to users, use the --user &lt;user&gt; option. Event Streams supports 2 types of users: actual user principal names, or application service IDs.   A quota defined for a user principal name is only applied to that specific user name. To specify a principal name, you must prefix the value for the --user parameter with u-, for example, --user \"u-testuser1\"  A quota defined for a service ID is applied to all applications that are using API keys that have been bound to the specific service ID. To specify a service ID, you must prefix the value for the --user parameter with s-, for example, --user \"s-consumer_service_id\"To apply to client IDs, use the --client &lt;client id&gt; option. Client IDs are defined in the application using the client.id property. A client ID identifies an application making a request. You can apply the quota setting to all users or client IDs by using the --user-default or --client-default parameters, respectively. Quotas set for specific users or client IDs override default values set by these parameters. By using these quota type and user or client ID parameters, you can set quotas using the following combinations: cloudctl es entity-config --user &lt;user&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --user-default --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client &lt;client id&gt; --config &lt;quota_type&gt;=&lt;value&gt; cloudctl es entity-config --client-default --config &lt;quota_type&gt;=&lt;value&gt; Examples For example, the following setting specifies that user u-testuser1 can only send 2048 bytes of data per second:cloudctl es entity-config --user \"u-testuser1\" --config producer_byte_rate=2048 For example, the following setting specifies that all application client IDs can only receive 2048 bytes of data per second:cloudctl es entity-config --client-default --config consumer_byte_rate=2048 The cloudctl es entity-config command is dynamic, so any quota setting is applied immediately without the need to restart clients. Note: If you run any of the commands with the --default parameter, the specified quota is reset to the system default value for that user or client ID (which is unlimited).For example:    cloudctl es entity-config --user \"s-consumer_service_id\" --default --config producer_byte_rate ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/quotas/",
        "teaser":null},{
        "title": "Managing a multizone setup",
        "collection": "2019.4",
        "excerpt":"If you have set up your Event Streams installation to use multiple availability zones, follow the guidance here if one of your nodes containing Kafka or ZooKeeper experiences problems. Topic configuration Only create Kafka topics where the minimum in-sync replicas configuration can be met in the event of a zone failure. This requires considering the minimum in-sync replicas value in relation to the replication factor set for the topic, and the number of availability zones being used for spreading out your Kafka brokers. For example, if you have 3 availability zones and 6 Kafka brokers, losing a zone means the loss of 2 brokers. In the event of such a zone failure, the following topic configurations will guarantee that you can continue to produce to and consume from your topics:   If the replication factor is set to 6, then the suggested minimum in-sync replica is 4.  If the replication factor is set to 5, then the suggested minimum in-sync replica is 3.Updating an existing installation If you are changing an existing installation, such as adding new nodes, follow these steps to change the zone configuration. For example, if one of your nodes containing Kafka or ZooKeeper pods fails, and it cannot be recovered, you can label another node with that role label, and then the pod can be moved to the new node.   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Label the node according to your requirements. For example, if you want another Kafka broker, then label the node with the Kafka role and zone labels:kubectl label node &lt;node-name&gt; node-role.kubernetes.io/kafka=true  List the ConfigMaps in your Event Streams installation:kubectl get configmaps -n &lt;namespace&gt;  Look for the ConfigMap that has the suffix zone-gen-job-cm.  Download and save the configuration for the job stored in the ConfigMap:kubectl get configmap &lt;release-name&gt;-ibm-zone-gen-job-cm -o jsonpath='{.data.job}' &gt; zonegen.yml  Run the following command to add the new node to your Kafka configuration:kubectl create -f ./zonegen.ymlWhen the new node role label is applied, Kubernetes schedules the failed Kafka or ZooKeeper to run on that node. ","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/managing-multizone/",
        "teaser":null},{
        "title": "Stopping and starting Event Streams",
        "collection": "2019.4",
        "excerpt":"You can stop your Event Streams cluster if required, for example, in preparation for maintenance. Use the following instructions to gracefully shut down your Event Streams cluster. You can then start the cluster up again when it is ready. Stopping To shut down your cluster gracefully, scale all deployments and stateful sets to 0 replicas as follows.       Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;         Retrieve all deployments and stateful sets in your Event Streams namespace associated with the release, and scale them down to 0 replicas:      kubectl get deployments -n &lt;namespace&gt; -l release=&lt;release-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; &lt;deploy-replicas-filename&gt; &amp;&amp; while read -ra deploy; do kubectl -n &lt;namespace&gt; scale --replicas=0 deployment/${deploy}; done &lt; &lt;deploy-replicas-filename&gt; kubectl get sts -n &lt;namespace&gt; -l release=&lt;release-name&gt; -o custom-columns=NAME:.metadata.name,REPLICAS:.spec.replicas --no-headers &gt; &lt;sts-replicas-filename&gt; &amp;&amp; while read -ra sts; do kubectl -n &lt;namespace&gt; scale --replicas=0 sts/${sts}; done &lt; &lt;sts-replicas-filename&gt;      Where:   &lt;namespace&gt; is the location of your installation.  &lt;release-name&gt; is the name that identifies your Event Streams installation.  &lt;deploy-replicas-filename&gt; is the file where deployment - replica pairs are to be saved (provide a name, for example, deploy-replicas.yaml).  &lt;sts-replicas-filename&gt; is the file where stateful set - replica pairs are to be saved (provide a name, for example, sts-replicas.yaml).Starting up To start up your cluster and scale it back up to its previous state, run the following commands.       Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;         Retrieve and apply the previous configuration for your Event Streams instance:      while IFS= read -ra deploy; do d=$(echo $deploy | cut -f1 -d\" \"); rep=$(echo $deploy | cut -f2 -d\" \"); kubectl -n &lt;namespace&gt; scale --replicas=$rep deployment/${d}; done &lt; &lt;deploy-replicas-filename&gt; while IFS= read -ra sts; do s=$(echo $sts | cut -f1 -d\" \"); rep=$(echo $sts | cut -f2 -d\" \"); kubectl -n &lt;namespace&gt; scale --replicas=$rep sts/${s}; done &lt; &lt;sts-replicas-filename&gt;      Where:   &lt;namespace&gt; is the location of your installation.  &lt;deploy-replicas-filename&gt; is the file where you saved deployment - replica pairs (for example, deploy-replicas.yaml).  &lt;sts-replicas-filename&gt; is the file where you saved stateful set - replica pairs (for example, sts-replicas.yaml).","categories": ["administering"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/administering/stopping-starting/",
        "teaser":null},{
        "title": "Troubleshooting overview",
        "collection": "2019.4",
        "excerpt":"To help troubleshoot issues with your installation, see the troubleshooting topics in this section. In addition, you can check the health information for your environment as described in monitoring deployment health and monitoring Kafka cluster health. If you need help, want to raise questions, or have feature requests, see the IBM Event Streams support channels. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/intro/",
        "teaser":null},{
        "title": "Diagnosing installation issues",
        "collection": "2019.4",
        "excerpt":"To help troubleshoot and resolve installation issues, you can run a diagnostic script that checks your deployment for potential problems. Important: Do not use the script before the installation process completes. Despite a successful installation message, some processes might still need to complete, and it can take up to 10 minutes before IBM Event Streams is available to use. To run the script:   Download the installation-diagnostic-script.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the script as follows: ./installation-diagnostic-script.sh -n &lt;namespace&gt; -r &lt;release-name&gt;If you have been waiting for more than an hour, add the --restartoldpods option to recreate lost events (by default, events are deleted after an hour). This option restarts Failed or Pending pods that are an hour old or more. For example, the following installation has pods that are in pending state, and running the diagnostic script reveals that the issue is caused by not having sufficient memory and CPU resources available to the pods: Starting release diagnostics...Checking kafka-sts pods...kafka-sts pods foundChecking zookeeper-sts pods...zookeeper-sts pods foundChecking the ibm-es-iam-secret API Key...API Key foundChecking for Pending pods...Pending pods found, checking pod for failed events...------------------Name: caesar-ibm-es-kafka-sts-0Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-1Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------Name: caesar-ibm-es-kafka-sts-2Type: PodFirstSeen: 2mLastSeen: 1mIssue: 0/4 nodes are available: 1 Insufficient memory, 4 Insufficient cpu.------------------No failed events found for pod caesar-ibm-es-rest-deploy-6ff498d779-stf79------------------Checking for CrashLoopBackOff pods...No CrashLoopBackOff pods foundRelease diagnostics complete. Please review output to identify potential problems.If unable to identify or fix problems, please contact support.","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/diagnosing-installation-issues/",
        "teaser":null},{
        "title": "Gathering logs",
        "collection": "2019.4",
        "excerpt":"To help IBM support troubleshoot any issues with your Event Streams installation, run the log gathering script as follows. The script collects the log files from available pods and creates a compressed file. It uses the component label names instead of the pod names as the pod names could be truncated.   Download the get-logs.sh script from GitHub.  Ensure you have installed the Kubernetes command line tool and the IBM Cloud Private CLI as noted in the installation prerequisites.  Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;      Run the script as follows: ./get-logs.sh -n &lt;namespace&gt; -r &lt;release-name&gt;     If you do not specify a namespace, the script retrieves logs from the default namespace as requested in the cloudctl login. If you do not specify a release name, the script gathers logs for all releases.   ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/gathering-logs/",
        "teaser":null},{
        "title": "Resources not available",
        "collection": "2019.4",
        "excerpt":"If IBM Event Streams resources are not available, the following are possible sypmtoms and causes. IBM Event Streams not available after installation After a successsful installation message is displayed, IBM Event Streams might not be available to use yet. It can take up to 10 minutes before IBM Event Streams is available to use. The IBM Cloud Private installation might return a successful completion message before all Event Streams services start up. If the installation continues to be unavailable, run the installation diognastics scripts. Insufficient system resources You can specify the memory and CPU requirements when IBM Event Streams is installed. If the values set are larger than the resources available, then pods will fail to start. Common error messages in such cases include the following:   pod has unbound PersistentVolumeClaims: occurs when there are no Persistent Volumes available that meet the requirements provided at the time of installation.  Insufficient memory: occurs when there are no nodes with enough available memory to support the limits provided at the time of installation.  Insufficient CPU: occurs when there are no nodes with enough available CPU to support the limits provided at the time of installation.For example, if each Kafka broker is set to require 80 GB of memory on a system that only has 16 GB available per node, you might see the following error message:  To get detailed information on the cause of the error, check the events for the individual pods (not the logs at the stateful set level). If a system has 16 GB of memory available per node, then the broker memory requirements must be set to be less than 16 GB. This allows resources to be available for the other IBM Event Streams components which may reside on the same node. To correct this issue, uninstall IBM Event Streams. Install again using lower resource requirements, or increase the amount of system resources available to the pod. Problems with secrets When using a non-default Docker registry, you might need to provide a secret which stores the user ID and password to access that registry. If there are issues with the secret that holds the user ID and password used to access the Docker registry, the events for a pod will show an error similar to the following.  To resolve this issue correct the secret and install IBM Event Streams again. Installation failure stating object already exists If a secret that does not exist is specified during installation, the process fails even if no secret is required to access the Docker registry. The default Docker image registry at ibmcom does not require a secret specifying the user ID and password. To correct this, install IBM Event Streams again without specifying a secret. If you are using a Docker image registry that does require a secret, attempting to install again might fail stating that an object already exists, for example: Internal service error : rpc error: code = Unknown desc = rolebindings.rbac.authorization.k8s.io \"elh-ibm-es-secret-copy-crb-sys\" already existsDelete the left over object cited and other objects before trying to install again. For instructions, see how to fully clean up after uninstallation. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/resources-not-available/",
        "teaser":null},{
        "title": "Full cleanup after uninstallation",
        "collection": "2019.4",
        "excerpt":"The uninstallation process might leave behind artifacts that you have to clear manually. Security resources A service ID is created as part of installing Event Streams, which defines the identity for securing communication between internal components. To delete this service ID after uninstalling Event Streams, run the following command: cloudctl iam service-id-delete eventstreams-&lt;release&gt;-service-id -f Kubernetes resources Use the following command to find the list of IBM Event Streams objects associated with the release’s namespace: kubectl get &lt;type&gt; -n &lt;namespace&gt; | grep ibm-es Where type is each of   pods  clusterroles  clusterrolebindings  roles  rolebindings  configmaps  serviceaccounts  statefulsets  deployments  jobs  pods  pvc (see Note later)  secretsEvent Streams 2019.2.1 and earlier created a number of objects in the kube-system namespace. You can safely remove these objects after uninstalling. Note: If you upgraded to 2019.4.1, there might be objects in the kube-system namespace from the previous release. To list these objects run the following command: kubectl get pod -a -n kube-system | grep ibm-es Note: These commands might return objects that should not be deleted. For example, do not delete secrets or system clusterroles if the kubectl output is not piped to grep. Note: If persistent volume claims (PVCs) are deleted (the objects returned when specifying “pvc” in the commands above), the data associated with the PVCs is also deleted. This includes any persistent Kafka data on disk. Consider whether this is the desired result before deleting any PVCs. To find which objects need to be manually cleared look for the following string in the output of the previously mentioned commands: &lt;release&gt;-ibm-es You can either navigate through the IBM Cloud Private cluster management console to Workloads &gt; or Configuration &gt; to find the objects and delete them, or use the following command: kubectl delete &lt;type&gt; &lt;name&gt; -n &lt;namespace&gt; For example, to delete a leftover rolebinding called eventstreams-ibm-eventstreams-secret-copy-crb-ns, run the following command: kubectl delete rolebinding eventstreams-ibm-eventstreams-secret-copy-crb-ns -n es Be cautious of deleting persistent volume claims (PVCs) as the data on the disk that is associated with that persistent volume will also be deleted. This includes Event Streams message data. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/cleanup-uninstall/",
        "teaser":null},{
        "title": "ConsumerTimeoutException when pods available",
        "collection": "2019.4",
        "excerpt":"Symptoms Attempts to communicate with a pod results in timeout errors such as kafka.consumer.ConsumerTimeoutException. Causes When querying the status of pods in the Kubernetes cluster, pods show as being in Ready state can still be in the process of starting up. This latency is a result of the external ports being active on the pods before the underlying services are ready to handle requests. The period of this latency depends on the configured topology and performance characteristics of the system in use. Resolving the problem Allow additional time for pod startup to complete before attempting to communicate with it. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/pod-timeout-error/",
        "teaser":null},{
        "title": "Error when creating multiple geo-replicators",
        "collection": "2019.4",
        "excerpt":"Symptoms The following error message is displayed when setting up replication by using the CLI: FAILEDEvent Streams API request failed:Error response from server. Status code: 400. The resource request is invalid. Missing required parameter topic nameThe message does not provide accurate information about the cause of the error. Causes When providing the list of topics to geo-replicate, you added spaces between the topic names in the comma-separated list. Resolving the problem Ensure you do not have spaces between the topic names. For example, instead of --topics MyTopicName1, MyTopicName2, MyTopicName3, enter --topics MyTopicName1,MyTopicName2,MyTopicName3. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/georeplication-error/",
        "teaser":null},{
        "title": "TimeoutException when using standard Kafka producer",
        "collection": "2019.4",
        "excerpt":"Symptoms The standard Kafka producer (kafka-console-producer.sh) is unable to send messages and fails with the following timeout error: org.apache.kafka.common.errors.TimeoutExceptionCauses This situation occurs if the producer is invoked without supplying the required security credentials. In this case, the producer fails withthe following error: Error when sending message to topic &lt;topicname&gt; with key: null, value: &lt;n&gt; bytesResolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command, include the --producer.config &lt;properties_file&gt; option, replacing &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-producer.sh --broker-list &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; --producer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/kafka-producer-error/",
        "teaser":null},{
        "title": "Standard Kafka consumer hangs and does not output messages",
        "collection": "2019.4",
        "excerpt":"Symptoms The standard Kafka consumer (kafka-console-consumer.sh) is unable to receive messages and hangs without producing any output. Causes This situation occurs if the consumer is invoked without supplying the required security credentials. In this case, the consumerhangs and does not output any messages sent to the topic. Resolving the problem Create a properties file with the following content: security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=&lt;certs.jks_file_location&gt;ssl.truststore.password=&lt;truststore_password&gt;sasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"&lt;api_key&gt;\";Replace &lt;certs.jks_file_location&gt; with the location of a trust store file containing the server certificate (for example, certs.jks), &lt;truststore_password&gt; with the password for the trust store and &lt;api_key&gt; with an API key able to access the IBM Event Streams deployment. When running the kafka-console-producer.sh command include the --consumer.config &lt;properties_file&gt; option, replacing the &lt;properties_file&gt; with the name of the property file and the path to it. For example: kafka-console-consumer.sh --bootstrap-server &lt;brokerIP&gt;:&lt;bootstrapPort&gt; --topic &lt;topic&gt; -consumer.config &lt;properties_file&gt;","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/kafka-consumer-hangs/",
        "teaser":null},{
        "title": "Command 'cloudctl es' fails with 'not a registered command' error",
        "collection": "2019.4",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED'es' is not a registered command. See 'cloudctl help'.Causes This error occurs when you attempt to use the IBM Event Streams CLI before it is installed. Resolving the problem Log in to the IBM Event Streams UI, and install the CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/cloudctl-es-not-registered/",
        "teaser":null},{
        "title": "Command 'cloudctl es' produces 'FAILED' message",
        "collection": "2019.4",
        "excerpt":"Symptoms When running the cloudctl es command, the following error message is displayed: FAILED...Causes This error occurs when you have not logged in to the IBM Cloud Private cluster and initialized the command line tool. Resolving the problem Ensure you log in to the IBM Cloud Private cluster as follows: cloudctl login -a https://&lt;cluster_address&gt;:&lt;cluster_router_https_port&gt;After logging in to IBM Cloud Private, initialize the IBM Event Streams CLI as follows: cloudctl es initFinally, run the operation again. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/cloudctl-es-fails/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on Ubuntu",
        "collection": "2019.4",
        "excerpt":"Symptoms When using a Google Chrome browser on Ubuntu operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes The Google Chrome browser on Ubuntu systems requires a certificate that IBM Event Streams does not currently provide. Resolving the problem Use a different browser, such as Firefox, or launch Google Chrome with the following option: --ignore-certificate-errors ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/chrome-ubuntu-issue/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no ImagePolicies' error",
        "collection": "2019.4",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no ImagePolicies in the \"&lt;name&gt;\" namespace, where &lt;name&gt; is the namespace into which you are deploying the chart. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs if there are no image policies defined for the target namespace. To confirm this, list the policies as follows: kubectl get imagepolicyYou should see a message stating No resources found. Resolving the problem If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioNote: IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages. To apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. Important: Always ensure you install Event Streams in a dedicated namespace. Do not use any of the default namespaces (default, kube-system, kube-public, and so on). The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/no-image-policy/",
        "teaser":null},{
        "title": "Chart deployment fails with 'no matching repositories in the ImagePolicies' error",
        "collection": "2019.4",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, an Internal service error occurs stating that there are no matching repositories in the ImagePolicies. Causes Image policies are used to control access to Docker repositories and it is necessary to ensure that there is a suitable policy in place before the chart is installed. This situation occurs when there are image policies defined for the namespace into which the chart is being deployed, but none of them include the required repositories. To confirm this, list the image policies defined as follows: kubectl get imagepolicyFor each image policy, you can check which repositories it includes as follows: kubectl describe imagepolicy &lt;imagePolicyName&gt;If you are using IBM Event Streams (not the Community Edition), you will need access to the following image repositories:   docker.io  mycluster.icp:8500If you are using Community Edition, you need access to the following image repositories:   docker.ioNote: IBM Event Streams Community Edition is no longer available from 1 May 2020. For information about trying out Kafka and Event Streams for free, see the product pages. Resolving the problem To apply an image policy, create a new yaml file with the following content, replacing the namespace with the namespace into which the chart will be deployed and the name with a name of your choice. Important: Always ensure you install Event Streams in a dedicated namespace. Do not use any of the default namespaces (default, kube-system, kube-public, and so on). The following is an example where we are adding both repositories: apiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1kind: ImagePolicymetadata:  name: &lt;imagePolicyName&gt;  namespace: &lt;namespace&gt;spec:  repositories:  - name: docker.io/*    policy: null  - name: mycluster.icp:8500/*    policy: nullThen run the following command to create the image policy: kubectl apply -f &lt;yamlFile&gt;Finally, repeat the installation steps to deploy the chart. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/image-policy-missing-repository/",
        "teaser":null},{
        "title": "Chart deployment starts but no helm release is created",
        "collection": "2019.4",
        "excerpt":"Symptoms When the IBM Event Streams chart is deployed, the process appears to start successfully but the helm release and set of expected pods are not created. You can confirm the helm release has not been created by running the following command: helm listIn this case you will not see an entry for the Helm release name you provided when you started the deployment process. In addition, you will see that only a single pod is initially created and then subsequently removed after a couple of minutes. You can check which pods are running using the following command: kubectl get podsImmediately after starting the deployment process, you will see a single pod created named &lt;releaseName&gt;-ibm-es-secret-copy-job-&lt;uid&gt;. If you ‘describe’ the pod, you will receive the error message Failed to pull image, and a further message stating either Authentication is required or unauthorized: BAD_CREDENTIAL. After a couple of minutes this pod is deleted and no pods will be reported by the kubectl command. If you query the defined jobs as follows, you will see one named &lt;releaseName&gt;-ibm-es-secret-copy-job: kubectl get jobsFinally if you ‘describe’ the job as follows, you will see that it reports a failed pod status: kubectl describe job &lt;releaseName&gt;-ibm-es-secret-copy-jobFor example, the job description will include the following: Pods Statuses:            0 Running / 0 Succeeded / 1 FailedCauses This situation occurs if there a problem with the image pull secret being used to authorize access to the Docker image repository you specified when the chart was deployed. When you ‘describe’ the secret copy pod, if you see the error message Authentication is required, this indicates that the secret you specified does not exist. If you see the error message unauthorized: BAD_CREDENTIAL, this indicates that the secret was found but one of the fields present within it is not correct. To confirm which secrets are deployed, run the following command: kubectl get secretsResolving the problem To delete a secret thats not correctly defined, use the following command: kubectl delete secret &lt;secretName&gt;To create a new secret for use in chart deployment, run the following command: kubectl create secret docker-registry &lt;secretName&gt; --docker-server=&lt;serverAddress:serverPort&gt; --docker-username=&lt;dockerUser&gt; --docker-password=&lt;dockerPassword&gt; --docker-email=&lt;yourEmailAddress&gt;For example: kubectl create secret docker-registry regcred --docker-server=mycluster.icp:8500 --docker-username=admin --docker-password=admin --docker-email=John.Smith@ibm.comAfter you have confirmed that the required secret is correctly defined, re-run the chart deployment process. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/no-helm-release-is-created/",
        "teaser":null},{
        "title": "The Messages page is blank",
        "collection": "2019.4",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Messages page loads, but then becomes blank when viewing any topic. Causes The vm.max_map_count property on one or more of your nodes is below the required value of 262144. This causes the message indexing capabilities to fail, resulting in this behavior. Resolving the problem Ensure you set the vm.max_map_count property to at least 262144 on all IBM Cloud Private nodes in your cluster (not only the master node). Run the following commands on each node:     sudo sysctl -w vm.max_map_count=262144    echo \"vm.max_map_count=262144\" | tee -a /etc/sysctl.confImportant: This property might have already been updated by other workloads to be higher than the minimum required. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/messages-page-blank/",
        "teaser":null},{
        "title": "Unable to connect to Kafka cluster",
        "collection": "2019.4",
        "excerpt":"Symptoms The following error is displayed when trying to connect to your Kafka cluster using SSL, for example, when running the Kafka Connect source connector for IBM MQ: org.apache.kafka.common.errors.SslAuthenticationException: SSL handshake failedCauses The Java process might replace the IP address of your cluster with the corresponding hostname value found in your /etc/hosts file. For example, to be able to access Docker images from your IBM Cloud Private cluster, you might have added an entry in your /etc/hosts file that corresponds to the IP address of your cluster, such as 192.0.2.24 mycluster.icp. In such cases, the following Java exception is displayed after the previously mentioned error message: Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching XXXXX found.Resolving the problem If you see the exception mentioned previously, comment out the hostname value in your /etc/hosts file to solve this connection issue. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/kafka-connection-issue/",
        "teaser":null},{
        "title": "Unable to remove destination cluster",
        "collection": "2019.4",
        "excerpt":"Symptoms When trying to remove an offline geo-replication destination cluster, the following error message is displayed in the UI: Failed to retrieve data for this destination cluster.Causes There could be several reasons, for example, the cluster might be offline, or the service ID of the cluster might have been revoked. Resolving the problem   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersLook for the destination cluster ID that you want to remove.  Run the following command:cloudctl es geo-cluster-remove --destination &lt;destination-cluster-id&gt; --force","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/error-removing-destination/",
        "teaser":null},{
        "title": "Geo-replication fails to start with 'Could not connect to origin cluster' error",
        "collection": "2019.4",
        "excerpt":"Symptoms When geo-replicating a topic to a destination cluster with 2 or more geo-replication worker nodes, the topic replication fails to start. The Event Streams UI reports the following error: Could not connect to origin cluster.In addition, the logs for the replicator worker nodes contain the following error message: org.apache.kafka.connect.errors.ConnectException: SSL handshake failedCauses The truststore on the geo-replication worker node that hosts the replicator task does not contain the certificate for the origin cluster. Resolving the problem You can either manually add the certificate to the truststore in each of the geo-replicator worker nodes, or you can scale the number of geo-replicator worker nodes down to 1 if suitable for your setup. Manually adding certificates To manually add the certificate to the truststore in each of the geo-replicator worker nodes:   Go to your origin cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following command to start the IBM Event Streams CLI: cloudctl es init  Retrieve destination cluster IDs by using the following command:cloudctl es geo-clustersA list of destination cluster IDs are displayed. Find the name of the destination cluster you are attempting to geo-replicate topics to.  Retrieve information about the destination cluster by running the following command and copying the required destination cluster ID from the previous step:cloudctl es geo-cluster --destination &lt;destination-cluster-id&gt;The failed geo-replicator name is in the list of geo-replicators returned.  log in to the destination cluster, and use kubectl exec to run the keytool command to import the certificate into the truststore in each geo-replication worker node:    kubectl exec -it -n &lt;namespace&gt; -c replicator \\&lt;releaseName&gt;-ibm-es-replicator-deploy-&lt;replicator-pod-id&gt; \\-- bash -c \\\"keytool -importcert \\-keystore /opt/replicator/security/cacerts \\-alias &lt;geo-replicator_name&gt; \\-file /etc/consumer-credentials/cert_&lt;geo-replicator_name&gt; \\-storepass changeit \\-trustcacerts -noprompt\"        The command either succeeds with a \"Certificate was added\" message or fails with a \"Certificate not imported, alias &lt;geo-replicator_name&gt; already exists\" message. In both cases, the truststore for that pod is ready to be used.     Repeat the command for each replicator worker node to ensure the certificate is imported into the truststore on all replicator pods.  Log in to the origin cluster, and restart the failed geo-replicator using the following cloudctl command:cloudctl es geo-replicator-restart -d &lt;geo-replication-cluster-id&gt; -n  &lt;geo-replicator_name&gt;Scaling the number of nodes To scale the number of geo-replicator worker nodes to 1:   Go to your destination cluster. Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;cluster-address&gt;:&lt;cluster-router-https-port&gt;  Run the following kubectl command:kubectl scale --replicas 1 deployment &lt;releaseName&gt;-ibm-es-replicator-deploy","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/georeplication-connect-error/",
        "teaser":null},{
        "title": "403 error when signing in to Event Streams UI",
        "collection": "2019.4",
        "excerpt":"Symptoms Signing into the Event Streams UI fails with the message 403 Not authorized, indicating that the user does not have permission to access the Event Streams instance. Causes The most likely cause of this problem is that the user attempting to authenticate is part of an IBM Cloud Private team that has not been associated with the  Event Streams instance. Resolving the problem Configure the IBM Cloud Private team that the user is part of to work with the Event Streams instance by running the iam-add-release-to-team CLI command. Run the command as follows: cloudctl es iam-add-release-to-team --namespace &lt;namespace for the Event Streams instance&gt; --release &lt;release name of the Event Streams instance&gt; --team &lt;name of the IBM Cloud Private team that the user is imported into&gt; The user can authenticate and sign in to the Event Streams UI after the command runs successfully. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/ui-403-error/",
        "teaser":null},{
        "title": "Kafka client applications are unable to connect to the cluster. Users are unable to login to the UI.",
        "collection": "2019.4",
        "excerpt":"Symptoms Client applications are unable to produce or consume messages. The logs for producer and consumer applications contain the following error message: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.serversThe Event Streams UI reports the following error: CWOAU0062E: The OAuth service provider could not redirect the request because the redirect URI was not valid. Contact your system administrator to resolve the problem.Causes An invalid host name or IP address was specified in the External access settings when configuring the installation. Resolving the problem Modify your Event Streams installation and provide the correct external host name or IP address. If using the UI, you can set the value in the External hostname/IP address field of the External access settings section. If using the CLI, use the proxy.externalEndpoint parameter, for example: helm upgrade --reuse-values --set proxy.externalEndpoint=my-loadbalancer1.com es1 ibm-eventstreams-prod-1.4.0.tgz --tls ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/client-connect-error/",
        "teaser":null},{
        "title": "The UI cannot load data",
        "collection": "2019.4",
        "excerpt":"Symptoms When using the IBM Event Streams UI, the Monitor and the Topics &gt; Producers tabs do not load, displaying the following message:  Causes The IBM Cloud Private monitoring service might not be installed. In general, the monitoring service is installed by default during the  IBM Cloud Private installation. However, some deployment methods do not install the service. Resolving the problem Install the IBM Cloud Private monitoring service from the Catalog or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/problem-with-piping/",
        "teaser":null},{
        "title": "Helm commands fail when running as Team Administrator",
        "collection": "2019.4",
        "excerpt":"Symptoms Running a Helm command as a Team Administrator fails with the following error message: Error: pods is forbidden: User \"https://127.0.0.1:9443/oidc/endpoint/OP#&lt;username&gt;\" cannot list resource \"pods\" in API group \"\" in the namespace \"kube-system\"Where &lt;username&gt; is the name of the user that logged in with the command cloudctl login. Causes Running any Helm command requires the user to have authorization to the kube-system namespace. The Team Administrator does not have access to the kube-system namespace. Resolving the problem The Helm Tiller service can also be configured to use a NodePort to bypass the default proxied connection and allowing users to use Helm CLI without requiring access to the kube-system namespace. The default port is 31514. To retrieve the port value if not using the default number, log in as the Cluster Administrator by using the cloudctl login command, and run the following command: kubectl get services -n kube-system tiller-deploy -o jsonpath='{.spec.ports[0].nodePort}'The port number for the &lt;tiller-nodeport&gt; is displayed. Provide the port number to the Team Administrator. The Team Administrator can then use the port number to run Helm commands in one of the following ways:   Export the HELM_HOST variable, for example:     export HELM_HOST=&lt;cluster-ip&gt;:&lt;tiller-nodeport&gt;        The Team Administrators can then run Helm commands.     Use the --host option when running Helm commands, and include the &lt;cluster-ip&gt;:&lt;tiller-nodeport&gt; value, for example:     helm install --tls --host &lt;cluster-ip&gt;:&lt;tiller-nodeport&gt; install/delete/list ...      For more information about the workaround, see the IBM Cloud Private documentation. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/helm-cli-team-administrator/",
        "teaser":null},{
        "title": "UI does not open when using Chrome on macOS Catalina",
        "collection": "2019.4",
        "excerpt":"Symptoms When using a Google Chrome browser on the latest versions of macOS operating systems, the IBM Event Streams UI does not open, and the browser displays an error message about invalid certificates, similar to the following example: 192.0.2.24 normally uses encryption to protect your information.When Google Chrome tried to connect to 192.0.2.24 this time, the website sent back unusual and incorrect credentials.This may happen when an attacker is trying to pretend to be 192.0.2.24, or a Wi-Fi sign-in screen has interrupted the connection.Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit 192.0.2.24 at the moment because the website sent scrambled credentials that Google Chrome cannot process.Network errors and attacks are usually temporary, so this page will probably work later.Causes Security rules in macOS version 10.15 Catalina and later do not allow the certificate presented by Event Streams to be used. Resolving the problem Use one of the following workarounds to avoid this problem:   Install Event Streams with provided certificates, or change them to provided instead of using generated (self-signed) certificates.  Use a different browser, such as Mozilla Firefox.  Launch Google Chrome with the following option: --ignore-certificate-errors For example, launch a terminal and run the following command: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --ignore-certificate-errors &amp;&gt; /dev/null &amp;      Install the IBM Cloud Private root CA in your macOS keychain. Obtain the certificate by using the following command: kubectl get secret cluster-ca-cert -n kube-system -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -D &gt; cluster-ca-cert.pem     Use the Keychain Access app in macOS to add the certificate and mark it as trusted.   ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/chrome-macos-issue/",
        "teaser":null},{
        "title": "Error when geo-replicating to an earlier version of Event Streams",
        "collection": "2019.4",
        "excerpt":"Symptoms When setting up geo-replication on a destination cluster that is running an earlier version of Event Streams than the origin cluster, the topic creation fails with the following error message: ERROR_CREATING_TOPICorg.apache.kafka.common.errors.InvalidConfigurationException: Invalid value 2.3-IV1 for configuration message.format.version: Version `2.3-IV1` is not a valid versionThe error message always includes the strings ERROR_CREATING_TOPIC and org.apache.kafka.common.errors.InvalidConfigurationException However, the type of the InvalidConfigurationException message can vary depending on the configuration options used for the origin topic. Causes When setting up a geo-replication job, a replica of the topic is created on the destination cluster that uses the same configuration settings as the topic on the origin cluster. If the origin and destination Event Streams clusters have different Kafka versions, then the destination cluster might not support the same topic configuration options as the origin cluster. Creating the replica of the topic on the destination cluster can fail in such cases. Resolving the problem Manually create a topic with the same name on the destination cluster before setting up geo-replication. Ensure you set the same number of partitions for the topic as you have set for the topic on the origin cluster you will be geo-replicating from. If a topic with a matching name already exists on the destination cluster, geo-replication will reuse the topic instead of creating one. You can create a topic by using the Event Streams UI or CLI. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/georeplication-version-error/",
        "teaser":null},{
        "title": "Error when downloading Java dependencies for schema registry in 2019.4.2",
        "collection": "2019.4",
        "excerpt":"Symptoms  In Event Streams 2019.4.2, when clicking on the link to download the schema registry Java dependencies .zip file, a network error is returned and no file is downloaded. The link appears in the following scenario when you are preparing Java applications to be used with a schema:   You have selected a schema, clicked the Connect to this version button.  You have provided or generated an API key.  You have clicked Generate connection details.  You have downloaded the Java truststore file which contains the server certificate.      In section 2. Download the schema registry dependencies or configure Maven to install dependencies in your project, you have clicked the Use JARs tab and clicked Java dependencies to download the Event Streams schema registry JAR files to use for your application.     A File not found error is displayed, and no file download option is displayed.   Cause The link published in the Event Streams 2019.4.2 UI is incorrect. Resolving the problem   Right-click on the existing link and select the option to copy the link.  Open a new browser tab.  Paste the copied link into the address bar and change the file name 2019.4.2 to 2019.4.1 in the URL: /dependencies-2019.4.1-java.zip  Submit the URL by pressing enter.You can now download the 2019.4.1 version of the dependencies, which are fully functional with 2019.4.2 as well. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/schemaregistry-java-deps-download-error/",
        "teaser":null},{
        "title": "Failed to read 'log header' errors in Kafka logs",
        "collection": "2019.4",
        "excerpt":"Symptoms When Event Streams is configured to use GlusterFS as a storage volume, the Kafka logs show errors containing messages similar to the following: [2020-05-12 06:40:19,249] ERROR [ReplicaManager broker=2] Error processing fetch with max size 1048576 from consumer on partition &lt;TOPIC-NAME&gt;-0: (fetchOffset=10380908, logStartOffset=-1, maxBytes=1048576, currentLeaderEpoch=Optional.empty) (kafka.server.ReplicaManager)org.apache.kafka.common.KafkaException: java.io.EOFException: Failed to read `log header` from file channel `sun.nio.ch.FileChannelImpl@a5e333e6`. Expected to read 17 bytes, but reached end of file after reading 0 bytes. Started read from position 95236164.These errors mean that Kafka has been unable to read files from the Gluster volume. This can cause replicas to fall out of sync. Cause See Kafka issue 7282 GlusterFS has performance settings that will allow requests for data to be served from replicas when they are not in sync with the leader. This causes problems for Kafka when it attempts to read a replica log segment before it has been fully written by Gluster. Resolving the problem Apply the following settings to each Gluster volume that is used by an Event Streams Kafka broker: gluster volume set &lt;volumeName&gt; performance.quick-read offgluster volume set &lt;volumeName&gt; performance.io-cache offgluster volume set &lt;volumeName&gt; performance.write-behind offgluster volume set &lt;volumeName&gt; performance.stat-prefetch offgluster volume set &lt;volumeName&gt; performance.read-ahead offgluster volume set &lt;volumeName&gt; performance.readdir-ahead offgluster volume set &lt;volumeName&gt; performance.open-behind offgluster volume set &lt;volumeName&gt; performance.client-io-threads offThese settings can be applied while the Gluster volume is online. The Kafka broker will not need to be modified, the broker will be able to read from the volume after the change is applied. ","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/failed-to-read-log/",
        "teaser":null},{
        "title": "Kafka cluster is not accessible by using OpenShift routes",
        "collection": "2019.4",
        "excerpt":"Symptoms If Event Streams is installed on Red Hat OpenShift Container Platform, Kafka clients receive timeout errors when trying to connect to the instance by using the address provided in the Event Streams UI and CLI. The following is an example error message: org.apache.kafka.common.errors.TimeoutException: Topic &lt;topicName&gt; not present in metadata after 60000 msCauses Event Streams deploys OpenShift Container Platform routes. When the hostname of the OpenShift Container Platform cluster is too long, the generated route addresses are truncated. This causes incorrect routing of the connection within Event Streams. The following table provides an example. The output of the oc get routes command shows that the NAME of the route is not the same as the prefix for the HOST/PORT address.             NAME      HOST/PORT      PATH      SERVICES      PORT      TERMINATION      WILDCARD                  &lt;releaseName&gt;-ibm-es-proxy-route-bootstrap      -ibm-es-proxy-route-bootstrap-eventstream-persistence.             &lt;releaseName&gt;-ibm-es-proxy-svc      bootstrap      passthrough/None      None              &lt;releaseName&gt;-ibm-es-proxy-route-broker-0      -ibm-es-proxy-route-broker-0-eventstream-persistence.             &lt;releaseName&gt;-ibm-es-proxy-svc      brk0-external      passthrough/None      None              &lt;releaseName&gt;-ibm-es-proxy-route-broker-1      -ibm-es-proxy-route-broker-1-eventstream-persistence.             &lt;releaseName&gt;-ibm-es-proxy-svc      brk1-external      passthrough/None      None              &lt;releaseName&gt;-ibm-es-proxy-route-broker-2      -ibm-es-proxy-route-broker-2-eventstream-persistence.             &lt;releaseName&gt;-ibm-es-proxy-svc      brk2-external      passthrough/None      None      Resolving the problem The issue is resolved in Event Streams 10.0 and later versions. The workaround for 2019.4 versions is as follows:       Edit the ConfigMap &lt;releaseName&gt;-ibm-es-proxy-cm, for example:     oc edit cm &lt;releaseName&gt;-ibm-es-proxy-cm -n &lt;namespace&gt;     Change bootstrapRoutePrefix from &lt;releaseName&gt;-ibm-es-proxy-route-bootstrap to &lt;truncatedAddress&gt;-ibm-es-proxy-route-bootstrap      Change brokerRoutePrefix from &lt;releaseName&gt;-ibm-es-proxy-route-broker- to &lt;truncatedAddress&gt;-ibm-es-proxy-route-broker-     Important: The trailing hyphen at the end is required.     Remove the externalListeners entry from the map.  Increase the revision value by 1.  Save the changes to the ConfigMap.","categories": ["troubleshooting"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/2019.4/troubleshooting/openshift-route-incorrect/",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-arangodb/Licensing%20and%20support",
        "teaser":null},{
        "title": "ArangoDB",
        "collection": "connectors",
        "excerpt":"Kafka Connect ArangoDB is a Kafka connector that takes records from Apache Kafka, translates them into database changes and performs them against ArangoDB. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-arangodb/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/jaredpetersen/kafka-connect-arangodb.git cd kafka-connect-arangodb mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-arangodb/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-cos/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM Cloud Object Storage",
        "collection": "connectors",
        "excerpt":"IBM Cloud Object Storage sink connector for copying data from a Kafka topic into IBM Cloud Object Storage. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-cos/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/ibm-messaging/kafka-connect-ibmcos-sink cd kafka-connect-ibmcos-sink gradle shadowJar            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-cos/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-couchbase/Licensing%20and%20support",
        "teaser":null},{
        "title": "Couchbase",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector subscribes to one or more Kafka topics and writes the messages to Couchbase. This plugin also includes the corresponding source connector for Couchbase. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-couchbase/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-couchbase/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-elastic/Licensing%20and%20support",
        "teaser":null},{
        "title": "Elasticsearch",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Elasticsearch subscribes to one or more Kafka topics and writes the records to Elasticsearch. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-elastic/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the connector plugin JAR file.     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-elastic/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-filestream/Licensing%20and%20support",
        "teaser":null},{
        "title": "FileStream",
        "collection": "connectors",
        "excerpt":"FileStream sink connector for reading data from a Kafka topic and writing it to a local file. This connector is meant for use in standalone mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-filestream/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Add the connector plugin to your Kafka Connect environment.     The FileStream sink connector plugin JAR file, connect-file-&lt;version&gt;.jar, is included in the libs directory of Kafka. To make the plugin available to your Kafka Connect runtime, ensure it is in the libs directory alongside the Kafka Connect runtime JAR files.         Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-filestream/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-http/Licensing%20and%20support",
        "teaser":null},{
        "title": "HTTP",
        "collection": "connectors",
        "excerpt":"The HTTP sink connector is a Kafka connector for invoking HTTP APIs with data from Apache Kafka. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-http/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/thomaskwscott/kafka-connect-http.git cd kafka-connect-http mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-http/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-memcached/Licensing%20and%20support",
        "teaser":null},{
        "title": "Memcached",
        "collection": "connectors",
        "excerpt":"Kafka Connect for Memcached provides a sink connector that can write data in real time from Apache Kafka to a Memcached environment. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-memcached/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest tar.gz and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-memcached/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mongodb/Licensing%20and%20support",
        "teaser":null},{
        "title": "MongoDB",
        "collection": "connectors",
        "excerpt":"The Kafka Connect MongoDB connector allows you to import data from Kafka topics into MongoDB. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mongodb/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/hpgrahsl/kafka-connect-mongodb.git cd kafka-connect-mongodb mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mongodb/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mq/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM MQ",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for IBM MQ for copying data from Apache Kafka into IBM MQ. Supports connecting to MQ in both bindings and client mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mq/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"IBM Event Streams provides additional help for setting up a Kafka Connect environment and starting the MQ sink connector. Log in to the Event Streams UI, click the Toolbox tab and scroll to the Connectors section. Alternatively, you can download the MQ sink connector from GitHub:       Download the connector plugin JAR file:     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-mq/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-redis/Licensing%20and%20support",
        "teaser":null},{
        "title": "Redis",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Redis is used to write data from Apache Kafka to a Redis cache. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-redis/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the connector plugin JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-redis/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-solr/Licensing%20and%20support",
        "teaser":null},{
        "title": "Solr",
        "collection": "connectors",
        "excerpt":"Kafka Connect sink connector for Solr takes plain JSON data from Kafka topics and pushes it to Solr. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-solr/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/bkatwal/kafka-solr-sink-connector.git cd kafka-solr-sink-connector mvn package            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["sink"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-sink-solr/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-couchbase/Licensing%20and%20support",
        "teaser":null},{
        "title": "Couchbase",
        "collection": "connectors",
        "excerpt":"Kafka Connect source connector publishes document change notifications from Couchbase to a Kafka topic. This plugin also includes the corresponding sink connector for Couchbase. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-couchbase/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-couchbase/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-filestream/Licensing%20and%20support",
        "teaser":null},{
        "title": "FileStream",
        "collection": "connectors",
        "excerpt":"FileStream source connector for reading data from a local file and sending it to a Kafka topic. This connector is meant for use in standalone mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-filestream/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Add the connector plugin to your Kafka Connect environment.     The FileStream source connector plugin JAR file, connect-file-&lt;version&gt;.jar, is included in the libs directory of Kafka. To make the plugin available to your Kafka Connect runtime, ensure it is in the libs directory alongside the Kafka Connect runtime JAR files.         Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-filestream/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mongodbz/Licensing%20and%20support",
        "teaser":null},{
        "title": "MongoDB (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s MongoDB connector can monitor a MongoDB replica set or a MongoDB sharded cluster for document changes in databases and collections, recording those changes as events in Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mongodbz/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mongodbz/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mq/Licensing%20and%20support",
        "teaser":null},{
        "title": "IBM MQ",
        "collection": "connectors",
        "excerpt":"Kafka Connect source connector for IBM MQ for copying data from Apache Kafka into IBM MQ. Supports connecting to MQ in both bindings and client mode. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mq/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"IBM Event Streams provides additional help for setting up a Kafka Connect environment and starting the MQ source connector. Log in to the Event Streams UI, click the Toolbox tab and scroll to the Connectors section. Alternatively, you can download the MQ source connector from GitHub:       Download the connector plugin JAR file:     Go to the connector releases page and download the JAR file for the latest release.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mq/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mysql/Licensing%20and%20support",
        "teaser":null},{
        "title": "MySQL (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s MySQL connector can monitor all of the row-level changes in the databases on a MySQL server or HA MySQL cluster and record them in Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mysql/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-mysql/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-postgresql/Licensing%20and%20support",
        "teaser":null},{
        "title": "PostgreSQL (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s PostgreSQL connector can monitor the row-level changes in the schemas of a PostgreSQL database and record them in separate Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-postgresql/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-postgresql/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-rss/Licensing%20and%20support",
        "teaser":null},{
        "title": "RSS",
        "collection": "connectors",
        "excerpt":"Kafka Connect RSS is a source connector that supports polling multiple URLs and sending output to a single Kafka topic. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-rss/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download the latest ZIP and extract the connector plugin JAR files.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-rss/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-sqlserver/Licensing%20and%20support",
        "teaser":null},{
        "title": "SQL Server (Debezium)",
        "collection": "connectors",
        "excerpt":"Debezium is an open source distributed platform for change data capture. Debezium’s SQL Server Connector can monitor the row-level changes in the schemas of a SQL Server database and record to separate Kafka topics. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-sqlserver/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Download and extract the connector plugin JAR files.     Go to the connector Maven repository. Open the directory for the latest version and download the file ending with -plugin.tar.gz.         Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-sqlserver/installation",
        "teaser":null},{
        "title": "Licensing and support",
        "collection": "connectors",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-weathercompany/Licensing%20and%20support",
        "teaser":null},{
        "title": "Weather Company Data",
        "collection": "connectors",
        "excerpt":"Kafka Connect for Weather Company Data is a source connector for importing data from the IBM Cloud Weather Company Data service into Apache Kafka. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-weathercompany/index",
        "teaser":null},{
        "title": "Installation",
        "collection": "connectors",
        "excerpt":"      Clone the Git repository and build the connector plugin JAR files:      git clone https://github.com/ibm-messaging/kafka-connect-weather-source cd kafka-connect-weather-source gradle build            Add the connector plugin to your Kafka Connect environment.     To add a connector, ensure you have your connector plugin directory or JAR files in the location specified in the plugin.path property of your Kafka Connect worker configuration (for example, &lt;kafka&gt;/config/connect-distributed.properties). The plugin.path property is a comma-separated list of paths to directories that contain connector plugins.     For example:      plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors            Restart Kafka Connect to make the new connector available in the environment.   For more information about how to set up a Kafka Connect environment, add connectors to it, and start the connectors, see how to set up and run connectors.   ","categories": ["source"],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/connectors/kc-source-weathercompany/installation",
        "teaser":null},{
        "title": "Schema registry",
        "collection": "keyFeatures",
        "excerpt":"  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/feature/schemaRegistry",
        "teaser":null},{
        "title": "Multizone support",
        "collection": "keyFeatures",
        "excerpt":"  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/feature/multizone",
        "teaser":null},{
        "title": "Open the Connector catalog",
        "collection": "keyFeatures",
        "excerpt":"No content ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/3.connectorsCatalog",
        "teaser":null},{
        "title": "Chat with us on Slack",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/support/chatOnSlack/",
        "teaser":null},{
        "title": "Give us feedback",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/support/giveUsFeedback/",
        "teaser":null},{
        "title": "IBM Support",
        "collection": "support",
        "excerpt":"","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/support/ibmSupport/",
        "teaser":null},{
        "title": "Running Kafka Streams applications",
        "collection": "tutorials",
        "excerpt":"You can run Kafka Streams applications in IBM Event Streams. Follow the steps in this tutorial to understand how to set up your existing Kafka Streams application to run in Event Streams, including how to set the correct connection and permission properties to allow your application to work with Event Streams. The examples mentioned in this tutorial are based on the WordCountDemo.java sample which reads messages from an input topic called streams-plaintext-input and writes the words, together with an occurrence count for each word, to an output topic called streams-wordcount-output. Prerequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  Ensure you have a Kafka Streams application ready to use. You can also use one of the Kafka Streams sample applications such as the  WordCountDemo.java sample used here.Creating input and output topics Create the input and output topics in Event Streams. For example, you can create the topics and name them as they are named in the WordCountDemo.java sample application. For demonstration purposes, the topics only have 1 replica and 1 partition. To create the topics:   Log in to your IBM Event Streams UI.  Click the Topics tab and click Create topic.  Enter the name streams-plaintext-input and click Next.  Set 1 partition for the topic, leave the default retention period, and select 1 replica.  Click Create topic.  Repeat the same steps to create a topic called streams-wordcount-output.Sending data to input topic To send data to the topic, first set up permissions to produce to the input topic, and then run the Kafka Streams producer to add messages to the topic. To set up permissions:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Select your input topic you created earlier from the list, for example streams-plaintext-input.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Produce only. The name of the input topic is filled in automatically, for example streams-plaintext-input.  Click the Sample code tab, and copy the snippet from the Sample configuration properties section into a new file called streams-demo-input.properties. This creates a new properties file for your Kafka Streams application.  Replace &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the API key generated for the input topic. For example:    security.protocol=SASL_SSLssl.protocol=TLSv1.2ssl.truststore.location=/Users/john.smith/Downloads/es-cert.jksssl.truststore.password=passwordsasl.mechanism=PLAINsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"abcAt0vjYZ1hEwXsRIuy8pxxXHNbEppOF\";      To send messages to the input topic, use the bootstrap address, the input topic name, and the new properties file you created. For example, run kafka-console-producer.sh with the following options:   --broker-list &lt;broker_url&gt;: where &lt;broker_url&gt; is your cluster’s broker URL copied earlier from the Bootstrap server section.  --topic &lt;topic_name&gt;: where &lt;topic_name&gt; is the name of your input topic, in this example, streams-plaintext-input.  --producer.config &lt;properties_file&gt;: where &lt;properties_file&gt; is the new properties file including full path to it, in this example, streams-demo-input.properties.For example: kafka_2.12-1.1.0 $ ./bin/kafka-console-producer.sh \\              --broker-list 192.0.2.24:31248 \\              --topic streams-plaintext-input \\              --producer.config streams-demo-input.properties&gt;This is a test message&gt;This will be used to demo the Streams sample app&gt;It is a Kafka Streams test message&gt;The words in these messages will be counted by the Streams appAnother method to produce messages to the topic is by using the Event Streams producer API. Running the application Set up your Kafka Streams application to connect to your Event Streams instance, have permission to create topics, join consumer groups, and produce and consume messages. You can then use your application to create intermediate Kafka Streams topics, consume from the input topic, and produce to the output topic. To set up permissions and secure the connection:   Log in to your IBM Event Streams UI.  Click Connect to this cluster on the right.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Produce, consume and create topics.The permissions are required to do the following:          Create topics: Kafka Streams creates intermediate topics for the operations performed in the stream.      Join a consumer group: to be able to read messages from the input topic, it joins the group streams-wordcount.      Produce and consume messages.        Click the Sample code tab, and copy the snippet from the Sample connection code section into your Kafka Streams application to set up a secure connection from your application to your Event Streams instance.  Using the snippet, import the following libraries to your application:    import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;        Using the snippet, reconstruct the Properties object as follows, replacing &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the generated API key, for example:    Properties properties = new Properties();properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, \"192.0.2.24:31248\");properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, \"SASL_SSL\");properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, \"TLSv1.2\");properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, \"&lt;certs.jks_file_location&gt;\");properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, \"&lt;truststore_password&gt;\");properties.put(SaslConfigs.SASL_MECHANISM, \"PLAIN\");String saslJaasConfig = \"org.apache.kafka.common.security.plain.PlainLoginModule required \"    + \"username=\\\"token\\\" password=\\\"&lt;api_key&gt;\\\";\";properties.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);        Ensure you download the JAR files for SLF4J and add them to your classpath.Run your Kafka Streams application. To view the topics, log in to your Event Streams UI and click the Topics tab. For example, the following topics are created by the WordCountDemo.java Kafka Streams application: streams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-changelogstreams-wordcount-KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartitionViewing messages on output topic To receive messages from the input topic, first set up permissions so that the output topic can consume messages, and then run the Kafka Streams consumer to send messages to the topic. To set up permissions:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Select your output topic you created earlier from the list, for example streams-wordcount-output.  Click Connect to this topic on the right.  On the Connect a client tab, copy the address from the Bootstrap server section. This gives the bootstrap address for Kafka clients.  From the Certificates section, download the server certificate from the Java truststore section, and make a note of the password.  To generate an API key, go to the API key section and follow the instructions. Ensure you select Consume only. The name of the output topic is filled in automatically, for example streams-wordcount-output.  Click the Sample code tab, and copy the snippet from the Sample configuration properties section into a new file called streams-demo-output.properties. This creates a new properties file for your Kafka Streams application.  Replace &lt;certs.jks_file_location&gt; with the path to your truststore file, &lt;truststore_password&gt; with the password for the JKS file, and &lt;api_key&gt; with the API key generated for the input topic.To view messages on the output topic, use the bootstrap address, the output topic name, and the new properties file you created. For example, run kafka-console-consumer.sh with the following options:   --bootstrap-server &lt;broker_url&gt;: where &lt;broker_url&gt; is your cluster’s broker URL copied earlier from the Bootstrap server section.  --topic &lt;topic_name&gt;: where &lt;topic_name&gt; is the name of your output topic, in this example, streams-wordcount-output.  --consumer.config &lt;properties_file&gt;: where &lt;properties_file&gt; is the new properties file including full path to it, in this example, streams-demo-output.properties.For example: $ ./bin/kafka-console-consumer.sh \\&gt; --bootstrap-server 192.0.2.24:31248 \\&gt; --topic streams-wordcount-output \\&gt; --consumer.config streams-demo-output.properties \\&gt; --from-beginning \\&gt; --group streams-demo-group-consumer \\&gt; --formatter kafka.tools.DefaultMessageFormatter \\&gt; --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\&gt; --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer \\&gt; --property print.key=truethis    1is    1a    1test    1message    1this    2will    1be    1used    1to    3demo    1the    1streams    5sample    1app    1it    1is    2a    2kafka    7streams    6test    2message    2the    2words    1in    1these    1messages    1will    2be    2counted    1by    1the    3streams    7app    2Processed a total of 34 messages","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/kafka-streams-app/",
        "teaser":null},{
        "title": "Monitoring cluster health with Datadog",
        "collection": "tutorials",
        "excerpt":"Event Streams can be configured such that Datadog can capture Kafka broker JMX metrics via its Autodiscovery service. For more information about Autodiscovery, see the Datadog documentation. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  Ensure you have the Datadog Kubernetes agent with JMX installed deployed into the IBM Cloud Private cluster.When installing the agent, ensure the following settings:   The Kubernetes agent requires a less restrictive PodSecurityPolicy than required for Event Streams. It is recommended that you install the agent into a different namespace than where Event Streams is deployed.  For the namespace where you deploy the agent, apply a PodSecurityPolicy to allow the following:          volumes:                  hostPath                    Configuring  for Autodiscovery When installing Event Streams, firstly ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure the Kafka brokers’ JMX ports are accessible to the Datadog Agent. Then supply the YAML object containing the required Check Templates for configuring Kafka JMX monitoring. The example configuration supplied provides an overview of the required fields. You can set the YAML object on the Configuration page by using the configuration option External monitoring &gt; Datadog - Autodiscovery annotation check templates for Kafka brokers. The YAML object is then applied to the Kafka pods as annotations to enable the pods to be recognized by the Datadog agent AutoDiscovery service. The Datadog annotation format is ad.datadoghq.com/&lt;container identifier&gt;.&lt;template name&gt;. However, Event Streams automatically adds the Datadog prefix and container identifier to the annotation, so the YAML object keys must only be &lt;template name&gt; (for example check_names). Providing Check Templates Each Check Template value is a YAML object: check_names: - kafkainstances:  - host: %%host%%See an example Kafka Check Template. Supplying JMX connection values As part of the Kafka instances Check Template, provide values to ensure the Datadog agent can communicate with the Kafka JMX port via SSL as an authenticated user.   rmi_registry_ssl=true  trust_store_path=&lt;path to trust store&gt;  trust_store_password=&lt;password for trust store&gt;  user=&lt;username for authenticating JMX connection&gt;  password=&lt;password for user&gt;Release-specific credentials for establishing the connection are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. Because these values are not known at install time, they cannot be supplied explictly as part of the check templates configuration. Template variables should be used to reference environment variables that will be supplied to each Datadog Agent pod after installing Event Streams. In addition, files contained inside the release-specific secret should be mounted into the Datadog Agent pod using the paths supplied in the configuration. Example Kafka check template content check_names:  - kafkainstances:  - host: %%host%%    rmi_registry_ssl: true    #This path must be used to mound the trust store into each Datadog Agent pod    trust_store_path: /etc/es-jmx/es-kafka-dd.jks    #Note the environment variable used    trust_store_password: %%env_TRUST_STORE_PASSWORD%%    #Note the environment variable used    user: %%env_JMX_USER%%    #Note the environment variable used    password: %%env_JMX_PASSWORD%%    #Port opened on Kafka broker for JMX    port: 9999    tags:      kafka: brokerinit_config:  - is_jmx: true    conf:    - include:        domain: kafka.server        bean: kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec        attribute:          count:            metric_type: rate            alias: kafka.net.bytes_in.rateInstalling through Helm CLI Check Templates can be supplied to Helm CLI installs using the following commands:   Log in to your cluster as an administrator by using the IBM Cloud Private CLI: cloudctl login -a https://&lt;Cluster Master Host&gt;:&lt;Cluster Master API Port&gt;The master host and port for your cluster are set during the installation of IBM Cloud Private.  Supply -f values.yaml to the helm install command, where values.yaml contains:    externalMonitoring:   datadog:     instances:       key: value     check_names:       key: value     ... remaining template values      Configuring your Datadog agent After installation of Event Streams, the DataDog DaemonSet must be edited to supply values for the environment variables and trust store referenced in the check templates. First, the JMX secret must be copied into the Datadog Agent namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;datadog-namespace&gt; create -f - That will create the secret &lt;release-name&gt;-ibm-es-jmx-secret in the DataDog namespace, which can then be referenced in the DaemonSet. The Datadog Agent DaemonSet must now be edited to add in the following information: spec:  containers:    - name: datadog      env:       #Add in new environment variables from the jmx secret. Note that the variable names match the names supplied in the `instances` check template        - name: JMX_USER          secretKeyRef:            key: jmx_username            name: &lt;release-name&gt;-ibm-es-jmx-secret        - name: JMX_PASSWORD          secretRef:            key: jmx_password            name: &lt;release-name&gt;-ibm-es-jmx-secret        - name: TRUST_STORE_PASSWORD          secretRef:            key: trust_store_password            name: &lt;release-name&gt;-ibm-es-jmx-secret      ...      # Mount the secret volume with the mount path that matches the path to the trust store in the `instances` check template      volumeMounts:        - name: es-volume          mountPath: /etc/es-jmx  ...  volumes:    # Mount the jmx secret as a volume, selecting the trust store item    - name: es-volume      fromSecret:        secretName: &lt;release-name&gt;-ibm-es-jmx-secret        items:          - name: truststore.jks            #Note the path should match the name of the trust store file in the `instances` check template            path: es-kafka-dd.jks","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/monitor-with-datadog/",
        "teaser":null},{
        "title": "Monitoring cluster health with Prometheus",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial  details how to deploy a Prometheus JMX exporter into your IBM Cloud Private cluster and export Kafka JMX metrics to an external Prometheus system. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, firstly ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure the Kafka brokers’ JMX ports are accessible to the Prometheus Exporter.  Ensure you have a Prometheus server installed that has network access to your  cluster.  Ensure you have configured access to the Docker registry from the machine you will be using to deploy the JMX exporter.  Ensure you have downloaded the Prometheus JMX exporter httpserver jar file to the machine you will be using to deploy the JMX exporter.Prometheus JMX exporter The Prometheus JMX exporter can be run as a HTTP server which will provide an endpoint for the external Prometheus server to query for metrics data. In order to deploy to your  cluster, the JMX exporter needs to be packaged into a Kubernetes solution. Release-specific credentials for establishing the connection between the JMX exporter and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying the JMX exporter in a different namesapce to your Event Streams installation, the secret must be copied to the required namespace. kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - That will create the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the prometheus-exporter-&lt;broker-num&gt;.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   JMX exporter packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where the JMX exporter will be deployed.  Kubernetes pod specification created that exposes the configuration to the JMX exporter via environment variables and a ConfigMap.  Kubernetes ConfigMap containing the JMX exporter YAML configuration.  Kubernetes NodePort service to expose access to the JMX exporter for the external Prometheus server.Example Dockerfile Create a Dockerfile as follows. FROM &lt;base OS Docker image with Java&gt;WORKDIR /opt/prometheusCOPY jmx_prometheus_httpserver.jar .COPY run.sh .CMD ./run.shExample run.sh Create a run.sh script as follows. The script copies the YAML configuration and appends the release-specific connection values. It then runs the JMX exporter as an HTTP server. cp /etc/jmx-config/config.yaml /tmp/jmx-config.yamlcat &lt;&lt; EOF &gt;&gt; /tmp/jmx-config.yamlssl: trueusername: ${JMX_USER}password: ${JMX_PASSWORD}hostPort: ${JMX_HOST}:9999EOFjava -Djavax.net.ssl.trustStore=/etc/jmx-secret/store.jks -Djavax.net.ssl.trustStorePassword=${STORE_PW} -jar jmx_prometheus_httpserver.jar 0.0.0.0:8080 /tmp/jmx-config.yamlAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Building the Docker image Build the Docker image as follows.   Ensure that the Dockerfile, run.sh, and jmx-exporter.jar are in the same directory.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the image: docker build -t &lt;remote-registry&gt;:&lt;remote-registry-port&gt;/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt; .  Push the image to your IBM Cloud Private cluster Docker registry: docker push &lt;remote-registry&gt;:&lt;remote-registry-port&gt;/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create and expose a JMX exporter for each Kafka broker. However, the configuration ConfigMap can be shared across each instance.   Copy the following into a file called prometheus-config.yaml:    ---apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-configdata:  config.yaml: |-    startDelaySeconds: 10    lowercaseOutputName: true    rules:      # The following rules match the Kafka MBeans in the jconsole order      # Broker metrics    - pattern : kafka.server&lt;type=BrokerTopicMetrics, name=(BytesInPerSec|BytesOutPerSec)&gt;&lt;&gt;(Count)      name: kafka_server_BrokerTopicMetrics_$1_$2        Create the ConfigMap in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f prometheus-config.yaml  Copy the following into a file called prometheus-exporter-&lt;broker-num&gt;.yaml       apiVersion: v1kind: Podmetadata:    name: prometheus-export-broker-&lt;broker-num&gt;    labels:       app: prometheus-export-broker-&lt;broker-num&gt;spec:    containers:    - name: jmx-exporter      image: &lt;full name of docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          # mountPath must match path supplied to -Djavax.net.ssl.trustStore in run.sh          mountPath: /etc/jmx-secret        - name: config-volume          # mountPath must match path supplied to the cp command in run.sh          mountPath: /etc/jmx-config      ports:      - containerPort: 8080      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-&lt;broker-num&gt;.&lt;release-namespace&gt;.svc      - name: STORE_PW        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              # Path must match the filename supplied to -Djavax.net.ssl.trustStore in run.sh              path: store.jks        - name: config-volume          configMap:             name: prometheus-config---apiVersion: v1kind: Servicemetadata:  name: prometheus-svc-&lt;broker-num&gt;spec:  type: NodePort  selector:    app: prometheus-export-broker-&lt;broker-num&gt;  ports:  - port : 8080    protocol: TCP               Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f prometheus-exporter-&lt;broker-num&gt;.yaml.  Repeat for the total number of Kafka brokers deployed in your Event Streams installation.  After the resources are created, find the NodePort by using the command kubectl -n &lt;target-namespace&gt; get svc. The NodePort is listed for the service prometheus-svc-&lt;broker-num&gt;. The connection to be supplied to your external Prometheus as static scrape target in the Prometheus configuration file is the url &lt;cluster name/IP&gt;:&lt;node-port&gt;Troubleshooting If metrics are not appearing in your external Prometheus, check the logs for the Prometheus agent with the following command: kubectl -n &lt;target-namespace&gt; get logs prometheus-export-broker-&lt;broker-num&gt; ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/monitor-with-prometheus/",
        "teaser":null},{
        "title": "Monitoring cluster health with a Splunk HTTP Event Collector",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial details how to deploy jmxtrans into your IBM Cloud Private cluster to export Kafka JMX metrics as graphite output, and then use Logstash to write the metrics to an external Splunk system as an HTTP Event Collector. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure that each Kafka broker’s JMX port is accessible to jmxtrans.  Ensure you have a Splunk Enterprise server installed or a Splunk Universal Forwarder that has network access to your  cluster.  Ensure that you have an index to receive the data and an HTTP Event Collector configured on Splunk. Details can be found in the Splunk documentation  Ensure you have configured access to the Docker registry from the machine you will be using to deploy jmxtrans.jmxtrans Jmxtrans is a connector that reads JMX metrics and outputs a number of formats supporting a wide variety of logging, monitoring, and graphing applications. To deploy to your  cluster, you must package jmxtrans into a Kubernetes solution. Release-specific credentials for establishing the connection between jmxtrans and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying jmxtrans in a different namespace to your Event Streams installation, copy the secret to the required namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - The command creates the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the hec.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   Jmxtrans packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where Logstash and jmxtrans will be deployed.  Logstash packaged into a Docker image to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where Logstash and jmxtrans will be deployed.  Kubernetes pod specification created that exposes the configuration to jmxtrans and Logstash via environment variables.Example Dockerfile.jmxtrans Create a Dockerfile called Dockerfile.jmxtrans as follows. FROM jmxtrans/jmxtransCOPY run.sh .ENTRYPOINT [ \"./run.sh\" ]Example run.sh Create a run.sh script as follows. The script generates the JSON configuration file and substitute the release-specific connection values. It then runs jmxtrans. #!/bin/shcat &lt;&lt;EOF &gt;&gt; /var/lib/jmxtrans/config.json{  \"servers\": [    {      \"port\": 9999,      \"host\": \"${JMX_HOST_0}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_1}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_2}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"localhost\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    }  ]}EOFexec /docker-entrypoint.sh start-without-jmxAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Example Dockerfile.logstash Create a Dockerfile called Dockerfile.logstash as follows. FROM docker.elastic.co/logstash/logstash:&lt;required-logstash-version&gt;RUN /usr/share/logstash/bin/logstash-plugin install logstash-input-graphiteRUN rm -f /usr/share/logstash/pipeline/logstash.confCOPY pipeline/ /usr/share/logstash/pipeline/COPY config/ /usr/share/logstash/config/Example logstash.yml Create a Logstash settings file called logstash.yml as follows. path.config: /usr/share/logstash/pipeline/Example logstash.conf Create a Logstash configuration file called logstash.conf as follows. input {    graphite {        host =&gt; \"localhost\"        port =&gt; 9999        mode =&gt; \"server\"    }}output {    http {        http_method =&gt; \"post\"        url =&gt; \"http://&lt;splunk-host-name-or-ip-address&gt;:&lt;splunk-http-event-collector-port&gt;/services/collector/event\"        headers =&gt; [\"Authorization\", \"Splunk &lt;splunk-http-event-collector-token&gt;\"]        mapping =&gt; {\"event\" =&gt; \"%{message}\"}    }}Building the Docker image Build the Docker images as follows.   Ensure that Dockerfile.jmxtrans, Dockerfile.logstash and run.sh are in the same directory. Edit Dockerfile.logstash and replace &lt;required-logstash-version&gt; with the Logstash version you would like to use as a basis.  Ensure that logstash.yml is in a subdirectory called config/ of the directory in step 1.  Ensure that logstash.conf is in a subdirectory called pipeline/ of the directory in step 1.  Edit logstash.conf, and replace &lt;splunk-host-name-or-ip-address&gt; with the external Splunk Enterprise, Splunk Universal forwarder, or Splunk Cloud host name or IP address.Replace &lt;splunk-http-event-collector-port&gt; with the HTTP Event Collector port number.Replace &lt;splunk-http-event-collector-token&gt; with the HTTP Event Collector token setup on the Splunk HTTP Event Collector Data input.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the jmxtrans image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;jmxtrans-image-name&gt;:&lt;image-version&gt; -f Dockerfile.jmxtrans .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;jmxtrans-image-name&gt;:&lt;image-version&gt;  Create the logstash image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;logstash-image-name&gt;:&lt;image-version&gt; -f Dockerfile.logstash .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;logstash-image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create a pod for jmxtrans and Logstash as follows.   Copy the following into a file called hec.yaml.    apiVersion: v1kind: Podmetadata:    name: jmxtrans-broker    labels:       app: jmxtrans-brokerspec:    containers:    - name: logstash      image: &lt;full name of logstash docker image pushed to remote registry&gt;    - name: jmxtrans      image: &lt;full name of jmxtrans docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          mountPath: /etc/jmx-secret      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST_0        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-0.&lt;release-namespace&gt;.svc      - name: JMX_HOST_1        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-1.&lt;release-namespace&gt;.svc      - name: JMX_HOST_2        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-2.&lt;release-namespace&gt;.svc      - name: SSL_TRUSTSTORE        value: /etc/jmx-secret/store.jks      - name: SSL_TRUSTSTORE_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password      - name: JAVA_OPTS        value: -Djavax.net.ssl.trustStore=$(SSL_TRUSTSTORE) -Djavax.net.ssl.trustStorePassword=$(SSL_TRUSTSTORE_PASSWORD)      # The SECONDS_BETWEEN_RUNS is the scrape frequency of the JMX values. The default value is 60 seconds. Change it to a value to suit your requirements.      - name: SECONDS_BETWEEN_RUNS        value: \"15\"    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              path: store.jks        Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f hec.yaml.Events start appearing in Splunk after running the command. Troubleshooting If metrics are not appearing in your external Splunk, check the logs for jmxtrans and for Logstash with the following commands: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c jmxtrans kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c logstash To get debug-level logs from jmxtrans, use the following steps:   Copy the following into a file called logback.xml.    &lt;configuration debug=\"false\"&gt;  &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;    &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;    &lt;encoder&gt;      &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;logger name=\"com.googlecode.jmxtrans\" level=\"${logLevel}\"/&gt;  &lt;root level=\"info\"&gt;    &lt;appender-ref ref=\"console\" /&gt;  &lt;/root&gt;&lt;/configuration&gt;        Add the file to the same directory as the Dockerfile and run.sh files.  Edit the Dockerfile to include the logback.xml file, for example:    FROM jmxtrans/jmxtransCOPY logback.xml /usr/share/jmxtrans/conf/logback.xmlCOPY configure.sh .ENTRYPOINT [ \"./configure.sh\" ]        Follow the instructions for building the docker image.  Add the following environment variable to the hec.yaml file after the env: property:      - name: JMXTRANS_OPTS    value: -Djmxtrans.log.level=debug        Delete jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; delete pod jmxtrans-broker  Check that it has been deleted with the following command kubectl -n &lt;target-namespace&gt; get pods  When it has been deleted, create it again with the following command: kubectl -n &lt;target-namespace&gt; apply -f hec.yaml  View the logs with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker -c jmxtrans","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/monitor-with-splunk-hec/",
        "teaser":null},{
        "title": "Monitoring cluster health with Splunk",
        "collection": "tutorials",
        "excerpt":"You can configure Event Streams to allow JMX scrapers to export Kafka broker JMX metrics to external applications. This tutorial  details how to deploy jmxtrans into your IBM Cloud Private cluster to export Kafka JMX metrics as graphite output to an external Splunk system using a TCP data input. Prequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1.  When installing Event Streams, ensure you select the Enable secure JMX connections check box in the Kafka broker settings. This is required to ensure that each Kafka broker’s JMX port is accessible to jmxtrans.  Ensure you have a Splunk Enterprise server installed or a Splunk Universal Forwarder that has network access to your  cluster.  Ensure that you have an index to receive the data and a TCP Data input configured on Splunk. Details can be found in the Splunk documentation.  Ensure you have configured access to the Docker registry from the machine you will be using to deploy jmxtrans.jmxtrans Jmxtrans is a connector that reads JMX metrics and outputs a number of formats supporting a wide variety of logging, monitoring, and graphing applications. To deploy to your  cluster, you must package jmxtrans into a Kubernetes solution. Release-specific credentials for establishing the connection between jmxtrans and the Kafka brokers are generated when Event Streams is installed with the Enable secure JMX connections selected. The credentials are stored in a Kubernetes secret inside the release namespace. See secure JMX connections for information about the secret contents. If you are deploying jmxtrans in a different namespace to your Event Streams installation, copy the secret to the required namespace with the following command: kubectl -n &lt;release-namespace&gt; get secret &lt;release-name&gt;-ibm-es-jmx-secret -o yaml --export | kubectl -n &lt;target-namespace&gt; create -f - The command creates the secret &lt;release-name&gt;-ibm-es-jmx-secret in the target namespace, which can then be referenced in the jmxtrans.yaml file later. Solution overview The tasks in this tutorial help achieve the following:   Jmxtrans packaged into a Docker image, along with scripts to load configuration values and connection information.  Docker image pushed to the IBM Cloud Private cluster Docker registry into the namespace where jmxtrans will be deployed.  Kubernetes pod specification created that exposes the configuration to jmxtrans via environment variables.Example Dockerfile Create a Dockerfile as follows. FROM jmxtrans/jmxtransCOPY run.sh .ENTRYPOINT [ \"./run.sh\" ]Example run.sh Create a run.sh script as follows. The script generates the JSON configuration file and substitutes the release-specific connection values. It then runs jmxtrans. #!/bin/shcat &lt;&lt;EOF &gt;&gt; /var/lib/jmxtrans/config.json{  \"servers\": [    {      \"port\": 9999,      \"host\": \"${JMX_HOST_0}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_1}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    },    {      \"port\": 9999,      \"host\": \"${JMX_HOST_2}\",      \"ssl\": true,      \"username\": \"${JMX_USER}\",      \"password\": \"${JMX_PASSWORD}\",      \"queries\": [        {          \"obj\": \"kafka.server:type=BrokerTopicMetrics,name=Bytes*PerSec\",          \"attr\": [ \"Count\" ],          \"outputWriters\": [            {              \"@class\": \"com.googlecode.jmxtrans.model.output.GraphiteWriterFactory\",              \"port\": 9999,              \"host\": \"$SPLUNK_HOST\",              \"typeNames\": [ \"name\" ],              \"flushDelayInSeconds\": 5            }          ]        }      ]    }  ]}EOFexec /docker-entrypoint.sh start-without-jmxAfter you have created the file, ensure that it has execution permission by running chmod 755 run.sh. Building the Docker image Build the Docker image as follows.   Ensure that the Dockerfile and run.sh are in the same directory.  Verify that your cluster IP is mapped to the mycluster.icp parameter by checking your system’s host file: cat /etc/hostsIf it is not, change the value to your cluster by editing your system’s host file: sudo vi /etc/hosts  Create a local directory, and copy the certificates file from the IBM Cloud Private master node to the local machine:  sudo mkdir -pv /etc/docker/certs.d/mycluster.icp\\:8500/  sudo scp root@&lt;Cluster Master Host&gt;:/etc/docker/certs.d/mycluster.icp\\:8500/ca.crt /etc/docker/certs.d/mycluster.icp\\:8500/  On macOS only, run the following command:sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/mycluster.icp\\:8500/ca.crt  Restart Docker.  Log in to Docker: docker login mycluster.icp:8500  Create the image: docker build -t mycluster.icp:8500/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt; .  Push the image to your IBM Cloud Private cluster Docker registry: docker push mycluster.icp:8500/&lt;target-namespace&gt;/&lt;image-name&gt;:&lt;image-version&gt;Example Kubernetes deployment file Create a jmxtrans pod as follows.   Copy the following into a file called jmxtrans.yaml.    apiVersion: v1kind: Podmetadata:    name: jmxtrans-broker    labels:       app: jmxtrans-brokerspec:    containers:    - name: jmxtrans      image: &lt;full name of docker image pushed to remote registry&gt;      volumeMounts:        - name: jmx-secret-volume          mountPath: /etc/jmx-secret      env:      - name: JMX_USER        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_username      - name: JMX_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: jmx_password      - name: JMX_HOST_0        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-0.&lt;release-namespace&gt;.svc      - name: JMX_HOST_1        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-1.&lt;release-namespace&gt;.svc      - name: JMX_HOST_2        value: &lt;release-name&gt;-ibm-es-kafka-broker-svc-2.&lt;release-namespace&gt;.svc      - name: SSL_TRUSTSTORE        value: /etc/jmx-secret/store.jks      - name: SSL_TRUSTSTORE_PASSWORD        valueFrom:          secretKeyRef:            name: &lt;release-name&gt;-ibm-es-jmx-secret            key: trust_store_password      - name: JAVA_OPTS        value: -Djavax.net.ssl.trustStore=$(SSL_TRUSTSTORE) -Djavax.net.ssl.trustStorePassword=$(SSL_TRUSTSTORE_PASSWORD)      # The SECONDS_BETWEEN_RUNS is the scrape frequency of the JMX values. The default value is 60 seconds. Change it to a value to suit your requirements.      - name: SECONDS_BETWEEN_RUNS        value: \"15\"      - name: SPLUNK_HOST        value: &lt;splunk-hostname-or-ip-address&gt;    volumes:        - name: jmx-secret-volume          secret:            secretName: &lt;release-name&gt;-ibm-es-jmx-secret            items:            - key: truststore.jks              path: store.jks        Create the resources in your IBM Cloud Private cluster with the following command:kubectl -n &lt;target-namespace&gt; apply -f jmxtrans.yamlEvents start appearing in Splunk after running the command. The amount of time it takes before events appear in the Splunk index depends on a combination of the scrape interval on jmxtrans and the size of the receive queue on Splunk. You can increase or decrease the frequency of samples in jmxtrans and the size of the receive queue. To modify the receive queue on Splunk, create an inputs.conf file, and specify the queueSize and persistentQueueSize settings of the [tcp://&lt;remote server&gt;:&lt;port&gt;] stanza. Troubleshooting If metrics are not appearing in your external Splunk, check the logs for jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker To get debug-level logs from jmxtrans, use the following steps:   Copy the following into a file called logback.xml.    &lt;configuration debug=\"false\"&gt;  &lt;appender name=\"console\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;    &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;    &lt;encoder&gt;      &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;logger name=\"com.googlecode.jmxtrans\" level=\"${logLevel}\"/&gt;  &lt;root level=\"info\"&gt;    &lt;appender-ref ref=\"console\" /&gt;  &lt;/root&gt;&lt;/configuration&gt;        Add the file to the same directory as the Dockerfile and run.sh files.  Edit the Dockerfile to include the logback.xml file, for example:    FROM jmxtrans/jmxtransCOPY logback.xml /usr/share/jmxtrans/conf/logback.xmlCOPY configure.sh .ENTRYPOINT [ \"./configure.sh\" ]        Follow the instructions for building the docker image.  Add the following environment variable to the jmxtrans.yaml file after the env: property:      - name: JMXTRANS_OPTS    value: -Djmxtrans.log.level=debug        Delete jmxtrans with the following command: kubectl -n &lt;target-namespace&gt; delete pod jmxtrans-broker  Check that it has been deleted with the following command kubectl -n &lt;target-namespace&gt; get pods  When it has been deleted, create it again with the following command: kubectl -n &lt;target-namespace&gt; apply -f jmxtrans.yaml  View the logs with the following command: kubectl -n &lt;target-namespace&gt; get logs jmxtrans-broker","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/monitor-with-splunk/",
        "teaser":null},{
        "title": "Setting up alert notifications to Slack",
        "collection": "tutorials",
        "excerpt":"Receiving notifications based on monitored metrics is an important way of keeping an eye on the health of your cluster. You can set up notifications to be sent to applications like Slack based on pre-defined triggers. The following tutorial shows an example of how to set up alert notifications to be sent to Slack based on metrics from Event Streams. Prerequisites   Ensure you have an Event Streams installation available. This tutorial is based on Event Streams version 2019.1.1 installed on IBM Cloud Private 3.1.1, using the default master port 8443.  Ensure you have Slack installed and ready to use. This tutorial is based on Slack version 3.3.8.  You need to be a Workplace Administrator to add apps to a Slack channel.Preparing Slack To send notifications from Event Streams to your Slack channel, configure an incoming webhook URL within your Slack service. The webhook URL provided by Slack is required for the integration steps later in this section. To create the webhook URL:   Open Slack and go to your Slack channel where you want the notifications to be sent.  From your Slack channel click the icon for Channel Settings, and select Add apps or Add an app depending on the Slack plan you are using.  Search for “Incoming Webhooks”.  Click Add configuration.  Select the channel that you want to post to.  Click Add Incoming Webhooks integration.  Copy the URL in the Webhook URL field.For more information about incoming webhooks in Slack, see the Slack documentation. Selecting the metric to monitor To retrieve a list of available metrics, use an HTTP GET request on your ICP cluster URL as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:8443.  Use the following request: https://&lt;Cluster Master Host&gt;:8443/prometheus/api/v1/label/__name__/valuesThe list of available metrics is displayed.  Select a metric to monitor.For example, to test the triggering of alerts, you can monitor the total number of partitions for all topics by using the kafka_server_replicamanager_partitioncount_value metric. When topics are created, this metric can trigger notifications. For production environments, a good metric to monitor is the number of under-replicated partitions as it tells you about potential problems with your Kafka cluster, such as load or network problems where the cluster becomes overloaded and followers are not able to catch up on leaders. Under-replicated partitions might be a temporary problem, but if it continues for longer, it probably requires urgent attention. An example is to set up a notification trigger to your Slack channel if the number of under-replicated partitions is greater than 0 for more than a minute. You can do this with the kafka_server_replicamanager_underreplicatedpartitions_value metric. The examples in this tutorial show you how to set up monitoring for both of these metrics, with the purpose of testing notification triggers, and also to have a production environment example. Note: Not all of the metrics that Kafka uses are published to Prometheus by default. The metrics that are published are controlled by a ConfigMap. You can publish metrics by adding them to the ConfigMap. For information about the different metrics, see Monitoring Kafka. Setting the alert rule To set up the alert rule and define the trigger criteria, use the monitoring-prometheus-alertrules ConfigMap. By default, the list of rules is empty. See the data section of the ConfigMap, for example: user$ kubectl get configmap -n kube-system monitoring-prometheus-alertrules -o yamlapiVersion: v1data:  alert.rules: \"\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"4564\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Example test setup As mentioned earlier, to test the triggering of alerts, you can monitor the total number of partitions for all topics by using the kafka_server_replicamanager_partitioncount_value metric. Define an alert rule that creates a notification if the number of partitions increases. To achieve this, add a new rule for kafka_server_replicamanager_partitioncount_value, and set the trigger conditions in the data section, for example: Note: In this example, we are setting a threshold value of 50 as the built-in consumer-offsets topic has 50 partitions by default already, and this topic is automatically created the first time a consumer application connects to the cluster. We will create a topic later with 10 partitions to test the firing of the alert and the subsequent notification to the Slack channel. user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertrulesapiVersion: v1data:  sample.rules: |-    groups:    - name: alert.rules      #      # Each of the alerts you want to create will be listed here      rules:      # Posts an alert if the number of partitions increases      - alert: PartitionCount        expr: kafka_server_replicamanager_partitioncount_value &gt; 50        for: 10s        labels:          # Labels should match the alert manager so that it is received by the Slack hook          severity: critical        # The contents of the Slack messages that are posted are defined here        annotations:          identifier: \"Partition count\"          description: \"There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"84156\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Important: As noted in the prerequisites, this tutorial is based on IBM Cloud Private 3.1.1. Setting up alert rules is different if you are using IBM Cloud Private 3.1.2 or later, as each alert rule is a dedicated Kubernetes resource instead of being defined in a ConfigMap. This means that instead of adding alert rule entries to a ConfigMap, you create a separate alert rule resource for each alert you want to enable. In addition, the alert rules don’t need to be in the kube-system namespace, they can be added to the namespace where your release is deployed. This also means you don’t have to be a Cluster administrator to add alert rules. For example, to create the rule by using a dedicated alert rule, you can save it to a file as follows: apiVersion: monitoringcontroller.cloud.ibm.com/v1kind: AlertRulemetadata:  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.4.0    component: prometheus    heritage: Tiller    release: RELEASENAME  name: partition-count  namespace: NAMESPACEspec:  data: |-    groups:      - name: PartitionCount        rules:          - alert: PartitionCount            expr: kafka_server_replicamanager_partitioncount_value &gt; 50            for: 10s            labels:              severity: critical            annotations:              identifier: 'Partition count'              description: 'There are {{ $value }} partition(s) reported by broker {{ $labels.kafka }}'  enabled: trueTo review your alert rules set up this way, use the kubectl get alertrules command, for example: $ kubectl get alertrulesNAME                          ENABLED   AGE   CHART                     RELEASE         ERRORSpartition-count               true      1h    ibm-icpmonitoring-1.4.0   es-demoExample production setup As mentioned earlier, a good metric to monitor in production environments is the metric kafka_server_replicamanager_underreplicatedpartitions_value, for which we want to define an alert rule that creates a notification if the number of under-replicated partitions is greater than 0 for more than a minute. To achieve this, add a new rule for kafka_server_replicamanager_underreplicatedpartitions_value, and set the trigger conditions in the data section, for example: user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertrulesapiVersion: v1data:  sample.rules: |-    groups:    - name: alert.rules      #      # Each of the alerts you want to create will be listed here      rules:      # Posts an alert if there are any under-replicated partitions      #  for longer than a minute      - alert: under_replicated_partitions        expr: kafka_server_replicamanager_underreplicatedpartitions_value &gt; 0        for: 1m        labels:          # Labels should match the alert manager so that it is received by the Slack hook          severity: critical        # The contents of the Slack messages that are posted are defined here        annotations:          identifier: \"Under-replicated partitions\"          description: \"There are {{ $value }} under-replicated partition(s) reported by broker {{ $labels.kafka }}\"kind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: prometheus    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertrules  namespace: kube-system  resourceVersion: \"84156\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertrules  uid: a87b5766-c89f-11e8-9f94-00000a3304c0Important: As noted in the prerequisites, this tutorial is based on IBM Cloud Private 3.1.1. Setting up alert rules is different if you are using IBM Cloud Private 3.1.2 or later, as each alert rule is a dedicated Kubernetes resource instead of being defined in a ConfigMap. This means that instead of adding alert rule entries to a ConfigMap, you create a separate alert rule resource for each alert you want to enable. In addition, the alert rules don’t need to be in the kube-system namespace, they can be added to the namespace where your release is deployed. This also means you don’t have to be a Cluster administrator to add alert rules. For example, to create the rule by using a dedicated alert rule, you can save it to a file as follows: apiVersion: monitoringcontroller.cloud.ibm.com/v1kind: AlertRulemetadata:  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.4.0    component: prometheus    heritage: Tiller    release: RELEASENAME  name: under-replicated-partitions  namespace: NAMESPACEspec:  data: |-    groups:      - name: UnderReplicatedPartitions        rules:          - alert: UnderReplicatedPartitions            expr: kafka_server_replicamanager_underreplicatedpartitions_value &gt; 0            for: 1m            labels:              severity: critical            annotations:              identifier: 'Under-replicated partitions'              description: 'There are {{ $value }} under-replicated partition(s) reported by broker {{ $labels.kafka }}'  enabled: trueTo review your alert rules set up this way, use the kubectl get alertrules command, for example: $ kubectl get alertrulesNAME                          ENABLED   AGE   CHART                     RELEASE         ERRORSunder-replicated-partitions   true      1h    ibm-icpmonitoring-1.4.0   es-prodDefining the alert destination To define where to send the notifications triggered by the alert rule, specify Slack as a receiver by adding details about your Slack channel and the webhook you copied earlier to the monitoring-prometheus-alertmanager ConfigMap. For more information about Prometheus Alertmanager, see the Prometheus documentation. By default, the list of receivers is empty. See the data section of the ConfigMap, for example: user$ kubectl get configmap -n kube-system monitoring-prometheus-alertmanager -o yamlapiVersion: v1data:  alertmanager.yml: |-    global:    receivers:      - name: default-receiver    route:      group_wait: 10s      group_interval: 5m      receiver: default-receiver      repeat_interval: 3hkind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: alertmanager    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertmanager  namespace: kube-system  resourceVersion: \"4565\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertmanager  uid: a87bdb44-c89f-11e8-9f94-00000a3304c0Define the Slack channel as the receiver using the incoming webhook you copied earlier, and also set up the notification details such as the channel to post to, the content format, and criteria for the events to send to Slack. Settings to configure include the following:   slack_api_url: The incoming webhook generated in Slack earlier.  send_resolved: Set to true to send notifications about resolved alerts.  channel: The Slack channel to send the notifications to.  username: The username that posts the alert notifications to the channel.For more information about the configuration settings to enter for Slack notifications, see the Prometheus documentation. The content for the posts can be customized, see the following blog for Slack alert examples from Prometheus. For example, to set up Slack notifications for your alert rule created earlier: user$ kubectl edit configmap -n kube-system monitoring-prometheus-alertmanagerapiVersion: v1data:  alertmanager.yml: |-    global:      # This is the URL for the Incoming Webhook you created in Slack      slack_api_url:  https://hooks.slack.com/services/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    receivers:      - name: default-receiver        #        # Adding a Slack channel integration to the default Prometheus receiver        #  see https://prometheus.io/docs/alerting/configuration/#slack_config        #  for details about the values to enter        slack_configs:        - send_resolved: true          # The name of the Slack channel that alerts should be posted to          channel: \"#ibm-eventstreams-demo\"          # The username to post alerts as          username: \"IBM Event Streams\"          # An icon for posts in Slack          icon_url: https://developer.ibm.com/messaging/wp-content/uploads/sites/18/2018/09/icon_dev_32_24x24.png          #          # The content for posts to Slack when alert conditions are fired          # Improves on the formatting from the default, with support for handling          #  alerts containing multiple events.          # (Modified from the examples in          #   https://medium.com/quiq-blog/better-slack-alerts-from-prometheus-49125c8c672b)          title: |-            [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}{{ range .Alerts.Firing }} @ {{ .Annotations.identifier }}{{ end }}{{ range .Alerts.Resolved }} @ {{ .Annotations.identifier }}{{ end }}{{ end }}          text: |-            {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}            {{ range .Alerts.Firing }}{{ .Annotations.description }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.description }}{{ end }}            {{ else }}            {{ if gt (len .Alerts.Firing) 0 }}            *Alerts Firing:*            {{ range .Alerts.Firing }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}            {{ end }}{{ end }}            {{ if gt (len .Alerts.Resolved) 0 }}            *Alerts Resolved:*            {{ range .Alerts.Resolved }}- {{ .Annotations.identifier }}: {{ .Annotations.description }}            {{ end }}{{ end }}            {{ end }}    route:      group_wait: 10s      group_interval: 5m      receiver: default-receiver      repeat_interval: 3h      #      # The criteria for events that should go to Slack      routes:      - match:          severity: critical        receiver: default-receiverkind: ConfigMapmetadata:  creationTimestamp: 2019-04-05T13:07:48Z  labels:    app: monitoring-prometheus    chart: ibm-icpmonitoring-1.2.0    component: alertmanager    heritage: Tiller    release: monitoring  name: monitoring-prometheus-alertmanager  namespace: kube-system  resourceVersion: \"4565\"  selfLink: /api/v1/namespaces/kube-system/configmaps/monitoring-prometheus-alertmanager  uid: a87bdb44-c89f-11e8-9f94-00000a3304c0To check that the new alert is set up, use the Prometheus UI as follows:   Log in to your IBM Cloud Private cluster management console from a supported web browser by using the URL https://&lt;Cluster Master Host&gt;:8443.  Go to the Prometheus UI at https://&lt;Cluster Master Host&gt;:8443/prometheus, and click the Alerts tab to see the active alerts. You can also go to Status &gt; Rules to view the defined alert rules.For example:   Testing Example test setup To create a notification for the test setup, create a topic with 10 partitions as follows:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Click Create topic.  Follow the instructions to create the topic, and set the Partitions value to 10.The following notification is sent to the Slack channel when the topic is created:  To create a resolution alert, delete the topic you created previously:   Log in to your IBM Event Streams UI.  Click the Topics tab.  Go to the topic you created and click  More options &gt; Delete this topic.When the topic is deleted, the following resolution alert is posted:  Example production setup For the production environment example, the following notification is posted to the Slack channel if the number of under-replicated partitions remains above 0 for a minute:  When the cluster recovers, a new resolution alert is posted when the number of under-replicated partitions returns to 0. This is based on the send_resolved setting (was set to true).  Setting up other notifications You can use this example to set up alert notifications to other applications, including HipChat, PagerDuty, emails, and so on. You can also use this technique to generate HTTP calls, which lets you customize alerts when defining a flow in tools like Node-RED or IBM App Connect. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/monitoring-alerts/",
        "teaser":null},{
        "title": "Installing a multizone cluster",
        "collection": "tutorials",
        "excerpt":"The following tutorial explains how to set up a multizone Event Streams cluster in a non-zone-aware cluster. The example used in this tutorial is for installing a 3 zone cluster. The example shows how to prepare your cluster for multiple zones by labelling your nodes, and then using those labels to set up the zones when installing Event Streams. Prerequisites This tutorial is based on the following software versions:   IBM Cloud Private version 3.2.1  IBM Event Streams version 2019.4.1Labelling the worker nodes To make your IBM Cloud Private cluster zone aware, label your worker nodes to be able to specify later the zones that each node will be added to. Labelling for zones Label your IBM Cloud Private worker nodes, so that they can later be allocated to zones. Run the following command to retrieve the nodes in your cluster: kubectl get nodes This will list the nodes of your cluster. In this example there are 12 nodes: 192.0.0.1 192.0.0.2 192.0.0.3 192.0.0.4 192.0.0.5 192.0.0.6 192.0.0.7 192.0.0.8 192.0.0.9 192.0.0.10 192.0.0.11 192.0.0.12Using the node IP addresses, set which zone you want each node to be in. In this tutorial, you are setting up a 3 zone cluster by using the following labels, each representing a data center:   es-zone-0  es-zone-1  es-zone-2Label your nodes with these zone labels by using the kubectl label nodes command. In this example, as you are creating a 3 zone cluster, and have 12 nodes, label every 4 nodes with the same zone label. Label the first 4 nodes 192.0.0.1-4 to allocate them to zone es-zone-0: kubectl label nodes 192.0.0.1 192.0.0.2 192.0.0.3 192.0.0.4 failure-domain.beta.kubernetes.io/zone=\"es-zone-0\" Label the next 4 nodes 192.0.0.5 192.0.0.6 192.0.0.7 192.0.0.8 to allocate them to zone es-zone-1: kubectl label nodes 192.0.0.5-8 failure-domain.beta.kubernetes.io/zone=\"es-zone-1\" Label the next 4 nodes 192.0.0.9 192.0.0.10 192.0.0.11 192.0.0.12 to allocate them to zone es-zone-2: kubectl label nodes 192.0.0.9-12 failure-domain.beta.kubernetes.io/zone=\"es-zone-2\" As a result, all nodes that Event Streams will use now have a label of failure-domain.beta.kubernetes.io/zone with value es-zone-0, es-zone-1, or es-zone-2. Run the following command to verify this: kubectl get nodes --show-labels Labelling for Kafka Kafka broker pods need to be distributed as evenly as possible across the zones by dedicating a node in each zone to a Kafka pod. In this example, there are 6 Kafka brokers. This means 6 worker nodes are needed to host our Kafka brokers. Splitting the 6 brokers across 3 zones equally means you will have 2 Kafka brokers in each zone. To achieve this, label 2 nodes in each zone. In this example, label the first 2 nodes of each zone: 192.0.0.1, 192.0.0.2, 192.0.0.5, 192.0.0.6, 192.0.0.9, and 192.0.0.10. For example: kubectl label node 192.0.0.1 192.0.0.2 node-role.kubernetes.io/kafka=true Labelling for ZooKeeper Event Streams deploys 3 ZooKeeper nodes. Distribute the 3 ZooKeeper pods across the zones by dedicating 1 or 2 nodes in each zone to a ZooKeeper pod. Note: Do not label more than 2 in each zone. In addition, it is preferred that you label nodes that are not already labelled with Kafka. In this example, label the last 2 nodes of each zone: 192.0.0.3, 192.0.0.4, 192.0.0.7, 192.0.0.8, 192.0.0.11, and 192.0.0.12. For example: kubectl label node 192.0.0.3 192.0.0.4 node-role.kubernetes.io/zk=true You can check that your nodes are labelled as required by using the following command: kubectl get nodes --show-labels Installing Event Streams When installing Event Streams, configure the Kafka broker and multizone options as follows. If you are installing by using the UI, set the following options for the example in this tutorial:   Set the number of Kafka brokers to 6 in the Kafka brokers field.  Set the  Number of zones field to 3.  Enter the zone label names for each zone in the Zone labels field:     es-zone-0 es-zone-1 es-zone-2      If you are installing by using the CLI, use the following settings for the example in this tutorial: helm install --tls --name my-es --namespace=eventstreams \\--set license=accept \\--set global.image.pullSecret=\"my-ips\" \\--set global.image.repository=\"&lt;image-pull-repository&gt;\" \\--set messageIndexing.messageIndexingEnabled=false \\--set global.zones.count=3 \\--set global.zones.labels[0]=\"es-zone-0\" \\--set global.zones.labels[1]=\"es-zone-1\" \\--set global.zones.labels[2]=\"es-zone-2\" \\--set kafka.brokers=6 \\ibm-eventstreams-prodCreating topics for multizone setup It is important that you do not configure topics where the minimum in-sync replicas setting cannot be met in the event of a zone failure. Warning: Do not create a topic with 1 replica. Setting 1 replica means the topic will become unavailable during an outage and will lose data. In this example, create a topic with 6 replicas, setting the minimum in-sync replicas configuration to 4. This means if a zone is lost, 2 brokers would be lost and therefore 2 replicas. The minimum in-sync replicas would still mean the system remains operational with no data loss, as 4 brokers still remain, with four replicas of the topics data. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/fhir-server-docs/tutorials/multi-zone-tutorial/",
        "teaser":null}]
